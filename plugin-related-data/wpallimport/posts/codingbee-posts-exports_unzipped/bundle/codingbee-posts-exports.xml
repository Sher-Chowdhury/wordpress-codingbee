<?xml version="1.0" encoding="UTF-8"?>
<data>
	<post>
		<Title><![CDATA[RHCSA - The Bash Terminal]]></Title>
		<Content><![CDATA[No quiz required. 

In Linux, nearly all day to day tasks are performed inside a command line terminal, aka the bash terminal. 


There's a few ways to access it depending on various circumstance. 

First of it depends on whether you are sitting in front of your Linux machine or you want to access it remotely (e.g. this is likely if your machine is locked away in data centre, or server room).  


<h2>Directly Accessing your Bash terminal</h2>

If you are sitting in front of your machine, then to access the terminal will depend on whether your machine has a graphical User Interface installed. If so then you can access the bash terminal by 

If your machine has the graphical UI, then you can access your bash terminal by either:


<pre>right click anywhere on the desktop -> click "open in terminal"  </pre>

If you machine doesn't have a UI installed then you are automatically presented with the bash terminal when your machine boots up. 


<h2>Remotely accessing the Bash terminal</h2>

If the machine you want to access is behind closed doors e.g. inside a data centre, server room, or in the cloud, then you can access the machine remotely from your local desktop. 

<h3>Remotely accessing a machine from a Linux Desktop</h3>


If your local desktop is a Linux machine of any distro (e.g. CentOS, Ubunto, Debian...etc.), Then you can access your remote server's bash terminal by establishing a remote session using the SSH protocol. SSH is a universal Linux protocol that is the secure remote connection of choice across all Linux and Unix distros.   


Using SSH is a bit like creating a secure tunnel between your machine and the remote machine. Once the SSH session has been established, any commands or tasks you perform are actually performed on the remote machine. We establish an ssh session using the ssh command. For example, if our local machine's fqdn is "puppetmaster.local" and we want to use this machine to connect as the "vagrant" user to  a remote machine which has the name "puppetagent01.local" then we run the command:



<pre>
[root@puppetmaster ~]$ ssh vagrant@puppetagent01.local
vagrant@puppetagent01.local's password:
[vagrant@puppetagent01 ~]$
</pre>











<h3>Remotely accessing a machine from Windows Desktop</h3>


Windows doesn't natively support ssh. Therefore tyou need to install an ssh client on your windows desktop machine in order to connect to a remote Linux machine. The most popular free ssh client is <a href="http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html" rel="nofollow">PuTTY</a>.




]]></Content>
		<Date><![CDATA[2014-05-04]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Redhat|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Top 100 essential Linux commands]]></Title>
		<Content><![CDATA[I've split all the commands into the following categories:


<h2>commands for navigating around your vm</h2>
cd
ls
pwd


<h2>commands for viewing file contents</h2>
cat
less


<h2>commands for creating/moving/modifying files and folder</h2>
mkdir
vim
rmdir
touch
redirection (technique)
rm
mv
cp 



<h2>Commands for finding things</h2>
grep 
find
locate
which
whereis


<h2>Commands for getting help</h2>
man
pinfo
info
whatis


<h2>Commands for installing software</h2>
rpm
yum





]]></Content>
		<Date><![CDATA[2014-05-04]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Redhat|RHEL]]></Tags>
		<Status><![CDATA[private]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Linux - Help at the Command Line]]></Title>
		<Content><![CDATA[<strong>Commands:</strong>

man {command}

info {command}

pinfo {command}         # displays man pages in a slight differently user friendly format.

man –k {search term}    # Searches all the search terms for the search-term, and lists all the man pages where there is a match.

apropros {search term} # same as the “man –k {search term}”

type {command}         # tells you if command is builtin or not, if not then shows the path.

which {command}         # shows command’s file location (even for builtin ones).

whatis {command}       # one line description of a command.

{command} --help       # quick tips on how to use a command.

{command} –h          # use this if the “--help” doesn’t work.

man hier               # Explains what’s stored in the various directories in RHEL (layout of filesystems).

<strong>Config files:</strong>



<pre>

$ cd /usr/share/doc     # directory containing all the man pages. This can also 
                        # contain diagram/pictures/graphics (you need to open 
                        # it via Firefox to view them). You can also find lots of 
                        # sample config files here. This is very useful!!!

$ cd /usr/share/info    # directory containing all the info pages.

</pre>


<strong>Notes:</strong>

When you install a new software, the man pages may not be immediately added to the man pages database. To manually refresh this DB, run the following command:

/etc/cron.daily/makewhatis.cron

Each man page is made up of sections (sections 1-9). For each command, some of these (if not most) sections are not applicable and hence empty. To see which sections are available for a given command, do:

whatis {command} # e.g. try “whatis passwd”, or “whatis mount”

The whatis command also lists what help sections are available and then you can access different sections like this:

<pre>man {section number} {command}</pre>

e.g. try:

<pre>man 5 passwd                  #This will display the help pages for the /etc/passwd config file.
</pre>

Note the man command quiries documentation stored in /usr/share/man

Some software packages don’t come with man pages. Instead, the man pages have to be installed separately (not sure how to identify these packages, might be possible using yum or rpm)

&nbsp;

<strong>Related Services:</strong>

Inserttexthere

<strong>Must survive reboot:</strong>

/etc/cron.daily/makewhatis.cron   #updates the man page’s db manually.

<strong>Software to install:</strong>

n/a

<strong>GUI tool:</strong>

system --=> help

yelp #start gui help from the command line interface.

<strong>Book ref:</strong>

page 28, chapter 3

<strong>Study guide ref:</strong>

RH124 - page 122 (this has a table showing listing all the available sections and describing it)

RH124 - page 21 (getting help via gnome)

<strong>Need to learn more about:</strong>

<a href="http://en.wikipedia.org/wiki/Man_page">http://en.wikipedia.org/wiki/Man_page</a>

<a href="http://www.freebsd.org/cgi/man.cgi?query=hier&amp;apropos=0&amp;sektion=0&amp;manpath=FreeBSD+6.1-RELEASE&amp;format=html">man page for “hier</a>”

&nbsp;]]></Content>
		<Date><![CDATA[2014-05-04]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|RedHat|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RedHat]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Linux - cat and nano]]></Title>
		<Content><![CDATA[&nbsp;

<strong>Commands:</strong>

cat

nano     # This is a text editor. no need to learn this. vim is much better.

<strong>Config files:</strong>

n/a

<strong>Notes:</strong>

Here are some example using cat (as a text file creator rather than a file content’s outputter):

example 1:

$ cat &lt;&lt;EOF =>testfile   # This lets you start writing stuff, until you type EOF, after that your content gets stored in testfile.

=> this is a testfile

=> another line

=> EOF

example 2:

$ cat &lt;&lt;EOF            # Here the output is printed directly pack to the command line.

=> this is a testline

=> another line

=> yet another line

=> EOF

this is a testline

another line

yet another line

<strong>Related Services:</strong>

Inserttexthere

&nbsp;

<strong>Must survive reboot:</strong>

n/a

<strong>Software to install:</strong>

n/a

<strong>GUI tool:</strong>

n/a

<strong>Book ref:</strong>

n/a

<strong>Study guide ref:</strong>

n/a]]></Content>
		<Date><![CDATA[2014-05-04]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Redhat|RedHat|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RedHat]]></Categories>
	</post>
	<post>
		<Title><![CDATA[c# - Introduction]]></Title>
		<Content><![CDATA[C# is a powerfull object oriented program language. It is heavily engineered programming language.

In this tutorials you are going to learn all kinds of things.

In our tutorials we will show you a lot of things that won't make a lot of things until much later. That's why we will show you a lot of things without explaining them in detail. But will cover them in more detail in later units. When this happens we will explain

Because c# is syntax, we will show you a lot of things but explain them later.

C# is a really powerful programming language. It is basically

C# is a syntax heavy language, that means that even writing a basic program involves typing out a lot of code that on first impressions look superfluous. However everything is important and you only come to appreciate them in later lessons.

So if we just touch on something new without explaining it, then don't panic, we will cover it in later lessons, and for the time being just accept what we show you as being something that is necessary.

Reference
http://blog.pluralsight.com/learning-path-csharp]]></Content>
		<Date><![CDATA[2014-05-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[.net|c#|csharp|Microsoft|tutorial|Visual Studio]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>c#]]></Categories>
	</post>
	<post>
		<Title><![CDATA[c# - What you should already know]]></Title>
		<Content><![CDATA[You should already be familiar with the following:
<ul>
	<li>Variables</li>
	<li>Conditonal construct, e.g. If statements</li>
	<li>Conditional Operators,</li>
	<li>regular expressions</li>
	<li>wild cards</li>
</ul>]]></Content>
		<Date><![CDATA[2014-05-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[.net|c#|csharp|Microsoft|tutorial|Visual Studio]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>c#]]></Categories>
	</post>
	<post>
		<Title><![CDATA[c# - Hello World]]></Title>
		<Content><![CDATA[Here we are going to first show you a simple "hello world" program in the form of an exe file. When you double-click on the exe file, all that will happen is that a console terminal will open a display the message "hello world", like this:

&nbsp;
<h3><strong><span style="color: #008000;">Step 1 -Write the code</span> </strong></h3>
Open notepad and write:

[csharp]

using System;
class HelloWorld
{
   static void Main()         [content_tooltip id=&quot;650&quot;]
     {
        string message = &quot;Hello World&quot;;
        Console.WriteLine (message);
        Console.ReadLine();
     }
}

[/csharp]

[vision_notification style="warning" font_size="12px" closeable="false"] In c#, everything is case sensitve. That means if you type "using system;" instead of typing "using System;" then you will get an error message. [/vision_notification]

[vision_notification style="neutral" font_size="12px" closeable="false"] Every c# statement must end with a semi-colon. [/vision_notification]

<strong>line 1 -</strong> Here we are importing a "namespace" called "System". A namespace is essentially a collection of "classes" that we want to make use of in our program. A class is basically a collection of blocks of code, of which, some of these blocks are known as "methods". We will cover Classes, Namespaces, and Methods, in more detail in later lessons. We are loading the System namespace into memory here because, it contains a class called "Console" which we will be using further down in the code.

<strong>line 2 - </strong>Here we are declaring a class, and we chose to call it "HelloWorld". In this class we have defined only one method, which we have called "Main". You can give a method any name you like, however calling a method "Main" has special meaning in c#. We will cover methods (including the Main method) in later lessons.

<strong>line 4 - </strong>Here we are defining a method called "Main". The keywords "static" and "void" defines some of the characteristics of this method. We will cover this in more detail in later lessons.

<strong>Line 6</strong> - Here we are declaring a string variable called "message".

<strong>Line 7</strong> - Here we are making use of a method called "WriteLine" which resides in a class called "Console", which in turn resides in the "System" namespace. The writeline method's purpose, as the name indicates, is to write something onto a console. I have instructed writeline what to display by placing the "message" variable into the round-brackets. If I didn't place anything in the round brackets, then I would get an error message because the writeline method requires a parameter input for it to work.

<strong>Line 8</strong> - Here we are using another method that belongs to the Console class, this method is called ReadLine. We are using this method to keep the console window open. If I try to run the program without including this line, then the terminal will pop up for a fraction of a second, and then closes again. This method forces the terminal to stay open.
<h3><span style="color: #339966;"><strong>Step 2</strong> -Save the code</span></h3>
Save the file as a "cs" file:

<a href="http://progeektips.com/wp-content/uploads/2014/01/2014-01-18-18_38_51-_new-2-Notepad++.png"><img class="aligncenter size-full wp-image-101" src="http://progeektips.com/wp-content/uploads/2014/01/2014-01-18-18_38_51-_new-2-Notepad++.png" alt="2014-01-18 18_38_51-_new  2 - Notepad++" width="710" height="213" /></a>
<h3><span style="color: #339966;"><strong>Step 3</strong> - Locate the compiler</span></h3>
Next we generate an executable from this cs file (aka we are going to compile the code). Open up cmd (as admin), and then navigate to the latest version of your .net framework's directory.

<a href="http://progeektips.com/wp-content/uploads/2014/01/2014-01-18-18_32_27-Program-Manager.png"><img class="aligncenter size-full wp-image-100" src="http://progeektips.com/wp-content/uploads/2014/01/2014-01-18-18_32_27-Program-Manager.png" alt="2014-01-18 18_32_27-Program Manager" width="854" height="561" /></a>
<h2><span style="color: #008000;">Step 4 - Compile the code</span></h2>
In this directory, you should find the csharp compiler, which is called "csc.exe". Now run the csc.exe (passing in the path to your helloworld csharp file as a parameter):

<a href="http://progeektips.com/wp-content/uploads/2014/01/2014-01-18-18_44_23-Program-Manager.png"><img class="aligncenter size-full wp-image-103" src="http://progeektips.com/wp-content/uploads/2014/01/2014-01-18-18_44_23-Program-Manager.png" alt="2014-01-18 18_44_23-Program Manager" width="854" height="561" /></a>

This will create the exe file which you will find in the .net folder.

Note: Your compiled program ends up in the folder where your terminal has been cd into. If you want your programs to be automatically placed in another folder, e.g. c:\, then first go to that directory and then run the compiler by fully stating it's path:

<a href="http://progeektips.com/wp-content/uploads/2014/01/2014-01-18-19_11_53-Administrator_-Command-Prompt.png"><img class="aligncenter size-full wp-image-105" src="http://progeektips.com/wp-content/uploads/2014/01/2014-01-18-19_11_53-Administrator_-Command-Prompt.png" alt="2014-01-18 19_11_53-Administrator_ Command Prompt" width="854" height="560" /></a>
<h3><span style="color: #008000;">Step 5 - Locate the Program
</span></h3>
Open the folder where the exe has been saved to (this is the directory that your terminal is in at the time of running the csc.exe).

<a href="http://progeektips.com/wp-content/uploads/2014/01/2014-01-18-19_18_01-temp.png"><img class="aligncenter size-full wp-image-106" src="http://progeektips.com/wp-content/uploads/2014/01/2014-01-18-19_18_01-temp.png" alt="2014-01-18 19_18_01-temp" width="1039" height="565" /></a>

Tip: Just type "explorer ." this will open up in explorer window in the terminal in the current directory (which is denoted by ".").
<h3><span style="color: #008000;"><strong>Step 6 - Run the Hello World program</strong></span></h3>
Double click the exe to run it:

<a href="http://progeektips.com/wp-content/uploads/2014/01/2014-01-18-19_19_31-C__temp_helloworld.exe_.png"><img class="aligncenter size-full wp-image-107" src="http://progeektips.com/wp-content/uploads/2014/01/2014-01-18-19_19_31-C__temp_helloworld.exe_.png" alt="2014-01-18 19_19_31-C__temp_helloworld.exe" width="852" height="559" /></a>

Here's another example csharp program, but this program is doing a bit of maths:

[csharp]
using System;

class SimpleMaths
{
    static void Main()
    {
        int x = 12 * 30;
        Console.WriteLine (&quot;Twelve times thirty is equal to: &quot; + x);
        Console.ReadLine();
    }
}
[/csharp]

The compiled exe outputs:

&nbsp;]]></Content>
		<Date><![CDATA[2014-05-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[.net|c#|csharp|Microsoft|tutorial|Visual Studio]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>c#]]></Categories>
	</post>
	<post>
		<Title><![CDATA[c# - Adding comments to your code]]></Title>
		<Content><![CDATA[It is always good practice to insert comments into your code so that you (as well as others) can understand what your code is doing.

You can place a single line comment after a line of code, or on a line of it's own using the "//" notation.

If you want to insert a multi-line comment, then you can begin it using /* and end the comment with */.

Here we the 2 comments types in actions

[csharp]
/*
Program name: Hello World
Version:      1.0
Description:  Opens up a terminal and displays &quot;Hello World&quot;
*/

using System; // here we are importing the &quot;System&quot; namespace

// We are about to define the HelloWorld class.
class HelloWorld // This line is called the &quot;class declaration&quot; line
{         // curly brackets are used to structure the program, here we are indicating the start of the &quot;HelloWorld&quot; class.
   static void Main() // This is the &quot;method declaration&quot; line

     {    // Start of method's code block
        string message = &quot;Hello World&quot;;
        Console.WriteLine (message);
        Console.ReadLine();
     }  // End of method
} // end of class

[/csharp]]]></Content>
		<Date><![CDATA[2014-05-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[.net|c#|csharp|Microsoft|tutorial|Visual Studio]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>c#]]></Categories>
	</post>
	<post>
		<Title><![CDATA[c# - Methods, Classes, and Namespaces]]></Title>
		<Content><![CDATA[When programming it is essential to organize and structure your code, otherwise it will become unwieldy to work with.

c# offer a number of ways to structure and and organize our code using:
<ul>
	<li>methods</li>
	<li>classes</li>
	<li>namespaces</li>
</ul>
Methods are used for storing blocks of codes and each method is designed to perform a specific task (aka function).

Methods, in turn, are stored in "classes". These classes  class can store one or more methods.

Classes in turn are stored in "namespaces".  You can think of Namespaces as a bit like folders on a windows machine. That means that namespaces have tree structures where a namespace can house classes as well as other lower-level namespaces.  As a result we have the following hierarchy:

namespaces => classes => methods

One of the key strength of c# is that it has access to a immensely powerful pre-installed namespace, that is in the form of the .net framework. This framework has a library containing thousands of ready-made classes that you can make use of for building your own applications.

The top level namespace in the .net framework is called "System". All  classes and lower-level namespaces are housed under the System tree structure. The <a href="http://msdn.microsoft.com/en-us/library/system%28v=vs.110%29.aspx" target="_blank">.net Framework library</a> website fully documents all the classes and their method.

With this in mind, let's revisit our hello world example again:

[csharp]

using System;
class HelloWorld
{
    static void Main()
	{
		string message = &quot;Hello World&quot;;
		Console.WriteLine (message);
                Console.ReadLine();
	}
}

[/csharp]

<strong>Line 1</strong> - Here we are telling our program to load all the classes stored directly under the "System" namespace into memory. This won't load any classes that are stored in other namespaces under System.  As a result, one of the classes that will be loaded into memory, will be a class called "Console".

<strong>Line 7</strong> - Here we are using the "<a href="http://msdn.microsoft.com/en-us/library/system.console%28v=vs.110%29.aspx" target="_blank">Console</a>" class (which we loaded into memory in line 1). In particular we are using a "." notation to specify that we want to use the "<a href="http://msdn.microsoft.com/en-us/library/xf2k8ftb%28v=vs.110%29.aspx" target="_blank">WriteLine</a>" method within the console line, to write something to the terminal.

<strong>Line 8</strong> - Here we are using the Console class again, but this time we are using the <a href="http://msdn.microsoft.com/en-us/library/system.console.readline%28v=vs.110%29.aspx" target="_blank">ReadLine</a>. Strictly speaking ReadLine should be used for capturing anything that a user types into the terminal, but here we are using to keep the terminal open.

[vision_notification style="neutral" font_size="12px" closeable="false"] You can identify a method, by the fact that a method is always followed by a pair of round brackets. These round brackets can either contain some content or are empty, depending on the type of method you are running. [/vision_notification]

In Line 1, the program loaded all the classes that are in the "System" namespace. However another way to tell the program where to find a class, is by using a class's fully qualfied name, like this:

[csharp]
// the &quot;using System;&quot; is no longer needed.
class HelloWorld
{
    static void Main()
	{
		string message = &quot;Hello World&quot;;
		System.Console.WriteLine (message);
                System.Console.ReadLine();
	}
}

[/csharp]

Line 7 - here we used the full path of the class, along with which class's method to use. The general syntax that we used to drill down to the is as follows:
<pre>namespace1.namespace2...namespaceX.classname.methodname()</pre>
Lets take a look at another example, let's say we want a program that works out how many days there are for a given number of weeks. Then we can create something like this:

[csharp]
using System;
class sampleclass
{
	static void Main()
	{
                int NumberOfDays = WeekToDays (4)
		Console.WriteLine (NumberOfDays);      // 28
	}

	static int WeekToDays (int Week)
	{
		int days = Week * 7;
		return days;
	}
}
[/csharp]

In this example, we have a class that contains 2 methods, they are "Main" and "WeekToDays". Also notice that the "Main" method actually triggers the "WeekToDays" method as part of its own running.

This program will work out how many days there are in 4 weeks. In this case running this program will display "28".

Line weektodays - you will notice that the round brackets contains some content, this is the input parameter. Also just before the "WeekToDays" we stated "int", this represents the form of the output parameter. We will cover more about input and output parameters later.

Classes essentially lets you define/create your very own custom types, on top of the types that are come included with .net, e.g. int, string, bool, console, etc.]]></Content>
		<Date><![CDATA[2014-05-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[.net|c#|csharp|Microsoft|tutorial|Visual Studio]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>c#]]></Categories>
	</post>
	<post>
		<Title><![CDATA[c# - Storing data inside objects (by using "fields")]]></Title>
		<Content><![CDATA[Whenever you create an object (aka an instance of a class), you often want to store data within that object. You can think of this info as the object's metadata. For example, lets say want to have a class called "Employee", and you want to use that class to create "employee" objects, and each employee-object needs to store the following info:

firstname
age
DateJoined
Salary

Note: the words "instance" and "object" are synonyms.

In order to make this possible, you first need to write your Employee class to accept this info. After that, you can call on the class to instantiate the object and pass the metadata into it.

There are a number of ways to write your
<h3>1st Approach: Store the data directly in an object, using "Fields"</h3>
In this approach we have declared inside the employee class, that an employee-object can store 3 bits of data:
age,
firstname,
salary.

In C#, these items are referred to as "fields", and whenever you declare a new field, you have to also specify what type of data it can hold. For example in in case of the "age" field, we have specified that it can only hold integers (as indicated by the "int").

We also have to specify the access level in the form of <a href="http://msdn.microsoft.com/en-us/library/ms173121.aspx">access modifiers</a>. We will cover more about access modifiers later.

[csharp]
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace ConsoleApp
{

    class Employee
    {
        public int age;
        public string firstname;
        public double salary;
    }

    class Program
    {
        static void Main()
        {
            // here we create a new object using the Employee class.
            Employee Dave = new Employee();

            Dave.age = 50;
            Dave.firstname = &quot;David&quot;;
            Dave.salary = 35461.32;
            Console.WriteLine(&quot;{0} is {1} years old and earns £{2} per year.&quot;, Dave.firstname, Dave.age, Dave.salary);
        }
    }
}

[/csharp]

This will output:<img alt="" src="data:image/png;base64,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" />

Here we have written a really simple "Employee" class. We then instantiated it from within the Main method, by creating an object called "Dave". After the object has been created, we then populated it with data.

However the above approach isn't best practice mainly because the fields' access modifiers was set to "public". This breaks one of the 5 pillars of OOP, called encapsulation, which requires that access to a class's field must be restricted so that only methods within the class can get/set the field's value.

If we simply replace "public" with "private", then the the program will fail with the following errors:

<img alt="" src="data:image/png;base64,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" />

"private" means that only methods within the class itself can read/modify (aka get/set) the field. We are allowed to make these methods public, so that we can call these methods from outside the class....which is what we do in the next approach.
<h3>2nd Approach : Access/Modify (private) fields via the use of (public) methods.</h3>
Writing the code like above isn't best practice. That's because a class should be encapsulated. hence a class's internal fields should not be readable/writeable directly. In other words the fields need to be set to "private", and we access/modify these fields using methods.

To make this possible, what we could do is write a method for setting a field's value, and another method for getting a field's value. That means we will end up creating 2 methods per field, and since we have 3 fields in the Employee class, it means   that we will have to write (3*2) 6 methods altogether.

i.e:

[csharp]

using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

class Employee
{
    private int age;
    private string firstname;
    private double salary;

    public string GetFirstname()
    {
        return firstname;
    }

    public int GetAge()
    {
        return age;
    }

    public double GetSalary()
    {
        return salary;
    }

    public void SetFirstname(string ProvidedName)
    {
        firstname = ProvidedName;
    }

    public void SetAge(int ProvidedAge)
    {
        age = ProvidedAge;
    }

    public void SetSalary(double ProvidedSalary)
    {
        salary = ProvidedSalary;
    }
}

class Program
{
    static void Main(string[] args)
    {
        Employee Dave = new Employee();

        Dave.SetAge(50);
        Dave.SetFirstname(&quot;David&quot;);
        Dave.SetSalary(35461.32);

        Console.WriteLine(&quot;{0} is {1} years old and earns £{2} per year.&quot;, Dave.GetFirstname(), Dave.GetAge(), Dave.GetSalary());
    }
}

[/csharp]

This again will output:

<img alt="" src="data:image/png;base64,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" />

As you can see, we have added a lot more code but the output is still the same. However this second approach, while better, isn't best practice either. That's because:
<ul>
	<li>A method is designed to do a lot more then setting/getting a variable info. Creating methods solely for setting/getting fields values  overkill.</li>
	<li>This approach has a lot more of an overhead in terms of lines of code.</li>
	<li>Setting/Getting fields via methods, means that the code has become a little harder to read. For example we now have to write "Dave.age(20)" which is harder to follow. It would have been better we could just do "Dave.age=20;"</li>
</ul>
That is why there is an even better approach....which is to use "properties". I'll cover this in the next lesson.

&nbsp;]]></Content>
		<Date><![CDATA[2014-05-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[.net|c#|csharp|Microsoft|tutorial|Visual Studio]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>c#]]></Categories>
	</post>
	<post>
		<Title><![CDATA[c# - Storing data inside objects (by using "Properties")]]></Title>
		<Content><![CDATA[Previously we looked at how to store data in an object using fields. Fields are easy to use, but only if you set them to public. Having public fields is bad practice.

To overcome this we can set the fields to  private and then write (public) methods within the class to access/modify the fields. This approach keeps the fields private, but unfortunately it is a much more messier solution....that requires writing a lot more code, and is more harder to follow.

That's where "properties" comes in to save the day. Properties, gives you the best of both worlds, i.e.:
<ul>
	<li>It keeps the object's data private</li>
	<li>It keeps the syntax simple and has minimal overhead in terms of number of lines.</li>
</ul>
Properties are type of class member that is specifically used for storing an object's data. Properties have their own syntax as well, as you will see now:

[csharp]
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

class Employee
{
    // First we declare the variable that we want the property member
    // to set/get. By convention, it must be in lower case,
    // e.g. not something like &quot;FirstName&quot;
    private string firstname;

    // Here we are creating a property called &quot;Firstname&quot;
    // By convention, the property's name is capitalised....to differentiate it
    // from the variable.
    // Notice that the first line doesn't end with round brackets, e.g.
    // &quot;private string firstname()&quot;, this helps to distinguish
    // a property from a method.
    public string Firstname
    {
        get
        {
            return firstname;
        }
        set
        {
            firstname = value;
            // Note: &quot;value&quot; is a special built-in keyword of properties.
            // It acts as hidden default input-parameter.
        }
    }

    private int age;
    public int Age
    {
        get
        {
            return age;
        }
        set
        {
            age = value;
        }
    }

    private double salary;
    public double Salary
    {
        get
        {
            return salary;
        }
        set
        {
            salary = value;
        }
    }
}

class Program
{
    static void Main(string[] args)
    {
        Employee Dave = new Employee();

        // Notice here that we are calling the
        // property, as indicated by the capitalised first letter,
        // The value we provide, automatically get's assigned to the properties
        // builtin &quot;value&quot; input parameter.
        Dave.Age = 50;
        Dave.Firstname = &quot;David&quot;;
        Dave.Salary = 35461.32;
        // Notice we also didn't have to do anything like:
        // Dave.Age()....
        // Dave.Age.get()
        // These are unnecessary, and would be wrong anyway.

        // Note, the property's &quot;set&quot; option was triggered in the above three
        // lines because we used the &quot;=&quot; sign to assign values.
        // But the &quot;get&quot; option gets triggered below, because we are calling
        // it.

        Console.WriteLine(&quot;{0} is {1} years old and earns £{2} per year.&quot;, Dave.Firstname, Dave.Age.ToString(), Dave.Salary.ToString());
    }
}
[/csharp]

The cool thing about using properties, is :
- when you are assigning values to properties, it appears like you are just assigning variable directly, i.e. you are assigning values using the equal sign rather then passing through via brackets (which you do in the methods option).
- the employee class's internal variables (which are in lower case) remain private, and the input values it receives get handled in a way that's similar to using methods.
- you can control get/set abilities of the properties, by omitting or modifying them.

Note, you rewrite above to make it shorter by removing a lot of whitespace, i.e.:

[csharp]
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

class Employee
{
    private string firstname;
    public string Firstname
    {
        get{return firstname;}
        set{firstname = value;}
    }

    private int age;
    public int Age
    {
        get{return age;}
		set{age = value;}
    }

    private double salary;
    public double Salary
    {
        get{return salary;}
        set{salary = value;}
    }
}

class Program
{
    static void Main(string[] args)
    {
        Employee Dave = new Employee();

        Dave.Age = 50;
        Dave.Firstname = &quot;David&quot;;
        Dave.Salary = 35461.32;

        Console.WriteLine(&quot;{0} is {1} years old and earns £{2} per year.&quot;, Dave.Firstname, Dave.Age.ToString(), Dave.Salary.ToString());
    }
}
[/csharp]

However, since properties including get/set are so commonly used, Microsoft has introduced an even shorter version of the above syntax:

[csharp]
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

class Employee
{
    //private string firstname;
    //public string Firstname
    //{
    //    get{return firstname;}
    //    set{firstname = value;}
    //}
    // The compiler will expand the following line
    // into the above code block:
	public string Firstname {get; set;}

	// Likewise we have:
	public int Age {get; set;}
	public double Salary {get; set;}

}

class Program
{
    static void Main(string[] args)
    {
        Employee Dave = new Employee();

        Dave.Age = 50;
        Dave.Firstname = &quot;David&quot;;
        Dave.Salary = 35461.32;

        Console.WriteLine(&quot;{0} is {1} years old and earns £{2} per year.&quot;, Dave.Firstname, Dave.Age.ToString(), Dave.Salary.ToString());
    }
}
[/csharp]

Note: the above "{get; set;}" syntax is indicator that the given item is a property.

&nbsp;

Note: the terminology "getter" and "setter" are used to refer to these special get and set methods.  ]]></Content>
		<Date><![CDATA[2014-05-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[.net|c#|csharp|Microsoft|tutorial|Visual Studio]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>c#]]></Categories>
	</post>
	<post>
		<Title><![CDATA[c# - Instantiate an object and set it's Properties simultaneously (using the Initialization approach)]]></Title>
		<Content><![CDATA[&nbsp;
<h2>1st Approach: Using Initialization</h2>
Here we use properties (as demonstrated in the previous), but this time we set all the properties in one go, by writing them as part of the object's actual declaration, using curly-bracket syntax:

[csharp]
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

class Employee
{
	public string Firstname {get; set;}
	public int Age {get; set;}
	public double Salary {get; set;}
}

class Program
{
    static void Main()
    {
        Employee Dave = new Employee()
		{
			Firstname = &amp;quot;David&amp;quot;,
			Age = 35,
			Salary = 35235.23
		};
		// Using the above syntax is called initialisation

        Console.WriteLine(&amp;quot;{0} is {1} years old and earns £{2} per year.&amp;quot;, Dave.Firstname, Dave.Age.ToString(), Dave.Salary.ToString());
    }
}
[/csharp]
]]></Content>
		<Date><![CDATA[2014-05-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[.net|c#|csharp|Microsoft|tutorial|Visual Studio]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>c#]]></Categories>
	</post>
	<post>
		<Title><![CDATA[c# - Instantiate an object and set it's Properties simultaneously (using the Constructors approach)]]></Title>
		<Content><![CDATA[<h3>2nd Approach - Using the "Constructor"</h3>
Constructor is a special type of method that is present in all classes, even if you don't explicitly define it in the class itself.

Constructors are used for creating an instance of the object.... this basically means that whenever you declare a new instance of a class, e.g.:
<pre>        Employee Dave = new Employee()</pre>
Then the constructor get's executed, and in the above case, the only thing it does is set a side a memory container, waiting to be filled in. You can identify a constructor in a class by the fact that the constructor's name must be the same as the class's name itself. Hence in the case of the the "Employee" class, we didn't explicitly declare the Employee constructor, so the following constructor method is implicitly included:

public Employee()
{

}

Note: not only is the constructor method named after the class name, but it also never has a return value....not even "void".

The above function is empty.....and comes into play when you run:
<pre>        Employee Dave = new Employee()</pre>
This line says:

"Create an object called 'Dave' that belongs to the 'employee' class, and do this by running the 'Employee()' (constructor) method."

Since the Employee() hasn't been changed from the default, it has only ended up creating an empty memory container.

However we can utilize the constructor to set all the properties as well, as shown here:

[csharp]
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

class Employee
{
    public string Firstname { get; set; }
    public int Age { get; set; }
    public double Salary { get; set; }

    public Employee(string firstname, int age, double salary)
    {

        Firstname = firstname;
        Age = age;
        Salary = salary;
    }

}

class Program
{
    static void Main()
    {
	Employee Dave = new Employee(&amp;quot;David&amp;quot;, 35, 35235.23);
        Console.WriteLine(&amp;quot;{0} is {1} years old and earns £{2} per year.&amp;quot;, Dave.Firstname, Dave.Age.ToString(), Dave.Salary.ToString());
    }
}

[/csharp]

As you can see, we have now modified the constructor method, to accept 3 input parameters. As a result you can no longer use the following statement:
<pre>        Employee Dave = new Employee()</pre>
That's because due to the concept of method-overloading, the employee class won't be able to identify method to use. Hence you have to include the required 3 input parameters within the round brackets.

http://msdn.microsoft.com/en-us/library/w86s7x04.aspx

http://msdn.microsoft.com/en-us/library/aa288470%28v=vs.71%29.aspx

http://www.tutorialspoint.com/csharp/csharp_properties.htm]]></Content>
		<Date><![CDATA[2014-05-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[.net|c#|csharp|Microsoft|tutorial|Visual Studio]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>c#]]></Categories>
	</post>
	<post>
		<Title><![CDATA[c# - Create a read-only property (aka a constant) ]]></Title>
		<Content><![CDATA[So far we have looked at 5 ways to store data in a class's object (aka an instance) in the form of "properties". Of which one of the recommended ways of setting values to properties, is by using the constructor method, e.g.:

[csharp]
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

class Employee
{
    public string Firstname { get; set; }
    public int Age { get; set; }
    public double Salary { get; set; }

    public Employee(string firstname, int age, double salary)
    {

        Firstname = firstname;
        Age = age;
        Salary = salary;
    }

}

class Program
{
    static void Main()
    {
    Employee Dave = new Employee(&quot;David&quot;, 35, 35235.23);
        Console.WriteLine(&quot;{0} is {1} years old and earns £{2} per year.&quot;, Dave.Firstname, Dave.Age.ToString(), Dave.Salary.ToString());
    }
}
[/csharp]

Now lets say all the employees of the company earn the same salary of £30000, no matter if they are the cleaner or the CEO. In that case how can we reflect this in the above code? We can do this by expanding salary member and then change "set" to only accept a fix value, i.e. we can do:

[csharp]
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;
 
class Employee
{
    public string Firstname { get; set; }
    public int Age { get; set; }
 
    private double salary;
    public double Salary
    {
        get
        {
            return salary;
        }
        set
        {
            this.salary = 30000;
        }
    }
 
    public Employee(string firstname, int age)
    {
 
        Firstname = firstname;
        Age = age;
        Salary = salary;     // You need to run this just to trigger the Salary property's get feature.  
    }
 
}
 
class Program
{
    static void Main()
    {
    Employee Dave = new Employee(&quot;David&quot;, 35);
        Console.WriteLine(&quot;{0} is {1} years old and earns £{2} per year.&quot;, Dave.Firstname, Dave.Age.ToString(), Dave.Salary.ToString());
    }
}
[/csharp]

In order to get this to work, we had to remove the constructor's "salary" input parameter, since it is now read only.
]]></Content>
		<Date><![CDATA[2014-05-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[.net|c#|csharp|Microsoft|tutorial|Visual Studio]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>c#]]></Categories>
	</post>
	<post>
		<Title><![CDATA[c# - Method Overloading: Have two or methods of the same name]]></Title>
		<Content><![CDATA[Method Overloading is basically a technique that lets you have different versions of the same method, where each version of the method does a different thing.

It essentially makes methods more versatile.

Each version of the method accepts different/types of input parameters.

The way you control which of version  method you want to trigger, is by specifying the correct number/type of input parameters for that corresponding method.

In the following example, we have 2 methods, which in this case achieves the same goal by different means. The here is to instantiate an object:

[csharp]
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

class Employee
{
    public string Firstname { get; set; }
    public int Age { get; set; }
    public double Salary { get; set; }

	// we are going to use a technique called method overloading
	// as a way to have 2 methods of the same name, 
	// and leave it to c# to work out which method we want based 
	// on the input parameter provided when the method is called. 

    public Employee(string firstname, int age, double salary)
    {
        
        Firstname = firstname;
        Age = age;
        Salary = salary;
    }

    public Employee()
    { }

}
 
class Program
{
    static void Main()
    {
        
		// Here we initialised the object using the 
		//default constructor
		// to create the empty memory container.
		Employee Dave = new Employee()
        {
            Firstname = &quot;David&quot;,
            Age = 35,
            Salary = 35235.23
        };
        Console.WriteLine(&quot;{0} is {1} years old and earns £{2} per year.&quot;, Dave.Firstname, Dave.Age.ToString(), Dave.Salary.ToString());

		// Here we used the constructor to create
        Employee Jenny = new Employee(&quot;Jennifer&quot;, 22, 25023.50);
        
		Console.WriteLine(&quot;{0} is {1} years old and earns £{2} per year.&quot;, Jenny.Firstname, Jenny.Age.ToString(), Jenny.Salary.ToString());
    }
}
[/csharp]]]></Content>
		<Date><![CDATA[2014-05-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[.net|c#|csharp|Microsoft|tutorial|Visual Studio]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>c#]]></Categories>
	</post>
	<post>
		<Title><![CDATA[c# -  What are Variables Types]]></Title>
		<Content><![CDATA[A variable is essentially a container that's used for storing data. This container has a name assigned to it. In programming, this name is used to refer to the variable's content.

In c# there are actually three high-level types of variables:
<ul>
	<li>Value Types</li>
	<li>Reference types</li>
	<li>Pointer types   (we will ignore this for now, but will cover it later)</li>
</ul>
<h2>Value Types</h2>
The following variable types fall under this category:
<table>
<tbody>
<tr>
<th>Type</th>
<th>Represents</th>
<th>Range</th>
<th>Default
Value</th>
</tr>
<tr>
<td>bool</td>
<td>Boolean value</td>
<td>True or False</td>
<td>False</td>
</tr>
<tr>
<td>byte</td>
<td>8-bit unsigned integer</td>
<td>0 to 255</td>
<td>0</td>
</tr>
<tr>
<td>char</td>
<td>16-bit Unicode character</td>
<td>U +0000 to U +ffff</td>
<td>'\0'</td>
</tr>
<tr>
<td>decimal</td>
<td>128-bit precise decimal values with 28-29 significant digits</td>
<td>(-7.9 x 10<sup>28</sup> to 7.9 x 10<sup>28</sup>) / 10<sup>0 to 28</sup></td>
<td>0.0M</td>
</tr>
<tr>
<td>double</td>
<td>64-bit double-precision floating point type</td>
<td>(+/-)5.0 x 10<sup>-324</sup> to (+/-)1.7 x 10<sup>308</sup></td>
<td>0.0D</td>
</tr>
<tr>
<td>float</td>
<td>32-bit single-precision floating point type</td>
<td>-3.4 x 10<sup>38</sup> to + 3.4 x 10<sup>38</sup></td>
<td>0.0F</td>
</tr>
<tr>
<td>int</td>
<td>32-bit signed integer type</td>
<td>-2,147,483,648 to 2,147,483,647</td>
<td>0</td>
</tr>
<tr>
<td>long</td>
<td>64-bit signed integer type</td>
<td>-923,372,036,854,775,808 to 9,223,372,036,854,775,807</td>
<td>0L</td>
</tr>
<tr>
<td>sbyte</td>
<td>8-bit signed integer type</td>
<td>-128 to 127</td>
<td>0</td>
</tr>
<tr>
<td>short</td>
<td>16-bit signed integer type</td>
<td>-32,768 to 32,767</td>
<td>0</td>
</tr>
<tr>
<td>uint</td>
<td>32-bit unsigned integer type</td>
<td>0 to 4,294,967,295</td>
<td>0</td>
</tr>
<tr>
<td>ulong</td>
<td>64-bit unsigned integer type</td>
<td>0 to 18,446,744,073,709,551,615</td>
<td>0</td>
</tr>
<tr>
<td>ushort</td>
<td>16-bit unsigned integer type</td>
<td>0 to 65,535</td>
<td>0</td>
</tr>
</tbody>
</table>
<h2>Reference Types</h2>
The following variable types are categorized under reference types:
<ul>
	<li>string</li>
	<li>object</li>
	<li>dynamic</li>
</ul>
<h1>Value Types verses Reference Types</h1>
So far we have listed all the variable types. But why is there a need to pigeonhole  into value types or reference types?

The main reason boils down to memory usage.

In value type variables, each variable's value is only accessible by one variable name. Whereas in reference types variables, each variable's value can be accessible be one or more variable in order to save space.

To be better understand this, let's take a look at an example:

Let's say later on we declare the following (value type) integer variable:

int MyNumber1 = 12025 ;

This variable will take up about 32Bytes of your computer's memory. This isn't a lot of memory since most modern computers, comes with 2GB of RAM (but don't forget about 1.5GB of your ram is already used up to run your operating system).

Now lets say further along in the program we create  a second variable like this:

int MyNumber2 = MyNumber1 ;

We have affectively made a copy of our first variable. As a result, another 32Bytes of memory is used up in order to store the value for MyNumber2.

Now let's take a look at what happens when we we deal with reference type variables.  Let's say somewhere in our code we declare the following (reference type) string variable, and this variable we are going to store the whole the entire text of the "Lord of the Rings" trilogy:

string LordOfTheRings = "blah blah blah.................
.......................hobbits....blah blah........legolas...blah blah.....................
.................................................................................................etc";

I don't know exactly how much memory a string this big would use up, but lets say it is something really big, like 50MB of memory.

Now lets say later on we declare a second string variable like this:

string MyFavouriteBook = LordOfTheRings;

In doing this you would think that another massive 50MB of memory gets used up to store the value of MyFavouriteBook.

If that were true then this would be bad because our program would end up hogging all the computer's memory and will either cause our PC to crash or really slow  down in performance.

However what really happens is that "MyFavouriteBook" basically becomes another reference to the same memory location that holds the value

Another way you can think of this is to think of a memory location as a large empty room with no doors or windows. When we created the LordOfTheRings variable, we basically created a door to give access to the room, and in the room we placed the LordoftheRings book. The door's nameplate reads the variable's name, i.e. "LordofTherings". So when the following code was run:

string MyFavouriteBook = LordOfTheRings;

All we did was create another doorway into the same room, but this time the new door's nameplate reads "MyFavouriteBook".

Hence both LordofTherings and MyFavouriteBook acts as reference to access the same room (memory location). This is only done to make efficient use of the memory.

However if I want to change the value of MyFavouriteBook to something like the entire text of the Harry Potter books, i.e.:

string MyFavouriteBook = "blah blah blah.................
....Voldemort....blah blah........Professor Dumbledore...blah blah......................................................................................................................etc";

then the MyFavouriteBook can't point to the LordOftheRings data anymore. It also can't overwrite the LordoftheRings, because the other variable, LordOfTheRings, might still need it. In this scenario, there is no other option but to place the data into a new memory location, in other words we place the new data in a different room, and then move the  MyFavouriteBook "door" from it's old place and place it as entry point to the new room.

If the Harry Potter texts use up 100MB, then to store both string variables would end up using a massive 150MB of memory.

Here is an example that shows that when you have to variables pointing to the same memory location, and then you change the value of one of variables, then it doesn't impact the other variable:

[csharp]

using System;
static class StringaAreReferenceTypes
{
static void Main()
{
string x = &quot;hello ProGeekTips&quot;;
Console.WriteLine (x);

string y = x;
Console.WriteLine (y);

x = &quot;goodbye ProGeekTips&quot;;
Console.WriteLine (x);  // This now outputs &quot;goodbye ProGeekTips&quot;
Console.WriteLine (y);    // This still outputs &quot;hello ProGeekTips&quot;

}
}

[/csharp]

Now with value types, you can't have 2 variables names (doorways) leading into the same memory location (room). Also value type gets a fixed size container to store it's data in. For example, the size of an integer (int) variable's memory 32bytes, that's big enough to store any integer between -2,147,483,648 to 2,147,483,647. If you want to store a variable that is even bigger, something bigger than you have to use another value type instead, e.g. a float variable type.

The key thing to an

In c# the re are different types of variables. These different types are known as <a href="http://msdn.microsoft.com/en-us/library/s1ax56ch.aspx" target="_blank">Value Types</a> (aka data types) some of the most common ones are :
<ul>
	<li>string</li>
	<li>int</li>
	<li>array</li>
	<li>float</li>
	<li>boolean</li>
</ul>
Lets take a look at each of them in practice:
<h2>String Data type</h2>
We have already encountered a string variable in our earlier hello world program:

[csharp]

using System;
class HelloWorld
{
   static void Main()
     {
        string message = &quot;Hello World&quot;;
        Console.WriteLine (message);
        Console.ReadLine();
     }
}

[/csharp]

Line 8 - here we are creating a new string variable called "message", and this variable contains the data "Hello World".

<strong>Useful Links</strong>

http://msdn.microsoft.com/en-us/library/s6938f28.aspx

http://msdn.microsoft.com/en-us/library/3ewxz6et.aspx]]></Content>
		<Date><![CDATA[2014-05-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[.net|c#|csharp|Microsoft|tutorial|Visual Studio]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>c#]]></Categories>
	</post>
	<post>
		<Title><![CDATA[c# -  An overview of commonly used variables]]></Title>
		<Content><![CDATA[Previously we looked at why variables are categorized as either "value types", or "reference types" (primarily for memory reasons).

Now lets take a look at these variables in action, starting with the string variable.
<h1>String variables</h1>
We already encountered string variables when we created the "message" string variable in our hello world program.

You can also Concatenate existing string variables to form new string variables:

[csharp]
using System;
class ConcatenateStrings
{
	static void Main()
	{
		string firstname = &quot;James&quot; ;
		string lastname = &quot;Bond&quot; ;
		string fullname = firstname + &quot; &quot; + lastname;
		Console.WriteLine (&quot;My name is &quot; + lastname + &quot;, &quot; + fullname );

                string country = &quot;uk&quot;;
                string CounterUpperCase = country.ToUpper();
                Console.WriteLine(fullname + &quot; lives in the &quot; + CounterUpperCase );
	}
}
[/csharp]

Line 8 - here we are using a non-static method called ToUpper(), which is a member of the string class. We will cover more about static and non-static methods later, but for now it just gives early sight of how methods are used.
<h1>Boolean</h1>
This variable can only have one of 2 values, "true" or "false".  Booleans are commonly used to store the result of something we check.
o

[csharp]
using System;
class BooleanExample
{
	static void Main()
	{
		bool AboolVariable = true;
		bool AnotherBoolVariable = false;
		Console.WriteLine(&quot;First boolean's value is:  &quot; + AboolVariable);
		Console.WriteLine(&quot;Second boolean's value is:  &quot; + AnotherBoolVariable);

		int x = 1;
		int y = 2;
		int z = 1;

		Console.WriteLine (&quot;x is equal to:&quot; + x);
		Console.WriteLine (&quot;y is equal to:&quot; + y);
		Console.WriteLine (&quot;z is equal to:&quot; + z);

		bool FirstComparison = (x == y);  // True
		Console.WriteLine (&quot;Is x equal to y: &quot; + FirstComparison);

		bool SecondComparison = (x != y);
		Console.WriteLine (&quot;Is x not equal to y: &quot; + SecondComparison);

		bool ThirdComparison = (x == z);
		Console.WriteLine(&quot;Is x equal to z: &quot; + ThirdComparison);

		bool FourthComparison = (x &lt; y);
		Console.WriteLine(&quot;Is x equal to y: &quot; + FourthComparison);

		bool FifthComparison = (x =>= z);
		Console.WriteLine(&quot;Is x greater than or equal to z: &quot; + FifthComparison);
	}
}

[/csharp]

In the above program we have taken our first look at comparison operators, we will cover more about comparison operators later.
<h1>Arrays</h1>
An array is essentially a list of values grouped together into list.

[csharp]
using System;
class ArrayExample
{
    static void Main()
    {
        string[] fruits = new string[5];
        fruits [0] = &quot;a&quot;;
        fruits [1] = &quot;e&quot;;
        fruits [2] = &quot;i&quot;;
        fruits [3] = &quot;o&quot;;
        fruits [4] = &quot;u&quot;;

        for (int i = 0; i &lt; fruits.Length; i++)
        Console.WriteLine (vowels[i]);
    }
}
[/csharp]

Here is another way to declare an array:

[csharp]
using System;
class FruitsArray
{
	static void Main()
	{
		string[] fruits = {&quot;apple&quot;,&quot;bananas&quot;,&quot;oranges&quot;,&quot;mangoes&quot;};

		Console.WriteLine (fruits[0]); // this prints &quot;apple&quot;
		Console.WriteLine (fruits[1]); // this prints &quot;banana&quot;
		Console.WriteLine (fruits[2]); // this prints &quot;oranges&quot;
	}
}
[/csharp]
<h1>Integers</h1>
Here are some examples of integer, integers are often used for doing maths:

[csharp]
using System;
class LotsOfNumbers
{
	static void Main()
	{

		int FirstNumber = 12;
		int SecondNumber = 3;
		int SumOfBothNumbers = FirstNumber + SecondNumber ;

		Console.WriteLine(&quot;The first integer is:   &quot; + FirstNumber);
		Console.WriteLine(&quot;The second integer is:   &quot; + SecondNumber);
		Console.WriteLine(&quot;Adding up both numbers gives:   &quot; + SumOfBothNumbers);
		Console.WriteLine(FirstNumber + &quot; minus &quot; + SecondNumber + &quot; gives:   &quot; + (FirstNumber - SecondNumber) );
		Console.WriteLine(&quot;Multiplying both numbers gives:   &quot; + (FirstNumber * SecondNumber) );
		Console.WriteLine(&quot;Dividing &quot; + FirstNumber + &quot; by &quot; + SecondNumber + &quot; gives:   &quot; + (FirstNumber / SecondNumber) );
	}
}
[/csharp]]]></Content>
		<Date><![CDATA[2014-05-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[.net|c#|csharp|Microsoft|tutorial|Visual Studio]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>c#]]></Categories>
	</post>
	<post>
		<Title><![CDATA[c# -  Static and Non-Static Methods]]></Title>
		<Content><![CDATA[C# is a object oriented language, and that means that whenever we declare a new variable, we are actually using a class that generates (aka instantiates) that variable for us. Let's says that somewhere in our program we have the following statement:

int MyFavouriteNumber = 7 ;

Here we are creating a integer variable called MyFavouriteNumber which has the value 7.

The word "int" is actually a c# builtin <a href="http://msdn.microsoft.com/en-us/library/x53a06bb.aspx" target="_blank">keyword</a>, When using the <a href="http://msdn.microsoft.com/en-us/library/5kzh1b5w.aspx" target="_blank">int</a> keyword before a variable's name, we are essentially saying that we want this variable instantiated using the <a href="http://msdn.microsoft.com/en-us/library/system.int32.aspx">System.Int32</a> class.

As a result "MyFavouriteNumber" becomes a member of the System.Int32 class. This class has a number of methods.  There a basically two types of methods, Static methods, and Non-static methods. Static methods are the ones indicated with the letter "S" label:

<img alt="" src="data:image/png;base64,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" />

We will look at static methods later, but for now let's look at non-static methods.

<strong>Non-Static Methods</strong>

Non-static methods are methods that you apply to an instance of a class, which is why they are also often called "instance methods".  It is a bit like doing experiments on an object.

Here's an example, in which we are using the <a href="http://msdn.microsoft.com/en-us/library/system.int32.aspx">System.Int32</a> class's "<a href="http://msdn.microsoft.com/en-us/library/y2ky8xsk.aspx">CompareTo(Int32)</a>" method:

[csharp]
using System;

static class ComparingNumbersExample
{
    static void Main()
    {

        int MyFavouriteNumber = 7;
        Console.WriteLine (MyFavouriteNumber);  // this simply outputs &quot;7&quot;
        ComparisonResult = MyFavouriteNumber.CompareTo(2);
        Console.WriteLine (); // this outputs &quot;1&quot;
        Console.WriteLine (MyFavouriteNumber.CompareTo(15)); // this outputs &quot;-1&quot;
        Console.WriteLine (MyFavouriteNumber.CompareTo(7)); // this outputs &quot;0&quot;
        Console.ReadKey();  // this keeps the console open.
    }
}

[/csharp]

line 8: Here we are using the CompareTo method on an instance of the String class, which in this case is the MyFavouriteNumber variable. We did this using a dot (.) notation format.

This program outputs a list of numbers, but what do these numbers mean? Well you can find this out by going to the "<a href="http://msdn.microsoft.com/en-us/library/y2ky8xsk.aspx">CompareTo" method's reference page</a>.  Based on the reference info, we can derive:

line 10 -  Here we  are comparing our variable (i.e. <code>MyFavouriteNumber) </code>with the number 2. Since our variable is greater is than 2, it means that this method returns "1".

line 11 - Here we  are comparing our variable (i.e. <code>MyFavouriteNumber) </code>with the number 15. Since our variable is less than 15, it means that this method returns "-1".

line 12 - Here we  are comparing our variable (i.e. <code>MyFavouriteNumber) </code>with the number 7. Since they are the same value, it means that this method returns "0".

You can also create your own non-static method, by simply omitting the "static" when declaring your method, e.g.:

[csharp]
using System;

namespace ANonStaticMethod
{
    class Program
    {
        static void Main()
        {
            Dog Charlie = new Dog();
			// here we are triggering a nonstatic method. 
			Charlie.Woof();
        }
    }

    class Dog
    {
		// here we define a non-static method.
		public void Woof()
		{
			Console.Beep();
		}
    }
}
[/csharp]
<h1>Static methods</h1>
With non-static methods you may have to do a lot of work beforehand to create an instance of that class, and then apply the non-static method to it.

Static methods on the other hand are more general tools, and work right out-of-the-box without any need to create an instance of that class. These methods are indicated by the letter "S".

We have already encountered a static method, called "WriteLine" which is a member of the

The Console.WriteLine is a perfect example of a static method. The WriteLine method is a static method that resides in the "Console" class, and if you want to use it to output something to the terminal

however in order to use it, we didn't need to]]></Content>
		<Date><![CDATA[2014-05-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[.net|c#|csharp|Microsoft|tutorial|Visual Studio]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>c#]]></Categories>
	</post>
	<post>
		<Title><![CDATA[c# -  Input Parameters and Return Values]]></Title>
		<Content><![CDATA[As mentioned earlier, a method is a block of code that performs a specific task.

Methods can require some information in order for it to run. This info has to be provided in the form of an "input parameter"

Methods can also output information after it has completed running. This info is often referred to as the  "return value",  and this is something that you can be captured into a variable. You can check whether a method requires an input parameter, or outputs a return value (or both) by looking at the  first line of the method's declaration.

For example, in an earlier lesson we encountered the WeekToDays method that not only requires an input parameter, but also outputs a return value:

[csharp]

class ConvertToDays
{
	static void Main()
	{
                int NumberOfDays = WeekToDays (4)
		System.Console.WriteLine (NumberOfDays);      // 28
	}

	static int WeekToDays (int Week)
	{
		int days = Week * 7;
		return days;
	}
}
[/csharp]

line xxx - What you will notice here is that when the Main method runs, it in turn invokes the WeekToDays method. The method requires an input parameter and we passed the input parameter of "4" using the round brackets. The WeekToDays then provides a return value after it finishes running, which is captured into a integer variable called "NumberOfDays".

[vision_notification style="neutral" font_size="12px" closeable="false"] Notice we didn't specify the full path of the WeekToDays method. That's because, the program always looks for the  method, in the class that triggers it first before looking elsewhere. [/vision_notification]

line 10 - There is a lot going on here, first off, it define's this method's name as "WeekToDays". The round brackets contains "int Week", this means that this method requires an integer (aka int) input parameter. After the method receives this integer, it assigns it to a variable called "Week" so that it can be used in the method's  code block. The "Int" just before the "WeekToDays" name, means that after this method finishes running, it returns an integer output parameter. You can identify what the method outputs by comes after the "return" statement (see line 12).

[vision_notification style="neutral" font_size="12px" closeable="false"] If a method is supposed to return an output-parameter then there must be a "return" command somewhere in the code, indicating what it is going to return.[/vision_notification]

So far, we haven't discussed what the "static" keyword means. But we will cover what Static means in a later lesson.

If we had a method, e.g. called "ProGeekTips" and  didn't require an input parameter, (but did output a string) then then the method's definition would look like this:
<pre>static string ProGeekTips ()</pre>
If this method didn't output a return value (but did require an array input parameter called "arg[]") then then the method's definition would look like this:
<pre>static void ProGeekTips (string[] arg1)</pre>
The "Void" keyword means that this method doesn't give out an output parameter.  Note, you can actually omit the Void key word, and c# will still assume that void is still implied:
<pre>static ProGeekTips (arg[])</pre>
If this method didn't have input parameters or return values, then the method's definition would simply look like this:
<pre>static void ProGeekTips ()</pre>
Going back to our ConvertToDays class, we can rewrite this into a more simpler form like this:

[csharp]

class sampleclass
{
	static void Main()
	{
                int NumberOfDays = WeekToDays (4)
		System.Console.WriteLine (NumberOfDays);      // 28
	}

	static int WeekToDays (int Week)
	{
		return Week * 7;
	}
}
[/csharp]]]></Content>
		<Date><![CDATA[2014-05-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[.net|c#|csharp|Microsoft|tutorial|Visual Studio]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>c#]]></Categories>
	</post>
	<post>
		<Title><![CDATA[c# -  The "Main" method]]></Title>
		<Content><![CDATA[In most case a c# file (aka .cs) will contain several (or hundreds of classes). After you have compiled it into an .exe file and double click on it. c# will need to know which part of the c# code needs to run first. You may some a code block in one of your classes that you want to run first when someone double clicks on the exe. But how do you identify this code block to c#? 

That's where the main method comes into play. In your cs file, you need to add a special method called the "Main" method to only one of your classes. The code in the main method will be executed someone double clicks on the exe file. The class that houses the main method can be thought of as the "primary class" or the "main class". The main method hence acts as your applications entry point. 



http://stackoverflow.com/questions/14688756/c-sharp-class-without-main-method

http://www.completecsharptutorial.com/basic/main-method.php]]></Content>
		<Date><![CDATA[2014-05-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[.net|c#|csharp|Microsoft|tutorial|Visual Studio]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>c#]]></Categories>
	</post>
	<post>
		<Title><![CDATA[c# - The if-else statements (using the "?:" shorthand syntax)]]></Title>
		<Content><![CDATA[http://msdn.microsoft.com/en-us/library/ty67wk28.aspx

This is a single line if-else statement that also has a builtin "return" statement.]]></Content>
		<Date><![CDATA[2014-05-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[.net|c#|csharp|Microsoft|tutorial|Visual Studio]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>c#]]></Categories>
	</post>
	<post>
		<Title><![CDATA[c# - Booleans]]></Title>
		<Content><![CDATA[booleans are variables that hold true or false.

&nbsp;

[csharp]

using System;

class BooleanDemo
{
	static void Main()
	{
		bool myFirstBoolean = 3 + 2 == 5;
		Console.WriteLine(myFirstBoolean.ToString());
		
		bool mySecondBoolean = 3 + 2 => 5;
		Console.WriteLine(mySecondBoolean.ToString());
		
		// Here we use the 'and', &quot;&amp;&amp;&quot; operator
		bool BothAreTrue = myFirstBoolean &amp;&amp; mySecondBoolean ;
		Console.WriteLine(&quot;Are both true: &quot; + BothAreTrue.ToString());

		// Here we use the 'or', &quot;||&quot; operator		
		bool IsOneTrue = myFirstBoolean || mySecondBoolean ;
		Console.WriteLine(&quot;Are both true: &quot; + IsOneTrue.ToString());

		// here we create a boolean that stores the opposite result
		// of another boolean using the &quot;not&quot; (!) operator:
		bool GetReverseInfo = !myFirstBoolean;
		Console.WriteLine(&quot;The opposite result of myFirstBoolean, is: &quot; + GetReverseInfo.ToString());
		
	}
}        
[/csharp]]]></Content>
		<Date><![CDATA[2014-05-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[.net|c#|csharp|Microsoft|tutorial|Visual Studio]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>c#]]></Categories>
	</post>
	<post>
		<Title><![CDATA[c# - Embed variables in a string using the substition technique]]></Title>
		<Content><![CDATA[Previously in the boolean chapter we saw:

[csharp]

using System;

class BooleanDemo
{
	static void Main()
	{
		bool myFirstBoolean = 3 + 2 == 5;
		Console.WriteLine(myFirstBoolean.ToString());

		bool mySecondBoolean = 3 + 2 => 5;
		Console.WriteLine(mySecondBoolean.ToString());

		// Here we use the 'and', &quot;&amp;&amp;&quot; operator
		bool BothAreTrue = myFirstBoolean &amp;&amp; mySecondBoolean ;
		Console.WriteLine(&quot;Are both true: &quot; + BothAreTrue.ToString() + &quot; That's becuase one of them is false&quot;);

		// Here we use the 'or', &quot;||&quot; operator
		bool IsOneTrue = myFirstBoolean || mySecondBoolean ;
		Console.WriteLine(&quot;Are both true: &quot; + IsOneTrue.ToString() +  + &quot; That's because only of them them needs to be true&quot;);

	}
}
[/csharp]

We can use a technique called substitution as a shorthand, and simpler way to embed objects in strings. It works by embedding curly-braced-numbers in the string, e.g.:

[csharp]

using System;
class SubstitionTechnique
{
	static void Main()
	{
		bool myFirstBoolean = 3 + 2 == 5;
		Console.WriteLine(myFirstBoolean.ToString());
		
		bool mySecondBoolean = 3 + 2 => 5;
		Console.WriteLine(mySecondBoolean.ToString());
		
		// Here we use the 'and', &quot;&amp;&amp;&quot; operator
		bool BothAreTrue = myFirstBoolean &amp;&amp; mySecondBoolean ;
		Console.WriteLine(&quot;Are both true: {0}. That's becuase one of them is false&quot;, BothAreTrue);

		// Here we use the 'or', &quot;||&quot; operator		
		bool IsOneTrue = myFirstBoolean || mySecondBoolean ;
		Console.WriteLine(&quot;Are both true: {0}. That's because only of them them needs to be true&quot;, IsOneTrue);

	}
}        
[/csharp]

The linux Awk command works in a similar way. 

Also see:

http://msdn.microsoft.com/en-us/library/fk49wtc1%28v=vs.110%29.aspx]]></Content>
		<Date><![CDATA[2014-05-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[.net|c#|csharp|Microsoft|tutorial|Visual Studio]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>c#]]></Categories>
	</post>
	<post>
		<Title><![CDATA[c# - Switch statements (aka case statements)]]></Title>
		<Content><![CDATA[Here is a quick switch statement demo:

[csharp]
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace SwitchStatementDemo
{
    class Program 
    {
        static void Main(string[] args)
        {
            Console.Write(&quot;enter a number: &quot;);
            //the parse converts the input into an integer.
            int input = int.Parse(Console.ReadLine());
        
            switch (input)
            {
                case 1:        // this is how to do an &quot;or&quot; in switches.
                case 2:
                    Console.WriteLine(&quot;You picked 1 Or 2&quot;);
                    break;
                case 3:
                    Console.WriteLine(&quot;You picked 3&quot;);
                    break;
                case 4:
                    Console.WriteLine(&quot;You picked 4&quot;);
                    break;
               default:
		    Console.WriteLine(&quot;You picked a non-matching number&quot;);
                    break;
            }
            
            Console.WriteLine(&quot;Switch statement has ended&quot;);
        }
    }
}

[/csharp]]]></Content>
		<Date><![CDATA[2014-05-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[.net|c#|csharp|Microsoft|tutorial|Visual Studio]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>c#]]></Categories>
	</post>
	<post>
		<Title><![CDATA[c# - Create a variable that can only hold a fixed set of values (aka an Enum Constant)]]></Title>
		<Content><![CDATA[A enum constant is a special kind of variable that can only take a set number of values, e.g.:

[csharp]
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace ConsoleApp
{
    class EnumConstantDemo
    {
        // notice that you have to place this outside the method, othewise the method
        // wont recognise this a enum constant (aka variable type)
        enum Colour
        {
            red,
            green,
            blue      // notice no comma for the last one. 
        }

        static void Main(string[] args)
        {
           
            Colour Favourite = Colour.green;
            
            switch (Favourite)        
            {
                case Colour.green:
                    Console.WriteLine(&quot;Your fave colour is green&quot;);
                    break;
                case Colour.red:
                    Console.WriteLine(&quot;Your fave colour is red&quot;);
                    break;
                case Colour.blue:
                    Console.WriteLine(&quot;Your fave colour is blue&quot;);
                    break;
                default:
                    Console.WriteLine(&quot;Sorry don't know your fave colour&quot;);
                    break;
            }
            
            Console.WriteLine(&quot;Switch statement has ended&quot;);
        }
    }
}

[/csharp]


The cool thing about this aproach is that VS2013 will recognise the enum constant as type of variable, and hence will offer intellisense, e.g.:

<img alt="" src="data:image/png;base64,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" />]]></Content>
		<Date><![CDATA[2014-05-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[.net|c#|csharp|Microsoft|tutorial|Visual Studio]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>c#]]></Categories>
	</post>
	<post>
		<Title><![CDATA[c# - Declare variables outside if-else statement scope]]></Title>
		<Content><![CDATA[If you declare (and initialize) a variable within an if-else statement, then that variable's value will only exist within that if-else statement. However if you want to initialize (i.e. assign a value to) the variable within the if-else statement, but want to make the variable accessible outside the if-else scope, then you need to first declare the variable, in the highest most scope where you want to make the variable accessible, and then initialize it within the scope, here's an example:

[csharp]
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

class Program
    {
        static void Main(string[] args)
        {

			string ContainsSeedsSentence;   // Have to declare this outside if-else statement.
            if(true)
            {
                Console.WriteLine(&amp;quot;True statement run&amp;quot;);
                ContainsSeedsSentence = &amp;quot;This fruit contains seeds.&amp;quot;;
                //Console.WriteLine(ContainsSeedsSentence);
            }
            else
            {
                Console.WriteLine(&amp;quot;False statement run&amp;quot;);
                ContainsSeedsSentence = &amp;quot;This fruit is seedless.&amp;quot;;
                //Console.WriteLine(ContainsSeedsSentence);
            }

            Console.WriteLine(ContainsSeedsSentence);
        }
    }
[/csharp]]]></Content>
		<Date><![CDATA[2014-05-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[.net|c#|csharp|Microsoft|tutorial|Visual Studio]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>c#]]></Categories>
	</post>
	<post>
		<Title><![CDATA[c# - The "this" keyword]]></Title>
		<Content><![CDATA[Earlier we cam across the following:

[csharp]
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

class Employee
{
    public string Firstname { get; set; }
    //Note, the above line is the shorthand way of writing the following:
	//
	//   private string firstname;   // here we are defining a property called &quot;firstname&quot;
    //   public string Firstname
    //   {
    //        get{return firstname;}
    //        set{firstname = value;}
    //   }
	//
	// The same could be done for the following two lines

	public int Age { get; set; }
    public double Salary { get; set; }

    public Employee(string Firstname, int age, double Salary)
    {

        this.Firstname = Firstname;
        this.Age = Age;
        this.Salary = Salary;
    }

}

class Program
{
    static void Main()
    {
        Employee Jenny = new Employee(&quot;Jennifer&quot;, 22, 25023.50);
        Console.WriteLine(&quot;{0} is {1} years old and earns £{2} per year.&quot;, Jenny.Firstname, Jenny.Age.ToString(), Jenny.Salary.ToString());
    }
}
[/csharp]

If you look at the above code, you will notice that the employee class's constructor has input parameters that are called Firstname, Salary, and Age. But these names are also used as the names of the Employee class's properties.

As a result it can get a bit confusing as to what we are referring to, and consequently the above code will result in an error message:

<img alt="" src="data:image/png;base64,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" />

One way to  fix this is by using changing the names for the properties/input-parameters.  However you would often want the names to match as a way to make it easier to see the link between an input parameter, and the property it sets/gets. That's why a better way is by using the "this" keyword as a label the members that are internal to the class (e.g. private members), i.e. we can rewrite the above code to:

[csharp]using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

class Employee
{
    public string Firstname { get; set; }	
	public int Age { get; set; }
    public double Salary { get; set; }
	
 
    public Employee(string Firstname, int Age, double Salary)
    {
        
        this.Firstname = Firstname;     // the &quot;this&quot; keyword that the 
        this.Age = Age;                 // left hand side of the equation 
        this.Salary = Salary;           // refers to the property.
    }

}

class Program
{
    static void Main()
    {
        Employee Jenny = new Employee(&quot;Jennifer&quot;, 22, 25023.50);
        Console.WriteLine(&quot;{0} is {1} years old and earns £{2} per year.&quot;, Jenny.Firstname, Jenny.Age.ToString(), Jenny.Salary.ToString());
    }
}[/csharp]]]></Content>
		<Date><![CDATA[2014-05-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[.net|c#|csharp|Microsoft|tutorial|Visual Studio]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>c#]]></Categories>
	</post>
	<post>
		<Title><![CDATA[c# - The 5 Pillars of Object Oriented Programming]]></Title>
		<Content><![CDATA[There are 5 concepts of OOP:
<ol>
	<li>Encapsulation</li>
	<li>Data Hiding</li>
	<li>Specialization</li>
	<li>Polymorphism</li>
	<li>Division of Responsibility</li>
</ol>
&nbsp;

&nbsp;
<h2>Encapsulation</h2>
This means that you can use a class without the need to view the code inside the class. All you have knowledge of the class, are it's exposed members.
<h2>Data Hiding</h2>
A class's internal data is not accessible from outside the class.
<h2>Specialization</h2>
This is related to "inheritence"
<h2>Polymorphism</h2>
This is related to "inheritence"
<h2>Division of Responsibility</h2>
This is to do with the fact that oop is designed to allow a team of people to develop a whole program together. First you design how the program will work in terms of what classes have to be written (along with what methods), then each class is assigned to a team member to develop.

&nbsp;

&nbsp;]]></Content>
		<Date><![CDATA[2014-05-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[.net|c#|csharp|Microsoft|tutorial|Visual Studio]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>c#]]></Categories>
	</post>
	<post>
		<Title><![CDATA[c# - A practical example of OOP in action]]></Title>
		<Content><![CDATA[Tip: it is best practice to only define one class per cs file.

Tip: For consistency, and to make life easier, the cs file's name should be named after the class's name.

Let's say we have a warehouse ordering system...which we breakdown into 3 cs files:
<ol>
	<li>Program.cs - this is the main starting point of our component, and hence it contains the "Main" method to reflect this. All this will do is create a "Warehouse" object and then apply a non-static method to that object.</li>
	<li>Warehouse.cs - the warehouse class has a constructor,  to make it easy to create an object using this class. It also has a method, which in turn instantiates an object belonging to the "item" class.</li>
	<li>Item.cs - this only returns</li>
</ol>
First let's create a new project:

<img alt="" src="data:image/png;base64,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" />

Notice the namespace is now the same name as the namespace:

<img alt="" src="data:image/png;base64,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" />

In visual studio, there are 2 ways to create a new cs file, containing the new class.

The first way is done by right clicking on the project:

<img alt="" src="data:image/png;base64,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" />

Second way is just to declare a new object of that class, as if it already existed, and the ide will automatically prompt you to create a new class (when you see and then click on the blue squigly line):

<img alt="" src="data:image/png;base64,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" />

Creating a new class, will automtically result in a new cs file created, hence vs2013 automatically carries out best practice for you:

<img alt="" src="data:image/png;base64,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" />

Notice also, that it is also created in the same namespace as the main program.

Now I create the various classes:

The content of the Program.cs file is:

[csharp]
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace ExampleOOP
{
    class Program
    {
        static void Main()
        {
            Warehouse NewWarehouse = new Warehouse(&quot;Manchester&quot;);

            Item ItemDetails = NewWarehouse.FindandReturnItem(101);

            Console.WriteLine(&quot;You have ordered: {0}&quot;,ItemDetails.ItemName);
            Console.ReadLine();
        }
    }
}
[/csharp]


The content of the Warehouse.cs file is:

[csharp]
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;

namespace ExampleOOP
{
    class Warehouse
    {

        public string WarehouseLocation {get; set;}

        public Warehouse (string WarehouseLocation)
        {
            this.WarehouseLocation = WarehouseLocation;
        }

        public Item FindandReturnItem(int ItemID)
        {

            Item RequestedItem = new Item(ItemID);
            return RequestedItem;
        }
    }
}

[/csharp]

The content of the Item.cs file is:

[csharp]
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;

namespace ExampleOOP
{
    class Item
    {
        public int ItemID { get; set; }
        public string ItemName { get; set; }
        public Item(int ItemID)
        {
            this.ItemID = ItemID;
            this.ItemName = &quot;Microsoft Office 2013&quot;;
        }
    }
}
[/csharp]


Note: for the purpose of this example, I have kept things simple by hardcoding the itemname to to "Microsoft Office 2013". In reality the item class is likely to query a database.

Here is the VS2013 project files for this:
<a href="http://progeektips.com/wp-content/uploads/2014/03/ExampleOOP.7z">ExampleOOP</a>]]></Content>
		<Date><![CDATA[2014-05-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[.net|c#|csharp|Microsoft|tutorial|Visual Studio]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>c#]]></Categories>
	</post>
	<post>
		<Title><![CDATA[c# - Inheritence in Action]]></Title>
		<Content><![CDATA[Lets say we have a supermarket and it sells a different types of fruits, then you could create a class for each type of fruit:

The above will output:

[csharp]
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace InheritenceInAction
{
    class Program
    {
        static void Main(string[] args)
        {
			// Here we create an object using the orange class
            Orange AnOrange = new Orange(1.25, 45, &quot;Spain&quot;, &quot;25-01-2015&quot;, &quot;sweet&quot;, true, &quot;clementine&quot;);
			string ContainsSeedsSentence;
            if (AnOrange.ContainsSeeds)
            {
                ContainsSeedsSentence = &quot;This fruit contains seeds.&quot;;
            }
            else
            {
                ContainsSeedsSentence = &quot;This fruit is seedless.&quot;;
            }

            Console.WriteLine(&quot;\n\nThis {0} is a {1} and is grown in {2} and costs £{3}. It's a {4} fruit and contains {5} calories. It's use-by date is {6}. {7}&quot;,
            AnOrange.FruitName, AnOrange.OrangeType, AnOrange.CountryOfOrigin, AnOrange.Price, AnOrange.SweetnessLevel, AnOrange.Calories, AnOrange.UseByDate, ContainsSeedsSentence);

			// Here we create an object using the banana class
            Banana ABanana = new Banana(0.43, 135, &quot;Kenya&quot;, &quot;25-05-2014&quot;, &quot; very sweet&quot;, false, &quot;Cavendish&quot;);
        	if (ABanana.ContainsSeeds)
            {
                ContainsSeedsSentence = &quot;This fruit contains seeds.&quot;;
            }
            else
            {
                ContainsSeedsSentence = &quot;This fruit is seedless.&quot;;
            }
            Console.WriteLine(&quot;\n\nThis {0} is a {1} and is grown in {2} and costs £{3}. It's a {4} fruit and contains {5} calories. It's use-by date is {6}. {7}&quot;,
            ABanana.FruitName, ABanana.BananaType, ABanana.CountryOfOrigin, ABanana.Price, ABanana.SweetnessLevel, ABanana.Calories, ABanana.UseByDate, ContainsSeedsSentence);

        }
    }

    class Orange
    {

		private string fruitname;
        public string FruitName
		{
			get{return fruitname;}
			set{fruitname = &quot;Orange&quot;;}
		}
		// notice that we have expanded the Fruitname property above, because we want it to be a constant.
		// All other properties are defined in the standard way below:
        public double Price { get; set; }
        public int Calories { get; set; }
        public string CountryOfOrigin { get; set; }
        public string UseByDate { get; set; }
        public string SweetnessLevel { get; set; }
        public bool ContainsSeeds { get; set; }
        public string OrangeType { get; set; }

        public Orange(double Price, int Calories, string CountryOfOrigin, string UseByDate, string SweetnessLevel, bool ContainsSeeds, string OrangeType)
        {
			FruitName = FruitName;		// This is inserted to trigger the FruitName property's set element.
            this.Price = Price;
            this.Calories = Calories;
            this.CountryOfOrigin = CountryOfOrigin;
            this.UseByDate = UseByDate;
            this.SweetnessLevel = SweetnessLevel;
            this.ContainsSeeds = ContainsSeeds;
            this.OrangeType = OrangeType;
        }
    }

    class Banana
    {
		private string fruitname;
        public string FruitName
		{
			get{return fruitname;}
			set{fruitname = &quot;Banana&quot;;}
		}
		// notice that we have expanded the Fruitname property above, because we want it to be a constant.
		// All other properties are defined in the standard way below:
        public double Price { get; set; }
        public int Calories { get; set; }
        public string CountryOfOrigin { get; set; }
        public string UseByDate { get; set; }
        public string SweetnessLevel { get; set; }
        public bool ContainsSeeds { get; set; }
        public string BananaType { get; set; }

        public Banana(double Price, int Calories, string CountryOfOrigin, string UseByDate, string SweetnessLevel, bool ContainsSeeds, string BananaType)
        {
			FruitName = FruitName;   // This is inserted to trigger the FruitName property's set element.
            this.Price = Price;
            this.Calories = Calories;
            this.CountryOfOrigin = CountryOfOrigin;
            this.UseByDate = UseByDate;
            this.SweetnessLevel = SweetnessLevel;
            this.ContainsSeeds = ContainsSeeds;
            this.BananaType = BananaType;
        }
    }
}
[/csharp]

The resulting output would be:

<img alt="" src="data:image/png;base64,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" />

Now there is a good chance that this supermarket sells a lot of different types of fruits, e.g apples, melons, mangoes, etc.

So what we could do is write a class for each fruit, however if we did this, we would discover that there are a lot of repeating members within each fruit class.

For example in the above 2 classes, orange and banana you will have noticed that these 2 classes had the following member in common:

this.Price = Price;
this.Calories = Calories;
this.CountryOfOrigin = CountryOfOrigin;
this.UseByDate = UseByDate;
this.SweetnessLevel = SweetnessLevel;
this.ContainsSeeds = ContainsSeeds;

The only two properties that were different in each class were:
<p style="padding-left: 30px;">fruitname         (they had different "set" values)
this.BananaType      (unique to banana class)
this.OrangeType      (unique to orange class)</p>
In this scenario, this presents an ideal scenario to take advantage of the concept of inheritence, to rewrite the code in fewer lines and make it more organised/structured at the same time.

Inheritance is to do with recognising which classes are actually are special versions (aka sub-classes) of more general classes. We do this by looking for classes that are similar (both in what they contain and what they represent) and then group them together.

We can then create a high-level class (aka base class) and move the common members out of the sub-classes and place them into the base-class.

For example orange, and banana both represent the same kind of logical object, which is a "fruit". Hence we can organise our code by creating a new "fruit" class:

[csharp]
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace InheritenceInAction
{
    class Program
    {
        static void Main(string[] args)
        {
            // Here we create an object using the orange class
            Orange AnOrange = new Orange(1.25, 45, &quot;Spain&quot;, &quot;25-01-2015&quot;, &quot;sweet&quot;, true, &quot;clementine&quot;);
            string ContainsSeedsSentence;
            if (AnOrange.ContainsSeeds)
            {
                ContainsSeedsSentence = &quot;This fruit contains seeds.&quot;;
            }
            else
            {
                ContainsSeedsSentence = &quot;This fruit is seedless.&quot;;
            }

            Console.WriteLine(&quot;\n\nThis {0} is a {1} and is grown in {2} and costs £{3}. It's a {4} fruit and contains {5} calories. It's use-by date is {6}. {7}&quot;,
            AnOrange.FruitName, AnOrange.OrangeType, AnOrange.CountryOfOrigin, AnOrange.Price, AnOrange.SweetnessLevel, AnOrange.Calories, AnOrange.UseByDate, ContainsSeedsSentence);

            // Here we create an object using the banana class
            Banana ABanana = new Banana(0.43, 135, &quot;Kenya&quot;, &quot;25-05-2014&quot;, &quot; very sweet&quot;, false, &quot;Cavendish&quot;);
            if (ABanana.ContainsSeeds)
            {
                ContainsSeedsSentence = &quot;This fruit contains seeds.&quot;;
            }
            else
            {
                ContainsSeedsSentence = &quot;This fruit is seedless.&quot;;
            }
            Console.WriteLine(&quot;\n\nThis {0} is a {1} and is grown in {2} and costs £{3}. It's a {4} fruit and contains {5} calories. It's use-by date is {6}. {7}&quot;,
            ABanana.FruitName, ABanana.BananaType, ABanana.CountryOfOrigin, ABanana.Price, ABanana.SweetnessLevel, ABanana.Calories, ABanana.UseByDate, ContainsSeedsSentence);
            Console.ReadLine();
        }
    }

	// Here we created a new class that is the baseclass (aka the parent class)
	// of the orange and banana class.
    class Fruit {

        public double Price { get; set; }
        public int Calories { get; set; }
        public string CountryOfOrigin { get; set; }
        public string UseByDate { get; set; }
        public string SweetnessLevel { get; set; }
        public bool ContainsSeeds { get; set; }

    }

	// Here we used the &quot;:&quot; notation to indicate that Orange is a child-class of the Fruit.
	// It means that all the members (which at the moment are properties) that are defined in the
	// base class, are also implicitly defined in the child class.
    class Orange : Fruit
    {

        private string fruitname;
        public string FruitName
        {
            get { return fruitname; }
            set { fruitname = &quot;Orange&quot;; }
        }
        // notice that we have expanded the Fruitname property above, because we want it to be a constant.
        // All other properties are defined in the standard way below:

        public string OrangeType { get; set; }

        public Orange(double Price, int Calories, string CountryOfOrigin, string UseByDate, string SweetnessLevel, bool ContainsSeeds, string OrangeType)
        {
            FruitName = FruitName;		// This is inserted to trigger the FruitName property's set element.
            this.Price = Price;
            this.Calories = Calories;
            this.CountryOfOrigin = CountryOfOrigin;
            this.UseByDate = UseByDate;
            this.SweetnessLevel = SweetnessLevel;
            this.ContainsSeeds = ContainsSeeds;
            this.OrangeType = OrangeType;
        }
    }

	// Here we used the &quot;:&quot; notation to indicate that Orange is a child-class of the Fruit.
	// It means that all the members (which at the moment are properties) that are defined in the
	// base class, are also implicitly defined in the child class.
    class Banana : Fruit
    {
        private string fruitname;
        public string FruitName
        {
            get { return fruitname; }
            set { fruitname = &quot;Banana&quot;; }
        }
        public string BananaType { get; set; }

        public Banana(double Price, int Calories, string CountryOfOrigin, string UseByDate, string SweetnessLevel, bool ContainsSeeds, string BananaType)
        {
            FruitName = FruitName;   // This is inserted to trigger the FruitName property's set element.
            this.Price = Price;
            this.Calories = Calories;
            this.CountryOfOrigin = CountryOfOrigin;
            this.UseByDate = UseByDate;
            this.SweetnessLevel = SweetnessLevel;
            this.ContainsSeeds = ContainsSeeds;
            this.BananaType = BananaType;
        }
    }
}
[/csharp]

This again gives the same output. Note also that the class "fruit" is something we might never to use directly, it is just being used here as way of oraganising/structuring/streamlining our code as well as categorising our classes.

Also, as you can see, the base class essentially "inherits" the members of it's parent class. That where this term comes from.

Now, a supermarket might sell other things on top of fruits. E.g. meat products, such as beef, chicken, fish,....etc.

In this scenario we can take a similar approach that we did for fruits, and create a parent class called "meat", and then subclasses for beef, chicken, and etc.

In this scenario, we would end up with 2 parent classes, "fruit" and "meat". And both of these parent-classes are very likely to have a number of properties in common, e.g. price.

So what we can do is make both the fruit and meat classes themselves sub classes of an even higher level class, e.g a class called "food", which in turn can be a subclass itself for an even higher level class called "Item". The item class could cover non-food items e.g. newspapers, washing-up liquid,....etc.

If we just stick to the fruit example for now, we can organize/structure the above code even more by creating the "food" and "item" classes:

[csharp]
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace InheritenceInAction
{
    class Program
    {
        static void Main(string[] args)
        {
            // Here we create an object using the orange class
            Orange AnOrange = new Orange(1.25, 45, &quot;Spain&quot;, &quot;25-01-2015&quot;, &quot;sweet&quot;, true, &quot;clementine&quot;);
            string ContainsSeedsSentence;
            if (AnOrange.ContainsSeeds)
            {
                ContainsSeedsSentence = &quot;This fruit contains seeds.&quot;;
            }
            else
            {
                ContainsSeedsSentence = &quot;This fruit is seedless.&quot;;
            }

            Console.WriteLine(&quot;\n\nThis {0} is a {1} and is grown in {2} and costs £{3}. It's a {4} fruit and contains {5} calories. It's use-by date is {6}. {7}&quot;,
            AnOrange.FruitName, AnOrange.OrangeType, AnOrange.CountryOfOrigin, AnOrange.Price, AnOrange.SweetnessLevel, AnOrange.Calories, AnOrange.UseByDate, ContainsSeedsSentence);

            // Here we create an object using the banana class
            Banana ABanana = new Banana(0.43, 135, &quot;Kenya&quot;, &quot;25-05-2014&quot;, &quot; very sweet&quot;, false, &quot;Cavendish&quot;);
            if (ABanana.ContainsSeeds)
            {
                ContainsSeedsSentence = &quot;This fruit contains seeds.&quot;;
            }
            else
            {
                ContainsSeedsSentence = &quot;This fruit is seedless.&quot;;
            }
            Console.WriteLine(&quot;\n\nThis {0} is a {1} and is grown in {2} and costs £{3}. It's a {4} fruit and contains {5} calories. It's use-by date is {6}. {7}&quot;,
            ABanana.FruitName, ABanana.BananaType, ABanana.CountryOfOrigin, ABanana.Price, ABanana.SweetnessLevel, ABanana.Calories, ABanana.UseByDate, ContainsSeedsSentence);
            Console.ReadLine();
        }
    }

    class Item
    {
        public double Price { get; set; }
        public string CountryOfOrigin { get; set; }
    }

    class Food : Item
    {

        public int Calories { get; set; }

        public string UseByDate { get; set; }

    }

    class Fruit : Food
    {

        public string SweetnessLevel { get; set; }
        public bool ContainsSeeds { get; set; }

    }

    class Orange : Fruit
    {

        private string fruitname;
        public string FruitName
        {
            get { return fruitname; }
            set { fruitname = &quot;Orange&quot;; }
        }
        // notice that we have expanded the Fruitname property above, because we want it to be a constant.
        // All other properties are defined in the standard way below:

        public string OrangeType { get; set; }

        public Orange(double Price, int Calories, string CountryOfOrigin, string UseByDate, string SweetnessLevel, bool ContainsSeeds, string OrangeType)
        {
            FruitName = FruitName;		// This is inserted to trigger the FruitName property's set element.
            this.Price = Price;
            this.Calories = Calories;
            this.CountryOfOrigin = CountryOfOrigin;
            this.UseByDate = UseByDate;
            this.SweetnessLevel = SweetnessLevel;
            this.ContainsSeeds = ContainsSeeds;
            this.OrangeType = OrangeType;
        }
    }

    class Banana : Fruit
    {
        private string fruitname;
        public string FruitName
        {
            get { return fruitname; }
            set { fruitname = &quot;Banana&quot;; }
        }
        public string BananaType { get; set; }

        public Banana(double Price, int Calories, string CountryOfOrigin, string UseByDate, string SweetnessLevel, bool ContainsSeeds, string BananaType)
        {
            FruitName = FruitName;   // This is inserted to trigger the FruitName property's set element.
            this.Price = Price;
            this.Calories = Calories;
            this.CountryOfOrigin = CountryOfOrigin;
            this.UseByDate = UseByDate;
            this.SweetnessLevel = SweetnessLevel;
            this.ContainsSeeds = ContainsSeeds;
            this.BananaType = BananaType;
        }
    }
}
[/csharp]]]></Content>
		<Date><![CDATA[2014-05-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[.net|c#|csharp|Microsoft|tutorial|Visual Studio]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>c#]]></Categories>
	</post>
	<post>
		<Title><![CDATA[c# - Creating objects from generic parent classes]]></Title>
		<Content><![CDATA[In the previous unit we came across:

[csharp]
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace InheritenceInAction
{
    class Program
    {
        static void Main(string[] args)
        {
            // Here we create an object using the orange class
            Orange AnOrange = new Orange(1.25, 45, &quot;Spain&quot;, &quot;25-01-2015&quot;, &quot;sweet&quot;, true, &quot;clementine&quot;);
            string ContainsSeedsSentence;
            if (AnOrange.ContainsSeeds)
            {
                ContainsSeedsSentence = &quot;This fruit contains seeds.&quot;;
            }
            else
            {
                ContainsSeedsSentence = &quot;This fruit is seedless.&quot;;
            }

            Console.WriteLine(&quot;\n\nThis {0} is a {1} and is grown in {2} and costs £{3}. It's a {4} fruit and contains {5} calories. It's use-by date is {6}. {7}&quot;,
            AnOrange.FruitName, AnOrange.OrangeType, AnOrange.CountryOfOrigin, AnOrange.Price, AnOrange.SweetnessLevel, AnOrange.Calories, AnOrange.UseByDate, ContainsSeedsSentence);

            // Here we create an object using the banana class
            Banana ABanana = new Banana(0.43, 135, &quot;Kenya&quot;, &quot;25-05-2014&quot;, &quot; very sweet&quot;, false, &quot;Cavendish&quot;);
            if (ABanana.ContainsSeeds)
            {
                ContainsSeedsSentence = &quot;This fruit contains seeds.&quot;;
            }
            else
            {
                ContainsSeedsSentence = &quot;This fruit is seedless.&quot;;
            }
            Console.WriteLine(&quot;\n\nThis {0} is a {1} and is grown in {2} and costs £{3}. It's a {4} fruit and contains {5} calories. It's use-by date is {6}. {7}&quot;,
            ABanana.FruitName, ABanana.BananaType, ABanana.CountryOfOrigin, ABanana.Price, ABanana.SweetnessLevel, ABanana.Calories, ABanana.UseByDate, ContainsSeedsSentence);
            Console.ReadLine();
        }
    }

    class Item
    {
        public double Price { get; set; }
        public string CountryOfOrigin { get; set; }
    }

    class Food : Item
    {

        public int Calories { get; set; }

        public string UseByDate { get; set; }

    }

    class Fruit : Food
    {

        public string SweetnessLevel { get; set; }
        public bool ContainsSeeds { get; set; }

    }

    class Orange : Fruit
    {

        private string fruitname;
        public string FruitName
        {
            get { return fruitname; }
            set { fruitname = &quot;Orange&quot;; }
        }
        // notice that we have expanded the Fruitname property above, because we want it to be a constant.
        // All other properties are defined in the standard way below:

        public string OrangeType { get; set; }

        public Orange(double Price, int Calories, string CountryOfOrigin, string UseByDate, string SweetnessLevel, bool ContainsSeeds, string OrangeType)
        {
            FruitName = FruitName;		// This is inserted to trigger the FruitName property's set element.
            this.Price = Price;
            this.Calories = Calories;
            this.CountryOfOrigin = CountryOfOrigin;
            this.UseByDate = UseByDate;
            this.SweetnessLevel = SweetnessLevel;
            this.ContainsSeeds = ContainsSeeds;
            this.OrangeType = OrangeType;
        }
    }

    class Banana : Fruit
    {
        private string fruitname;
        public string FruitName
        {
            get { return fruitname; }
            set { fruitname = &quot;Banana&quot;; }
        }
        public string BananaType { get; set; }

        public Banana(double Price, int Calories, string CountryOfOrigin, string UseByDate, string SweetnessLevel, bool ContainsSeeds, string BananaType)
        {
            FruitName = FruitName;   // This is inserted to trigger the FruitName property's set element.
            this.Price = Price;
            this.Calories = Calories;
            this.CountryOfOrigin = CountryOfOrigin;
            this.UseByDate = UseByDate;
            this.SweetnessLevel = SweetnessLevel;
            this.ContainsSeeds = ContainsSeeds;
            this.BananaType = BananaType;
        }
    }
}
[/csharp]

In C# you can create an object using the parent class, but instantiate it with a child classe's constructor method, e.g.:

<img alt="" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA+YAAAGDCAIAAAAppJ2wAAAgAElEQVR4nO3d748cVZ7v+fgnvE98xZN9ci12uN04r/bB7vrOzhQ4r7SrlixmW9rm9sx18KsRKmlL9aDt3pZAsmfEj7iSVyr3tKbAogtm1fiqaehNUNmojN0gft6uaa/ZhnTZFCz0GEMh0830MMNsnX0Qv06c8z3xKyMzIzLfL4XAFRVx4sSJyMxPnjoR4f32768ppZRSj3zwxJ4L/30yPfLBE0ry9NNPi/MBAAAA2P7m/3yx6vSf/ual/+Z/+F+TySOyAwAAAONz7dpnVSciOwAAADA53YzsQ9XzlD8YtZjZMPC9kN4g4swGDINe42VO1+ztEQAAmDl1I/t3kmkmIvsw6HneCCF3GPSaj31J7Pa8XjCsUYFRauVYd0IBVz8eY94ckR0AALReNyN7s4ZBrzgTFxfRaOyrGiQnFdkngRANAACQ0ZbI7nsqymhD1cv+2/OU56leEC058KM5eqjzPRUE5pJqEM0x5xukyJ6dp/2k9f+GczIdwnqPuPaLsKrDoNcLAt/zvOh/7lzqCsxWmbkrWDMdq2vd+f7AvUfyYBuhzIHv+UE8O9uKJf5qMPDlhqm1oaSgzKLxbNfwIaNB9Ll8lQAAAJPX6sg+8J2jX4KeGdk9P1xHefHqvqeiFDdQXk/ldKNHGS8TJAd+JnynIc8VJ7Pz9QWjouLe44Hveb1gOAx6ruQq9/vrc80lykR2oUrKmZDzvjSk8+UqadlWK93RdM4NJMHbH7gqL27IvZtxTbM1MffU9ZWByA4AAKbFlcv/8uEf50X2A99JpjFG9mHYce4LhdiR3V69fGSPVs0G9yTu6dk6WsYMbnn92UkXcLxQmuArRfacAF4qsktVynwzyd8jab6jSlrVtCUcTZe/4fgnZ+WtDclL5nxhsHdixFFSAAAADXPl9XCacmSPDITgXiayJ0NovCS7F9P6WKNMLYW9gTGuxQq4UvArH9nFnt5RI7ucRScX2bVdyw3umRoZbWYvam3IsUNEdgAA0F05ed2V2scV2cOcFPTSkS0pK8cXR/ah6pXoWTdl+7cHfi8IHFmvIA8LoTsvspuDLqRLMBsYGCMvI+6gI7qWHBjjjuxyvcyfrTFJYuXFDckDW0pHdvfFrwO/YBQ+AADAeOTndTG1/6e/eemPDnwnmZqJ7EPt4tEkf4fx3bjYVJ+ZzC/sZRe+BiS0iw3N/l8xSYtLpr8RLj8N5+VE9nBZqTs6W2Y6zxyqk62VOFOokjVX3Ff5OlvzEs60RDFJO5uu7L4LlXd8N5B2U4rsjj3KbxAGswMAgIkrzOt2ah9LZG/eIBPTB777jjF5hTBKAokG7gYKAABQQ707xnQvsvt1Hr3kvHcI5k3Yw05eBwAAUzG7kT07hKZqF3s4MoOIBgAAgKmrG9nvTKb2RnYAAABgBhDZAQAAgFa7du0zPWYT2QEAAIB2IbIDAAAArWZE9r/79W/EqWWRfah6de4AMxPcj/oBAADATCKyhwXKD8+ptH6zMTrzbFRzU0R2AACAOdLNyN6sBp6QM8HIDgAAgDnTlsjuJ489Gqpe9t/GjdUHfjRHj8i+p4LAugX7IL0ve96t2aXInp2n/aT1x4dzzKfeJ6tpvwirOgx6vSDwPc+L/peX8sXIHt4tPrviwPf8IN5UtsrSHw4Gfs0/JAAAAGBKakb2f3dnMo0xsg985+iXoGdGds8P10kfeup7KkqwA+X1VE6XdZRvMxl54GfCtx9XScy7di+7vmBUVDykZeB7Xi8Y5veju39rbEvL4OnTWrXntroXBwAAQCe0OrIPw45zXyjEjuz26uUje7RqNrgnmV1Pz9EyZua1IrvZ9x5Hdn+gJ/iGInv8U/Ib7evGGMbsAAAAYKJaHdkjAyG4l4nsyRAaL8nuxbL9071gKHasD4xxLVYu1kOzuRCRHQAAABXUjez/IZkai+xhxgx66ciWlJXjiyP7UPVK9KybsmPYB34vCOShMEYUtiK6Fv3NNaTIbo1XGSmyp3sR9vabA2O4sBUAAKBD2hLZh9rFo0n+DuO7cbGpPjOZX9jLLnwNSCQXddr3eDSTtOuiTv03wuWn4bycyG4la3NkTZLErZlSZNcW9QfZvxLYWwIAAEC7tSWyN2+QiekD333HmLxChPEtHWMOjGnglpYAAACYpHmJ7H6dRy8Jg1u6x+p6J68DAAB0S73Ifsu/+w/J1NbInh1CU7WLPRwsMwvplstPAQAAOm6WIzsAAAAwA4jsAAAAQKsR2QEAAIBWI7IDAAAArdbNyD5UvTp3gAEAAAC6h8geFqg9pKhOuQ3flsV8ZlKde9dwpxgAAIAZUTeyfzeZuj8wpoGHC40pH2cfXFoNkR0AAGBG1Izsf/zdZGomsvvJY4+Gqpf9t3Fj9YEfzdHzqO+pILBuwT5I78ued2t2KbJn52k/aR3g4RyrRzxeTftFWNVh0OsFge95XvS/wkhtRXarzPAG8vFCA9/zesHQWSVzeQAAAHRAqyP7wHeOfgl6ZmT3/HCd9KGnvqeiqDpQXk/ldKNHGTeT2wd+JnynCVmqkt2lrS8YFTUMep7nD7RgXdS3b2xNKDP6p+cPMrPEKiULE9kBAAC6pNWRfRh2nPtCIXZkt1cvH9mjVbPBPUnAeraOljEzr5WPzY7uOLJr0bpyZJfKdNeKgTEAAAAzotWRPTIQgnuZyJ4MofGS7F4s7rJWSVSXOtYHxrgWKx9ne7yzC9WN7FKZScm9nvClgcgOAAAwA1oU2cM0GvTSkS0pK8cXR/ah6pXoWTdlx7AP/F4QOC4BzUZiK05r0d9cQ4rs8ngVe2CMY6xL3Imv10FO+AO/1v1nAAAAMDVtiexD7eLRJH+H8d242FSfmcwv7GUXvgYkwi5z8R6PZpLOjE1xjIQRLj8N5+VE9nDZosgulBkPjE9/aw2XyUZ0x5YAAADQWm2J7M0bZGL6wHffMSavEOdYlM5q4JaWAAAAmKR5iex+nUcvOQaidFbYw05eBwAA6JbZjezZITRVu9jDwTKkWwAAAEzdLEd2AAAAYAYQ2QEAAIBWqxvZ/zyZiOwAAADAGNWL7P/mj/88mYjsAAAAwBh1M7IPVa/OHWC6TnoOa4OM56UOg16LbpjDw1wBAMD8unbtM/Wul0zzGtn1RxTVKbfhQJl5YlP6MKQOR3bHHtWuHAAAwBypGdn/xz9Ppu4PjGng4UJjCpRGRp9sZB+L2rtAZAcAAPOrLZHdTx57NFS97L+NG6sP/GiOHuB8TwWBdQv2QXpf9rxbs0uRPTtP+0nrLg7nWP3H8WraL8KqDoNeLwh8z/Oi/xVmUCmyh3eM13qpB77nB8m24uWtrav4ZvPm3xIyOxDNTpbUNp/ZTto2eqHFfedmZC+skt6aSTOmCwtLpuUS8QEAwGxodWQf+M7RL0HPjOyeH66TPvTU91SU4gbK66mcLBkFPzP02XHR1Uts9wHrC0ZFxSNNBr7n9YLhMOhVDLhaDtUezBoVl1lB2LqjvlpB5k4YP4tb1wqXtlO0R2KVxIWiX+s7m1cckR0AAMyQVkf2Ydhx7guF2JHdXr18ZI9WzQb3JILq2TpaxgyDVmQ3+97jyO4P9ARfPbLrnejugCttPVrU6NTWc3ZxZLe3PnJkt6skN3K8S9m1HYcDAABgprQ6skcGQnAvE9mTITRekt2LZXude8FQ7MkdGONarMie17U9/sgup2fp7wajRnY9cpe5HsDeI+FPGekvPeNPANrRcS45Ud49HhMTExOTPU3+DRmYbS2K7GF0C3rpyJaUleOLI/tQ9Ur0rJuyY9gHfi8IHGMvshHTCslCuMyL7PJAjrqRXdq6vmvp1tKZmSHiwh6KW8/52lFmj8QqmXVOR7D7A/d9bKRRPCNeU1wGH0sAYOO9EWhcWyL7ULt4NMnfYXw3LjbVZybzC3vZha8BiewFlPbVkeaND8Ul098Il5+G83Iiu52X483Xi+zC1vUd1b+IJAv6g6Qkc2CNOT5e7mW3/u5QvEdilcRGNjeZtGje4Rh/vzsfSwBg470RaFzdyP4XydTWmzwOMjF94LvvGJNXSJnx2XMs25vfptZq4M6dZfCxBAA23huBxs1LZPfrPHrJMXIaiUwLtaW5wh72yXx54GMJAGy8NwKNm93Inh1CU7WLPRyw0Zo+49bKDE2Zw+biYwkAbLw3Ao2bTmRPTHBPgebxsQQANt4bgcZNIbIDM4OPJQCw8d4INK5MZM/cd8XziOxAhI8lALDx3gg0jsgO1MfHEgDYeG8EGud5/0WP7J73X+xJiOx/8hfJRGTH/HJ+LF1b7Z86l/y0vd7vr29Ppkpjdm75+Op09uTisqc1qW57vb98cXv1eH/1Wt5i+TKFlK9D3c3VXzG/nMJiq2734rL2PMvcxmmcq7XN+bnnZK12nqEXbDnVWqnUmwCRHWgckR2oj8jeBqXSdiOFNJWzmy2qUrFVw5ke0y8ue5M8+q6qEtkbR2QHuqDewJhv/MlfJBORHfNrpiL7tdV+8SdxGyO7uri6ek2dO7V8LplTal+KChGWqZuz7fqUKar8XiRLuootXEBkLjzq96JqphfZ1cXl5YuVV6qgxvlZY5XyZRLZgS4gsgP15UX2dDiB593j9de3ldpePR79GAWCi8ve8X7/Hm/5ovUrpbQ+TuMf4sKh5FfxKBFH+dvrfblup84J9dTm6P2sViFiTZJVnLuvre5uolPL2bo5suPFZe/Uat6+XFzun1oON7q8Hh0mRz4rUeGLy0Utph0vYwSLXJSKI5GxluNkWM/ur9CkKnNwy7S5dnyNw3rulLd8Ma18/7h1AuSW7zwTzAbPnL1mE1ltnu37two5dU44UbVXaH992/wxJM60N1HmPMkUdc4+HPHprbRA7DqFct5Goh8dZ7ixYtWzwtXgMiI70Lh6N3n8xp/8x2QismN+Vepl1wLQueV7ls+pdKSB8Kt4xTAh9Y/3o2Bx6pxr4XD56IM/7D8rKj9dUvuHvXBmjvhpbXUB2qvk7H7OptNlkuyl76BdkzDPlSwwt4sx3Za9bpKb41rZLZapp9jLLhWVaeTsWuLJYOyv3aTmASrT5sJBjGiRPbs7JcsvOtvFs9fsDLbb3PgaaRSiH1ytVvq3BePHnJn2Js7lnCeOzQnNZR191ymUe1oqpZxnuNDsuUfN2Erxm0AWkR1oHJEdqK9SZD93Su93z6Q04VdaOedO9VevnVs+vnpuvb980b1wkqgSeeWfW74n22cWf1rbC2vFGp/WViFCTaJVcna/1DIXl71T58wdtGUjrFig2avtiOz6ts6dMq9t1efktlh8HN0DY6zC5cgungxCZDc7a7MVKNPmUiWVUunAGL0Q4wSQyi8+E6wGV1JRemR3nZOOQoQTNa5GVAHjRyUtI24i9zxJ65Ypyj4cee2mlMq8PHNOy7Ao8QwXmj33rDC24n4TkBHZgcYR2YH66vayx+JPSvdg9+3V4/3+8eVzSp071e8f769eyxsZ7+pidHaXKmUHRHvhdE42ROZ8itur5Oy+1P9qLTNqL3u6QMnILndnSnPEFivuZXcV7uopl06G0SN77oUWjstPta+C5gmQU37OQREbLTeyu85JsZC8uGnlXeFkyM4s1csuHgXlPl7W0a/Wy14isgvNnntWOHvZy42qJ7IDjSOyA/VViuyZwaBmN7P1K23dJHbbQ4GtId3CQF65/IvazfvSERrhv+3y407K4/3Mp7VQSMJeJWf37fG7ci97ubHs5/L2pXRklwcN32PNMcZVm+ObtQZ0NF2aJsM5p5aXzSPiPhmM/XWGxZwFck6n7CFOfqs3XVEve6kzwWxwx+mRGcvuOCfFQqx6al3OURey2Otf6lUmnifOP2Vkzwf77xXp0XedQjmbi350nOGO13XRWWGen2mD53W3E9mBxhHZgfr4WCo2jjtdtIcz7sMy22cCsnhvBBpHZAfq42PJRbtHx2SfvzNJF5c9591mEJmLMwEW3huBxhHZgfr4WAIAG++NQOOI7EB9zX8sXVvta52RZZ/BVG94xuiDOspcujd6sU3KG3177pTn3bN8rs7WpWJLl2M+eFUcPt4wfVizeClCuVJqPHd2AueqPuRdXrHJJ4JN80FpJZtlGsO3iOxA44jsQH3jiezpM3fGmwaI7GV/VavYmpHdcZOW8RjxBJtcZK9hHiJ7SUR2YCYQ2YH6xhLZo3tvK1WYBupdz1fpKeX5m3DeYXA0U4nsI10cqRVbozUurq5eU+dOSTfsUxUDcUWjxk295mVXGeHg5h8j47eTiuzq4vLyxVZeXNv4C7MKIjvQOCI7UN+YIrt0B2v388PTXFJ07zl7rexTyq3nuuuPSbd/G0aB1WqPPc+/yWD6LEajHJXpfo76nssUnt3rdASI4+HtyQ3vyuxLTrFlSzDlPnC037/H6x+XjoJ7Q479tTdnNa82Pqd0/c1CzsnnzLn800Ao3DjTQuGfpKLCz5kvByGyl2kWa05mK9uOk7PEK6VwjrVpuZzj/b5xwhdWqdapOCIiO9C4mpH9Tw8nE5Ed82tckT1+wnmSqITnpyizF81+wov8IPrCh9vrc8S+Q6P3rspjz3Mf5aPdflvcenzD6TDFlik8ZG+i4OHtUcpxPWEq9xHuua0hHxFNbmR3HKP8DeVuOrO5bPNqW4yCeJn6m4WItZUie0HhxpkWV754E1pkF5vF+cAgbY75OFXx5Cx8pWQbs1rzCuVozVJYpVqn4oiI7EDjiOxTN/A9fzBiGcOg541eSuu1bzfHF9nDj9LV+JNVeEq5MjOK/Rx1+SrGvCdlWs91z4QAx9Ppqzz2PHcZ5xM0VVqB5XNVCk+awthE8cPb856G4yXP3xGeqZnbGsXXlboGxtjP3LnH+fSiUvsblp75hpBpXr0mySPrS1wXmy3EVduqjSNFdpWu5T6Cmb0QmsU4hcRqZLbiOjnLvFK0DZVr3rxy0mYprFK9U3E0RHagcZOI7E+7TW/Hx23ge7qcnDmmyD4MehNIt+V3M128FwyLinVUfr4ie/Rn8SZ62a1NlI56ehBxZusqjz3PPJHR6PDL+y4ROXfKWz617GwTq/CkJY1NFD+8Pa8b2Fls/l6UGzjuuPw0Lq3MN5xS+2vsgtW82S8JRg907g5ohThrm3MaiByRXf5tYS+7+69AedUomY8Lj5H9FyHHpnPLqVKlmqfiSIjsQOMmFNnFbc90ZA+VieNNRHbBZCJ7qOwuDPxeMAh6xZl9kpUfyVgjezjeNHfsqfG8cedY9mxHmvsp5Ret58+nj0mXfntRe3S8WGDOY8/TB7NbD0J3R3Z1bbWf/h2/TOHZ+emz1q3GKYrszue9Z4rNbV7nEcm6KN3kUc+4Rb3spfY3bEEjuunNq21IGLufU39HIcI5kzlSUvMmrWqcaeHenpIOx3HtagRzLHtes7j20dxK3p+Acl8pRmOWad68Y601S2GVap+KIyCyA41rS2T3PRUEyvOU56leEM0c+NEcz1NhgBsGqhco31NeTwV+Ol8NVS+7pDEzKbPkhuz5A6vMEpnSyLID3/ODoBf2SMfBdeB7/iDurU7T7DBeTutXzqwfz046upPFtFWzpQplilXKFJsuK1Wp9G6qMLEP1dDI7MaGXJW3d7PqHo0DH0tNKnPPDXPcCHJVvY2J3rzTb+pG7+syXXZjTr95x4v3RqBxLYrsnq+UUmqg5ePEUPU8NVBqGES/9T3VC6IEH64+0IqKQ56QqtMNDVUvXtLeULh6tGR2dXtDbnaWjaPlwNf/Jc3Ug7oWWzNJP6231S1tzxHLFLeu/zN/9fK7GSd2lcns0obEykvzh0Ev820kb4/GhI+l0ZV/oP32ep+H3pdRvkmttbJjcmY6U04UkR3AyCYX2f8sS1mRXe/GjkNe2sntJZHdj5YPhnFk13q+wymMcWG+N2K3nrmDXpzprQ0pMZE7NuQmdD9rPdZWOk9mml3NaWR3JNASkV0u01Ulq3vaUaXSu6nVJ83s0obk3RHmGwvFWxW3Pi58LAGAjfdGoHH1Ivs3//RwMjXWy25H9jQ0673sdmQfKK+nnOF5kAnuYmS3N6TEyJ6/IWnbNSO7I8qOFNnlMktHdle6Lrmbzi8MRHYAmDW8NwKNa3Fk1wauhGPKnZFdKT9/ZHk2iOtd+K4NKaWCnjwwpkoGrBvZHUM6qkR2Ow6LZTp7xKWBMa6tF++mUb+4n915CxhHli85MKZNkb3Gn7+dF8+NQZny69dBv8QtvoSuSmml7iIydaNUoyvjJaZeq2wFJnDDkzEab2OOfAFAQ01NZAca1+LIrl39GV5ymhPZM0NW4o7woCdcVOpLw1rsDSmrBL16xoYk4t0Py0f2bLe0PH7dXi57dWb6G+HyU2GsjdWFbRYpr15mN6Uuf/3rgVV3q/KO3Uy3rw+Pb1Nkr2EyYV3f1ujL5DKe10Nkb2bdSWpZPbsd2cer4Wt2iexAe0wosrfnvuy+fW0rUFf9j6WcW3kQ2bX7LeY9TX26OVK46XV1tdeteiuYEbUkspc8MdppQoesemQvem5rvaYmsgONm0RkbxUiOxpUdmDMxWXveL9/jxc/GikeMZK5mbp2w2ZjYMy11X48tiSzur5KpnxVvIpyrOiqjzk/eeKP/Q+TGdmzW9Rub2I3jpL2pUrls/eqt7Yl3Wa7VNNplSxVDSXPt9eVW9vYEauJyuymWcmqtVpe1m8SH4c8YVvO5wOYjwuINmc0uHi4Ly57p1bLnRjlTtR1R4Not0I3mm75otRQzn0XGjM9YcRD7DjPyx1NbRCanr+LT07rXKrQ1HmI7EDj6kZ2P5kqR/bpPkGJyI4GVYns2YeJCo+lzD4WUfvv9npfT37yKlaxBask1UsfvhOlPbE+4nMily8qdXG5f7wfJS1HR6wV2c3nqkasxklWl/v5SlTeeCJsyQdDlmo6vYTCYxoXJTwCM7sLSWsX7IjVRMW76Wj2vFpZj5eynmmqsR8pGj+F1/FQ3uyJpNVHKFx6+qm4ZNkT1dUg4hHJeZRvmZeP1JjiIS7ozy55yom7437gq9AC5Zo6H5EdaNzcRXagQRUiu92Ve3x1O/MEx+zjFbMrxosZj2P0cp7IWLCKXr1o+bzC9ZQWLhlGn3On+qvXzi0fXz3n/lB3DIyxHtWZdBBasVLfF7NtcytvhUtrW7Wbzh36XasIbWjtQtLaBTuSbaJ6u1miVtpjcaOHjy6fEx+xqW3LqIz9o904xvEVDreUI8Uly56orgZxHJGihip4+WQOmeMQyzte7mhqdXPtjr2k1j5Fkd1dNyciO9C4mpF9wU8mIjvm1+iRXbi6K+eOMWb/XHYVvfzCVewV464+sXChc05trx7v948vn1Pq3Kl+/7jzszw/sgtpwzXqV2qN/MobPb6r9raSQrTBHsaOy02X208vriJ300pzCnckv5c9bzetM6SgVtnVz53ylk8t63WzD1/pXnaL8/g6c6S9ZNkTtTiym61R5vDlvTadvewF7SC8QHJOOTF/Z5es0cvuqlsOIjvQOCI7UF/9yB72nB1f3c7cCVHuZdc65MzRwK5e9uJVkurFHXjWmGmjcGF8dvLxv73eT0cIWJ/0Bb3sWh08s3GUEvalSuWNgdSubd3jeaeWqzVdUknh4LpWyRsM7bpywDEiPNNExbuZ86XOefWCEjLitdW+PjpIPnxlx7Jbf8cQfsyeNmVOjHInak5kl49IucOX/9rMnDAF7ZDXwjl/qjrez4/sUvtkz6UKTZ2HyA40jsgO1MfHEhpWuhdzOlpevdFV2sGZb40R8N4INK5dkT26y/Z4b6UNNIaPJTTp4rJn3mSmRbbX++U7WbuKyN4Q3huBxrUpso//wTdAs/hYAgAb741A49oU2R0PqQdai4+luWZ3sooXDRc+3aZWZ+30nwBKHzPceG8EGkdkB+rjY2muuTLrnER2wI33RqBxZSK78jx9GmdkZ1wMOoWPpbk21ciu6j5JflSu2/8BGt4bgca1JLIPfK46RQfxsTRnrHspGk+DzwyMcTxDXiwnvPN9fC+/9B7b+hzxofGuJ8lrdwY0nnK/fDHvZoWOx9pndta7xxN3M1qy+sPtMXt4bwQaVzey35VMjfWyc/UpOoePpbliPs/I8Sx64Yn0rqcXJeXovezaY7b0fnTxofHOJ8mnT4Y6t5x+wXA8TMdaOFN5YxnnI4HSxwm19qY3mBjeG4HGtSiyM5YdncPH0lzRH/aulPuRQ/EDsORnyDvLsZ5Lbz0lXnxovPwkee1rQPRcev0JpsYj662FtUfnWI8EykZ2YcnqD7fH7OG9EWgckR2oj4+lueLuHRciu/MZ8o5yciK+OdhdHPvuXEbrJk+eAOr4ld5Tbo5pcUT2vKtguaXMHOO9EWgckR2oj4+lOSOMQVdKiZHd+Qx5VznWc+mNp8SLD40v85T7dDB6VNu8sezx1wZtIL69s2EN7SH7cj2Lrr7FLOK9EWhcvZs83rpwVzLxKCXMLz6W0FI8xRNTxXsj0Lg2RfYwtHPrGHQHH0toKSI7por3RqBxNSP7bXclU5ORHegWPpYAwMZ7I9A4IjtQHx9LAGDjvRFoHJEdqI+PJQCw8d4INI7IDtTHxxIA2HhvBBo3y5F9561nfhw6e7nqShXWwBzjYwkAbLw3Ao2rd1/2W2+7O5naGtkvn/3xM2/tKBWm8LIRfOetZ6K1gGJ8LAGAjfdGoHEzG9l33nomiemZHL7z1jNnz56NO9KzPfGXz2o/mJ30eStqv9W66DPFCb3+ye/5ltBV2qPamZiYmJjSadpvz8CsmdnIfvms1rN++Wwmsie/0XJ9tEiS7sVf5a/442TNZ97aMWograLXCkBzfvfl76ddBQAAmjSzkT2vlz35Ienm1rvWw9/m/Krot2EUN4fY2KswcB4YDyI7AGDGzGxkd45lN5K3EZf1yO76VdFvk070zDL2KlLBAEZHZAcAzJjZjeyuoeLZgKz1fWcHxuT+qmjF6F/GWAycbpoAACAASURBVHZjlXRoO93sQBN+9+Xvw8n4NwAAXVf3Jo9diOzAWHz99bRrAAAA5guRHShtZ2f3oYfU2bPTrgcAAJgvRHagnJdf3r31VuV5u9/9rtraUu++O+0KAQCAeWFEdj2auyYiO+ZM2Lnuecrzdp9+Wl28GP0DAABgIspEduHy09vvTiYiO2Za0rn+7W+rrS2llNreVp63e+LEtGsGAADmRd3Ifk8yEdkxo4zO9a++SuYrz9t96KGpVg4AAMyRKUT26Rr40Y74g2lXBW1md64nvvxSed6u70+pZgAAYO7Ui+z7b78nmYoj+9Nu09rtoDfuyP7h0RPP3hxPR98b67bQKFfnus7zdg8enHjNAADAnJpQZBe3PeuR/Y2N8J+fvnPHiZce/3Ssm0ND3nzT2bmu2T14UHneJOsFAADm2YxH9mQYjOcpPaIbkV1fLJ0/VL14Zi/InSnQIru68fhTUUf7xgvPHn3tnTui3nc901v98e+9cbPWT3/Hazdcq2+8YHbnX33tpTtee+foiWdvfuqdx1+gm7+cL7/cPXGioHM9tuv7yvPUjRsTqx0AAJhn9W7y2I3IPvCV5xhv7OxlH6peHO4HvrCMOFOiRfb33rg57mXfeOHZm59656qK/n30vWwffPrvD48mM997Q1/FXF336Tt3nHhjQ6mrr71084k3NtSHR088e8drN66+9lKY+OG0uRl2nOd3rid2H35YeZ7a2ZlA1QAAACYX2f8sS40/svueCobyr8zIPsj8JSH8zTBQnmeGfnGmRB/Lno6KsXP21ddeuvmFD5Mf4wWckV3oL8/0x8eR/YUPk0KI7HmqdK4nolW2t8ddOwAAADXbvezlI3u6pNbLHhlIGV2cmaEPjEmVjuz6cJe8xJ8J93ovO5G9jKRz/eBBtblZfr3dp59WnqcuXRpf1QAAABLXrn2mx+yZiuxBr9zAmKHqxZE9HNRujnyxc7xrZqpsZJcHxnz6zh1xz3r51TdeoJe9NL1z/cQJ9eWXldbeffFF5XnqzTfHVDsAAADdLEd2Fab27HAXfU5ysWly+WkvUL60ZJLvxZmS0pFd6SNbMh3qxnAX1+rJkne89s5RInsZdTvXU2fPKs9TZ882XTMAAADBhCJ72+7L3nbvvXGzlvg3XniWzN2M0TrXU5ubyvN2n3uu0coBAADIJhHZUVkmsn94lLs0NmL0zvXEu+9Gl6sCAACMX93Ifm8yVY7sdK6XcOPxp8ybsqO+pjrXE9vbyvN2H364icoBAAAUILJj1l261FjnemJnR3ne7kMPNVMaAABArpqR/eC9yURkR1t99VV0N8amOtcTX36pPG/XL74/PwAAwOiI7JhRW1u73/52w53rOs/bvfXW5osFAACwENkxc/TO9YcfbrJzXRMOthlHyQAAAIbZj+wD3/M8z8u9iTpmR9K5fuut6uWXx7edXd9XnqducGUwAAAYu1mP7MOgR1qfE3rn+kMPqZ2dsW5t96GHlOepTz4Z61YAAADU7Ef2ge/1guHktocpmVTneiK6a+T29gS2BQAA5hyRHR032c71RLTRS5cmszkAADDP6kX23sF7k6n1kZ1xMTNs4p3rid3nnlOep958c5IbBQAA86luZL8vmVob2Qc+V53OsCl1rqfOnlWep86enfR2AQDA/JnhyK4UV5/Oqul1rqc2N5Xn7Z4+PZ2tAwCAeTLjkZ2x7LPm6693X3xxmp3riUuXlOft8pABAAAwfkR2dMfHH+8uLk65cz2xvR09qgkAAGDMiOzoAr1zfXGxFXdD39lRnrf7/e9Pux4AAGD2zXpkZzD7DIg715Xn7b74ovr662lXSCml1I0byvN2fX/a9QAAALOvZmTv35dM7Y7sYWjn1jEdZXSuf/zxtCuUFQ7RAQAAGLPZj+zoqnZ2rmt2Dx5UntfCigEAgBlDZEf7tLxzPbbr+8rz1I0b064IAACYcUR2tEzrO9cTuw89pDyvFdfCAgCAmUZkR2t0pHM9sXvihPI8tb097YoAAIAZR2SvZuetZ34cOnu56koV1phD3elcT+w+/bTyPLW5Oe2KAACAGVc3sn8vmeYpsl8+++Nn3tpRKkzhZSP4zlvPRGvB4eWXO9S5nth97jnleerNN6ddEQAAMOOI7BXsvPVMEtMzOXznrWfOnj0bd6Rne+Ivn9V+MDvp81bUfqt10WeKE3r9k9935FvCzk40Itzzdp97rhOd66mzZ5XnqbNnp10PAAAw44jsFVw+q/WsXz6biezJb7RcHy2SpHvxV/kr/jhZ85m3dowaSKvotWq/l1/evfVW5Xm73/622tqadm2q29xUnrd7+vS06wEAAGYckb2CvF725Iekm1vvWg9/m/Orot+GUdwcYmOv0pWB83rn+tNPq6++mnaFarl0Kao/AADAOBHZq3CNZTeStxGX9cju+lXRb5NO9Mwy9ipSwa3T9c71xPa28rzdhx+edj0AAMCMI7JXIw8VzwZkre87OzAm91dFK0b/MsayG6ukQ9vb2c0+G53riZ0d5Xm73//+tOsBAABmHJEdkzIzneuJGzeU5+36/rTrAQAAZly9yP5v+99LJiI7isxY57rO85TnTbsSAABgxhHZMWaz17mu2T14UHlex+5NCQAAuqZmZP/330smIjscZrhzPbbr+8rz1I0b064IAACYZUR2jMebb85w53oi+k7SnSe2AgCALqob2e9PJiI7sr78cvfEiahz/a//eiY71xPRnm5vT7siAABglk0hskMwVD1P+YOpbmioep4atQqbm+Hw7t3/7r/+qeeV36OBX2HhYiO15zDoeaG0hIEfzeoFQ23R3aefVp6nNjdHrG9xjbLbBQAAc2USkf1pt+nteMOCXtmAKC/Z/sg+8NO4Ogx6dhzXO9dPnFC//ithGVelgl6ZRcs38siR3RGQrd/sPvec8jz16qv1tlTewK/yBQgAAMyWCUV2cdtE9nbJjezDoNfrRXl1GPi+n82uSef6wYOVu5xLBvbJNV2FyK7OnlWep86eHX+tBr5HTzsAAHOKyF7RUPW86GbcvUAppYZB9GMyhblq4KdzwqBZuKSRR/US3ElaeT011FYJa6XXMylW3pC2ZO6Ger1gEPjBUA2DYJBk143vepfjlbfvPqG+/FJpo0i0DQ18zw/iESd69hSiaJVG9oN04YF7N31PBUGmTKWUGmTKjOdXieybm8rzdk+f1ncnobW8tvPJbH3RpE3SUTlms5T+agMAAGYNkb2agS939OZ1AGd7r11LGvMHvvLKPVXTj/Nr+O9BvMV4EIv2b2lDfhJti3rZ/YEaBn4Q+P4gzq565/pz/1s2URoJc+AnYTUzyENI7OUbeeCn31iMtezdjNpzkIb7tOkG+jefKpH90iXlebunTskLx7upjyqKR+1rO579p/tEIrMDADCnJhfZ/yxLdTOyR929VpgWgni2B7dqZPezOTtH0rM+DKKKJf9IFnBmWT2q5kb2KErGqfHqX33zyv/8g3DfLnve/2L3qguRPf4p8xsho5ZvZFe4txf2k13TdrOByL69rTxv9+GH9TppXedpZLfqKUf2qJNd3ivGxgAAMKfoZa9lYGZKO0362X7u8UV2NVS9nhoqFfRUPLZibJE9tLm5+43/Snne7sFv/F/ev9bCZjORPa1bUSOPGNn1oUdaa1eJ7Ds7yvN2v/99bTd7doOIO6lle2tz4e+sb4FEdgAA5hORva5swE0HkWu/1YeqO5eMGREz6JUdGBMuHAyi4G5svWBgjPbboJc3lj0KndptYS7/27vVr/+qF4dIbeBLVHTpyO6OofmNPGJkH2otlt1qhch+44byvF3fTxeQGkT8U0LBfRvtYTAMjAEAYF4R2asJc23+RZzGRaW9QMuL0pJ6mXqx+vyCoDaw6jMQ+o/FDSVXdvqDbD0zhkHP+9nKpXTk+v/xrWREdtxRHMS5VLuCMh0d4orsQmav1Mh2hhV3s7CXXWvkKpFdqWhlbXesBinsZU/CfabpjFVI7AAAzC3uy44SvvoqemaQ5+0+9JDa2Wmy8GlF0UHmu5DWhV8tsu/eeqvyPPX11xW37nu531ukFRgVAwDAnJpEZEe3bW3tfvvbyvN2b71VvfzyOLYwndCejezpnXOqRnbfV56nbtyouHXjtjkFDcCjlAAAmGdTiOx0rnfGWDvXs/Jubjg2+hAabZT80LqDema8ixnZH3pIeZ76+OOKG8+MgSnoP59K6wAAgNYgssNh/J3rs2H34YeV56mtrWlXBAAAzKwmI7v67EH1rpdOnz0obpLI3nYT7FyfAVFbbW5OuyIAAGBmEdmRRed6RbvPPac8T7366rQrAgAAZhaRHTE61+s5e1Z5njp7dtr1AAAAM4vIXkd0LWKnrwj88svMj3Su1/bmm8rzdrt/VgMAgNYislc3A4+02dzc9f3oVuJ0ro/ik092X3xRed7u97+vzp5VZ89yHSoAAGgckb26rj/TZmcnfPrP7unTSec6Qzsq+/LLzKNTk4nrUAEAQNOI7NV1OrJ/9VX06B9t2l1crH5bcajdv/5rIbJXfaYSAABAESJ7dV1+ro2dMndPn45GyKCqrS2zMX1/2nUCAAAziMheycDv9FWnL79s9wpz3eQodhcXaUwAADBuRPbKunr16fa2PPaaJ3eO4tVXGcgOAADGjcheXRfHsn/5ZXqZaXYgx+7TTxPZ6/v66/BaXgayAwCA8SGyV9e5yP7117sPPZTG9Ftv3T1xQr36KvdzbET09FMGsgMAgLEhslfXtcgeZsrd739/97nn6FBv3iefMJAdAACMFZG9um4NZv/qK7W5aT7rFI3affhhBrIDAIDxIbLXMQx6XqdvHYNmvfsuA9kBAMD4ENmBkYVXCwAAAIxHk5H9kQ+e0Mt65IMnxE0S2TX7xz9hFKXb+ca/4VACAIAxIbJPF5G95SZwgDiUAACgAJF9ush5LTf1mM6hBAAARPYpI+e13NRjeuZQDvxZvub585WFtT2L76v183v2PL/S3N1It1ae37Pn/Pp4tz6uyrdMp1up08eofOXfX9yztmfPWolzflzKveJE8m4Og16nbqwMjAmRfbr2K7X/6In9N2vTRhzRHn9q/9H3mo7sA9/zStxWPronDvfFiZvx0/13xAeoiYNSZ7JuLlr6xkXp0UyOvHaARzjGYSnx2iOXub64tmfxfbW1uaB9Wm+tPL8niiBri7VSQMkAIW69vILKL2xG87Y2F+LdKdwjYfW0hOI9ElcvW6ajnuNopbHY2lywmmjkrQc9NbX3w2qVl3a/eY6tjBDZXbs58Of9owhQRPZpSwPZUS2sjy+yD/xeMCjssdDeHodBr1sPjmpYenSiY/Hp/jtO7H/804lH9uH+bGAPH+hV5hkBw6BnH8wmHi4w8L2e7/cykX2kMtcX1xZWPtdzwNbK85lkOU721kddfWtzYfF9pVTYd7iw8nk0s2T56+f36O0QFrV+fs/C5laZQsStly/TsYnmW2lMHJF9tK1PObJXqPxUI/sonLvZtUcYAmNAZJ8uObJffS3T735zkhGtvt6rr+2/47X9R0/sv/mp/Y+/IPYB6wZ+Lxhm/so48D0/iHtHw7lG8kreKTOLxksMfLtfVSjTXNTTo2PD3fm+p4IgfCCp6gXx3KHqedHMcEPmp+9AeT1lfSLEh+OFtEk3Xth/x2vRP46+lh6Rjfi3N1v98UdP7H88PqbhukrtV+9lDnE039Gd7/i8qpiS0+9io0d2+zvDGJ4xJkf29fN7FjdXFsIO4KQrLhkPoPcWS4ME5NXHK+o7VBVSTpRdlIpGC5TI04Vbr1DmZDJfuKn0DynPr2wJm06qrS2ZLmDN/Dw+uGvC3xNqGsZvKckUvhwHvvKD9L0lPP39+B/h247+b/0tqBHaaR99Q1NKCW0otdLzKyvn9+xZW1jZXNSaVGhk4SXjauQKrzjxaLoJ7y8Df+7/Doz5QmSfrmq97PoyR0/sf/zTKNxvqP1HT+y/47UowTsje5jYMyMDtfe8OM8Z6TB5oxz4aQAXBlWnb6hCmZlS03/qxeR2omTHXOS/Tfue8vywSOlDNMz0QzXwo0Af/moYxGtlZDK6keA3Xth/81P7r8bLmF+WPt1/R3y8jp6IQ/97abg/mnwTey8txz7E4b99b7/UONVSsra03qB1+q7io2VG9lHKFEXJIA6aSqmwqzgaqiFm+rT/OPo5G1+01dMkPV6fryzEY0v0ASd5UTJZ5f3FKFRp3y6q5emkqCpllq3n6N5fNPfl/ahi6+f3LL6fVjv6USkV/2VAOWbau9MMu5d94Kdf9Qd+9FsxsttvQaN7f9E1tso+581WMkaNf76yoLe5vqT7JeNq5DKvONeBc7Pf7YjsmDNE9umqEtm1/tek6z2Jj0mCd0f2bGCzQ3P8+5zILrw5ZvrOk3Rud7xKkd0M4s3kPOHzUuvfSnrIhoHqBUoNVK+n/EH8o6kgsgsjl7J95xvGwc3meDOyS4c4iezSB1OVyO4YDToMepU/9dIjLFegTpm5ssFd/7DXwsH6otbnlx/Z49+a4X48HF8M4syULLPH7MVcXE8y2fv5kd1aXdx6tTLFeo5BdHC16BnFx/XF5xcWzq/HP+pXNSRfJMSZubszCjGy2yd6ybegkeUk3ezuS62kfRGKfnQ2svMlUz6yW6s7D1wOxsZg3hHZp6tKZNf6Yu34WBzZ5XwsxOu8gTHmh5P2HppJ58JYCS3b293tRSr2spufl+Kgl4HyfDUMVDBQPV/+6C0eGGNF9qPZUUw5kV0fQmN3txvTiJE9L0NXvlzBOhj2t62xXAKRjMMWI7sWX4p72ScX2fPybv5I/fVFLX/XHLVibr1emZO5oiD81pH0xS6uf76yuLm+8vzietQNr43qyaxlz1SqdZFdHnc3otKRXWolObLL7TmGyO48cDmI7Jh3RPbpyovsRueu0i+CrB7ZjXAX97NL8VrPW9q/HYNh0t/m9bLLd+kay20AXH+Vtiqvej3l+2qoVOArvyf2e6U5Ww/i4b9dg2GS3+b1sn+6/w4pnduHuPLAGOuPxfl93vp4J3H1XPJ3htHKdEiDo5YAki5kLVa+v9hoL/sIlc/vnw5Hp7hTixbIzH76UnlU2nqdMovqqZRq8BDHR3Nh8fziyudqa3Nx8fxCOjzDqqE4M6p22QsVSlc+GU2nzxEje3j2B73M2Dx5E/WbbmtzofzAGLOV5MjubGT5JeNo5DKvOOeBc3IMjCHFY34Q2acrHugs3eRRHyZhX34adseWjuzWu12UluUeca1LPHP5qBzOouUCv2wvuyffGLCht97Ca7+S7i4/vj41vLZM+tAUhrskg1XEXvak7zy8LLhkL/vN2QX0QxwWa/UvZZtT+8jP3njR9QcKba7QQV4+QTjGso9UpiYz1kW/GE74e3py3dvzKytxRMjeqdDupC8T2WtX3vzTf7h1rUqFOVi4IaO+77nX7clbL19mlXqqEVope4z0Q6xd5qgfL6tW4kx5T0euvPY2kl5+KgwMSy9/93PfgqptXaC3Xs45L7SSI7KL7el+yZiNXOUV5zpwDkL3wEhNB3QPkX26hP7Upqd2yPand+cPnGM7Lu9lvp7Zf1ExJ/Mmj+Mzjj99NFumPjBmArgldBmdbqXpVr7TTTcx4ofGvN+EGPOGyD5d8xnZO/QRNaHI7hoPo09juIeiKey0avYjcAxlTi6yj6NBZk+nW2m6le90002S/ZlB02EOEdmna24ie3Z8RnfeZ8d4aB5/yrope9GhlG/aM3cm3MsOYKp44wOUUkR2INcEvlN17tsXAACYtCYju/rsQfWul06fPShuksgOAAAAlEdkBwAAAFqNyA4AAAC0GpEdAAAAaDUiOwAAANBqRHYAAACg1YjsAAAAQKsR2QEAAIBWI7IDAAAArUZkBwAAAFqNyA4AAAC0GpEdAAAAaDUiOwAAANBqRHYAAACg1YjsAAAAQKsR2QEAAIBWazKyP/LBE3pZj3zwhLhJIjsAAABQHpEdAAAAaDUiOwAAANBqRPY5svHCszefePbmE88efW/8GxsGPc/zB60rc+CnJXy+srC2Z/F9tX5+z57nV7aUUkptbS7sOb9epqTyS4rWz+9ZfF8ueOX5PaOUrN5f3LO2Z8/aYl4Rn68sFC7TYJXKa77yVW2tPO86NJllJtQgzWvhCVZVmWPUStNvOkO3X9pbmwt71vbsWUvfwKuY8Ku4VSftePZ9Wqd3tN1R9qiFb+lSlYjsrZCE6ZtPvPT4p2Pd1I3Hn6oT2fWkK/1sEeL1MOiVCtzDoNcLhuXKrMaswfri2p7F99XW5oL2jr++WPbdpNyS0TuUNj2/sjXGyG5/MGytPB9temEz87m2ft6ck1PmRN7OhE+16FPZ2nrpyjdQB3GZlr2/l9fCE2z0OjS9gdG+kKcZIvMWYVU7Xayxpls/n3m3STbn2qP181N7aSsVvz1mK1B639cX1xZWPh+pSsamRz3uRZtrzUk7jncwcwfTL1RWRh/HO8Nox24Sb+kNvDaJ7FN34/Gnnr35hQ8nubl6kT0Towsju2DkyD4ie/vRO77xgtnaXCj5blJ+SWVldHdkH5H5Maa97KXE0K7cKVR+YXNLfkcbV+Vb1RnWQuYx2tpciJrr85UFI0J19hiN9vGfNlHaONn5pqabzn57yYkF4/laZW5H2vf1xbWFxfNWxUru++crC03/LWJuIvs4CO8MzvqM4Z1h2rtf1kivTSL7tH36zh0n3tiQ5xvjWD48euKNx197KZx5x2s37CXFmdmAbkV255IZZoyOI/vA9/wg6HmhKBAP/OzPUf+4Li5M+4W+sB3Z7TKNjaerSGUq+1uH2+crC9k/s66f37O4GXeW678qv6Qc2dcXjSXlP/ClvZgl/v5rvG9qP9odWmXeN0esktaPWOL9VM409SO7tHXHMdL2aC33k3XUYxQfdH3h9xf3nF9JSoi3rpV5fl1qh6i5cs66MpWvsLq2UdeuVf5OOI5jlC3W1Z7mzHBD1p/F4k/N0ofYGSXzI3t2lQYju3OPRJN7aTu/kBfuu7VH0c5KryOtKbTQbL8QKrXSVE9aYUOuyovHqMKbwCgfPU1EdrtB3GedtTnp9S6+0xa+pee8XVQ30muTyD5lV1976ean3rlqzP30nTuSETLpvz88eiLuj3/vjZvjoL/xghW15dVDRmT/8Gj6heHDo+5hOXGMTqN6GH8HfhqWsz3vdp+2PUdfI03U7l52o4SBn6Tyga/9SyizSmJXUryO/65nhpJxLGm+9VT78M4O10miwPuLe9YWVjYXM2807y+WfN8ZrUpRGSU6meSxRs7IXuFNM9262PJ5f4sQi6vbIFrHifYJFx6d5N/n18Nq6GFxYXMr2eXoXIoPbs65VKbyFVd3jwcTc+dUjtH7i9ZgWak9lfPA1cmRIeMz2PyKZTZd/LdyK8pXazpT+Z4808Re2vEOyk1dZt/F8818HTkie/iztenqPbVTOGldL9jcyhfsu1zmKB892YExZhItc4iFBtHl75H79W6dIdLqmVdQ+qbdxB8HRnptEtmnTIzsV197SR8qE4dyLV5rffNXw353bXnH6qFsZNe62AtG0odJeOD7vu8P0lTtHiBTIrKbfe81Inv8U/IbR5lVh/Jk31By3vTLL+keGFP0QVL6eh3tbVdfd3E9ee+z3yg/X1ko+GY/UpWMTuWcTyyh8q6tV6i8sHWp5fX+oVqRvcoxkiO7sYNGHAkXiP6qs774/MJC+mPu+Vmi8uVXzzlGzrg/jWMk/EFZbE/lPHDCKVf+nJd62XObTsmtV+61KRdXOxZM6KWdxju5YmX2vdyfJsYT2ad50rr2SKq88xiVehMY5aPH3JHKp7djyFa5PXK93t2Z2078Qud3E9fOjvTaJLJPm9ZfnqgU2bVyouBeIbK/94bQxy8a+J4/GAZ+MBz4/qCZyO7o+B4psjs70yuOvt9aeT7t98rNNGWXrB/Z4zUWy7xZmB9j64tr7vvh1O5lL10l7W23RJp0jCio3csubn0skT3eYPExEq84LBnZw6Dz+cri5vrK84vrJfoRy1S+2uriMRI/ktXUjlGFyB6vYRw494doiUPsGhiTO/barPO0etnjAsb60s5emGj9nWGUXvaJRPbpnrTlI3vOMaryJlDvo0en93nH+1LhnUGcmbtHo0b2/CubyzWIa+VRXptE9qmTLj91DoxxR3Z9ZrWBMY4h7Nqok+jHnu/74eAYPwh69jCULOFiUytOp8NZjDXrR3ZHmRUHxiiVeW0XZJpyS44c2R1bN8lXcMa/qj7UeKQqGX+RrFz5vK0XV17eutTy6ZJhmKgb2VWJBpE+CYR9Ef+wu7Xy/MLi+cWVz9XW5uLi+YXkD+WTi+z2MXLldXm/zLqM4xhtbS6UHRiTXSAtMy9SFLVSjbHs9q+EpltfLDVqXCkpFlT/DjCJl7YadQxSqcjuOm3qjsmZ8knrfMGalc87RhXfBGq9MyT04SjJnKJDXNAgBXtUbSCctXrhnZSMBpnUa5PI3gYfHrWHpoS95pnBKmJkv/H4U+nIlkz3ubl6Zsl0YX1sjNbjHg4wSbPvMOjFo0z0X0mR3RybYhTiuvxUnBevLpYpRna5TFUjs6dvQEVvZ6WWLBPZzc6n5KLDkS7zSv/GJwwoLPW+OUKVkk7l51dWim+SI93uxjUyuNKljdrWc/8EvGfP+fX8m/k01iDG9U/OPvvMgVs/n7ngLG+PSld+tMhu/gW5DcfI2NN4SaE93QfOfNWUPcR515PlnN7SWHZxZE5Re7puJJf3PqDv9qRe2pktjjOyp0dEq6f4QgirU6aVpnvSul+wVuWlepZ/Exjxo0dbvcTpXbJBSu+R2HTidh2r6+9s6Q2g5AaZ2GuTyI65UfYmk4mJPVapUVXuVdz6mzzmaV3ly8j+oWO0wQ9TMvPHaHzmuenmed/nBId4/IjsmCOVQ3sXyoaN8AAAHI9JREFUlRoaEZrULZnL63Tly8hE9qK/vbbTzB+j8ZnnppvnfZ8THOLxI7JjvtR5BlTXlLmqfaIPRa+i05UvIfOn1e51sSulZv8YjdE8N9087/uc4BCPG5EdAAAAaLUmI7v67EH1rpdOnz0oblKP7Nev/46JiYmJiYmJiYmJKWcisjMxMTExMTExMTG1empFZC/19wAAAABgzhDZ0Q7DoCc//AhAV9jPXKi8dqk3Ad4uMPcGvvXME1gaaKU2vdsQ2dtk4Jf6qNMeEtSS0yh5VdSvlPCqsG7GmGmfKd2qsbPHaIpVMp+AVS/Pjai7By7z+mqq5UZ7+bhayX5QWaX2nG5k7/IZUkF3d1N/H2msThP6HHE/zHv0gkeof3dPhmrvig283U1ix0scjuvXf3f99b8ksrfCwO8Fg8JX9sBPzp5h0JtG+LGN556JUmTv9eL2mU5k7+wxakOVpnlrzc4euDF9XIzy8nG2kvglu2Xt6dTZM6Saru7muKox15G9qydD5XfFbjyLpczhuH79P3/H84jsbTDwe8Ew89Ie+J4fxN9vw7nGqZf0amUWjZfQO7/jlYQyzUWT+aW/W4tZTKqStqC2J/afrqx+2V4wjPZ2EPT8QWZ9ezeHQa8XBOE31sDXSh61t6Crx6gdVcqcJnbYi4+xtKF5PXDOzxphdfHF5dhNT6iRvZviMRJbySozXr16e9p/WDMqKi3paPkKOnuGzMluCmFR7NMxT/HsO05mpuOFUOUtSJspLpnMlCO7WCVtufSvVs5WGu27d4dPBuH30tYdK0gbqvC+VP5kqKDU4Xj9oW963/nPRPYWCI9X5rU98JMzIv6ia/zlOTkTB356mggB2si32TIzpab/1Iux/+Bt1N1+8YlVkiO7+LPzHXkYZnbpVRvPi7+DR1WIm7TCHrn2s6PHqB1VMrag/VbcUPrxNb8HTu7V0ueKrVRQJSW+fBwvWPMYuVrJLrR6e7oLMBvJXlLczdI6e4bMz25GiUhItFEh2hlql+nakP1CcFVeWF1oBMdMKbK76plJbDlbd9S/tM6eDMV9/fnpQn7/rPS+VPZkqKDc4fjb73jfeGiTyD512jmRHjHhMzjn9VPwrdN+8WdeVNbrx+yCKIjs5d65mojs0bkt9dLHu2m0VtSiVfZI0uFj1I4qmVvTV5LeKjPfwObzwIkfTo4PENfWxVecHNnFV7FxjBqJ7I7PtlEiu+uNpYwOnyHztpt6cI92YuD3ej1/kPs+79yQGNmtyjtWj2bbn1vSTHPfHGVm2iYsJK+Vakf2Dp8MzsgubN3aWfd7R9n3pSonQ2llDweRvR3kk0A4YK7PNEc+Fnsxhc827VR3fUbmmXBkV4Po70XxW4e5m3Jkr7JHgi4fo3ZUydpaemCS+VJkn+MDp/VLZXdoUpHdPkY5udz4VZX2lGslNF1+G8h7VqTTZ8g87uYg7QH2B8PAD4dLRlsVy3RuqFxkz6/nQB98Kc8UIrurzPSbR5lWqhvZO30ySO+Kjq1bGx49stc4GQqVPhwMjGkF+6Mu6k2Wv+7Lf9ORPti1v/jkvH6EtxPlelkYxSUzSkd2/Qt1fmS3Xhrpz8Og1+ul7yjmbsqRvcoe2bp9jNpQJWlrA78XBPpsbZm0pDk+cOErxVhUrqf44nJmWftzx/WJZR4jRytZW6jSnmIBctOJm3JH9tk/Q+ZpN42NDoNez/f9YKiGge/76d+BxI8e1+ltvBBcX31zm1dMztpMae9dZQ6DnnEZYs7WpQ3P/slQ8K5oNYD1xjLK+1LVk6HRw3H99b/8hvdNIvt0Wa+66JRwfA6l308zr2kxNMfLJR+5hV95sz0DUieXHbdLR/a0SK1K5hfM7EtL27z+5pp9SzDKdER25x4JuyRVvNPHaOpVcn6JM68jdO/PfB44sU2kekovLneWNV5cOZ9Y9keOtHWprcu3p/wmIDWduGTRbs74GTIHuymvPEiGH2dCnFhm/ntIOtfRIMLqmVNRWi6tv3HWpkFerJL9iivaumd/bs7wyWAWoCd1Y+uOVhrtfan0ydD84bh+/XfX//a7RPb5lv3SaHe/zYGi781T18Jj1EiVzNXcPR1lKtFCLTxwVU2r0s003ZycIXOymyiDk6FVmjwcPEoJxhlV6/S66y71x3+s+n319dddmnP5soq/A7f9LaOFx+j5/xjV4q671B//ke95/vNVy7HX+iPf+1eDwq1fuaLm58BN2fTqPHLTzckZMie7Of3Pi5bP+fBDxcnQnjnjORwtiuxM05s2H/pG+jedbzy0WWn1z/6fK1/9+//p+m8//4N/zx/+3L9+/Xdf/u8PdmPOJ19Mu+U7fYz+TK/Srf/ttyuV8/N/Ja7107/w/vWzReV89ne/mfbhmNCBm+70t9+Zbp073HTsZrO72eFPmUnN2Xn7/572IeZkmNDhILIz1Z92XvvVp+///fXrv7v+28//6fb+P97x7a+/8c3OzJmPqdvHyJ7Tpe9aTExMo06z9g42hjnXr92Y+mHiZJjM4ZhyZAcAAACQj8gOAAAAtBqRHQCAjvn4l3cztXya9jmCWUNkBwCgY0ZMhPt/zDTe6eNf3v0v//L/XX3/2uavr7z9q8tMXZ82f33l8pXf/vM//0tTL+EaiOwAAHTM6JEd4xNG9qvvX/vgw08+uf7531/7jKnr07VPdj767afvDj+a4nlFZAcAoGOI7G0WRvbNX18hr8/SdO2Tnbd/dXmK5xWRHQCAjiGyt1kY2d/+1eWpp0ymZiciOwAAqCAnsr/yyit33nnnwsLCwsLCk08+KS5DZB8rIvusTkR2AABQgSuyv/LKKwuxRx55xLV6NrJvnzx0YO++A3v3Hdh7aG2r4ZqWsHEs2vq+A/3VbXN+UiXXYokra/19h09ecWwl/7eNIrLP6kRkBwAAFbgie9K/rud1u689jexX1vr7DixtpD+e3FAju7BUPhxvHNubLrx98lASxy8s6RVzLtZGRPZZnYjsAACgAldkt7vYH3nkkYWFBWOxOLJvnzykxeLGlI/sVgWurPX3HTtjFpKzWBvVj+xvBLfddPujb9RIk28/evtNN9100023B683nFPjksdSeInp9APx5m+67dG3zflJlVyLlWzbci1PZAcAABUURvYwtYd53RnZc4LvlbV+PAolzsp6hk7+fWFp3+GTq8e08SoXluIV9x65EC6wdOTw3n2H+4eyveaH1raECmyfPHT45JW0kP7qtlTPcDGllX/sjF7DuP791bVMVeU6N6xEZP/Z/TWjec6UlNlc4W8Et9100/2n0x8fPT16sVWqd/qBm9KF33709iSO/+x+vWLOxRqeiOwAAKCCMpE9ceeddxqLpZFdHryuDUpJh4C7InsYzcOxK3YH+YWlJBZvHIuWVOrMkQNLG2IFkg71bP7OW0wfS5PWKqz/1urhvUJkt+vcpFmJ7G8/ersWi5uvZ/UKvBHcdtMDz5iF5CzW8ERkBwAAFYiR/d577xUj+yuvvGIsWdDLnp0fxeucXvZSM8N/x5k+jOBF3edal3mJxZJ/ZyK+XStX9RojRfZ0eMn9p392fzKG4/6fhenz/vtvv+mmB54xYvejD5gjPd4IbovmBFbwDdc1Ck9XuUkPu6ejmfef/uz1uGZmz3RO8E3LTLKynqFz9sK177ffdnu21/z24HWhAm8/entmH2979G2pnuFinznaVmzGopYnsgMAgKrsyK7n9W9961tJ/7qd11XhWPZxRXa1tXq4v7od/leuQOWx7B2J7KcfiAK00NP8s/tv0sd7JMExztynH9DSdpRrX3/09pvkyO5Kz5/9/ekH0tAcDgE//UCaStOtaLlcHryuDUpJh4C7Iru4F9K+a030zP033X9arEDSoZ7N33mLyW1rNWN+nYnsAACgOiOy63k9596OifSOMRvH9gp3jDEGxoThWMvN6f1bqkV2dWWtf+jY0iFtTmZoijjKpfxi5QfGTDyyvxHclumydaTq/Nidyab28BJpXa07XO/Yzt10US97dn4Ur8vvRV4F4kwf7mZR97m2jyUWK2jGwuoR2QEAQEVGZK+U15VxX3btStP0JujpTCM3h9eVHsuPv2eOZC4/1TPxmSMHkhHtZrH6lwc7TBcvpv17I7m61HH56aQHxiR9ujkBd0yR3YizpSO7ayz7uCL7Z68/evttj74d/leuQOWx7ER2IjsAANMjRvaSeV1N9emn8TCbSZnG7SBzLj+N82ityF5zYIwxILvKpk8/cJNwxxhjYEwYjrXcnN6/pVpk//s3gttuf+D+27U5maEp4iiX8ouVHxhDZI8R2QEAGIXrjjElTS2yO+9RMy5njkzhka7yWPbMNaCfPXN//kgV53j0+LJI4/LTzH3ZtcKzY2NKDYyxYnSyetI5nc40cnO4lQfy98Kx7/GvMoP+Mzdc17rSrb8JFC+W34xEdgmRHQCAUXQwsm+fPJQdZjP2bYXTFJ64NImnn47tPobTneJhNpOaKjYjkR0AAFTQwcg+RyYQ2Z+5f0rPIh13gJ7sTlVtRiI7AACoYPTIzjTWaTyRPb25u3lDxs5P4a41/niphpuRyA4AACoYMbJj3MY+MIZpGhORHQAAVEBkb7mPf3n3r/7uyifXd6aeMpkanIjsAACgAiJ7y338y7svb3380W8/vfYJqX1mpp13fvPBFE8qIjsAAB1DZG+5j3959z/989e/ee//fftXl5lmY3rnNx/84z/+0xRPKiI7AAAdQ2RvOQ4QGkdkBwCgY4xE+Pt/+MNPHzt/3y0rP33s/O//4Q+li7mwtC+5f/noDyW9sDSh2653AJEdjSOyAwDQMXoi3Hz58g/6P3nihy9+8OH1J3744g/6P9l8ueRFcs2GbCJ7isiOxhHZAQDomCQR/mjpF0cPPnnp9atfxS69fvXowSd/tPSLEsUQ2ceFyI7GEdkBAOiYJBHed8vKY3edNiL7Y3edvu+WlRLFSCH7ylp/34G9+w70V9fi3+qLpf8+cyQeVHPkQvZX2ycPNTXYpquI7GgckR0AgI7RI/vOF1/8fOXVZGDMz1de3fnii9KRPRnLfuxMPCfM2Vurh/fmRnatkOxiG8fiED+/iOxoHJEdAICO0SN72Lk+vPTRfbesDC99FP5Ys5f9ylr/0NqW+VtHZN84Fsf97GJX1vr7DvRXt5vZ1W4isqNxRHYAADrGjuxhTNf/XaKYESL7lbV+1DG/ffKQvVjYSc/AGKAxRHYAADpGvPw0jOyjXX4qDozZPnkoDt8bx/Ymo1/CcH9lrS8k+6iEue1rJ7KjcXUj+/eSicgOAMBEiTd5vO+Wleo3eUzvyx7F63i4i3b5qTYG5sgx8xrTQ8eWDllj2TPj4+cRkR2NI7IDANAxDT1KKR83bayPyI7G1Yzs/e8lE5EdAICJmkgiJLLXR2RH44jsAAB0DJG95YjsaFy9yN7rfy+ZiOwAAEwUibDlOEBoHJEdAICOIRG2HAcIjSOyAwDQMSTCluMAoXFEdgAAOoZE2HIcIDSOyA4AQMeQCFuOA4TGEdkBAOgYEmHLcYDQOCI7AAAdQyJsOQ4QGlc3st+XTER2AAAmqsmnn15Z6+87sDecDq1tFa9g36+9/B3cc5bcPnmoUjUqbXfSiOxoHJEdAICO0RPh5suXf9D/yRM/fPGDD68/8cMXf9D/yebLl8sWdGWtr6XeM0eOnZGX08PxGCL7lbX+vgNLG+mPJzesZcqX1gJEdjSOyA4AQMckifBHS784evDJS69f/Sp26fWrRw8++aOlX5QoZvvkIS0o5xlrZC9fjTKltQKRHY0jsgMA0DFJIrzvlpXH7jptRPbH7jp93y0rxaVcWevvc3Srp6Nljp1RF5aSkTNHLuRH9jNH9CWVPtxlaSNeciMqvL+6XboamT74ePW1tCaZCk8fkR2NI7IDANAxemTf+eKLn6+8mgyM+fnKqztffFE2ssejxrdWD+/dd2BvlIC1UL5xrL+6bfWyx7k8nRwhfuNYnN21dcONbhyL4rVWjawLS0lSTwfwpDO3Vg+7KzxlRHY0jsgOAEDH6JE97FwfXvrovltWhpc+Cn+s1cseB1/9glShZz13YMzGsUyIv7LWT3rTzXX1zUld49n5Z44cWNow8n1OhaeMyI7G1YzsB+9LJiI7AAATZUf2MKbr/y5RjBG+czJ0ucierrh98lC6TNiFnw6MMctxjGWvFtlbMR4mQWRH44jsAAB0jHj5aRjZq1x+GobpJOwmGfrCUqZrXJWN7BvHokEv2RvRhBuSBthk+uatO8YYA2PCeroGxhxow3iYBJEdjSOyAwDQMeJNHu+7ZaXyTR6VPpRFS736UJMjF1RyXWnB5afxxaaHji0dikN85qpQd/QXbw+fztSWjMt0XH7KwBjMJiI7AAAd0+SjlDAGRHY0jsgOAEDHkAhbjgOExhHZAQDoGBJhy3GA0Li6kf3eZCKyAwAwUSTCluMAoXH1Ivv+g/cmE5EdAICJIhG2HAcIjSOyAwDQMSTCluMAoXFEdgAAOoZE2HIcIDSOyA4AQMeQCFuOA4TGEdkBAOgYEmHLcYDQOCI7AAAdQyJsOQ4QGkdkBwCgY5p8+umVtf6+A3vD6dDaVtHiZ47EC8dTf3W72hbnAJEdjasZ2W+/N5mI7AAATJSeCDdfvvyD/k+e+OGLH3x4/YkfvviD/k82X75ctqAra/19h09eiX46c+TYGXm5C0vaYo45SBHZ0TgiOwAAHZMkwh8t/eLowScvvX71q9il168ePfjkj5Z+UaKY7ZOHDixtlNkgkb0aIjsaVzey35NMRHYAACYqSYT33bLy2F2njcj+2F2n77tlpbiUK2v9fY5u9XS0zLEz6sJSMgzmyIV4CT2yZ6P/xrG9h9a2wgU2onLSwTOZkmcWkR2NI7IDANAxemTf+eKLn6+8mgyM+fnKqztffFE2sseD17dWD+/dd2BvlMK1OL5xrL+6XdzLvnEsSfNnjoTx/cJSMjg+HX5jlzybiOxoHJEdAICO0SN72Lk+vPTRfbesDC99FP5Yq5c9ztP6BalRz3rhwJgLS1FRF5airwGZBaIcL5Q8m4jsaByRHQCAjrEjexjT9X+XKMaO3UlkN0atFI9l31o93F/dDv9rLRCPnMkZijNbiOxoHJEdAICOES8/DSN7lctPw/EwSYZOQvaFJfPWjSUuP72y1j90bOlQMlMrJE3qdsmziciOxhHZAQDoGPEmj/fdslL5Jo9KqY1jwh3W9REsRy6o5Hbs8uWnkTNHzAWWjoRD5LWLU62SZxKRHY0jsgMA0DFNPkqpOfGFp6G5vgskkR2NI7IDANAxbUyE2v1nlFJE9mlXAbOGyA4AQMe0LBFunzyU3CAyQWQHmkRkBwCgY0iELccBQuPqRfZbb78nmYjsAABMFImw5ThAaByRHQCAjiERthwHCI2rG9nvTiYiOwAAE0UibDkOEBpXM7LfdncyEdkBAJgoEmHLcYDQOCI7AAAdQyJsOQ4QGkdkBwCgY3IS4SuvvHLnnXcuLCwsLCw8+eSTk6wVEkR2NI7IDgBAx7gS4SuvvLIQe+SRR4qKubC078DeaJLuoX5lrT/H91YfBZEdjSOyAwDQMa5EmPSv63nd3deePu1oa/Xw3syzSzESIjsaR2QHAKBjXInQ7mJ/5JFHFhYWHMXoDyid64eVNo7IjsYR2QEA6JjCyB6m9jCvV4zsF5b2HV46cnjvvmNnMgtsnzwUjaJZ2lBKhcNmwjnHzjS7e91HZEfjiOwAAHRMmcieuPPOOx3FiANjLiztO9Bf3c4usH3yUDLTXFdtHMv+CkR2NI/IDgBAx4iJ8N577xUj+yuvvOIoRr/8NOkpl7rer6z1ja70tIv9wN59B/YeudDczs0CIjsaR2QHAKBj7ESo5/VvfetbSf+6O68rx/j18pGd8TBORHY0rm5kvyuZiOwAAEyUkQj1vF7i3o6J0pFdHhhzgPEwLkR2NI7IDgBAxxiJsFZeV1Uiu9JH0ViXnzIwxkRkR+OI7AAAdIwY2SvmdYwRkR2NI7IDANAxJMKW4wChcUR2AAA6hkTYchwgNK5mZF+4K5nSyP7IB0/oZT3ywRPiJonsAACMgkTYchwgNI7IDgBAx5AIW44DhMbVi+zfXLgrmYjsAABMFImw5ThAaByRHQCAjiERthwHCI0jsgMA0DEkwpbjAKFxRHYAADqGRNhyHCA0rm5k95OJyA4AwESRCFuOA4TGEdkBAOgYIxH+/h/+8NPHzt93y8pPHzv/+3/4Q8lCzhw5sHdfZuqvbjuX3jgWL3bsTE6hV9b6+w6fvFKyCjOLyI7GEdkBAOgYPRFuvnz5B/2fPPHDFz/48PoTP3zxB/2fbL58uUphF5YKQ/aVtX6S1K+sndwot9YcI7KjcUR2AAA6JkmEP1r6xdGDT156/epXsUuvXz168MkfLf2idGElwvfGsb2H1raqrjXHiOxoHJEdAICOSRLhfbesPHbXaSOyP3bX6ftuWSldmBW+r6z146EySxvJMvqwmQtLyXCaIxfCEpaOHN6779iZtLQLS/sOn1w9Zg65iQvvr67NcOgnsqNxRHYAADpGj+w7X3zx85VXk4ExP195deeLL0aI7BeWkqSeGZi+ffKQEeKTX+mBXo/sYaAPx8GH42rSwrdWD+8lsgOlEdkBAOgYPbKHnevDSx/dd8vK8NJH4Y/1I7s+bF2pM0eSjJ78NpxjRHb739LMK2v9dIDNLA+tIbKjcUR2AAA6xo7sYUzX/126sCqRPewdjwfDENldiOxoHJEdAICOES8/DSP7yJefGgNjjp1RSm2s6cNj+qvbNSM7A2OAumpG9j/1k4nIDgDARIk3ebzvlpVmbvKYXn6aGa2uXW+qVHJbd2ePuyPTbyQXpHL5KVBBk5FdffagetdLp88eFDdJZAcAYBSNPEppyrIjcGYMkR2NqxvZDycTkR0AgImagUR45sgB617vs2MGDhDahsgOAEDHdDYRRneK3LvvwN7Z7WJXHT5AaC8ie5t8EvQe9AdlZqKlBn7Owfok6D3o+ZcmWZ9ck6hPboOMffXJGZz2vAc9r1XHFzONRNhyHCA0jsjeIsNzvd65YZmZqUu+dzKIfz0MTs5bYmjbLo8pso9nN5uP7HY95yWyK6XadzZilpEIW44DhMbVi+zf+NPDyURkb8rAf7AXfFJmpobI3q5dHlPEbNtuuhDZO3GYMAs+/uXdTC2fpn2OYNYQ2Vvjku+dtsKJONNYQIzsnwS9B82/1A9Oe/65ZH6chKQlLVYWSbZbavW0Al526+WrpK0bzRye6yVzvAc9L/luU7pKdplhRgzikvP+viFvaOA/6A8u+dnVh8FJe0POehqt5NxNaYf8TBsK7ZnskTSQw7Hv8e7kt4mjnmKDlD9G8urlD9xoS7qrZZ3JYRsYLxO5TOlwyDMBAIgR2VtiGJy0o5g4Mysbp+JkoPdNpv30g9Neku8Hp/OWtA1Oh0kiWn54ruedHpRfPdq69fWjTpUyg/vzu3WL/kYhlDnwH4zrecn3Cka56F8SkoQar64vINTWfYyEL2klu2/lyB43rM0oVtx3rQ0z3w/LFJhTZsljlN+eVQ5cnSUddZKPkco7TEWHw32MAABQisjeFmIYKk5Ijl52rcdO75cVYoFjSVs0pP6S3zvZ8y/FP5Ze3ZXMKlQp8+XEHdkrVEks09FRLTWIntuk7xt2gJP/DJLtkxYrPFJkj/q/i78JiKuPHtmtMqudNlJ7lj9wIy3pqpKrwtIfo6wyxcPhPkYAACjVRGT//wFdRsMIuWInnQAAAABJRU5ErkJggg==" />

This means that the object is technically an "item" object and not an "orange" object.

However you can convert high level object to the object that it has been constructed by using the following notation, highlighted:

<img alt="" src="data:image/png;base64,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" />

After that, the new object variable can now access the properties that it has been hiding:

[csharp]
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace InheritenceInAction
{
    class Program
    {
        static void Main(string[] args)
        {
            Item AnOrange = new Orange(1.25, 45, &quot;Spain&quot;, &quot;25-01-2015&quot;, &quot;sweet&quot;, true, &quot;clementine&quot;);

            Orange AnotherOrange = AnOrange as Orange;

            Console.WriteLine(&quot;\n\nThis {0} is a {1} and is grown in {2} and costs £{3}. It's a {4} fruit and contains {5} calories. It's use-by date is {6}.&quot;,
            AnotherOrange.FruitName, AnotherOrange.OrangeType, AnotherOrange.CountryOfOrigin, AnotherOrange.Price, AnotherOrange.SweetnessLevel, AnotherOrange.Calories, AnotherOrange.UseByDate);
            Console.ReadLine();
        }
    }

    class Item
    {
        public double Price { get; set; }
        public string CountryOfOrigin { get; set; }
    }

    class Food : Item
    {

        public int Calories { get; set; }

        public string UseByDate { get; set; }

    }

    class Fruit : Food
    {

        public string SweetnessLevel { get; set; }
        public bool ContainsSeeds { get; set; }

    }

    class Orange : Fruit
    {

        private string fruitname;
        public string FruitName
        {
            get { return fruitname; }
            set { fruitname = &quot;Orange&quot;; }
        }

        public string OrangeType { get; set; }

        public Orange(double Price, int Calories, string CountryOfOrigin, string UseByDate, string SweetnessLevel, bool ContainsSeeds, string OrangeType)
        {
            FruitName = FruitName;		// This is inserted to trigger the FruitName property's set element.
            this.Price = Price;
            this.Calories = Calories;
            this.CountryOfOrigin = CountryOfOrigin;
            this.UseByDate = UseByDate;
            this.SweetnessLevel = SweetnessLevel;
            this.ContainsSeeds = ContainsSeeds;
            this.OrangeType = OrangeType;
        }
    }

}

[/csharp]

This will output:

<img alt="" src="data:image/png;base64,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" />

This technique is useful if you want to create an object, but only makes it's lower level info available later on.

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;]]></Content>
		<Date><![CDATA[2014-05-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[.net|c#|csharp|Microsoft|tutorial|Visual Studio]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>c#]]></Categories>
	</post>
	<post>
		<Title><![CDATA[c# - Over-riding generic methods]]></Title>
		<Content><![CDATA[Child  classes can inherit methods from it's parent class, in the same way that it inherits properties. Here is an example of the Orange class inheriting the "SomeInfo" method, which have been originally defined in the Item class:

&nbsp;
[csharp]
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace InheritenceInAction
{
    class Program
    {
        static void Main(string[] args)
        {
            Orange AnOrange = new Orange(1.25, 45, &quot;Spain&quot;, &quot;25-01-2015&quot;, &quot;sweet&quot;, true, &quot;clementine&quot;);

            // The &quot;SomeInfo&quot; method hasn't been explicitly defined in the Orange class, 
			// Howeever the following method call works thanks to inheritence.
            AnOrange.SomeInfo();
        }
    }

    class Item
    {
        public double Price { get; set; }
        public string CountryOfOrigin { get; set; }

        public virtual void SomeInfo()
        {
            Console.WriteLine(&quot;This item costs: {0}&quot;, Price);
        }
    }

    class Food : Item
    {
        public int Calories { get; set; }
        public string UseByDate { get; set; }
    }

    class Fruit : Food
    {

        public string SweetnessLevel { get; set; }
        public bool ContainsSeeds { get; set; }

    }

    class Orange : Fruit
    {
		// Notice, that this class doesn't explicitly contain a method called &quot;SomeInfo&quot;, however it does 
		// exist implicitly due to inheritence. 

        private string fruitname;
        public string FruitName
        {
            get { return fruitname; }
            set { fruitname = &quot;Orange&quot;; }
        }

        public string OrangeType { get; set; }

        public Orange(double Price, int Calories, string CountryOfOrigin, string UseByDate, string SweetnessLevel, bool ContainsSeeds, string OrangeType)
        {
            FruitName = FruitName;		// This is inserted to trigger the FruitName property's set element. 
            this.Price = Price;
            this.Calories = Calories;
            this.CountryOfOrigin = CountryOfOrigin;
            this.UseByDate = UseByDate;
            this.SweetnessLevel = SweetnessLevel;
            this.ContainsSeeds = ContainsSeeds;
            this.OrangeType = OrangeType;
        }
    }
}
[/csharp]

&nbsp;

However you can make some changes to the above code to allow the child class t either over-ride the inherited  method, or add extra tasks before/after the method. This is done by adding the "virtual" and "override" labels:

[csharp]
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace InheritedMethodOverrided
{
    class Program
    {
        static void Main(string[] args)
        {
            Orange AnOrange = new Orange(1.25, 45, &quot;Spain&quot;, &quot;25-01-2015&quot;, &quot;sweet&quot;, true, &quot;clementine&quot;);

            AnOrange.SomeInfo();
        }
    }

    class Item
    {
        public double Price { get; set; }
        public string CountryOfOrigin { get; set; }

		// The &quot;virtual&quot; keyword here is telling all the child classes that they 
		// can override this method if they want to.
        public virtual void SomeInfo()
        {
            Console.WriteLine(&quot;This item costs: {0}&quot;, Price);
        }
    }

    class Food : Item
    {
        public int Calories { get; set; }
        public string UseByDate { get; set; }
    }

    class Fruit : Food
    {

        public string SweetnessLevel { get; set; }
        public bool ContainsSeeds { get; set; }

    }

    class Orange : Fruit
    {
 
        private string fruitname;
        public string FruitName
        {
            get { return fruitname; }
            set { fruitname = &quot;Orange&quot;; }
        }

        public string OrangeType { get; set; }

        public Orange(double Price, int Calories, string CountryOfOrigin, string UseByDate, string SweetnessLevel, bool ContainsSeeds, string OrangeType)
        {
            FruitName = FruitName;
            this.Price = Price;
            this.Calories = Calories;
            this.CountryOfOrigin = CountryOfOrigin;
            this.UseByDate = UseByDate;
            this.SweetnessLevel = SweetnessLevel;
            this.ContainsSeeds = ContainsSeeds;
            this.OrangeType = OrangeType;
        }


        // The &quot;override&quot; keyword here means that we are overriding the inherited  
        // method. 
        public override void SomeInfo()
        {
            Console.WriteLine(&quot;override successful&quot;);
        }


    }
}

[/csharp]
In the above example we did a complete override. But what if we want to run the inherited method as is, but just prefix/suffix (or both prefix and suffix) the method with some additional c# statement, in that case we can use the "base." syntax like this:

[csharp]
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace InheritedMethodOverrided
{
    class Program
    {
        static void Main(string[] args)
        {
            Orange AnOrange = new Orange(1.25, 45, &quot;Spain&quot;, &quot;25-01-2015&quot;, &quot;sweet&quot;, true, &quot;clementine&quot;);

            AnOrange.SomeInfo();
        }
    }

    class Item
    {
        public double Price { get; set; }
        public string CountryOfOrigin { get; set; }

		// The &quot;virtual&quot; keyword here is telling all the child classes that they 
		// can override this method if they want to.
        public virtual void SomeInfo()
        {
            Console.WriteLine(&quot;This item costs: {0}&quot;, Price);
        }
    }

    class Food : Item
    {
        public int Calories { get; set; }
        public string UseByDate { get; set; }
    }

    class Fruit : Food
    {

        public string SweetnessLevel { get; set; }
        public bool ContainsSeeds { get; set; }

    }

    class Orange : Fruit
    {
 
        private string fruitname;
        public string FruitName
        {
            get { return fruitname; }
            set { fruitname = &quot;Orange&quot;; }
        }

        public string OrangeType { get; set; }

        public Orange(double Price, int Calories, string CountryOfOrigin, string UseByDate, string SweetnessLevel, bool ContainsSeeds, string OrangeType)
        {
            FruitName = FruitName;
            this.Price = Price;
            this.Calories = Calories;
            this.CountryOfOrigin = CountryOfOrigin;
            this.UseByDate = UseByDate;
            this.SweetnessLevel = SweetnessLevel;
            this.ContainsSeeds = ContainsSeeds;
            this.OrangeType = OrangeType;
        }


        // The &quot;override&quot; keyword here means that we are overriding the inherited  
        // method. 
        public override void SomeInfo()
        {
            Console.WriteLine(&quot;prefix override successful&quot;);
            base.SomeInfo();  // the &quot;base&quot; keyword makes the original inherited method to run. 
            Console.WriteLine(&quot;Suffix override successful&quot;);

        }

    }
}
[/csharp]]]></Content>
		<Date><![CDATA[2014-05-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[.net|c#|csharp|Microsoft|tutorial|Visual Studio]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>c#]]></Categories>
	</post>
	<post>
		<Title><![CDATA[c# - Random Number Generator]]></Title>
		<Content><![CDATA[Here's a quick a simple class for creating a a series of random numbers, by creating an instance of the "Random" class, and applying this class's next() method:
 
http://msdn.microsoft.com/en-us/library/system.random%28v=vs.110%29.aspx

Here is the code:

[csharp]
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace RandomNumberGenerator
{
    class Program
    {
        static void Main(string[] args)
        {
			Random RandomNumber = new Random();
			
			Console.WriteLine(&quot;Here is a list of random numbers:&quot;);
			Console.WriteLine(RandomNumber.Next());
			Console.WriteLine(RandomNumber.Next());
			Console.WriteLine(RandomNumber.Next());
			Console.WriteLine(RandomNumber.Next());
			Console.WriteLine(RandomNumber.Next());
			
		}
    }
	
}
[/csharp]]]></Content>
		<Date><![CDATA[2014-05-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[.net|c#|csharp|Microsoft|tutorial|Visual Studio]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>c#]]></Categories>
	</post>
	<post>
		<Title><![CDATA[c# - An intro to Arrays and Generics]]></Title>
		<Content><![CDATA[Here are some terminologies:

Arrays: This is a object variable, that is designed to store a collection of object-variable. Each object variable is stored in numbered containers (i.e. they are indexed), with the first container starting at number 0.

Generics: these are like arrays, but they can only store builtin c# variables, i.e. variables that are int, string, double,...etc. However you can't add new items to an array.

There are different types of generic collections:
<ul>
	<li>List  - these are like arrays, but you can also add new items to the list. Hence you can grow the list by using the "add" keyword</li>
	<li>Queues - This is based on the idea that the first item in the queue gets processed first....i.e. it is based on first-in, first-out concept.</li>
	<li>Stack - This is like queues, except it is based in last-in, first-out. Think of it as a stack pancake, where the last pancaked stacked on the pile is also the first pancaked that gets eaten.</li>
	<li>Dictionaries - These are essentially hash-tables, i.e key/value pairing.  This is very powerful, because you can search/sort keys, which in turn holds the underlining data.</li>
</ul>]]></Content>
		<Date><![CDATA[2014-05-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[.net|c#|csharp|Microsoft|tutorial|Visual Studio]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>c#]]></Categories>
	</post>
	<post>
		<Title><![CDATA[c# - Arrays]]></Title>
		<Content><![CDATA[Arrays are used to store a a group of items as a collection:

[csharp]
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace Arrays
{
    class Program
    {
        static void Main(string[] args)
        {
            // Here we are creating an array called &quot;vowels&quot;, that can house up to 5 items. 
            string[] vowels = new string[5];
            vowels[0] = &quot;a&quot;;
            vowels[1] = &quot;e&quot;;
            vowels[2] = &quot;i&quot;;
            vowels[3] = &quot;o&quot;;
            vowels[4] = &quot;u&quot;;
			
            for (int i = 0; i &lt; vowels.Length; i++)
			{
				Console.WriteLine(vowels[i]);
			}

			// Here is another way to declare an array, where you set all the values in a single line:
			string[] fruits = {&quot;apple&quot;,&quot;bananas&quot;,&quot;oranges&quot;,&quot;mangoes&quot;};
	
			for (int i = 0; i &lt; fruits.Length; i++)
			{
				Console.WriteLine(fruits[i]);
			}

			// Here is an array of integers:
			int[] PrimaryNumbers = {1,2,3,5,7,11,13};
	
			for (int i = 0; i &lt; PrimaryNumbers.Length; i++)
			{
				Console.WriteLine(PrimaryNumbers[i]);
			}
			
			
			// The following only outputs &quot;System.String[]&quot; or &quot;System.Int32[]&quot;, hence not that useful. 
			Console.WriteLine(vowels);
			Console.WriteLine(fruits);
			Console.WriteLine(PrimaryNumbers);
		}
    }
}
[/csharp]


Tip: In the above examples we used the for-loop to process each item in the variable. However, it is often easier to use the foreach loop instead (we will  do this in our next example). 

Arrays can also be used to store custom variable objects:

[csharp]
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace Arrays
{
    class Program
    {
        static void Main(string[] args)
        {

            Employee[] Employees = new Employee[3];
            Employees[0] = new Employee(&quot;David&quot;);
            Employees[1] = new Employee(&quot;Jennifer&quot;);
            Employees[2] = new Employee(&quot;Jonathon&quot;);
			
			// We used the for loop above, however when 
			// working with arrays it is often easier 
			// to output using the foreach loop instead:
			foreach (Employee item in Employees)
			{
				Console.WriteLine(item.Name);
			}

		}
    }
	
	class Employee
    {
        public string Name {get; set;}
 
        public Employee (string Name)
        {
            this.Name = Name;
        }
    }
	
}
[/csharp] 



For more info about arrays, checkout:

http://msdn.microsoft.com/en-us/library/system.array.aspx

http://msdn.microsoft.com/en-us/library/aa288453%28v=vs.71%29.aspx



However arrays have one big limitation, and that is that you can't add extra items to an array that is already contains the maximum number of items. 



]]></Content>
		<Date><![CDATA[2014-05-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[.net|c#|csharp|Microsoft|tutorial|Visual Studio]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>c#]]></Categories>
	</post>
	<post>
		<Title><![CDATA[c# - Generics: Lists]]></Title>
		<Content><![CDATA[Arrays have a big limitation is that you cant extend the array to hold more than the number items it was originally defined to hold.

As a result a lot of people uses "lists" as an alternative.

You can find the lists class in the following namespace:

System.Collections.Generic

Here is the link for the "generic" namespace

http://msdn.microsoft.com/en-us/library/system.collections.generic(v=vs.110).aspx

The word "generic" comes from the fact that lists (along with queues, stacks, ) can hold any kind of builtin object variables, e.g. string, int, double,...and etc.

The list's class is:

http://msdn.microsoft.com/en-us/library/6sh2ey19%28v=vs.110%29.aspx

(The "" indicates type)

Lists, as with all the generics comes with it's own style of syntax.....in the form of angle brackets, "&lt;=>". Inside these brackets you specify the data type that the list will hold.

The way lists works is that you first create an empty list, and then you add items to it. In the following example we create a list (ListOfRandomNumbers) that is designed to hold a series of random integers e.g.:

[csharp]
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace List
{
    class Program
    {
        static void Main(string[] args)
        {

			// Here you can think of &quot;List&quot;
			List ListOfRandomNumbers = new List();

			// first we create an instance of the random class
			Random RandomNumberGenerator = new Random();

			// Here we add random numbers to the list
			for (int i=0; i			{
				ListOfRandomNumbers.Add(RandomNumberGenerator.Next());
			}

			// Here we sort the numbers, using the list class's &quot;Sort&quot; method:
			ListOfRandomNumbers.Sort();

			foreach (int number in ListOfRandomNumbers)
			{
				Console.WriteLine(number);
			}
		}
    }

}
[/csharp]

lists can often be used as a way to get a list of items as an output parameter for a method. 


Also list's can also hold custom object, here's an example:

[csharp]
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace List
{
    class Program
    {
        static void Main(string[] args)
        {		
			// Here you can think of &quot;List&lt;int=>&quot; 
			List&lt;CountryStat=> ListOfCountryData = new List&lt;CountryStat=>();
			
			CountryStat EnglandObject = new CountryStat(&quot;London&quot;, 60000000, &quot;English&quot;);
			CountryStat FranceObject = new CountryStat(&quot;Paris&quot;, 90000000, &quot;French&quot;);
			CountryStat RussiaObject = new CountryStat(&quot;Moscow&quot;, 180000000, &quot;Russian&quot;);
			
			ListOfCountryData.Add(EnglandObject);
			ListOfCountryData.Add(FranceObject);
			ListOfCountryData.Add(RussiaObject);
			
			foreach (CountryStat CountryData in ListOfCountryData)
			{
				Console.WriteLine(CountryData.Capital);
			}
		}
    }

	public class CountryStat
    {

        public string Capital { get; set; }
        public int Population { get; set; }
        public string Language { get; set; }

        public CountryStat(string Capital, int Population, string Language)
        {
            this.Capital = Capital;
            this.Population = Population;
            this.Language = Language;
        }
	}
}
[/csharp]]]></Content>
		<Date><![CDATA[2014-05-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[.net|c#|csharp|Microsoft|tutorial|Visual Studio]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>c#]]></Categories>
	</post>
	<post>
		<Title><![CDATA[c# - Generics: Queue]]></Title>
		<Content><![CDATA[Queues are like lists, but works on the basis of first-in, first-out, here's an example:

[csharp]
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace Queue
{
    class Program
    {
        static void Main(string[] args)
        {
			// here we create an empty queue that we define to only hold
			// integers.
			Queue&lt;int=> MyQueue = new Queue&lt;int=>();
			
			// Here we add the first number (57) to the queue, using the 
			// the Enqueue method. 
			MyQueue.Enqueue(57);
			
			// Here we add another number to the queue
			MyQueue.Enqueue(101);

			// Here we add another number to the queue 
			MyQueue.Enqueue(18);

			// Here we add another number to the queue
			MyQueue.Enqueue(123);			
			
			
			
			Console.WriteLine(MyQueue.Dequeue());
			// The Dequeue method does 2 things,
			// - it outputs the next item in the queue
			// - it deletes removes the item from the queue
			
			Console.WriteLine(MyQueue.Peek());
			// here we just access the next item without 
			// dequeueing it.

			Console.WriteLine(MyQueue.Dequeue());
			
			
			// You can also view all the items in the queue like this:
			Console.WriteLine(&quot;Here are the remaining items in the list:&quot;);
			
			foreach (int number in MyQueue)
			{
				Console.WriteLine(number);
			}
		}
    }	
}
[/csharp]]]></Content>
		<Date><![CDATA[2014-05-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[.net|c#|csharp|Microsoft|tutorial|Visual Studio]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>c#]]></Categories>
	</post>
	<post>
		<Title><![CDATA[c# - Generics: Stack]]></Title>
		<Content><![CDATA[Stacks are practically the same thing as Queues, but works on the basis of last-in, first-out.

Here is the stack's reference page:

http://msdn.microsoft.com/en-us/library/3278tedw(v=vs.110).aspx



It basically works in the opposite order of how a queue works. Here's an example:

[csharp]
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace Queue
{
    class Program
    {
        static void Main(string[] args)
        {
			// here we create an empty stack that we define to only hold
			// integers.
			Stack&lt;int=> Stack = new Stack&lt;int=>();
			
			// Here we add the first number (57) to the stack, using the 
			// the Enqueue method. 
			Stack.Push(57);
			
			// Here we add another number to the queue
			Stack.Push(101);

			// Here we add another number to the queue 
			Stack.Push(18);

			// Here we add another number to the queue
			Stack.Push(123);			
			
			
			
			Console.WriteLine(Stack.Pop());
			// The Pop() method does 2 things,
			// - it outputs the last item added to the stack
			// - it  removes the item from the stack
			
			Console.WriteLine(Stack.Peek());
			// here we just access the next item without 
			// dequeueing it.

			Console.WriteLine(Stack.Pop());
			
			
			// You can also view all the items in the queue like this:
			Console.WriteLine(&quot;Here are the remaining items in the list:&quot;); 
			
			foreach (int number in Stack)
			{
				Console.WriteLine(number);
			}
		}
    }	
}
[/csharp]]]></Content>
		<Date><![CDATA[2014-05-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[.net|c#|csharp|Microsoft|tutorial|Visual Studio]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>c#]]></Categories>
	</post>
	<post>
		<Title><![CDATA[c# - Generics: Dictionary (aka hashtable)]]></Title>
		<Content><![CDATA[In c#, dictionaries are the same thing as hashtables.

Dictonaries are a bit like arrays where you can customise the default index numbers to something more meaningful, here's an example:

[csharp]
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace Dictionary
{
    class Program
    {
        static void Main(string[] args)
        {
			// here we create an empty dictionary
			// called &quot;CountriesAndCapitals&quot;
			// where we define that
			// the index, is a string, and the main value is
			// also a string:
			Dictionary&lt;string, string=> CountriesAndCapitals = new Dictionary&lt;string, string=>();

			// Here we add items to the dictionary
			CountriesAndCapitals.Add(&quot;England&quot;,&quot;London&quot;);
			CountriesAndCapitals.Add(&quot;France&quot;,&quot;Paris&quot;);
			CountriesAndCapitals.Add(&quot;Italy&quot;,&quot;Rome&quot;);

			Console.WriteLine(CountriesAndCapitals[&quot;Italy&quot;]);
		}
    }
}
[/csharp]

Dictionaries are really powerful, and one of things it often gets used for is to create a dictionary where the key is string, but the value itself is actually a custom object, and to instantiate this type of object we do:

Dictionary<string, {Custom Object's Name}> {Dictionary's Name} = new Dictionary<string, {Custom Class's Name}>();


Now something that is common practice is that we create a custom class, which we'll use to create the custom objects needed to populate the dictionary. A (static) method (that gets triggered by the main program), which in turn instantiates the dictionary object, the method then instantiates a number of objects from the custom class and adds them to the dictionary along with a key. Finally the method returns the dictionary back to it's caller, which would capture it in a dictionary-variable.

Here is an example of this in action:

[csharp]
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace Dictionary
{
    class Program
    {
        static void Main(string[] args)
        {

            // Here we call the CountryStat's class's (static) GetCountryListInfo method,
            // in order for it to generate the dictionary, populate it, and 
            // then return the dictionary:
            var RetrievedCountryInfo = CountryStat.GetCountryListInfo();

            Console.WriteLine
			(
				&quot;England has a population of {0} and it's official language is {1}. It's capital city is called {2}.&quot;,
				RetrievedCountryInfo[&quot;England&quot;].Population,
				RetrievedCountryInfo[&quot;England&quot;].
				Language,RetrievedCountryInfo[&quot;England&quot;].Capital
			);
        }
    }

    public class CountryStat
    {

        public string Capital { get; set; }
        public int Population { get; set; }
        public string Language { get; set; }

        public CountryStat(string Capital, int Population, string Language)
        {
            this.Capital = Capital;
            this.Population = Population;
            this.Language = Language;
        }

        // Here we instantiate the dictionary, and add
        // entries to the dictionary
        public static Dictionary&lt;string, CountryStat=> GetCountryListInfo()
		{
			// Here we create the empty dictionary that 
			// we will populate:
			Dictionary&lt;string, CountryStat=> DictionaryOfCountries = new Dictionary&lt;string, CountryStat=>();
			
			// Here we create the first object to be added to the dictionary:
			CountryStat EnglandObject = new CountryStat(&quot;London&quot;, 60000000, &quot;English&quot;);
			// Now we add this object to the dictionary:
			DictionaryOfCountries.Add(&quot;England&quot;,EnglandObject);

			// Here we create the next object to be added to the dictionary:
			CountryStat FranceObject = new CountryStat(&quot;Paris&quot;, 90000000, &quot;French&quot;);
			// Now we add this object to the dictionary:
			DictionaryOfCountries.Add(&quot;France&quot;,FranceObject);
			
			
			// Here we create the next object to be added to the dictionary:
			CountryStat RussiaObject = new CountryStat(&quot;Moscow&quot;, 180000000, &quot;Russian&quot;);
			// Now we add this object to the dictionary:
			DictionaryOfCountries.Add(&quot;Russia&quot;,RussiaObject);
			
			// Now we return the populated dictionary back to the caller:
			return DictionaryOfCountries;		
		}
    }
}
[/csharp]]]></Content>
		<Date><![CDATA[2014-05-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[.net|c#|csharp|Microsoft|tutorial|Visual Studio]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>c#]]></Categories>
	</post>
	<post>
		<Title><![CDATA[c# - Using the "Null" object]]></Title>
		<Content><![CDATA[Sometimes a method will ask for an input parameter, but that input parameter is optional. In these situation you can still trigger the method, without passing an input parameter, and instead pass in a "null" object in it's place:

[csharp]
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace ExceptionExample
{
    class Program
    {
        static void Main(string[] args)
        {

            FirstMethod(&quot;hello world&quot;);
            // FirstMethod();           // this will cause an error because the &quot;firstmethod&quot; method must require
										// an input parameter in order to operate
            FirstMethod(null);

        }

        public static void FirstMethod(string message)
        {
            if(String.IsNullOrEmpty(message))
            {
                Console.WriteLine(&quot;There is no message that has been passed in&quot;);
            }
            else
            {
                Console.WriteLine(&quot;The message is: &quot; + message);
            }
            
        }

    }
}
[/csharp]]]></Content>
		<Date><![CDATA[2014-05-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[.net|c#|csharp|Microsoft|tutorial|Visual Studio]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>c#]]></Categories>
	</post>
	<post>
		<Title><![CDATA[c# - Intro to Exceptions]]></Title>
		<Content><![CDATA[Sometimes you might have a block of "dangerous code" in your program. These are code that could fail. When these blocks of "dangerous codes" fail, it will stop the program straight, and won't attempt to carry on any further, and instead it will just output an error message.

However sometimes you might want your program to do some (or all) of the following when the "dangerous code" fails:

- continue with the rest of the program
- run a specific block of code when a failure occurs (aka error-handling)
- Customize the error message that get's outputted.
- Create artificial error messages, that simulates a failure, when in fact, there is none. E.g. say you have a string variable called "FirstName". A valid entry for this is "Mr". However you want this to generate an error, because it looks like someone accidently entered their title in place of where there name is supposed to be (unless in the extremely rare occasion, a person has a first name of "Mr").

Lets start of by looking at what happens when there is an ordinary error in the code.
In the following example, we have a class that contains the Main method. This main method calls FirstMethod, which in turn calls SecondMethod, which in turn calls ThirdMethod. I have also commented out the dangerous code to see what the output is when everything runs ok:

[csharp]
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace ExceptionExample
{
    class Program
    {
        static void Main(string[] args)
        {

            FirstMethod();

        }

        public static void FirstMethod()
        {
            Console.WriteLine(&quot;First Method Begins&quot;);
            SecondMethod();
            Console.WriteLine(&quot;First Method Ends&quot;);
        }

        private static void SecondMethod()
        {
            Console.WriteLine(&quot;Second Method Begins&quot;);
            ThirdMethod();
            Console.WriteLine(&quot;Second Method Ends&quot;);
        }

        private static void ThirdMethod()
        {
            Console.WriteLine(&quot;Third Method Begins&quot;);

            // The following line is the dangerous code, since we are trying ot
            // divide by zero!
            //int x = 0;
            //var y = 10 / x;
            Console.WriteLine(&quot;Third Method Ends&quot;);
        }

    }

}

[/csharp]

This outputs:

<img alt="" src="data:image/png;base64,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" />

Now if we enable the dangerous code and try again, we get:

<img alt="" src="data:image/png;base64,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" />

As you can, this time the program grinds to a halt as soon as it encounters the error.

If you want the program to continue, and also run some error-handling, then this is where exception handling comes into play....via the use of the exception handling's  "try" and "catch" construct:

<img alt="" src="data:image/png;base64,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" />

As you can see, that we got our custom error message, and also that the program didn't crash/stop.

There is an extension to the try-catch construct, and that is the "finally" construct. This block of code runs regardless whether the try block passes or fails (causing the catch block to run). Here it is in action:

[csharp]
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace ExceptionExample
{
    class Program
    {
        static void Main(string[] args)
        {

            FirstMethod();

        }

        public static void FirstMethod()
        {
            Console.WriteLine(&quot;First Method Begins&quot;);
            SecondMethod();
            Console.WriteLine(&quot;First Method Ends&quot;);
        }

        private static void SecondMethod()
        {
            Console.WriteLine(&quot;Second Method Begins&quot;);
            ThirdMethod();
            Console.WriteLine(&quot;Second Method Ends&quot;);
        }

        private static void ThirdMethod()
        {
            Console.WriteLine(&quot;Third Method Begins&quot;);

            try
            {
                //The following line is the dangerous code, since we are trying to
                //divide by zero!
                int x = 0;
                var y = 10 / x;
            }
            catch (Exception ErrorObject)
            {
                Console.WriteLine(&quot;Third method failed with the following error message: &quot; + ErrorObject.Message);

            }
            finally
            {
                Console.WriteLine(&quot;This code block alwaaaaaaaaays runs!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!&quot;);
            }

            Console.WriteLine(&quot;Third Method Ends&quot;);
        }
    }
}

[/csharp]

This will output:

<img alt="" src="data:image/png;base64,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" />

You might wonder what is the point of the finally construct since the code after the try-catch construct will be executed anyway. The reasoning is that sometimes you might want to include some code at the end of the try-block, which you must always run. These code may fail to run if the try-block fails in midway, to cover that possibility, you would also add the same lines in the catch-block. However, then you will have ended up with the same lines of code in both blocks, i.e. we have duplicate lines of code. To overcome this, you take these codes out of both try-catch blocks and place them in the finally block.

However since the code after the catch-block are run anyway, then why bother with the catch block? That's because there is a possibility that the catch block might also fail. In those instances, the "finally" block acts as the catch-block's fail-safe.
Here's an example where I intentionally placed an error in both the try and catch blocks:

[csharp]
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace ExceptionExample
{
    class Program
    {
        static void Main(string[] args)
        {

            FirstMethod();

        }

        public static void FirstMethod()
        {
            Console.WriteLine(&quot;First Method Begins&quot;);
            SecondMethod();
            Console.WriteLine(&quot;First Method Ends&quot;);
        }

        private static void SecondMethod()
        {
            Console.WriteLine(&quot;Second Method Begins&quot;);
            ThirdMethod();
            Console.WriteLine(&quot;Second Method Ends&quot;);
        }

        private static void ThirdMethod()
        {
            Console.WriteLine(&quot;Third Method Begins&quot;);

            try
            {
                //The following line is the dangerous code, since we are trying to
                //divide by zero!
                int x = 0;
                var y = 10 / x;
            }
            catch (Exception ErrorObject)
            {
                int x = 0;
                var y = 10 / x;
                Console.WriteLine(&quot;Third method failed with the following error message: &quot; + ErrorObject.Message);

            }
            finally
            {
                Console.WriteLine(&quot;This code block alwaaaaaaaaays runs!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!&quot;);
            }

            Console.WriteLine(&quot;Third Method Ends&quot;);
        }
    }
}

[/csharp]

This outputs:

<img alt="" src="data:image/png;base64,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" />

For more info see:

http://stackoverflow.com/questions/547791/why-use-finally-in-c]]></Content>
		<Date><![CDATA[2014-05-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[.net|c#|csharp|Microsoft|tutorial|Visual Studio]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>c#]]></Categories>
	</post>
	<post>
		<Title><![CDATA[c# - The Interface]]></Title>
		<Content><![CDATA[We previously came across the concept of inheritance, which is where a child class (implicitly) inherits the properties and methods from it's parent class.

However what if you don't want a class to inherit methods and properties, but we do want class to have certain members (properties and methods) with particular names. That's where "interfaces" comes in.

If you want to create a collection of classes and you want them all to have methods of certain names (although the content of the methods can be completely different), then you use interfaces.

A common example of where interfaces comes in handy is when you create a class (e.g. called DBclass) that's designed to interact with a db and perform crud operations. For this class you might have a method called "Read()" for retrieving data from db, and another method called "Write()" for writing data to db.

Now lets says you have another class (e.g. called XMLclass), which this time is designed to interact with an xml file, and perform crud operation. Both these classes do the same job, the only difference being that they have different data sources, one is a db, and the other is an xml file. So it would be best practice for consistency purposes that both these classes have methods of the same names. Hence XMLclass should also contain methods called Read() and Write().

To make sure this standard and consistency gets enforced, we create an interface. An interface is basically a template (aka specification) that we use to create classes from. These classes that get created from a interface, must contain the methods an properties that have the same name as those specified in the interface.

The interface works on the basis of "minimum-requirements". This means that a class can have many more methods/properties specific to the class, but still comply with the interface as long as it includes the methods and properties that have names that match with those listed in the interface.

Here's an example, in this example we have a class called "Transactions", and this class is designed to comply with an interface called "Itransactions":

[csharp]
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;

namespace InterfaceApplication
{

	// Here we define the interface using the &quot;interface&quot; keyword
   public interface ITransactions
   {
      // Here we have defined a property called &quot;country&quot;. This means that all classes that conforms
	  // to this interface, must also have a string property called country (with read/write enabled)
	  string country {get; set;}

      // Here we say that all classes that are created to conform with this interface, must contain at least 2 methods.
// one of these methods must have the name &quot;showTransaction&quot; (which has a void output parameter) and another
	  // method with the name &quot;getAmount&quot; (which has a double output parameter)
	  // note: due to purpose of the interface, the interface, the names of the methods has to be, and not 
// what task the method actually performs. That info is defined within the complying class. 
      void showTransaction();
      double getAmount();
   }
   // Note, you don't prefix any of the above items with &quot;public&quot;, that is the default anyway, and you
   // cannot change this either.

   // Here we are creating a new class called &quot;Transactions&quot;, and on the
   // first line we have specified that this class must conform to the
   // ITransactions interface.
   public class Transaction : ITransactions
   {
		// here we declared the &quot;country&quot; in order to comply with the interface's requirements.
		public string country {get; set;}

		// The interface, defines what is essentially &quot;minimum requirements&quot;. That means
		// that we can add extra properties/methods to our class on top of those that we
		// must include according to the interface. Here we have added 3 additional
		// properties:
		public string TCode{get; set;}
		public string Date{get; set;}
		public double Amount{get; set;}

	    // here is the constructor.
		public Transaction(string c, string d, double a)
		{
		this.TCode = c;
		this.Date = d;
		this.Amount = a;
		this.country = &quot;England&quot;;
		}

		// Here we created a method called &quot;getAmount&quot; in accordance with
		// the interface's requirements.
		public double getAmount()
		{
		return Amount;		// The content of the method doesn't matter. As long as method by this
							// name exists.
		}

		// Here we created a method called &quot;showTransaction&quot; in accordance with the interface's
		// requirements.
		public void showTransaction()
		{
		Console.WriteLine(&quot;Transaction: {0}&quot;, TCode);
		Console.WriteLine(&quot;Date: {0}&quot;, Date);
		Console.WriteLine(&quot;Amount: {0}&quot;, getAmount());

		}

		// This is an additional method that the interface doesn't require. However
		// we haved include this as it is a method we want in this class.
		public void CountryOfTransaction()
		{
			Console.WriteLine(&quot;This transaction took place in &quot; + country);
		}

   }

   class Program
   {
      static void Main(string[] args)
      {
         Transaction t1 = new Transaction(&quot;002&quot;, &quot;9/10/2012&quot;, 451900.00);
         t1.showTransaction();
		 t1.CountryOfTransaction();
      }
   }
}
[/csharp]

This outputs:

<img alt="" src="data:image/png;base64,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" />

[csharp]
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace InterfaceDemo
{
    // Here we have defined the interface. On this occasion
    // we are saying that all complying class must have at least
    // 2 methods, where 2 of those methods have the names &quot;Read&quot; and 
    // &quot;Write&quot;
    public interface DataSourceInteractionTemplate
    {

        string Read();

        void Write(object objectdata);
    }



    public class DBInteraction : DataSourceInteractionTemplate
    {

        // this is an additional property. 
        public string HelloDBMessage { get; set; }

        // Here we created a method called &quot;Read&quot; in accordance with 
        // the DataSourceInteractionTemplate interface
        public string Read()
        {
            Console.WriteLine(&quot;reading data from database&quot;);
            return &quot;test&quot;;
        }


        // Here we created a method called &quot;Write&quot; in accordance with 
        // the DataSourceInteractionTemplate interface
        public void Write(object InputData)
        {
            Console.WriteLine(&quot;writing data to database&quot;);
        }

        // this is an additional method. 
        public void HelloDBMethod()
        {
            this.HelloDBMessage = &quot;Hello Database&quot;;
            Console.WriteLine(this.HelloDBMessage);
        }

    }


    public class XMLInteraction : DataSourceInteractionTemplate
    {

        // this is an additional property. 
        public string HelloXMLMessage { get; set; }

        // Here we created a method called &quot;Read&quot; in accordance with 
        // the DataSourceInteractionTemplate interface
        public string Read()
        {
            Console.WriteLine(&quot;reading data from xml file&quot;);
            return &quot;test&quot;;
        }

        // Here we created a method called &quot;Write&quot; in accordance with 
        // the DataSourceInteractionTemplate interface
        public void Write(object InputData)
        {
            Console.WriteLine(&quot;writing data to xml&quot;);
        }

        // this is an additional method. 
        public void HelloXMLMethod()
        {
            this.HelloXMLMessage = &quot;Hello XML&quot;;
            Console.WriteLine(this.HelloXMLMessage);
        }
        
    }


    class Program
    {
        static void Main(string[] args)
        {
            DBInteraction DBobject = new DBInteraction();
            DBobject.Read();
            DBobject.Write(null);
            DBobject.HelloDBMethod();

            XMLInteraction XMLobject = new XMLInteraction();
            XMLobject.Read();
            XMLobject.Write(null);
            XMLobject.HelloXMLMethod();



        }
    }
}

[/csharp]

The above will output:

<img alt="" src="data:image/png;base64,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" />

So far we have seen that a interface gives some high-level rules that complying class must follow. Due to the nature of an interface, you cannot instantiate a object from a interface, since interface isn't really a class. However, you can convert an existing object (that got instantiated by a complying class) to belong to that interface. When you do this, the object can still access it's original method/properties, but only those that are named in the interface. 

The cool advantage of this is that you can have an object that belongs to "DataSourceInteractionTemplate" interface, and use it's read/write methods, without knowing whether you are interacting with a db,xml file, or something else!

Here's an example:  
   
[csharp]
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace InterfaceDemo
{
    // Here we have defined the interface. On this occasion
    // we are saying that all complying class must have at least
    // 2 methods, where 2 of those methods have the names &quot;Read&quot; and 
    // &quot;Write&quot;
    public interface DataSourceInteractionTemplate
    {

        string Read();

        void Write(object objectdata);
    }



    public class DBInteraction : DataSourceInteractionTemplate
    {

        // this is an additional property. 
        public string HelloDBMessage { get; set; }

        // Here we created a method called &quot;Read&quot; in accordance with 
        // the DataSourceInteractionTemplate interface
        public string Read()
        {
            Console.WriteLine(&quot;reading data from database&quot;);
            return &quot;test&quot;;
        }


        // Here we created a method called &quot;Write&quot; in accordance with 
        // the DataSourceInteractionTemplate interface
        public void Write(object InputData)
        {
            Console.WriteLine(&quot;writing data to database&quot;);
        }

        // this is an additional method. 
        public void HelloDBMethod()
        {
            this.HelloDBMessage = &quot;Hello Database&quot;;
            Console.WriteLine(this.HelloDBMessage);
        }

    }


    public class XMLInteraction : DataSourceInteractionTemplate
    {

        // this is an additional property. 
        public string HelloXMLMessage { get; set; }

        // Here we created a method called &quot;Read&quot; in accordance with 
        // the DataSourceInteractionTemplate interface
        public string Read()
        {
            Console.WriteLine(&quot;reading data from xml file&quot;);
            return &quot;test&quot;;
        }

        // Here we created a method called &quot;Write&quot; in accordance with 
        // the DataSourceInteractionTemplate interface
        public void Write(object InputData)
        {
            Console.WriteLine(&quot;writing data to xml&quot;);
        }

        // this is an additional method. 
        public void HelloXMLMethod()
        {
            this.HelloXMLMessage = &quot;Hello XML&quot;;
            Console.WriteLine(this.HelloXMLMessage);
        }

    }


    class Program
    {
        static void Main(string[] args)
        {
            List&lt;DataSourceInteractionTemplate=> ListOfDataObjects = new List&lt;DataSourceInteractionTemplate=>();
            
            DBInteraction object1 = new DBInteraction();
            ListOfDataObjects.Add(object1); // this converts object1's type from DBInteraction to DataSourceInteractionTemplate. 
            XMLInteraction object2 = new XMLInteraction();
            ListOfDataObjects.Add(object2); // this converts object2's type from XMLInteraction to DataSourceInteractionTemplate.
            DBInteraction object3 = new DBInteraction();
            ListOfDataObjects.Add(object3);
            XMLInteraction object4 = new XMLInteraction();
            ListOfDataObjects.Add(object4);


            foreach (DataSourceInteractionTemplate databoject in ListOfDataObjects)
            {
                databoject.Read();
                databoject.Write(null);
                // DBobject.HelloDBMethod();   // the converted object can no longer access the class's members.  
                // XMLobject.HelloXMLMethod(); // except for the ones named in the interface specs.  
            }
            
            
        }
    }
}
[/csharp]

The above shows how you can standardize interaction with many different data-sources by using interfaces. This means that objects originating from different classes are doing the right thing in the loop. This means they are behaving polymorphically. 


]]></Content>
		<Date><![CDATA[2014-05-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[.net|c#|csharp|Microsoft|tutorial|Visual Studio]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>c#]]></Categories>
	</post>
	<post>
		<Title><![CDATA[c# - Abstract Classes]]></Title>
		<Content><![CDATA[When you organise your code into parent and child classes (in order to fully utilize the concept of inheritance in order to cut down code duplication), you may end up with parent classes, that are just there for the purpose of holding base-class code. The class itself isn't needed for instantiating objects.

These types of class (which you never instantiate objects from) are referred to as "abstract classes". You never use an abstract class directly to initialize an object.

Here is an example of abstract classes in action:

[csharp]
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace AbstractClass1
{

    // hwer
    public abstract class Employee
    {
        public string Name { get; set; }
        public double Salary { get; set; }
        private DateTime HireDate;

        // The &quot;virtual&quot; key word means that
        // this method can be over-rided
        // by the child class, if required, otherwise it is implicitly
        // inherited
        public virtual void Hire()
        {
            HireDate = DateTime.Now;
        }

        // This is an abstract method.
        // This line forces child classes to include a method with the same name, along with a definition.
        // this is a bit like the same thing that you also find in interfaces.
        public abstract void GiveRaise();

    }

    public class Worker : Employee
    {
        public override void GiveRaise()
        {
            Salary += Salary * 0.03;
        }
    }

    public class Manager : Employee
    {
        public override void GiveRaise()
        {
            Salary += Salary * 0.05;
        }
    }

    class Program
    {
        static void Main(string[] args)
        {
            Worker David = new Worker();
            David.Name = &quot;David Smith&quot;;
            David.Salary = 60000.00;
            David.GiveRaise();
            Console.WriteLine(&quot;New salary for {0} is {1}&quot;,David.Name,David.Salary);

            Manager Goliath = new Manager();
            Goliath.Name = &quot;Goliath Banks&quot;;
            Goliath.Salary = 60000.00;
            Goliath.GiveRaise();
            Console.WriteLine(&quot;New salary for {0} is {1}&quot;, Goliath.Name, Goliath.Salary);

            List ListOfEmployees = new List();

            // this essentially converts this object into an instance of the abstract class.
            // It will still have access to original properties/method as defined in the child class, but only the ones
            // that are also listed in the abstract. It will lose access to everything else, that has been defined in the child class.
            // The same thing happens when using interfaces.
            ListOfEmployees.Add(David);
            ListOfEmployees.Add(Goliath);

            foreach (Employee Person in ListOfEmployees)
            {
                Person.Salary = 100000.00;      // here we reset the salary, and make them the same for each object, to make a better comparison.
                Person.GiveRaise();       // Depending on which class the object was instantiated from, a different method is applied.
                Console.WriteLine(&quot;New salary for {0} is {1}&quot;, Person.Name, Person.Salary);
            }

        }
    }
}
[/csharp]

this gives the following output:

&nbsp;

<img alt="" src="data:image/png;base64,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" />

&nbsp;

As you can see, interfaces and abstract classes are quite similar.

&nbsp;

However the biggest difference between the two is that you can make a class comply with multiple interfaces, using the following syntax:

&nbsp;
class {classname} : {interface1}, {interface1},......{interfaceX}
&nbsp;

Note: the more interfaces a class has to comply to, the more methods and properties it needs to comply with all of them. 

Whereas a class can only be derived from up to one abstract class, but you can define the abstract class's method's tasks which can be inherited....and if necessary over-rided. The abstract class can also force it's classes to contain a method with a specific name (just like in interfaces) by the use of abstract methods as shown above. ]]></Content>
		<Date><![CDATA[2014-05-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[.net|c#|csharp|Microsoft|tutorial|Visual Studio]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>c#]]></Categories>
	</post>
	<post>
		<Title><![CDATA[c# - Delegates]]></Title>
		<Content><![CDATA[What if you want the output of one method, to be treated as an input parameter for another method? i.e. you want to do something similar to linux bash piping in the world of c#, but instead of piping from one command, to another, you are piping from one method to another.

Here is an example

[csharp]

using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace DelegateDemo
{
    class Program
    {
        static void Main(string[] args)
        {
            // Step 5
            // Here we create an instance of the 
            // Mediatester class, using it's default constructor class.
            // We use this object to access the RunTest method
            // later on. 
            MediaTester ObjectContainingDelegate = new MediaTester();

            // Step 6
            // Here we create ordinary instances
            // of the MP3Player and MP4Player 
            // classes
            MP3Player mp3 = new MP3Player();

            // Step 7
            // We now want to create a reference instance
            // to this object's, PlayMP3File() method,
            // in the form of a delegate:
            MediaTester.TestMedia mp3Delegate = new MediaTester.TestMedia(mp3.PlayMP3File);
            // This syntax is a bit unusual because it is similar to the syntax 
            // you use to create an instance of a class, but in this case 
            // you use it to create a delegate.
            // we use this syntax because we need to tell c# where the delegate is located, so that it can find out
            // it's requirement (i.e. what behaviours matching methods needs to have, e.g. output parameter of integer)
            // as well as the actual object and it's method that we want to reference. 



            // Step 8
            // Now we do the same thing for the 
            // MP4Player class:
            MP4Player mp4 = new MP4Player();
            MediaTester.TestMedia mp4Delegate = new MediaTester.TestMedia(mp4.PlayMP4File);
            
            
            // step 9
            // Here the RunTest method requires a delegate as a input parameter,
            // when it calls on the delegate, the delgate in turn runs the method
            // against it's object, and retrieves the parameter output, which it
            // in turn passes into the RunTest as an input parameter. 
            // After which the RunTest method starts it's execution. 
            ObjectContainingDelegate.RunTest(mp3Delegate); // This line executes &quot;mp3.PlayMP3File()&quot; as part of it's execution. 
            ObjectContainingDelegate.RunTest(mp4Delegate); // This line executes &quot;mp4.PlayMP4File()&quot; as part of it's execution. 

        }
    }

    public class MediaTester
    {
        // Step 1
        // Here we created a delegate called &quot;TestMedia&quot;.
        // This delegate can act as delegate for to pipe  
        // any methods that requires no-input but gives out an 
        // integer output parameter. 
        public delegate int TestMedia();

        // Step 2
        // Here we create a method, that accepts an input 
        // parameter in the form of a &quot;TestMedia&quot; delegate. 
        public void RunTest(TestMedia testDelegate)
        {
            // Her we capture the output of the delegate into 
            // an integer variable. This should work since,
            // we defined that the TestMedia is a reference 
            // for any methods that gives an integer output. 
            int result = testDelegate();
            if (result == 0)
            {
                Console.WriteLine(&quot;Works!&quot;);
            }
            else
            {
                Console.WriteLine(&quot;Failed.&quot;);
            }
        }
    }

    // // Step 3
    // This is an ordinary class the consists 1 method called PlayMP3File,
    // which happens to fit the input/output parameter requirements of the  
    // &quot;TestMedia&quot; delegate
    class MP3Player
    {
        public int PlayMP3File()
        {
            Console.WriteLine(&quot;Playing MP3 file...&quot;);
            return 0;
        }

    }


    // Step 4
    // This is an ordinary class the consists 1 method called PlayMP4File,
    // which happens to fit the input/output parameter requirements of the  
    // &quot;TestMedia&quot; delegate 
    class MP4Player
    {
        public int PlayMP4File()
        {
            Console.WriteLine(&quot;Playing MP4 file...&quot;);
            return 1;
        }

    }


}


[/csharp]

This outputs:

<img alt="" src="data:image/png;base64,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" />

A delegate essentially act's as a placeholder for a method that matches it's reguirements. See:

http://stackoverflow.com/questions/2019402/when-why-to-use-delegates

http://stackoverflow.com/questions/31497/where-do-i-use-delegates (contains a good analogy of the president of the united states)]]></Content>
		<Date><![CDATA[2014-05-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[.net|c#|csharp|Microsoft|tutorial|Visual Studio]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>c#]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Linux - Compressing files in Linux]]></Title>
		<Content><![CDATA[## File compression.

## Commands covered in this article
gzip    # A file compression utility
bzip2     # A file compression utility
tar        # A tool that can &lt;l=>archive a whole directory&lt;l=> and convert it into a single file.
star #.secure tar. (not as important but worth learning.)


## gzip and bzip
Linux comes pre-installed with a number of file compression utilities. However the 2 most commonly used compression utilities are gzip and bzip2. Both of these tools work pretty much the same way. The only difference is that gzip is slighter better at compressing (resulting in an even smaller file when compared to bzip2) whereas bzip2 is slightly faster at compressing.

&lt;insert code=>


So your choice of whether to use bzip2 or gzip really comes down to what is more important to you, better compression, or faster compression.

To uncompress, you can use the equivalent gunzip and bunzip2 commands:

&lt;insert code=>


See the gzip and bzip2 man pages for more info about these commands.

gzip and bunzip can only compress singles files, and it won't let you compress a directory along with whatever it contains. To do this you need to look at &lt;l=>how to compress a directory using tar&lt;l=>.


## Also see
How to compress a Whole Directory in Linux? (coming soon)
20 practical example on how to use tar (coming soon)
Secure your compression using secure tar (star) (coming soon)]]></Content>
		<Date><![CDATA[2014-01-05]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[zip]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Linux - Compress a whole folder in Linux (using tar)]]></Title>
		<Content><![CDATA[<h2>What is tar</h2>
You may have come across the gzip and bzip2 commands which are used for compressing individual files. But if you want to compress a whole directory, then the command you need to use is <code>tar</code>. The tar command requires you to declare a number of options for it work, so the best way to understand how tar works is to see it in action:


<h2>Creating a tar file.</h2>
The tar command has several different modes, and to generate a tar file, you need to enable the "create" mode. Here's how you compress a whole directory.

<pre>tar -cf {tar-filename} /path/to/dir</pre>

For the above, you can read the options "-cf" as: (c)reate a tar (f)ile with the name {tar-filename}, using the content from /path/to/dir.

If you want to do the above, but also see the progress of the tar activity, then do:

<pre>tar -cvf {tar-filename} /path/to/dir</pre>

For the above, you can read "-cvf" as: (c)reate a tar (f)ile in (v)erbose mode, called {tar-filename}, using the content from /path/to/dir.


<h2>Creating a compressed tar file.</h2>
The tar files that we have created so far, are not compressed files. A compressed tar file can be created in 2 steps, like this:
<pre>
tar -cf {tar-filename} /path/to/dir       # step 1 - create the tarfile.
gzip {tar-filename}                    # step 2 - compress the tarfile.
</pre> 

However you can instruct the tar command to also do the gzipping for you:

<pre>
tar -cvzf {tar-filename} /path/to/dir        # Here, tar compresses the tar file using the gzip utility.
</pre>

For the above, you can read "-cvzf" as: (c)reate a g(z)ipped tar (f)ile in (v)erbose mode, called {tar-filename}, using the content from /path/to/dir.

Alternatively, you can use bzip2 utility like this:

<pre>tar -cvjf {tar-filename} /path/to/dir </pre>      # Here, the tar command compresses the tar file using the bzip2 utility.

Note: Options z and j don't work in Solaris 10.


<h2>Extracting a tar file</h2>
To extract a tar file, you need to enable the tar command's "extract" mode:

<pre>tar -xf tarfile.tar</pre>

For the above, you can read the options "-xf" as: e(x)tract the tar file of the (f)ilename, tarfile.tar.

<pre>tar -xvf name.tar </pre>      # This does the same as above, but in verbose mode.

For the above, you can read "-xvf" as: e(x)tract, in (v)erbose, the tar file of the (f)ilename, tarfile.tar.


<h2>Extracting a zipped tar file</h2>
So far we have only looked at how to extract an uncompressed tar file. But when dealing with compressed tar files, you can extract them by taking a 2 step approach:

<pre>gunzip tarfile.tar.gz </pre>   # step 1 - unzip the compressed tarfile.
<pre>tar -xvf tarfile.tar </pre>   # step 2 - extract the tarfile.

However it is easier to just do both steps in a single command like this:

<pre>tar -xvzf tarfile.tar.gz</pre>

For the above, you can read "-xvzf" as: gun(z)ip and then e(x)tract, in (v)erbose mode, the tar file, of the (f)ilename, tarfile.tar.gz

Similarly, if you are dealing with tar files that have been compressed with bzip2, you do:

<pre>tar -xjvf tarfile.tar.bzip2</pre>


<h3>Also see</h3>
How to compress a Whole Directory in Linux? (Coming soon)
The 5 main modes of tar (Coming soon)
20 practical example on how to use tar (Coming soon)]]></Content>
		<Date><![CDATA[2014-01-05]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[tar]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Linux - Introducing "VIM", a powerful text editor]]></Title>
		<Content><![CDATA[## Howto: open and edit files in Linux

In Centos, you can view files either from the command line, or using a gui text editor called emacs.

But in this article, we will focus on doing this from the command line.


## View files

In linux there are several commands to view a file. Here are a few of them:

cat {filename}        # Displays the file's content.
less {filename}        # Displays the file's content. this command also has few other options, such as you can also scroll up/down using the arrow keys.
head {filename}        # Displays the first few lines of a file.
tail {filename}        # Displays the last few lines of a file.


# Create and edit files.

There are a lot of ways to create/edit files in linux. One of the ways is by using a based text editor called vim. Vim lets you create/edit files straight from the command line.

vim {filename}    # This opens up the file in the vim editor. If the filename doesn't exist, then it creates a new file by that name.

vim is a powerful text editor which is operated entirely via the keyboard, and doesn't involve using a mouse (apart from the occasional copy&amp;paste). And once you become proficient at vim, you will find that it is quicker to edit files with vim than any other graphical text editor. As a consequence and it has quite a steep learning curve.

The first thing to realise about vim is that it has 3 modes and you have switch between these modes depending on what you want to do. These modes are:

1. Edit mode  - You can access the mode at any time by hittig the Esc key.
2. Write mode - This is the mode for typing content.
3. Command mode - This is a mode that lets you do a number of tasks, including saving changes to your file.

When you open vim, the default mode that you get placed in is the edit mode:

- press ":" to access the command mode.
- press "i" to start (i)nserting text. There are a number of other ways to access the write mode, but this is a good starting point.


Note: you can't directly jump from command mode to write mode, or vice-versa. This means you can only access the command/write mode from the edit mode.

To access the edit mode, simply hit the Esc key. If already in the Esc mode, then hitting Esc again won't do anything.


The best way to learn vim is by going through a pre-installed tutorial:

vimtutor    # A handy step-by-step guide for vim.

vim is something that takes time to get used to. But once you are, working with files becomes really quick and easy.


## Useful tips
1. If you plan to edit a config file, it is always a good idea to make a backup of the file before you make any changes:

cp {config-file} {config-file}-backup

2. As an extra precaution, If editing a config file, always make a duplicate of the line about to be change, then comment out the original line, and then make the edits on the duplicate line. This approach is useful to keep track of the lines you have changed and how the lines looked before.]]></Content>
		<Date><![CDATA[2014-01-05]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[vim]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Linux - Temporarily switch to another Linux user (using su)]]></Title>
		<Content><![CDATA[## How to switch users or temporarily become root

## Commands covered in this article
id        # Displays the username of the currently logged in user. It also displays what groups the user belongs to.
su      # Lets you (s)witch to another (u)ser while logged in as someone else.
sudo     # xxxxxxxxxxxxxxxxx

## Become the root user using "su"
In linux it is always recommended that you avoid logging in as the root user directly. That's because it can cause security issues and increases the chances of damaging something. In fact, Centos will display a warning message when you attempt a direct root login.

So the best practice is that you always login as a normal user. However there are lot of commands that can only be run as the root user, and in those instances, the recommended way to run them is by using the (s)witch (u)ser utility, which lets you switch from one user to another:

su - root    # This lets you switch to the root user. I'll cover the "-" argument later.
password    # The su command prompts for the password of the user you are switching to. In this case it is the root user's password.
id             # The id command simply displays which user youre session is currently running under.

Note, you can only use the su command if you know the password for the user you are switching to. The only exception to this rule is if you are logged in as the root user, In which case you can su to any user without a password prompt.

login:
password:
id
su -        # If you leave out the username, like here, then by default, su will assume you mean root.
password
id
su - Tom
id             # Notice that we didn't need to enter Tom's password.

## Switching back to the previous user
In the above example we switched from John-=>root-=>Tom. This meant that our current shell is 2-sessions-deep. So if you want switch back to John, then you can use the exit command:

id
exit    # This takes you backup one step, which in this case is the root user.
id
exit     # Now you are back to the John, and you can't exit any further. If you try to exit again, then the terminal itself will close.

Instead of doing the above, you could have simply done:

su - John    # in this case, you would be 3-sessions deep, i.e. John-=>root-=>Tom-=>John
# It is good practice, where possible to never go deeper than 3 levels. Otherwise you might start to experience odd behaviours.

## What does the "-" mean in 'su -'?

The first parameter, "-",  tells su to pretend as if the user logged in directly. Otherwise, the environment is passed  along, with the exception of $PATH, which is controlled by PATH and SUPATH in /etc/default/su.

This "-" basically means start the new session from fresh, and dont carry any 'baggage' from the previous session. For example, if you create a variable, and export it, then this variable is only available if you don't start a fresh session:

testvar=12345    # Here I created a new variable.
export testvar            # This sets the variable to become available in all sessions.
su Tom            # Since "-" is ommitted, carry over is allowed. So variable should be accessible in the new session.
password
echo testvar      # variable exists in the new session.
exit
su - Tom         # Since "-" is specified , it blocks any carry over, in order to mimic a close a direct login as possible.
echo testvar    # Returns a blank since this variable doesn't exist in a "fresh" session.

## Run a single command as another user.
You can use su to switch user just long enough to run a single command before backing out again:

su - Tom -c 'echo "Hello and Goodbye Tom'

Or you can merge multiple commands in to single line using semi-colon:

su - Tom -c 'id ; echo "Hello and Goodbye Tom'

## Give non-admins access to some root level using sudo
In Linux, all admin level  commands/tasks that can only be performed by the root user. So if there is a particular admin-level activity that you want a colleague to perform, then one way to do this is to tell them the root password. However this is not a good idea because:
1. of security reasons, it is best practice to limit as far as possible, who has root access.
2. you are inadvertantly giving them full control of everything else.

A way round this dilemma is to use the sudo utility. Sudo allows the root user to give a user the ability to run one or more admin-level commands (as if they are logged in as root user but without actually logging in as root user).

For example, only root can shutdown the machine:

shutdown    # This shutdowns a machine. To switch it on again, you need to press the power-button on the machine.

If user, Tom tries to use the shutdown command, then he gets a permission denied message:

&lt;code=>

If the root user gives Tom, sudo permission to use the "shutdown" command. Then Tom should now be able to run the admin-level command be preceding it with the word sudo:

sudo {command}

As indicated above, the syntax for initiating sudo is the word sudo following by the command to be run:

Hence Tom can invoke his sudo privelege like this:

shutdown        # this wont work because sudo hasn't been invoked.
sudo shutdown    # This worked, and machine shuts down.
&lt;password=>        # Like su, you have to enter a password. But unlike su, you actually enter the password of the current user.
# This is a little overkill since user is already logged in as that user. Fortunately this &lt;l-anchor=>sudo password&lt;l=> prompt can be disabled.

For security reasons, a log is kept of all sudo related activities, in case a user abuses their sudo priveleges.

The root user needs to &lt;l=>configure and assign sudo priveleges&lt;l=> in order to assign priveleges.

## The main difference between using su and sudo (for running admin commands)
To summarise, The main difference for running admin-level commands as the main user are:

1. To use su, you need to know the root password, whereas for sudo, you don't
2. With sudo, you need to prefix every command with the word "sudo", but no prefixing is necessary with su.
3. With su, you can run any admin-command, but with sudo, you can only run the admin-commands as authorised in the sudo's config file.

## Some useful facts
The root user is sometimes referred to as the "superuser".
An alternative to the "id" comamnd "whoami" (but it isn't as informative).
There is a convention that for a root user, the prompt user ends in "#" whereas for anyone else it ends with "$". Speaking of which, there are a few ways to &lt;l=>customise the command prompt&lt;l=>.

The id command shows more than just username and group names. It also shows corresponding id numbers for the user and groups. You can also use the id command to view details of other users aside from the currently logged in user:

id {username}

The id command gets its information from /etc/passwd and /etc/group

## Related topics
Setting up and assigning sudo privileges.

## Useful links
http://www.cyberciti.biz/open-source/command-line-hacks/linux-run-command-as-different-user/
http://www.cyberciti.biz/faq/how-can-i-log-in-as-root/
http://www.cyberciti.biz/tips/allow-a-normal-user-to-run-commands-as-root.html]]></Content>
		<Date><![CDATA[2014-01-05]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Centos|RHEL|switch user]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Give a user more priveleges using sudo]]></Title>
		<Content><![CDATA[## Setting up and assign sudo priveleges

The Sudo utility lets you give a users custom privileges so that they can run commands that they are not normally allowed to run.

For example, only the root user can run the "Shutdown" command. But if you want give another user permission to run this command, then you can do this using sudo.

To configure privileges, all you have to do is edit the sudo's config file (which is called sudoers):

/etc/sudoers

## Use visudo instead of vim.
Be careful that you don't make any errors when editing the sudeors file. Thats becuase any errors can stop your machine from booting up. One way to avoid making errors is to use visudo to edit the sudoers file. visudo is a specialized version of vim that has been specifically designed for editing the sudoers file. visudo has builtin error-checking that will try to warn you if you make any errors.

There are a lot of different sudo configurations that are available. So lets start with the most basic one.

## Grant sudo permission for a regular user
To grant permissions, You simply add a line in the following syntax:

{username} localhost={/path/to/command}

Therefore there are 2 things we need to know in order to  assign sudo priveleges:

1. Who is to receive the new privilege
2. The command which the user will now be able to run. Note: you need to specify the full pathname of the command, and not just the command itself.

Lets assume we have a regular user called Tom and we want to let him run the "shutdown" command. First we need to find the shutdown command's full path (which we can do using the "which" command):

&lt;which shutdown=>

Now we add the following entry in the /etc/sudoers file:

Tom localhost=/usr/sbin/shutdown

If you you want Tom to also have sudo permission for the "reboot" command, then do:

Tom localhost=/usr/sbin/shutdown, /usr/sbin/reboot

If you want to give Tom sudo permission for all command stored in /usr/sbin, then do:

Tom localhost=/usr/sbin/*








## useful links

http://www.thegeekstuff.com/2010/09/sudo-command-examples/
http://cavepopo.hd.free.fr/wordpress/linux/sudo-command-sudoers-file-concepts-and-practical-examples/
http://www.garron.me/en/linux/visudo-command-sudoers-file-sudo-default-editor.html


http://www.thegeekstuff.com/2010/08/bash-shell-builtin-commands/
http://linux.die.net/man/5/sudoers]]></Content>
		<Date><![CDATA[2014-01-05]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Centos|RHEL|sudo]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Linux - Accessing the linux terminal]]></Title>
		<Content><![CDATA[The way you access the  linux command line terminal, depends on the scenario:

Scenario 1 - Access your Linux desktop machine's terminal

Using the gnome ui, interface, simply go to:

Useful Tip: you can create a desktop shortcut to save time.

Scenario 2 - Access a remote linux server's  terminal from a windows desktop machine.

In most corporate workplaces, linux servers are locked away in server rooms. In these cases, you can access the server's terminal remotely by using a software called putty. Putty creates a connection to the linux server and opens up a virtual terminal. After that you can use this terminal just as if you were access the linux server directly.

Useful Tip: In a corporate workplace, you may need to remotely connect to several remote linux machines. Then you might want to use a putty-addon called putty connection manager (PCM).  The cool thing thing about pcm is that it can be:
<ul>
	<li>used to store all your linux login credential....so that you don't have to have keep them all memorised.</li>
	<li>used automatically log in to a linux machine, so no need to repetitively type in the username and password</li>
	<li>lets you open up several terminals in a tabbed interface....so that you can work on multiple linux machines simultaneously</li>
</ul>
Scenario 3 -  Access a remote linux server's  terminal from a linux desktop machine

If your local machine is a linux machine, then all you need to do is open a terminal (as described in scenario 1), and then use the ssh command to establish a remote linux server connection. After that you can manage the remote server as if you are accessing it directly.

Note: This scenario isn't very common because most desktop machines in a corporate workplace run on windows (rather than linux), and only the servers run on linux.

[vision_content_box style="autumn" title="Content box title"] Awesome content goes here. [/vision_content_box]

Scenario 4 (more advanced)- Create a connection between 2 remote linux servers

However ssh is often used in shell scripting, and when you want to remotely connect to a linux server from another linux server.]]></Content>
		<Date><![CDATA[2014-01-05]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Centos|gnome|putty|RHEL|ssh]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[sample code to test syntax highligter plugin]]></Title>
		<Content><![CDATA[[powershell]
# this is a sample of powershell code
$currentdirectory = get-location

get-childitem -path $currentdirectory
[/powershell]

http://wordpress.org/plugins/syntaxhighlighter/ 

http://www.viper007bond.com/wordpress-plugins/syntaxhighlighter/adding-a-new-brush-language/ ]]></Content>
		<Date><![CDATA[2014-01-09]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[wordpress]]></Tags>
		<Status><![CDATA[draft]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[What is PowerShell?]]></Title>
		<Content><![CDATA[Powershell is the main command line utility for Microsofts Windows Operating Systems. It comes pre-installed with Windows 7 and onwards. As well as Windows Server 2008 and onwards. It is immensely powerfull and is regularly used my system administrators to automate repetitive windows tasks.

To open up the Powershell Console, go to:

start -> all programs -> accessories -> Windows PowerShell

You might find 2 pairs of links, those tagged with x86, and those without. The x86 are actually the 32bit versions and should be avoided. The "PowerShell ISE" is actually a more feature reach version of the standard PowerShell console, and is as an IDE for writing PowerShell scripts.

Tip, you should also do the following: 

<ol>
	<li>right click on the powershell icon</li>

	<li>go to properties,</li>

	<li>select the shortcut tab</li>

	<li>click on the "advanced" button</li>

	<li>enable the "Run as Administrator" checkbox</li>

	<li>Then apply and save changes</li>
</ol>


The latest version of powershell is v4.0. To see what version you have installed, do:


<pre>
PS C:\=> $PSVersionTable

Name                           Value
----                           -----
PSVersion                      4.0
WSManStackVersion              3.0
SerializationVersion           1.1.0.1
CLRVersion                     4.0.30319.18444
BuildVersion                   6.3.9600.16406
PSCompatibleVersions           {1.0, 2.0, 3.0, 4.0}
PSRemotingProtocolVersion      2.2
</pre>


If you or using an older version, then you can upgrade to the latest version by installing the latest <a href="https://www.microsoft.com/en-us/download/details.aspx?id=48729">Windows Management Framework</a>.

In this course we will be showing you how to write and run powershell scripts. However, by default PowerShell's script running capability is disabled (aka restricted). You can check what this setting is by running the following command:



<pre>PS C:\Windows\system32=> Get-ExecutionPolicy
Restricted</pre>



To follow this course, you need to change this to unrestricted, like this:

<pre>
PS C:\> Set-ExecutionPolicy -ExecutionPolicy unrestricted
PS C:> Get-ExecutionPolicy
Unrestricted
</pre>


You only need to do this once, and your machine will remember it, even if you restart it.

We'll explain more about <a title="PowerShell – Security Alert" href="http://codingbee.net/tutorials/powershell/powershell-security-alert/">Execution Policies</a> later in the course, but for now just accept this in order to get started.

Another key feature that is disabled by default is PowerShell's ability to send commands to other windows machines and run those commands remotely. This feature is immensely powerful because it allows Windows SysAdmins to remotely manage an army of windows machines from a single place. We'll explain more about <a href="http://codingbee.net/tutorials/powershell/powershell-run-commands-on-remote-machines/">remoting</a> later in the course, but for now, all you need to do is:


<pre>
PS C:\Windows\system32=> enable-psremoting
</pre>

(You may need to say "Y" to several prompts.)

Once again, you only need to do this once, and your machine will remember it, even if you restart it.

PowerShell comes with a handy built-in help guide. This guide is an comprehensive reference manual that tells you everything you need to know about PowerShell. However this guide gets regularly updated and to ensure you have the most up-to-date guide installed, simply run:


<pre>
PS C:\Windows\system32=> Update-Help
</pre>


We'll cover more about how to use this built-in help guide in the next lesson.]]></Content>
		<Date><![CDATA[2014-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell - Using the builtin help system]]></Title>
		<Content><![CDATA[<h2>Accessing the built-in help</h2>
Powershell comes with a builtin help manual, which is really useful and is something you will rely on heavily.

There are hundreds of commands at your disposal, you can view a whole list of them by running the get-command command:


<pre>PS C:\Windows\system32=> Get-Command 

CommandType     Name                                               ModuleName
-----------     ----                                               ----------
Alias           Begin-WebCommitDelay                               WebAdministration
Alias           End-WebCommitDelay                                 WebAdministration
Alias           Resize-Bitmap                                      Pscx
Function        A:
Function        Add-DirectoryLength                                Pscx
.
.
.
....etc
</pre>

You'll notice there are different types of commands, functions, aliases, and cmdlets. However the majority of them are "cmdlets" and Cmdlets are what we traditionally recognize as normal commands. Powershell uses a "verb-noun" structure for it's cmdlets. E.g. get-service, stop-process, and etc.

You can view powershell's built-in help guide for a given cmdlet using the get-help cmdlet. For example if you want to look up the help info for get-service command then do:


<pre>PS C:\Windows\system32=> help Get-Service

NAME
    Get-Service

SYNOPSIS
    Gets the services on a local or remote computer.
.
.
.
...etc</pre>


However, this doesn't give the full info. You need to use the "full" option to get all the info:


<pre>get-help -name get-service -full</pre>


A small problem with using get-help is that it usually shows the very bottom of the help page when you run this cmdlet, which means that you often have to scroll to the top every time you run this command, which can be annoying. One way to avoid this is by using the "help" instead of get-help:


<pre>get-help get-service</pre>


This only works in the powershell console, and not the powershell ise. If you are using the Powershell ISE, then another option is to use on the following get-help options:


<pre>get-help get-service -showWindows   # This displays the help info in a pop-up window
get-help get-service -online        # This displays the help info in web browser</pre>


"help" is actually a function (hence it doesn't follow the verb-noun structure), and in this case, it acts as a wrapper for the get-help cmdlet, it does all the same things as get-help, and the only difference is that default's to the top of the page rather than the bottom.

An alternative to using "help" is to use:


<pre>man get-service</pre>


"man" is actually just another name for the "help" function. In other word's "man" is an alias for "help". A lot of people use man, because man is the main linux command for accessing the built-in help on linux machines.

To summarise, "man" is an alias of the "help" function which in turn is a wrapper for the get-help command, which you can see here:



<pre>
PS C:\Windows\system32=> Get-command -Name man

CommandType     Name                                               ModuleName
-----------     ----                                               ----------
Alias           man -=> help                                                           

PS C:\Windows\system32=> Get-command -Name help

CommandType     Name                                               ModuleName
-----------     ----                                               ----------
Function        help                                                                  

PS C:\Windows\system32=> Get-command -Name Get-Help

CommandType     Name                                               ModuleName
-----------     ----                                               ----------
Cmdlet          Get-Help                                           Microsoft.PowerS...
</pre>

If you want to get a list of all the help pages available, then do:


<pre>
Get-Help *  # Lists all help topics that are available.
</pre>


A large number of the help pages are not tied to particular cmdlet, but covers more general information and concepts in PowerShell, e.g. try:


<pre>PS C:\Users\Mir=> help about_aliases</pre>


<h2>Understanding the help pages</h2>
The builtin-help has a lot of valuable information, and it's important to understand how to read the help pages to make the most it.

While not always true, a cmdlet's (full) help page usually contains several sections:
<ol>
	<li>Synopsis  - a one line description of a cmdlet</li>
	<li>Syntax - explain how to write the command (more on this later).</li>
	<li>Description - describes what the command does.</li>
	<li>Parameters - Gives info about the available cmdlet parameters.</li>
	<li>Inputs - Describes what can be piped into the command</li>
	<li>Outputs - Describes what type of object is outputted.</li>
	<li>Notes - Gives extra info about the command, e.g. if it has any aliases.</li>
	<li>Examples - Gives examples on how to use this command.</li>
	<li>RelatedLinks - Gives info about other related commands.</li>
</ol>
<h3>The "Syntax" section</h3>
At first sight the syntax section looks really complicated and scary. However it gets a lot easier once you understand how to read it.

To understand this, let's look at the help page for the move-item cmdlet:

<pre>
PS C:\Windows\system32=> Get-Help Move-Item -full
</pre>


The move-item cmdlet is used for moving files/folders (Note, if you want to copy files/folders then you need to use the copy-item cmdlet instead).

The move-item, syntax section has 2 entries:

<a href="http://codingbee.net/wp-content/uploads/2014/06/move-item-syntax.png"><img class="alignnone size-full wp-image-998" src="http://codingbee.net/wp-content/uploads/2014/06/move-item-syntax.png" alt="" width="710" height="130" /></a>

Let's ignore the second entry for now, and just look at the first entry.

As you can see the move item, has quite a lot of options, e.g. -Path, -Destination....etc. The help page's Parameter section describes what these parameters are along with additional. For the time being let's take look at the -Path and -Destination:

<a href="http://codingbee.net/wp-content/uploads/2014/06/RpT9J3B.png"><img class="alignnone size-full wp-image-999" src="http://codingbee.net/wp-content/uploads/2014/06/RpT9J3B.png" alt="" width="676" height="127" /></a>

&nbsp;

<a href="http://codingbee.net/wp-content/uploads/2014/06/hp1bj9d.png"><img class="alignnone size-full wp-image-1000" src="http://codingbee.net/wp-content/uploads/2014/06/hp1bj9d.png" alt="" width="710" height="169" /></a>

&nbsp;

When you want to move a file from one location to another, then there 2 things you have to tell the move-item command, where the file is currently located (by passing a value into the  "-Path" parameter) and where to move it to (by passing a value into the  "-Destination" parameter).

For example:

[powershell]
PS C:\Users\Public=&gt; Move-Item -Path C:\Users\Public\testfile.txt -Destination C:\Temp
[/powershell]

Now lets look at the syntax info again:

<a href="http://codingbee.net/wp-content/uploads/2014/06/move-item-syntax.png"><img class="alignnone size-full wp-image-998" src="http://codingbee.net/wp-content/uploads/2014/06/move-item-syntax.png" alt="" width="710" height="130" /></a>

The info for the first parameter is shown as:

[-Path] &lt;string[]=>

The "&lt;string[]=>" means that the Path can accept a value in the form of a string. the empty [] means that the path can also accept a string of arrays.  This means that move-item can pickup multiple items and mover them to the same location:

[powershell]
PS C:\Users\Public&amp;gt; Move-Item -Path C:\Users\Public\testfile.txt,C:\Users\Mir\Desktop\testfile2.txt -Destination C:\Temp
[/powershell]

The square brackets around -Path, i.e "[-Path]", means that, for your convenience, you can omit typing "-Path" into your command, and powershell will assume you meant "-path" instead. That is,  you can rewrite:

[powershell]
PS C:\Users\Public=&gt; Move-Item -Path C:\Users\Public\testfile.txt -Destination C:\Temp
[/powershell]

as:

[powershell]

PS C:\Users\Public=&gt; Move-Item C:\Users\Public\testfile.txt -Destination C:\Temp

[/powershell]

This means that you don't have to do as much typing and you are being more implicit than explicit, however it also makes it a little harder to understand what your command is doing. That's why I prefer writing my commands in the long-hand form.

&nbsp;

Note, this shorthand system only works if you specify the short-handed parameter values in the same order as shown in the syntax. I.e. the "-path" parameter is specified first, so this means that the following will fail:
[powershell]
PS C:\Users\Public&amp;gt; Move-Item -Destination C:\Temp C:\Users\Mir\Desktop\testfile2.txt
[/powershell]
The above failed because we specified the -path parameter's value as the second value in the string (destination's value being first). To fix this, we can either rearrange the ordering of the parameters declared, by making the -path parameter's value first, or include the "-path" into the command, or do both.

For this reason, any parameter, where you don't have to explicitly specify the parameter's name in the command, is referred to as a "positional parameter", and it must be declared in the 1st position, when declaring implicitly. You can confirm the position by looking at the parameter section of the help page again:

<a href="http://codingbee.net/wp-content/uploads/2014/06/p184BB4.png"><img class="alignnone size-full wp-image-1001" src="http://codingbee.net/wp-content/uploads/2014/06/p184BB4.png" alt="" width="677" height="124" /></a>

In general, anything that is encased in square brackets, indicates that it is optional. The only exception to this is when we have empty square bracket, [], which denotes arrays.

Notice that while [-path] is optional, it's corresponding value &lt;string[]=> is not encased in square brackets, that means that you have to declare the -path parameter's value every time, i.e. it is mandatory. That's why parameters like this are referred to as a "mandatory positional parameters". Again you can confirm this in the parameter section of the help page:

<a href="http://codingbee.net/wp-content/uploads/2014/06/TyENxcR.png"><img class="alignnone size-full wp-image-1002" src="http://codingbee.net/wp-content/uploads/2014/06/TyENxcR.png" alt="" width="677" height="124" /></a>

Now let's take a look at syntax info for the -destination parameter:

[[-Destination] &lt;String=>]

here's you'll see that note only is the this parameter also a positional parameter (since "destination" is encased in square brackets), but the outer bracket indicates that the whole thing is optional, i.e. this is a "optional positional parameter". How is this possible? How will move-item know where to send the file, if you don't provide a destination path? That's because the destination parameter has a default value which is the current directory. Once again, you can confirm this in the parameter section of the help page:

<a href="http://codingbee.net/wp-content/uploads/2014/06/u6CWikE.png"><img class="alignnone size-full wp-image-1003" src="http://codingbee.net/wp-content/uploads/2014/06/u6CWikE.png" alt="" width="690" height="167" /></a>

&nbsp;

based on all this information, you can rewrite the following long-form command:
[powershell]
PS C:\Temp&amp;gt; Move-Item -Path C:\Users\Public\testfile.txt -Destination C:\Temp
[/powershell]
....into a more shorthand form by implicitly declaring the parameter values:
[powershell]
PS C:\Temp&amp;gt; Move-Item C:\Users\Public\testfile.txt C:\Temp
[/powershell]
but since we are already in the destination folder, we can write this in even more shorthand mode:
[powershell]
PS C:\Temp&amp;gt; Move-Item C:\Users\Public\testfile.txt
[/powershell]
If you check the notes section of the move-item help page, you'll find that move-item has an alias "mv", this mean's that you can write this command in an even more shorthand mode:
[powershell]
PS C:\Temp&amp;gt;  mv C:\Users\Public\testfile.txt
[/powershell]
Earlier you will have noticed that the move-item command has 2 syntax. That's because move-item actually has 2 operational modes. If you closely compare the 2 entries, you'll notice a subtle difference:

<a href="http://codingbee.net/wp-content/uploads/2014/06/XSiABAL.png"><img class="alignnone size-full wp-image-1005" src="http://codingbee.net/wp-content/uploads/2014/06/XSiABAL.png" alt="" width="715" height="123" /></a>

Here this means, that if you don't declare the path parameter value, but explicitly declare the LiterallyPath parameter, then you are actually activating the second mode. Notice that -Path and -LiteralPath are both mandatory, but only mandatory in their respective modes, and the one you declare will dictate which mode move-item operates in. Hence you can't declare both of them at the same time, because that would be meaningless and will result in an error.

&nbsp;

Another good example of a command that has multiple operational modes, is the command get-help command itselft:

<a href="http://codingbee.net/wp-content/uploads/2014/06/q8YaIQk.png"><img class="alignnone size-full wp-image-1006" src="http://codingbee.net/wp-content/uploads/2014/06/q8YaIQk.png" alt="" width="716" height="435" /></a>

Here it's more obvious why there are more modes. For example you can't view the "-full" help page, and also only view the "-examples" help age.

These "operational mode" controlling parameters are quite hard to identify by reading the help pages, you have to look really closely to spot which parameters are controlling the operational mode, luckily there is a way to see them more clearly with the help of show-command, e.g. :

[powershell]
show-command -Name Get-Help
[/powershell]

This opens up a gui window where the tabs along the top shows the various operation mode of the given command. 

show-command is a tool that you can use to write your commands via a user-friendly gui interface, which can be handy if you are struggling to write your commands.  

Parameter values for a given parameter can take many forms (i.e. datatypes). E.g they can be a boolean, string, integer, or a custom object. In fact, in powershell,  all data types are a kind of an object. We will cover more about "objects" in later lesson.

Tip: The PowerShell console can get a little cluttered when you use it for viewing help pages. In such instances you can clear your console using the clear-host cmdlet. ]]></Content>
		<Date><![CDATA[2014-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell - Running commands]]></Title>
		<Content><![CDATA[There are different types of commands that you can run on the PowerShell Command line:
<ul>
	<li>cmdlet - Name for any powershell builtin command, e.g. get-service, set-location, get-childitem....etc</li>
	<li>aliases - these are nicknames for other commands. You can view a list of aliases using get-alias. It's best to avoid aliases where possible.</li>
	<li>function - Small blocks of powershell codes that are found in powershell scripts. You can also load a function into the current powershell session so that you can call the function in the same was as an ordinary cmdlet. A PowerShell  session comes preloaded with a default set of functions, for example the "help" function that we saw earlier.</li>
	<li>workflow - A special kind of function, which we'll cover later on.</li>
	<li>application - an external application. E.g. try running "notepad" or "mspaint" straight from the command line.</li>
</ul>
The word "command" is a generic term that and this course is used to refer to any of the above.

There are a large number of aliases you can use instead of the typing out the whole cmdlet, here are some commonly used aliases:


<pre>CommandType     Name                                               ModuleName
-----------     ----                                               ----------
Alias           % -=> ForEach-Object
Alias           ? -=> Where-Object
Alias           ac -=> Add-Content
Alias           asnp -=> Add-PSSnapin
Alias           cat -=> Get-Content
Alias           cd -=> Set-Location
Alias           chdir -=> Set-Location
Alias           clc -=> Clear-Content
Alias           clear -=> Clear-Host
Alias           clhy -=> Clear-History
Alias           cli -=> Clear-Item
Alias           clp -=> Clear-ItemProperty
Alias           cls -=> Clear-Host
Alias           clv -=> Clear-Variable
Alias           cnsn -=> Connect-PSSession
Alias           compare -=> Compare-Object
Alias           copy -=> Copy-Item
Alias           cp -=> Copy-Item
Alias           cpi -=> Copy-Item
Alias           cpp -=> Copy-ItemProperty
Alias           curl -=> Invoke-WebRequest
Alias           cvpa -=> Convert-Path
Alias           dbp -=> Disable-PSBreakpoint
Alias           del -=> Remove-Item
Alias           diff -=> Compare-Object
Alias           dir -=> Get-ChildItem
Alias           dnsn -=> Disconnect-PSSession
Alias           ebp -=> Enable-PSBreakpoint
Alias           echo -=> Write-Output
Alias           epal -=> Export-Alias
Alias           epcsv -=> Export-Csv
Alias           epsn -=> Export-PSSession
Alias           erase -=> Remove-Item
Alias           etsn -=> Enter-PSSession
Alias           exsn -=> Exit-PSSession
Alias           fc -=> Format-Custom
Alias           fl -=> Format-List
Alias           foreach -=> ForEach-Object
Alias           ft -=> Format-Table
Alias           fw -=> Format-Wide
Alias           gal -=> Get-Alias
Alias           gbp -=> Get-PSBreakpoint
Alias           gc -=> Get-Content
Alias           gci -=> Get-ChildItem
Alias           gcm -=> Get-Command
Alias           gcs -=> Get-PSCallStack
Alias           gdr -=> Get-PSDrive
Alias           ghy -=> Get-History
Alias           gi -=> Get-Item
Alias           gjb -=> Get-Job
Alias           gl -=> Get-Location
Alias           gm -=> Get-Member
Alias           gmo -=> Get-Module
Alias           gp -=> Get-ItemProperty
Alias           gps -=> Get-Process
Alias           group -=> Group-Object
Alias           gsn -=> Get-PSSession
Alias           gsnp -=> Get-PSSnapin
Alias           gsv -=> Get-Service
Alias           gu -=> Get-Unique
Alias           gv -=> Get-Variable
Alias           gwmi -=> Get-WmiObject
Alias           h -=> Get-History
Alias           history -=> Get-History
Alias           icm -=> Invoke-Command
Alias           iex -=> Invoke-Expression
Alias           ihy -=> Invoke-History
Alias           ii -=> Invoke-Item
Alias           ipal -=> Import-Alias
Alias           ipcsv -=> Import-Csv
Alias           ipmo -=> Import-Module
Alias           ipsn -=> Import-PSSession
Alias           irm -=> Invoke-RestMethod
Alias           ise -=> powershell_ise.exe
Alias           iwmi -=> Invoke-WmiMethod
Alias           iwr -=> Invoke-WebRequest
Alias           kill -=> Stop-Process
Alias           lp -=> Out-Printer
Alias           ls -=> Get-ChildItem
Alias           man -=> help
Alias           md -=> mkdir
Alias           measure -=> Measure-Object
Alias           mi -=> Move-Item
Alias           mount -=> New-PSDrive
Alias           move -=> Move-Item
Alias           mp -=> Move-ItemProperty
Alias           mv -=> Move-Item
Alias           nal -=> New-Alias
Alias           ndr -=> New-PSDrive
Alias           ni -=> New-Item
Alias           nmo -=> New-Module
Alias           npssc -=> New-PSSessionConfigurationFile
Alias           nsn -=> New-PSSession
Alias           nv -=> New-Variable
Alias           ogv -=> Out-GridView
Alias           oh -=> Out-Host
Alias           popd -=> Pop-Location
Alias           ps -=> Get-Process
Alias           pushd -=> Push-Location
Alias           pwd -=> Get-Location
Alias           r -=> Invoke-History
Alias           rbp -=> Remove-PSBreakpoint
Alias           rcjb -=> Receive-Job
Alias           rcsn -=> Receive-PSSession
Alias           rd -=> Remove-Item
Alias           rdr -=> Remove-PSDrive
Alias           ren -=> Rename-Item
Alias           ri -=> Remove-Item
Alias           rjb -=> Remove-Job
Alias           rm -=> Remove-Item
Alias           rmdir -=> Remove-Item
Alias           rmo -=> Remove-Module
Alias           rni -=> Rename-Item
Alias           rnp -=> Rename-ItemProperty
Alias           rp -=> Remove-ItemProperty
Alias           rsn -=> Remove-PSSession
Alias           rsnp -=> Remove-PSSnapin
Alias           rujb -=> Resume-Job
Alias           rv -=> Remove-Variable
Alias           rvpa -=> Resolve-Path
Alias           rwmi -=> Remove-WmiObject
Alias           sajb -=> Start-Job
Alias           sal -=> Set-Alias
Alias           saps -=> Start-Process
Alias           sasv -=> Start-Service
Alias           sbp -=> Set-PSBreakpoint
Alias           sc -=> Set-Content
Alias           select -=> Select-Object
Alias           set -=> Set-Variable
Alias           shcm -=> Show-Command
Alias           si -=> Set-Item
Alias           sl -=> Set-Location
Alias           sleep -=> Start-Sleep
Alias           sls -=> Select-String
Alias           sort -=> Sort-Object
Alias           sp -=> Set-ItemProperty
Alias           spjb -=> Stop-Job
Alias           spps -=> Stop-Process
Alias           spsv -=> Stop-Service
Alias           start -=> Start-Process
Alias           sujb -=> Suspend-Job
Alias           sv -=> Set-Variable
Alias           swmi -=> Set-WmiInstance
Alias           tee -=> Tee-Object
Alias           trcm -=> Trace-Command
Alias           type -=> Get-Content
Alias           wget -=> Invoke-WebRequest
Alias           where -=> Where-Object
Alias           wjb -=> Wait-Job
Alias           write -=> Write-Output
</pre>

If you have experience in using the linux command line, then a lot of these aliases will be familiar to you.

Sometime it can be difficult to write a command even after reading the help pages, that's why powershell comes with a gui wizard in the form of show-command that helps you to write your commands. E.g. if you need help writing a move-item command, then just do:


<pre>show-command -Name Move-Item</pre>

]]></Content>
		<Date><![CDATA[2014-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell - Working with providers]]></Title>
		<Content><![CDATA[Chapter 5 - Working with providers


Wordpress has things called plugins, which can extend wordpress's features. Powershell has similar features which are called snapins and modules.

In PS, we have things called "PSDrives". These are a bit like virtual hard drives (vhdds). Some of these virtual hdds stores a normal files system, that contains files and folders, whereas others are specialized "storage containers", which can store all kinds of things, that are not of the traditional files/folders structure. So in other words we have different types of psdrives. These types have names called "psproviders". To view a list of all the various types of psproviders do:

<pre>
PS C:\> Get-PSProvider
Name            Capabilities                           Drives
----            ------------                           ------
Alias           ShouldProcess                          {Alias}
Environment     ShouldProcess                          {Env}
FileSystem      Filter, ShouldProcess, Credentials     {C, E, A, D}
Function        ShouldProcess                          {Function}
Registry        ShouldProcess, Transactions            {HKLM, HKCU}
Variable        ShouldProcess                          {Variable}
Certificate     ShouldProcess                          {Cert}
WSMan           Credentials                            {WSMan}
</pre>

Here we have several types of psdrives, alias, environment, filesystem.....and etc. 

As you can see, 4 of our psdrives (C, E, A, D) are of the psprovider type of "filesystem". These are like the traditional hdds. 



So PSProviders are essentially storage containers, where each of these containers behave like virtual hdds, but only a subset do actually have a hdd like folder-tree/file appearance. 

But remember, all psdrives, (regardless of what psprovider type they are) are essentially) hdds. For that reason, it means you can cd into them, e.g.:

<pre>
PS C:\> cd Env:\
PS Env:\>
</pre>

Like wise, for the same reason, you can also do "ls":

<pre>
PS Env:\> ls

Name                           Value
----                           -----
ACPath                         C:\Program Files (x86)\Lenovo\Access Connections\
ALLUSERSPROFILE                C:\ProgramData
APPDATA                        C:\Users\schowdhury\AppData\Roaming
CommonProgramFiles             C:\Program Files\Common Files
CommonProgramFiles(x86)        C:\Program Files (x86)\Common Files
CommonProgramW6432             C:\Program Files\Common Files
COMPUTERNAME                   ND26460
ComSpec                        C:\WINDOWS\system32\cmd.exe
deployment.expiration.check... false
FP_NO_HOST_CHECK               NO
HOMEDRIVE                      C:
HOMEPATH                       \Users\schowdhury
LOCALAPPDATA                   C:\Users\schowdhury\AppData\Local
LOGONSERVER                    \\OSDC01
.                                                                                         
.
....etc.
</pre>



Since the "env" psdrive is not a typical hdd, it means that it shows us whatever is inside it. As it turns out, this drives contains an associative array. In powershell, there is a quick-hand way to access a key's value (from a psdrive that has an associative arrays stored in it):

<pre>PS C:\> $env:COMPUTERNAME	
codingbee
</pre>


<pre>PS C:\> Get-PSDrive
Name           Used (GB)     Free (GB) Provider      Root                                               CurrentLocation
----           ---------     --------- --------      ----                                               ---------------
A                                      FileSystem    A:\
Alias                                  Alias
C                  34.68          5.22 FileSystem    C:\
Cert                                   Certificate   \
D                                      FileSystem    D:\
E                   2.00         38.00 FileSystem    E:\
Env                                    Environment
Function                               Function
HKCU                                   Registry      HKEY_CURRENT_USER
HKLM                                   Registry      HKEY_LOCAL_MACHINE
Variable                               Variable
WSMan                                  WSMan
</pre>

We previously mentioned that a psprovider is a way to describe what type a psdrive is. Actually it is much more than that. A psprovider acts as a connector between the vhdd and the machine. This means that if the "alias" psprovider connector is used....then that vhdd becomes a psdrive of the "alias" psprovider type. This means that the psprovider that is used to connect the machine to the psdrive, essentially defines the type of that psdrive. Before a psdrive is connected, you can think of them as generic empty shell, that providers use to create psdrives. 

This means that the "get-psprovider" actually shows the list of available psproviders (connectors).....whereas "get-psdrive" shows all the available vhdds that have already been connected, along with the (ps)provider used to connect them.  

An analogy of a psprovider is essentially a bunch of cables, e.g. ethernet,hdmi,usb,scart....etc. 

Now you can make new psproviders (e.g. sata) available to powershell. This is usually done in one of the following ways:

	- installing a module  (covered later)
	- installing a snap-in  (covered later)
	- enabling a feature

Going back to the output of get-provider:
	
PS C:\> Get-PSProvider
Name                 Capabilities                                      Drives
----                 ------------                                      ------
Alias                ShouldProcess                                     {Alias}
Environment          ShouldProcess                                     {Env}
FileSystem           Filter, ShouldProcess, Credentials                {C, E, A, D}
Function             ShouldProcess                                     {Function}
Registry             ShouldProcess, Transactions                       {HKLM, HKCU}
Variable             ShouldProcess                                     {Variable}
Certificate          ShouldProcess                                     {Cert}
WSMan                Credentials                                       {WSMan}	
	
Here is a quick quide on what each of the capabilities means:

	shouldProcess - means that the provider supports the -whatif/-confirm parameters. 
	filter        - means that the provider supports the -filter parameter.
	credentials   - means that the provider lets you specify alternate credentials when connecting to data stores. There's the "-credential"
					parameter for this. 
	Transactions  - Means that the provider supports the use of transactions, which allows you to use the provider to make several changes,
					and then either rolls back or commit those changes as a single unit. 
	
	
To see and manipulate the data in each psdrive, you use commands that contains the word "item" in the second part of the name (this actually is a rule of thumb). To list these commands simply do:

PS C:\> get-command -noun *item*

However each psprovider's drives are essentially different tech, so each have their own quarks and while some commands work for one psprovider's psdrives, that might not be the case for another psprovider's psdrive. 

In powershell, an "item" is a generic term that refer to a folder or file.

New-Item testfile.txt -ItemType file 			#this creates a new file.
New-Item testfolder -ItemType directory 		#this creates a file.

new-item is effectively the "touch" and "mkdir" command combined into one. Files and folders are not allowed to have special characters  in their name (e.g. * and ?).  


However other items (e.g. registry items) are allowed to have wild cards in their name (and we set them using the "Set-ItemProperty" command - which is covered later). as a result, we have 2 parameter options for the "set-itemproperty" command:

-path 			# this allows wildcards
-literalpath 	# this doesn't allow parameters

now to alter a property of a registry, you use the following command:

<pre>Set-ItemProperty -Path {path-to-item} -PSProperty {name of item's property} -Value {new value for the property}
</pre>
Software vendors takes advantage of the concept of providers to allow powershell to change their software's settings. Providers can be used a bit like an API.

]]></Content>
		<Date><![CDATA[2014-07-27]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell - Combine commands together using pipes]]></Title>
		<Content><![CDATA[compare-object # similar to the diff command. but has a lot more feature for comparing tabular data. Hence can be used to compare
# what is and is-not installed as well as versions for ordnance survey.

<pre>## Redirection
Get-ChildItem | Out-File testfile.txt # This is the same as "ls -l=> testfile.txt"

Get-ChildItem | Out-File testfile.txt -append # This is the same as "ls -l =>=> testfile.txt"
</pre>

If you don't specify the out-file command, then powershell will always assume the default which is "out-default", which in turn sends it to "out-host". "Out-host" is another name for the command line terminal.

<pre>measure-object # equivalent to "wc -l"
</pre>


<pre>Get-ChildItem | Out-null # same as: ls -l =>/dev/null
</pre>

Note, if you want to discard error messages then, at the end of the command, add "2=>&amp;1 | Out-Null". This is equivalent to Linux's "2=>&amp;1 1=>/dev/null". For example:

<pre>Remove-Item C:\WINDOWS\system32\WindowsPowerShell\v1.0\Modules\pscx -Recurse 2=>&amp;1 | Out-Null
</pre>
In the above example, we are deleting a folder, which might, or might not exist in the first place.

Note you can also do:

<pre>Remove-Item C:\WINDOWS\system32\WindowsPowerShell\v1.0\Modules\pscx -Recurse 2=>Out-Null
</pre>

<pre>get-content .\testfile.txt | ConvertTo-Html | Out-File homepage.html # This takes the content of a file (which in this case
# contains ls -l data) and outputs as html.
</pre>
there are other convert-to... commands. E.g. you can do:

get-service | ConvertTo-Csv # this outputs to the terminal

get-service | ConvertTo-Csv | out-file testfile.csv # This creates a csv file, a shorter way to write this is like this:

get-service | Export-Csv testfile.csv

Commands that are from the same family are better at communicating with each other:

Get-Process -name notepad++ | Stop-Process

Here we are closing the notepad++ app. and the 2 "process" commands are working well together. The linux equivalent is a 2 step command:

ps -aux | grep -v grep | grep 'notepad++' # to identify the process id. Then do:
kill 9 pid

You can fit this into a single command using awk, but takes longer to write that command. You should always be careful when using commands like stop-process and stop-service.

Windows has an "are you sure" system. You can see the current-level of the are-you-sure system like this:

PS C:\Users\mir\Desktop=> $ConfirmPreference
High

You can change the threshold this like this:
$ConfirmPreference="low"
PS C:\Users\mir\Desktop=> $ConfirmPreference
Low

For more info, see:
help about_Preference_Variables

You can manually make a command ask the are-you-sure command like this:

Get-Process -name notepad++ | Stop-Process -Confirm

This will open a are-you-sure popup window.

There is also the what-if command:

Get-Process -name notepad++ | StopProcess -whatif
What-if: Performing operation "StopProcess"
on Target "notepad++ (7596)".

This option shows what would happen if you run the command, without actually running it.]]></Content>
		<Date><![CDATA[2014-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell - Extend PowerShell's capability by installing Modules and Snapins]]></Title>
		<Content><![CDATA[When you install a new app on your smart phone, you actually ending enabling your phone to do more things that it originally couldn't do out of the box. In powershell we can add features (e.g. new cmdlets) to powershell in the same way, but by installing extensions. There are two types of extension that you install:

<ul>
	<li>PSSnapin</li>
	<li>modules </li>
</ul>

Modules are are better than PSSnapin, because they are more self-contained and easier to distribute, also they don't require advanced registration, instead you simply drop them in certain directories, which is covered later)

<h2>PSSnapins</h2>
first lets talk about PSSnapins

<pre>Get-PSSnapin -Registered</pre>

This lists all the available snapins that have been installed (regardless of whether they are active or not) but excludes snapins that came pre-installed with powershell.

Note, I think PSSnapin gets installed as part of a much larger software, e.g. things like microsoft sql developer, sharepoint,....etc. I don't think they come as standalones that you can download from the internet.

This lists all the snapins that are currently running (including snapins that came preinstalled with powershell):

<pre>Get-PSSnapin</pre>

If the above command shows something that you want to load, then you can load it using the following command:

<pre>
add-pssnapin -name {snapinname}
</pre>


Installing a snapin introduces one or both of the following:


<ul>
	<li>psdrive/psprovider</li>
	<li>cmdlets</li>
</ul>

To see what new commands a snapin has introduced, do:

<pre>Get-Command -name PSSnapin {snapin's name}</pre>

Note: the above doesn't work. 

Unfortunately there are no way to to see what psdrives have been added by a PSSnapin. The only way to check this by first running the following command before installing the PSSnapin:

<pre>
get-psprovider
</pre>


This should output something like:

<pre>
PS C:\> Get-PSProvider

Name                 Capabilities                                      Drives
----                 ------------                                      ------
WSMan                Credentials                                       {WSMan}
Alias                ShouldProcess                                     {Alias}
Environment          ShouldProcess                                     {Env}
FileSystem           Filter, ShouldProcess                             {C, E, A, D}
Function             ShouldProcess                                     {Function}
Registry             ShouldProcess, Transactions                       {HKLM, HKCU}
Variable             ShouldProcess                                     {Variable}
Certificate          ShouldProcess                                     {cert}
</pre>

And then run the above command again to see what new items are now listed.

Note: get-psprovider is what you use to start exploring the <a href="http://codingbee.net/tutorials/powershell/powershell-accessing-registry/" title="Powershell – Accessing the registry">registry</a>. This works in the same way as using regedit, but via the command line. 

############ detour - start
Note: A lot of softwares, e.g. scp comes with it's own command line shell. It could be that this command line shell is actually just a powershell
shell with the correct module/snapin preloaded. To check if that is really the case, right click the custom shell icon then do:

properties -=> find-target

Here you will find something like this:

%windir%\system32\WindowsPowershell\v1.0\powershell.exe -noexit -command import-module {module-name}
In these scenarios, instead of running this tool, you can simply open up powershell and run the "import-module {module-name}" command to achieve the same results.
############ detour - end

<h2>modules</h2>

Modules are designed to be a little more:

<ul>
	<li>self-contained</li>
	<li>easier to distribute</li>
	<li>dont require advanced registration</li>
</ul>

You can find and download modules from the internet, here are some popular module hosting websites:

<a href="http://powershell.codeplex.com/site/search?query=powershell&ac=3">http://powershell.codeplex.com/site/search?query=powershell&ac=3</a>

<a href="http://poshcode.org/">http://poshcode.org/</a>

Note, if you download them from the internet, then you first need to:

rightclick the zipped file -> properties -> unblock. 

Powershell automatically looks for available modules by looking at list of certain paths that are defined in the "psmodulepath" variable (which is stored in the "env" psdrive):

<pre>
Get-Content Env:\PSModulePath
C:\Documents and Settings\SChowdhury\My Documents\WindowsPowerShell\Modules;C:\WINDOWS\system32\WindowsPowerShell\v1.0\Modules\
</pre>

The cool thing about modules is that they are really easy to install (and activate). All you have to do is copy them into one of the folders defined in "Env:\PSModulePath".

Once you have placed the module into one of the "approved" folders, you can then check that the module has been installed and activated by running the following command:


<pre>
PS C:\> Get-Module -ListAvailable

Directory: C:\Users\SChowdhury\Documents\WindowsPowerShell\Modules

ModuleType Name ExportedCommands
---------- ---- ----------------
Script hmrctool {new-hmrcobject, get-maths}

Directory: C:\Program Files (x86)\PowerShell Community Extensions\Pscx3

ModuleType Name ExportedCommands
---------- ---- ----------------
Script Pscx {Add-PathVariable, Clear-MSMQueue, ConvertFrom-Base64, ConvertTo-Base64...}
</pre>


In the above example I also installed a really useful module called "Pscx" (short for "Powershell Community Extensions") which you can download from here:

http://pscx.codeplex.com/

Note: in this instance, the module is provided in the form of an msi file....which makes it even easier by walking you through an installation wizard. This msi file, behind the scene actually, use the unblock-file command (covered later), adds a new approved path to the $env:psmodulepath variable, and then place the module into that path.

Note: whenever you download a module from the internet, Windows tags these downaloads as "internet downloads" and fore safety+security reasons it will not automatically install+activate them. In this you can simply first use unblock-file command.

By the way, if you want to see what new commands are available, you can either do:


<pre>PS C:\> Get-Module -ListAvailable </pre>


This only gives a preview, so you can drill down like this:

<pre>
PS C:\> (Get-Module -ListAvailable | Where-Object -FilterScript {$_.Name -match "Pscx"}).ExportedCommands
</pre>



But a much better way is to simply do:


<pre>PS C:\> Get-Command -Module pscx

CommandType Name ModuleName
----------- ---- ----------
Alias Resize-Bitmap Pscx
Function Add-DirectoryLength Pscx
Function Add-ShortPath Pscx
Function Dismount-VHD Pscx
Function Edit-File Pscx
Function Edit-HostProfile Pscx
Function Edit-Profile Pscx
Function Enable-OpenPowerShellHere Pscx
Function Get-ExecutionTime Pscx
Function Get-Help Pscx
Function Get-Parameter Pscx
Function Get-ScreenCss Pscx
Function Get-ScreenHtml Pscx
Function Get-ViewDefinition Pscx
Function help Pscx
Function Import-VisualStudioVars Pscx
Function Invoke-BatchFile Pscx
Function Invoke-Elevated Pscx
Function Invoke-GC Pscx
Function Invoke-Method Pscx
Function Invoke-NullCoalescing Pscx
Function Invoke-Ternary Pscx
Function less Pscx
Function Mount-VHD Pscx
Function Out-Speech Pscx
Function QuoteList Pscx
Function QuoteString Pscx
Function Resolve-ErrorRecord Pscx
Function Resolve-HResult Pscx
Function Resolve-WindowsError Pscx
Function Set-LocationEx Pscx
Function Set-ReadOnly Pscx
Function Set-Writable Pscx
Function Show-Tree Pscx
Function Start-PowerShell Pscx
Function Stop-RemoteProcess Pscx
Filter New-HashObject Pscx
Cmdlet Add-PathVariable Pscx
Cmdlet Clear-MSMQueue Pscx
Cmdlet ConvertFrom-Base64 Pscx
Cmdlet ConvertTo-Base64 Pscx
Cmdlet ConvertTo-MacOs9LineEnding Pscx
Cmdlet ConvertTo-Metric Pscx
Cmdlet ConvertTo-UnixLineEnding Pscx
Cmdlet ConvertTo-WindowsLineEnding Pscx
Cmdlet Convert-Xml Pscx
Cmdlet Disconnect-TerminalSession Pscx
Cmdlet Expand-Archive Pscx
Cmdlet Export-Bitmap Pscx
Cmdlet Format-Byte Pscx
Cmdlet Format-Hex Pscx
Cmdlet Format-Xml Pscx
Cmdlet Get-ADObject Pscx
Cmdlet Get-AdoConnection Pscx
Cmdlet Get-AdoDataProvider Pscx
Cmdlet Get-Clipboard Pscx
Cmdlet Get-DhcpServer Pscx
Cmdlet Get-DomainController Pscx
Cmdlet Get-DriveInfo Pscx
Cmdlet Get-EnvironmentBlock Pscx
Cmdlet Get-FileTail Pscx
Cmdlet Get-FileVersionInfo Pscx
Cmdlet Get-ForegroundWindow Pscx
Cmdlet Get-Hash Pscx
Cmdlet Get-HttpResource Pscx
Cmdlet Get-LoremIpsum Pscx
Cmdlet Get-MountPoint Pscx
Cmdlet Get-MSMQueue Pscx
Cmdlet Get-OpticalDriveInfo Pscx
Cmdlet Get-PathVariable Pscx
Cmdlet Get-PEHeader Pscx
Cmdlet Get-Privilege Pscx
Cmdlet Get-PSSnapinHelp Pscx
Cmdlet Get-ReparsePoint Pscx
Cmdlet Get-RunningObject Pscx
Cmdlet Get-ShortPath Pscx
Cmdlet Get-TerminalSession Pscx
Cmdlet Get-TypeName Pscx
Cmdlet Get-Uptime Pscx
Cmdlet Import-Bitmap Pscx
Cmdlet Invoke-AdoCommand Pscx
Cmdlet Invoke-Apartment Pscx
Cmdlet Join-String Pscx
Cmdlet New-Hardlink Pscx
Cmdlet New-Junction Pscx
Cmdlet New-MSMQueue Pscx
Cmdlet New-Shortcut Pscx
Cmdlet New-Symlink Pscx
Cmdlet Out-Clipboard Pscx
Cmdlet Ping-Host Pscx
Cmdlet Pop-EnvironmentBlock Pscx
Cmdlet Push-EnvironmentBlock Pscx
Cmdlet Read-Archive Pscx
Cmdlet Receive-MSMQueue Pscx
Cmdlet Remove-MountPoint Pscx
Cmdlet Remove-ReparsePoint Pscx
Cmdlet Resolve-Host Pscx
Cmdlet Send-MSMQueue Pscx
Cmdlet Send-SmtpMail Pscx
Cmdlet Set-BitmapSize Pscx
Cmdlet Set-Clipboard Pscx
Cmdlet Set-FileTime Pscx
Cmdlet Set-ForegroundWindow Pscx
Cmdlet Set-PathVariable Pscx
Cmdlet Set-Privilege Pscx
Cmdlet Set-VolumeLabel Pscx
Cmdlet Skip-Object Pscx
Cmdlet Split-String Pscx
Cmdlet Stop-TerminalSession Pscx
Cmdlet Test-AlternateDataStream Pscx
Cmdlet Test-Assembly Pscx
Cmdlet Test-MSMQueue Pscx
Cmdlet Test-Script Pscx
Cmdlet Test-UserGroupMembership Pscx
Cmdlet Test-Xml Pscx
Cmdlet Write-BZip2 Pscx
Cmdlet Write-Clipboard Pscx
Cmdlet Write-GZip Pscx
Cmdlet Write-Tar Pscx
Cmdlet Write-Zip Pscx
</pre>



Note:  if you run just "get-module" it will only list modules that have been activated. This can be useful to identify any modules that have failed to load properly, as they won't appear in this list.
]]></Content>
		<Date><![CDATA[2014-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell - Data in the form of "objects"]]></Title>
		<Content><![CDATA[In PowerShell, there are lots of commands that gives tabular data, e.g.

<pre>get-process</pre>

In PS, we have some terminologies that describes different parts of these tables:

<ul>
	<li><strong>collection</strong>: This refers to the whole table.</li>
	<li><strong>object</strong>: single row in the table</li>
	<li><strong>property</strong>: column in the table</li>
	<li><strong>method</strong>: action that you can take on a given row (object)</li>
</ul>

Now to see what methods you can apply to each object in get-process, we do:



<pre>
PS C:\> Get-Service | get-member

   TypeName: System.ServiceProcess.ServiceController

Name                      MemberType    Definition
----                      ----------    ----------
Name                      AliasProperty Name = ServiceName
RequiredServices          AliasProperty RequiredServices = ServicesDependedOn
Disposed                  Event         System.EventHandler Disposed(System.Object, System.EventArgs)
Close                     Method        void Close()
Continue                  Method        void Continue()
CreateObjRef              Method        System.Runtime.Remoting.ObjRef CreateObjRef(type requestedType)
Dispose                   Method        void Dispose(), void IDisposable.Dispose()
GetType                   Method        type GetType()
.
...etc.
</pre>

On our machine, one of the get-service collection's object is called "browser":

<strong>
PS C:\> Get-Service -Name browser

Status   Name               DisplayName
------   ----               -----------
Running  browser            Computer Browser
</strong>


Let's say we wan to apply the "GetType" method on this object, then we do this using the following syntax:

<pre>
PS C:\> (Get-Service -Name browser).GetType()

IsPublic IsSerial Name                                     BaseType
-------- -------- ----                                     --------
True     False    ServiceController                        System.ComponentModel.Component
</pre>

This in turn can have it's own methods:

<pre>
PS C:\> (Get-Service -Name browser).GetType() | Get-Member

   TypeName: System.RuntimeType

Name                           MemberType Definition
----                           ---------- ----------
AsType                         Method     type AsType()
Clone                          Method     System.Object Clone(), System.Object ICloneable.Clone()
Equals                         Method     bool Equals(System.Object obj), bool Equals(type o), bool _MemberInfo.Equa...
FindInterfaces                 Method     type[] FindInterfaces(System.Reflection.TypeFilter filter, System.Object f...
</pre>



	
Note: here is another example of using methods, but this time for "get-date". 

<pre>
PS C:\> get-date

03 December 2015 13:37:59


PS C:\> (Get-date).ToShortTimeString()
13:38
PS C:\>
</pre>


Here is another very popular way of accessing a method:

<pre>
PS C:\> $date = Get-Date
PS C:\> $date

03 December 2015 13:38:52

PS C:\> $date.ToShortTimeString()
13:38
PS C:\>
</pre>

In Powershell, a "method" is simply a way of telling the object to do something, usually to itself. This concept, i.e. treating a table as a "collection" of "objects" means that you can work with data more easily (especially when compared to unix where you need to use grep, awk, and sed). In Powershell, you can extract data based on property names, and object's field entries. By default, the get-process
command only shows a subset of properties, because the screen isn't big enough to show all of them. However the command will show all the properties if you pipe it to a file. If you want to see all the available properties and methods for a collection, you need to use the get-member command. E.g. for get-process, you do:

<pre>get-process | get-member</pre>

Note, that sometimes you may need to use "-force" to access to the complete list, i.e.:

<pre>get-process | get-member -force</pre>

Note, everything about a collection, i.e. properties, object, are referred to as a member. Hence that'd where get-member gets it's name from. When you use get-member, you will discover a number of different types of properties:
<ul>
	<li>aliasproperty</li>
	<li>scriptproperty</li>
	<li>noteproperty</li>
	<li>property</li>
</ul>

	
You can ignore the various types and just think of them all as the same thing.

Now lets start by manipulating objects in a table. For get-process, if you want to re-arrange the rows based on the virtualmemory (vm) property, then do this:

<pre>get-process | sort-object -property VM</pre>

This sorts it by vm, followed by id:

<pre>get-process | sort-object -property VM,id</pre>

If you want to specify which property (columns) to output, then do:

<pre>get-process | select-object -Property ProductVersion,FileVersion</pre>

If you want to view the first 10 lines, you can do this using the select-object command:

<pre>
get-process | select-object -Property ProductVersion,FileVersion | select-object -first 10     
</pre>

Similarly for the last 10 lines you do:

<pre>
get-process | select-object -Property ProductVersion,FileVersion | select-object -last 10		</pre>


If you want to filter out rows, like we do in linux using grep, awk, and sed, then we need to use <strong>where-object</strong>. This will be covered later.

Note: When you look at the help files of any command that outputs a tabular output, you will find that they don't give a whole list of all available properties (columns), that's why you have to use the get-member command instead to get all this info.
]]></Content>
		<Date><![CDATA[2014-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell - A deeper understanding of the pipeline]]></Title>
		<Content><![CDATA[Chapter 9 - A deeper understanding of the pipeline


When piping data from one command to the next, there are times when you need to help the receiving command understand what data it is receiving from the inputing pipe. This is done by forwarding the piped data to a command's particular parameter. This concept is known as "pipeline parameter binding". By default, powershell will automatically try to pipe to a command using the first method (aka "ByValue") and if that fails then it will then attempt the 2 method (aka "ByPropertyName"). Hence the powershell makes up to 2 attempts in the following order:

1. Byvalue			(based on object compatibility)
2. ByPropertyName    (based on property names, might need to create a hashtable to simulate the match)
3. Feed data directly into a parameter (e.g. an array using something like $(get-content computerlist.txt))

Plan A - Byvalue 
When you do a "get-member", you will find something called "typename" near the top. This basically defines an object to belong to a specific object type.

For example if you want to find out whether get-process can feed into stop-process, first do:

Get-Process | gm | Select-Object -First 10


the above will display the typename to be:

	TypeName: System.Diagnostics.Process

You can also find this in the full help page, i.e. do "help get-process -full", and then go down to the "INPUTS" section:

	INPUTS
		System.Diagnostics.Process
			You can pipe a process object to Get-Process.
  
Now see the full help for stop-process, and in the input section you will find:

	INPUTS
		System.Diagnostics.Process

You can pipe a process object to Stop-Process. 

However the above only tells you what types of objects (i.e. typenames) it can accept from the incoming pipeline, it doesn't actually explain what it does with the input it receives.  To find this out, you need to view the command's full help and look up the description for the 
is for the "InputObject" parameter:

InputObject <Process[]>						# Here, "process" is actually a name of a typename (i.e. object type). 
	Stops the processes represented by the specified process objects. Enter	a variable that contains the objects, or type a command or expression that
	gets the objects.

	Required? true
	Position? 1
	Default value
	Accept pipeline input? true (ByValue)
	Accept wildcard characters? false

This means that this command can accept any object of the type "process".

	

I think that all commands that can accept an input, has a parameter called "InputObject", which tells you which parameter is mapped to incoming
piped data. So if data is different to what it is expecting then it will not work. If there is a mismatch using the byvalue system, then ps will try then try to do complete piping using Plan B:


  
Plan B: piping using ByPropertyName
This occurs when plan A fails. It basically looks for a name-match between an outgoing property's (column) name and the receiving command's parameter name. E.g. if command A generates a collection (table) with a property (column) called "name", and command B has a parameter with the name "-name", then it will pipe it to that parameter. 

However this will fail if the values within the property is a different context to what -name was expecting. 

What if Plan B fails!
If you know that a property should be directed to a certain parameter, but isn't working because
the property's name and parameter's name don't match, in this case you can make some configurations to 
do a manual mapping instead. This is done by creating a hash table:

http://en.wikipedia.org/wiki/Hash_table 

http://www.howtogeek.com/114344/5-cmdlets-to-get-you-started-with-powershell/ 

Lets assume we have command1 and command2 which both have a property called "ServiceName", where command2 can also accept "ServiceName" by ByPropertyName, in this case the following will successfully pipe:

command1 | command2


Now if you we have Command3 (a command which outputs column "name") | Command4 (a command where property "servicename" can accept by ByPropertyName)....then the following will not work:

command3 | command4


To fix this, set up manual mapping using the select-object command:

CommandA | 
select-object -property *,
@{name='service' ; expression={$_.servicename}},
@{name='B1' ; expression={$_.B}}

Notice here, that we used commas to break down a single command line into several lines to improve 
readability. 

Here is an example:

Get-Service | 
select-object -property *,
@{name='Sher-name' ; expression={$_.Name}},
@{name='Sher-stopper' ; expression={$_.Displayname}}

In this example, our custom property names are "Sher-name" and "Sher-stopper".


Note, the expression between the curly brackets can be a command, a whole script, or a whole scripts filename. 
In this case we used an existing value in the pipeline using the placeholder "$_". Hence something like
$_.B will retrieve the current pipeline object, but only the part under the B property (column). Hence $_.B
effectively pulls out single entry from a row. 

The "$_" houses the current pipelined object (row). Also the period right after it basically is our way of telling PS that we are focussing on one 
entry (property) in the row, which we identify by the property's name. 

http://stackoverflow.com/questions/3494115/what-does-mean-in-powershell 

You can think of it as one of ps's special variable, like $? is a special variable (but for exit code) in linux.
For linux, the closest thing to this could be in a loop where we do "for row in `ls -l`; do;....done". Also the latest
version of bash can now do hash tables:

http://stackoverflow.com/questions/1494178/how-to-define-hash-tables-in-bash



Sometimes, you will want to pipe into a parameter which (according to full help) will not accept piped in data regardless of whether we are trying to pipe-in matching typenames (byalue), or doing it by matching linking properties (ByPropertyName). One way to overcome this is by rewriting the command to feed data into the parameter in another way. 



e.g. The -computername parameter for get-wmiobject doesn't accept piped data (although it can accept an array), 
hence the following won't work:

get-content .\computers.txt | get-wmiobject -class win32_BIOS (the text file only contains a list of computer names)

However the work-around for this is by doing this:

get-wmiobject -class win32_BIOS -computername (get-content .\computers.txt)

Note, in the above we just read from a file, however you might want to get content from another commands output. 
In this case, just take out single property:

get-service -computername (get-process | select-object -property name)

This above command is made up anyway, however there is another reason it work work, the select-object's property command
outputs the data in a tabular form, including heading, whereas we want it in array form, to fix this, simply use the 
-expandproperty option instead:

get-service -computername (get-process | select-object -expandproperty name)
 
 
You can think of "select-object -property" as selecting a box out of a number of boxes, 
whereas "select-object -expandproperty" takes it one step further and takes the content out of the box, and 
throws away the box.

Note: the -expandproperty can give a a further  
 

 
]]></Content>
		<Date><![CDATA[2014-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell - Changing a command output's default formatting]]></Title>
		<Content><![CDATA[Chapter 10 - changing the default formats of a command's output

Every time a command outputs a table of data, the output actually gets (invisibly) piped to the Out-Default command. This in turn pipes the data (invisibly) 
to the defualt destination, which is, which by default is "out-host", which is the terminal.

out-host formats the data into a tabular form using formating info contained in the following file:
   
C:\windows\systems32\windowspowershell\v1.0\DotNetTypes.format.ps1xml
(Note: DO NOT edit this file, becuase it is digitally signed. So powershell will stop working if you make the tiniest change)

It does this by looking up the xml file for the entry for the matching object (which you can find out using get-member)

All out-* commands uses the same xml file too (although write-host is an exception which is covered later). 

However it is possible that a command outputs a table that does not have a pre-defined view in the xml file. In this case the out-* command
looks up a secondary xml file for formatting guidance:

C:\windows\systems32\windowspowershell\v1.0\Types.ps1xml

If this also fails, the the out-* command will format the data based on this simple rule:
Display table if 4 or fewer properties, otherwise display data in list format. 

There is also another out-* command called write-host, e.g.:

get-service | write-host 	

write-host also displays content to the terminal, but it does this by bypassing out-default, and hence bypassing out-host. As a result it 
doesn't make use of the xml formatting data. 

Now with PS you can over-ride the default formatting system, with the help of a number of commands:

   - format-table
   - format-list
   - format-wide

format-table
The format-table command has a number of parameters that are useful:

	-autosize  - with this, ps tries to adjust the widths to fill the column content, and leave minimal spacing either sides of the column.
				 This leads to a tighter layout. 
	-property  - Lets you define which properties you want in the table (similar to select-object -property name1,name2,name3....)
	-groupby   - This breaks down a table, into smaller tables based on using a sensible property (e.g. status property for get-service command)
				 e.g. try:
				 get-service | sort-object -property status | format-table -groupby status
	-wrap	   - PS may truncate some fields (indicated by "...") to fit everything into the screen. Use the -wrap parameter to disable this suppression
				 and show everything on screen by wrapping around long objects to a new line. 
				 
				 
format-list
This essentially shows a table of data in list format, a bit like a large number of mini-lists, one for each object (row). Also here is another way 
(aside from the get-member command) to view all properties:

get-service | format-list * | select-object -first 3 


format-wide
This takes a property from an existing table, and then creates a new table with just the entries of this column. You can also specify how many
columns you want the table to be broken down into. e.g.:

get-service | format-wide -property status -column 10


Another thing you can do is change the names of the table's default headers. This is done using the hashed table technique (like we did with select-object)

get-service | format-table -property @{name='alias'; expression={$_.name}},status,displayname

However when using hash tables with the format-* command, you can do some more custom formatting using some builtin hash parameters:

get-service | format-table -property @{name='alias'; expression={$_.name}; align='left';width=8},@{name='status'; expression={$_.status}; align='right';width=8},displayname  

Here we used the "align" and the "width" builtin parameters.

There is also the FormatString parameter, see here for info:

http://msdn.microsoft.com/en-us/library/26etazsy.aspx


You can also do some simple maths with hash tables (when used in conjunction with format-* commands). For example if a column shows data in bytes, 
and you want to change this to MB, then you can do:

get-process | format-table name,@{name='VM(MB)' ; expression={$_.VM / 1MB -as [int]}}

1MB is a special notation in PS for representing a megabyte. 
]]></Content>
		<Date><![CDATA[2014-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell - Filtering and Comparisons]]></Title>
		<Content><![CDATA[Chapter 11 - Filtering and comparisons

There is basically 2 approaches to filtering a command's output:

1. Using wildcards
2. Using the where-object command. 

1. Using wildcards

e.g.:

get-service -name e*,*x*  	# returns anything where the name property either begins with "e" or contains the letter "x"



Here are some comparisons done on the command line:

PS C:\> 5 -eq 5
True
PS C:\> 5 -eq 2+3
True
PS C:\> 5 -eq 2+4
False
PS C:\> 5 -ne 5
False
PS C:\> 5 -ne 6
True
PS C:\> 5 -gt 3
True
PS C:\> 5 -gt 5
False
PS C:\> 5 -ge 5
True
PS C:\> 'test' -eq 'TEST'     #Notice that it isn't case sensitive
True
PS C:\> 'test' -ceq 'TEST'		# The "c" means that it is case sensitive. 
False
PS C:\> 'test' -ceq 'test'
True
PS C:\> 'test' -cgt 'test'
False
PS C:\> 't' -cgt 'to'
False
PS C:\> 'to' -cgt 't'
True
PS C:\> 5 -eq 5 -and 4 -gt 3 					# This is equivalent to linuxes && operator
True
PS C:\> 5 -eq 5 -and 4 -eq 3 
False
PS C:\> 5 -eq 5 -or 4 -eq 3 
True
PS C:\> 5 -eq 4 -or 4 -eq 3 
False


PS C:\> 'hello' -like 'hello'
True
PS C:\> 'hello' -like '*ll*'
True
PS C:\> 'hello' -like '*LL*'
True							#notice it is case insensitive. use "clike" to make it sensitive.
PS C:\> 'hello' -clike '*LL*'
False
PS C:\> 'hello' -like '*pp*'	# the key thing about like is that it accepts wild cards. 
False
PS C:\> 'hello' -notlike '*LL*'
False
PS C:\> 'hello' -notlike '*xx*'
True
PS C:\> 'hello' -cnotlike '*LL*'
True
PS C:\> 'hello' -cnotlike '*ll*'
False


PS C:\> 'hello' -match '[a-z]*'						# The Match operator is where you can use regular expressions.
True

PS C:\> 'hello' -match '[o-z]'
True
PS C:\> 'hello' -match '[p-z]'
False



PS C:\> 'hello' -match '[a-z][a-z][a-z][a-z][a-z]'
True
PS C:\> 'hello' -match '[a-z][a-z][a-z][a-z][a-z][a-z]'
False

PS C:\> 'hello' -match '[A-Z]'		# Notice it isn't case sensitive
True
PS C:\> 'hello' -cmatch '[A-Z]'		# used the c option to now make it case sensitive.
False
PS C:\> 'hello' -cnotmatch '[A-Z]'
True


Also checkout:
help about_comparison_operators



All of these above comparison operators are most often used in conjunction with PS's main generic filter command, which is called "where-object" 
Here are some examples (taken from the help pages):

C:\PS>get-service | Where-Object -FilterScript {$_.Status -eq "Stopped"}
  
C:\PS>get-process | Where-Object -FilterScript {$_.workingset -gt 25000*1024}
   
C:\PS>get-process | Where-Object -FilterScript { $_.ProcessName -match "^p.*" }
  
]]></Content>
		<Date><![CDATA[2014-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell - Run commands on remote machines]]></Title>
		<Content><![CDATA[PowerShell lets you run commands on a remote machine. You can even run commands that exist on the remote machine, but doesn't exist in your local machine (since you have not installed it on your local machine).

PowerShell manages to do this by using it's own communication protocol called "Web Services for MANagement", aka WS-MAN. WS_MAN is Microsoft's equivalent of Linux's SSH. It operates over http/https, so is easy to route through firewalls, but has a different default port number.

ws-man comes in the form of a background service called "Windows Remote Management" (winrm):


<pre>
PS C:\> get-service winrm
</pre>


When you run a remote command, the output has to travel over a network, and therefore the command output (object) is serialised int xml to make this possible, and then deserialized on the local machine.

There are lots of help pages about remoting:

<pre>
PS C:\> help about_remote*
</pre>


In order for remoting to work:

<ul>
	<li>both machines must have an instance of PSv2 (or above) running, and both instance are for the same user. If this is not the case, then</li>
	<li>Ideally both machines to be on the same domain, or trusting domains (more trickier if seperate domains)</li>
</ul>

WinRM is not a powershell thing only, it is a more general tool that is used by the OS to route traffic to and from other application between machines. Therefore any incoming traffic is tagged with the same name of the recieving application. These applications are  registered with WinRM as endpoints. E.g. for an analogy, WinRM is the royal mail, and each newly built house's address has to be registered with the royal mail, in order to send and recieve letters. Hence powershell also has to register itself with WinRM.

WinRM can have lots of endpoints, and PS calls these endpoints "session configurations". An application can have a number of endpoints, each with
different permissions/functionality.

So before you can start remoting, you first have to set up a listener for your ps, this can be done by running the following command:

&nbsp;

<pre>
enable-psremoting
</pre>


By default, winrm uses ports 5985 (for http) and 5986 (for https)

Note: You can run winrm as a standalone command in ps (just like regedit, and cmd):
winrm # just run this on its own.

note: "enable-psremoting" on it's own might now work, you might also need to update <a href="http://technico.qnownow.com/the-winrm-client-cannot-process-the-request-if-the-authentication-scheme-is-different/">gpedit.msc</a>.

Once you have run the enable-psremoting command, you should now be ready to do a remote connection, which is done like this:


<pre>
enter-pssession -computername {machine name}
</pre>


after that you can run commands remotely and after you are finished, you can exit the remote machine by closing your ps session, or running:


<pre>
exit-pssession
</pre>



If you want to run some commands on multiple machines at the same time, then use:

<pre>invoke-command -computername name1,name2,name3 -scriptblock {command-you-want-to-run}</pre>

Let's first give a quick intro to invoke-command. This command is actually a pretty generic command which basically means
"run the following command". As a result, the first positional+mandatory command that it accepts is "-scriptblock". Therefore
if you want to run the get-service and get-process command (on the local machine) at the same time, then do:

<pre>invoke-command -scriptblock {get-service; get-process}</pre>

However what makes invoke-command really valuable is that it has the "-computername" option. This lets you run the -scriptblock
simultaneously across several machines (as shown further above).

The invoke-command also adds an extra "pscomputername" property to the output, so to keep track of
which machine each object came from

Some commands (e.g. get-eventlog) have native remote functionality builtin. This is indicated by the fact that the command has a parameter called "-computername". In general it is always better, faster, and more reliable to use invoke-command/enter-pssession rather than the native command remoting functions.

If you have lots of commands you want run on several remote machines, then it can become tedious constantly declaring them in the command line. Instead a better approach is to create reusable persistant connection (session) instead. This persistant session can be created using:

<pre>New-PSessionOption</pre>

This command will create the persistant session which is in the form of a "PSSessionOption" object. This object can then be fed into the
-SessionOption parameter in order to establish the connection.

Note: you cannot do a remote connection using a machine's IP address, you have to use the machine's (NetBIOS) name:

https://en.wikipedia.org/wiki/NetBIOS

you can view your machine's netbios name like this:

control panel | system management | general tab

Or alternatively, the name field in:

Get-WmiObject Win32_ComputerSystem # this command is covered later.

If you don't know what you machines name is then you can find it using the machines ip number. However if you don't
know your machine's ip number is, then do.

<pre>ipconfig</pre>

Then to find the name, do:

<pre>nbtstat -a {ip-number}</pre>



Earlier we ran commands remotely using "enter-pssession" and "invoke-command". However the problem with using them is that each time you use these commands, you may have to enter additional info such as passwords and port numbers. This can become repetitive and tedious.

One way around this issue is to create a reusable persistant session:

<pre>New-PSSession -computername manchine1 </pre>

The above can also specify an alternative username, port number...and etc.

The above will create a new (session) object. You can view all session objects like this:

<pre>get-PSSession</pre>

It is handy to store sessions in a variable:

<pre>$server = New-PSSession -ComputerName machine1,machine2,machine3</pre>

The above command creates 3 objects, one for each machine. As a result, $server is actually an array.

Now you can use a session like this:

<pre>enter-pssession -session $server[1] </pre>

another way to write this if you forget the list item and don't want to use gm to check:


<pre>
enter-pssession -session ($server | Where-Object -FilterScript {$_.computername -eq "localhost"}) 
</pre>

Everything in the bracket gets evaluated first. Here is another way:

<pre>get-pssession -computername localhost | enter-pssession</pre>

Notice that we are using "-session" option instead of "-computername". If you check the syntax section with the enter-pssession help pages, you will find that we are using enter-pssession in a different mode.
You can exit a session like this:

<pre>Exit-PSSession # or just do:
Exit</pre>

However the reusable persistant session itself will not be deleted. You have to use the remove-session command to do that:

<pre>remove-PSSession -id {id number}</pre>

Creating an array for storing sessions is useful for 2 purposes:

<ul>
	<li>It lets you create multiple sessions in a single command, rather than running the new-pssession command multiple times.</li>
	<li>It can be used with the invoke command to perform simultaneous task across multiple remote machines.</li>
</ul>

Here is how to use the invoke command using an arrayed session variable:

<pre>
Invoke-Command -ScriptBlock {Get-Service;get-process} -Session $server
</pre>

Notice that this time we used the invoke-command's "-session" option instead of the "-computername" option. A really cool think you can do is kind of create your alias commands, which only run on the remote machine.

E.g. say you want to run get-service and get-process on server[1], then you do:

<pre>
Invoke-Command -ScriptBlock {Get-Service;get-process} -Session $server[1]
</pre>

Now if you want to run the above command numerous times, then typing the above will become tedious.

So one thing you can do is run the above command once, then run:

<pre>import-pssession -session $server[1] -prefix sher</pre>

After that, if you try to run the following commands on your local machine:

<pre>get-sherService
get-sherProcess</pre>

These commands will work, but will actually run the get-service and get-process commands on the remote machine ($server[1]) and retrieve the output.

These aliases are temporarily and will stop working if you close the terminal, or remove the sessions from get-pssession.

This technique has a big drawback - the output retreived are deserialised, hence harder to manage and manipulate. You can see this for yourself
by comparint the following:

<pre>get-sherService | gm # The typename will indicate that it is deserialised.
get-service | gm # this will have a lot more properties and methods.</pre>

You can also disconnect connections, using disconnect-pssession (PS v3 only). But it is something that you will use that much, for more info see:
http://technet.microsoft.com/en-us/library/hh849747.aspx

If you want to change some of the session settings then go to:

<pre>set-location -path WSMan:\localhost\Shell </pre>

Here you will find the following settings:

- idletimeout # specifies how long shell can be idle before timing out (default: 2000 hours)
- maxconcurrentusers # specifies how many different users can have a session open at once.
- maxshellruntime # maximum amount of time a session can stay open, the default is infinite, although idletimeout can over
- maxshellperuser # Maximum of sessions a single user can have open at once.

There is also another setting here:

set-location -path WSMan:\localhost\Service

- MaxConnections # Total number of session that can be opened at once for entire machine.

Useful links:
http://www.howtogeek.com/117192/how-to-run-powershell-commands-on-remote-computers/]]></Content>
		<Date><![CDATA[2014-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell - Windows Media Instrumentation (WMI)]]></Title>
		<Content><![CDATA[WMI is a standalone component and Powershell only interacts with it.

WMI essentially houses a massive collection of system information and tries to organise this data in a meaningful way.

Here is very useful guide:

[powershell]
help about_WMI_Cmdlets
[/powershell]

At the top level, WMI is organised into namespaces (which are a bit like folders that ties to specific product or
technology.). Here are some example namespaces:

<strong>namespace name:</strong> root\CIMv2
<strong> Description:</strong> This houses all OS and Hardware info.

<strong>namespace name:</strong> root\MicrosoftDNS
<strong>Description:</strong> This houses info about the DNS server, assuming your machine is running as a dns server.

<strong>namespace name:</strong> root\securitycenter
<strong>Description:</strong> This houses info about firewalls, antivirus, and antispyware utilities.

Within each namespace, WMI is divided into a series of classes, a class is something that WMI knows how to query. Here are a few example classes, along with the namespace they reside in:

<strong>class name:</strong> AntiVirus-Product
<strong>located within the namespace:</strong> root\securitycenter
<strong> description:</strong> This class is designed to hold information about anti-spyware products

<strong>Class name:</strong> Win32_LogicalDisk
L<strong>ocated within the namespace:</strong> root\CIMv2
<strong> Description:</strong> This class is designed to hold information about logical disks

Note: There can be classes can exist, but doesn't hold any infomation, e.g. maybe the floppy disk class.

A class is essentially a text based equivalent of a config-window. The settings (content) of each class can be modified
using a command called "invoke-wmimethod". This command will be covered later.

Now going back to namespaces, to view all the available namespaces, do:

[powershell]
Get-WmiObject -Namespace root -Class __Namespace
[/powershell]

Note, this might give the output in list mode, in which case use select-object to get rid of some properties to make it all fit.

Note, the properties starting with 2 underscores, are referred to as "system properties".

If you want to view, a list of all the classes for a given namespace, then do:

[powershell]
Get-WmiObject -Namespace root\{namespace-name} -List
[/powershell]

e.g.:

[powershell]
Get-WmiObject -Namespace root\CIMV2 -List
[/powershell]

And if you just want to view the win32_logicaldisk class which resides in CIMV2, then do:

[powershell]
Get-WmiObject -Namespace root\CIMV2 -List | Where-Object -FilterScript {$_.name -eq &quot;win32_logicaldisk&quot;}
[/powershell]

If you want to view more info about this class, then do:

[powershell]
Get-WmiObject -Namespace root\CIMv2 -class {class-name}
[/powershell]

Note: You should always declare namespace because class names are not always unique across namespaces.

In most cases, there is class names are unique throughout WMI. For example, all computers have one BIOS, hence there is

always only one instance of the win32_bios class:

[powershell]
Get-WmiObject -class win32_bios
[/powershell]

However in some cases there can be more than instances classes with the same name. For example a machine can have many services running (win32_services):

[powershell]
Get-WmiObject -class win32_service
[/powershell]


Note: You can uniquely refer to each instance by their "__path" property's value, which are unique.

You can control a class, by using a "method". However Microsoft are trying to move away from methods, and
instead are developing custom commands for interacting with classes instead. These commands interacts
with WMI internally, so that you won't have to.

If you are looking for a particular class, then it can be time consuming to find it if you don't know which namespace it is in.
That's why, a good way to browse through classes is using a third party GUI software called Sapien Windows Explorer,
which is made by sapien.

In PS, there are 2 groups of commands that can be used to interact with WMI:

1. wim cmdlets - e.g. get-wmiobject and invoke-wmimethod. These are legacy commands which slowly be phased out.
- They communicate over Remote Procedure Calls (RPC) instead of WS-MAN
- It can take awhile to get these commands to work through a firewall.

2. cim based commands - These are the more up to date commands that microsoft are investing money in.
- They are only available in PS v3
- They work over ws-man
- There are a lot more of these command available.

The "get-CIMinstance" is the equivalent version of "get-wmiobject".

Note: Unfortunately there is no documentation for what individual namespaces and classes do. Microsoft themselves have not written any information on them. So you are stuck with using google for finding info.

For more info, checkout the book "powershell and wmi" by Richard Siddaway (2012)]]></Content>
		<Date><![CDATA[2014-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell - Running tasks in the background]]></Title>
		<Content><![CDATA[PowerShell lets you run commands in the background and you can then retrieve the results later.This is useful if you want to run
time consuming commands and you want to keep the terminal free. Here's how to start a background job.

[powershell]
start-job -scriptblock {get-childitem}
[/powershell]

or if you have multiple commands, you can do:

[powershell]
start-job -scriptblock {
    get-childitem
    pwd
}
[/powershell]

or if you want to run a whole script in the background, then do:

[powershell]
start-job -filepath {c:\path\to\powershell-script.ps1}
[/powershell]

The "start-job" cmdlent is designed to be self-contained so it cannot access any outside variables. If you want your script block to make use of an outside variable then you need to pass it into the start-job command using the "argumentlist" option. This approach also requires you to  makes use of the "param" construct (which is commonly used for parametizing functions and scripts) Here's an example:

[powershell]

$message1 = &quot;Hello&quot;
$message2 = &quot;Goodbye&quot;

Start-Job -ScriptBlock {
    param(
        $message1,
        $message2
    )

    &quot;First arguement is: $message1&quot;
    &quot;Second argument is: $message2&quot;

} -ArgumentList $message1,$message2

[/powershell]

Whenever you start a new job, it ends up running inside a newly generated "powershell.exe" process. You can view these new process using the task manager, or just use the get-process command.  So the more background jobs you are running, the more "powershell.exe" processes it creates.

Note, you can start "Task Manager" using the taskmgr command:

[powershell]
taskmgr
[/powershell]

This will create a job object (row). To view a list of jobs that we have started, do:

get-job

Some jobs can start child jobs, you can see them, if any, by passing the above through format-list:

get-job | format-list

To get more info about all the childjobs, you can do:

get-job | select-object -expandproperty childjobs

# the above is like taking the content out of the child property (box), but since the content is also another (get-job) object,
then it displays the child jobs as jobs rather than just strings. If you just want the child job names, then do:

get-job | select-object -expandproperty childjobs | select-object -expandproperty name

If you want to see info for the childjobs of a given job id, then simply do:

get-job -id {number} | select-object -expandproperty childjobs

Note, you can't use recieve-job command twice on the same job id. That's because the cache gets emptied on the first
run (however, you can use the "-keep" parameter to retain it in cache). Note, the "get-job" will still show the job
even if the cache is emptied. If you want to remove a "completed" job from the job list, then do:

remove-job -id {jobs id}

If the job is still running, or hanging, then you first do:

stop-job -id {jobs id}

This command might stop the job slowly, which I thinks is because it stops the process gracefully. if you want to stop it faster, then I think this might be possible by stopping the corresponding powershell.exe process, using "stop-process".

&nbsp;

Then you use the remove-job.

The output of recieve-job can be piped into other command like normal, e.g.

receive-job -id {job's id} | select-object -property name

############ Detour - Start
If you want to do start-job, and pass variables into it, as well as doing initilization (e.g. import a module), then do this:

Start-Job -InitializationScript {Import-Module -Name SSH-sessions} -ScriptBlock {
param($ServerName,$Username,$Password,$LinuxCreateTarGzCommand) # you have to include this.
New-SshSession -ComputerName $ServerName -Username $Username -Password $Password
Invoke-SshCommand -ComputerName $ServerName -Command $LinuxCreateTarGzCommand
} -ArgumentList $ServerName,$Username,$Password,$LinuxCreateTarGzCommand

Got this info from:

http://stackoverflow.com/questions/12293701/function-not-recognized-in-start-job

The above could be a solution to run code in the background in order to prevent the winform gui from freezing. Need to investigate this in more detail as I haven't got this working yet.

############ Detour - end

You can also create jobs, where the commands run on a remote machines. This can be done by:

1. Use commands that supports -computername parameter, e.g.
start-job -scriptblock {get-eventlog -logname security -computername &lt;machine1,machine2,machine3=&gt;}
2. Use get-wmiobject command, which has a builtin "-asjob" parameter as well as the "-computername" parameter. So both
are used together. e.g.:
get-wmiobject -namespace root\cimv2 -class win32_service -computername {machine1,machine2,machine3} -asjob
(However you can also ignore using -asjob, and just run the command within start-job instead). The downside is
that this method is quite slow because get-wmiobject runs on one machine at a time.
3. One way around the slow performance problem with using get-wmiobject, is to run get-wmiobject in conjunction with "invoke-command". This is made possible because
invoke-command which also accepts both -computername and -asjob parameters, e.g.:
invoke-command -computername machine1,machine2 -asjob -scriptblock {get-wmiobject -namespace root\cimv2 -class win32_service}
..or go one step further and replace -asjob with start job, effectively having a 3 level nest:
start-job -scriptblock {invoke-command -computername machine1,machine2 -scriptblock {get-wmiobject -namespace root\cimv2 -class win32_service}}

Note, we did machine1,machine2....this will mean 2 child process will be created for each job.
Note, the invoke-command alway adds an extra "pscomputername" property to the output, so to keep track of which machine each object came from.

wait-job # this is something you can put in a script to allow a job to end before moving on to the next part.
# Although not sure why this is necessary.

You can also schedule jobs like the linux equivalent of "cron" or "at". Setting up a
scheduled job is done in 2 parts:

First you create a schedule using the following 2 command:

new-jobtrigger
register-scheduledjobs

See the examples in help pages to see how they work. Looks like the best way to do is merge both commands into one
by nesting them.

&nbsp;

&nbsp;

You can allocate more cpu resource to a job, by changing the corresponding process's priority:

http://powershell.com/cs/blogs/tips/archive/2014/01/02/lowering-powershell-process-priority.aspx

http://forums.extremeoverclocking.com/showthread.php?t=350101

&nbsp;

Another way to increase performance is by using "runspaces", need to do more research on this:

http://thesurlyadmin.com/2013/02/11/multithreading-powershell-scripts/

http://learn-powershell.net/2012/05/13/using-background-runspaces-instead-of-psjobs-for-better-performance/

&nbsp;

Also check out the "start-powershell" cmdlet (which is in the pscx module). Starts a new Windows PowerShell process using PowerShell's parameter parsing engine to parse the parameters for the PowerShell executable.
This command exposes a few of the Start-Process commands it uses such as -Wait, -Credential and -WorkingDirectory. Note: If -NoNewWindow is specified, PowerShell is invoked using the call operator (&amp;) instead of with the Start-Process cmdlet.]]></Content>
		<Date><![CDATA[2014-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell - Processing each row in a table (for-each)]]></Title>
		<Content><![CDATA[Chapter 16  

some commands can run without giving any output at all, e.g. 

start-service {service name}
stop-service {service name}

However you can make them give a response using the -passthru option:

PS C:\Documents and Settings> start-service spooler -PassThru

Status   Name               DisplayName                           
------   ----               -----------                           
Running  spooler            Print Spooler                         

Note, for scripting you can output this to a file and check if it is successful by looking for a null file. 

Note: the "$?" exit code in linux works exactly the same way in PS too!!!!!!!!!!!!!!!!!!

For more about $? and others, check out:

help about_Automatic_Variables


In PS, there is a number of ways to do the equivalent of the for-each loop:

	1. Use a command collection's builtin for-each loop, e.g.:
			get-service | stop-service
		This is the simplest approach, but it isn't possible to take this approach all in all cases. 	
	2. This is the same technique as 1, but this time we will be changing a classes property using the 
	   WMI-invokemethod command (this feels like it shouldn't be bullet point here, and should have been covered 
	   earlier in this book). All the classes content (configuration) might not be configurable, but the ones 
	   that are, can be configured via the class's methods. To view all the available methods for a class, do:
	   
			get-wmiobject -class win32_networkadapterconfiguration | get-member
		
	   Once you found the method you want to change (e.g. anableDHCP), then use the invoke-wmimethod command:
	  
			get-wmiobject -class win32_networkadapterconfiguration | invoke-wmimethod -name enabledhcp
			
		The output can be a little confusing, however just look at the "returnvalue" output, which is a number, and 
		then google for more info about what that number means (since PS doesn't provide help info about classes).
		A return value of "0" means it is successful, anything else tends to indicate a problem.
		
	    Note, the CIM equivalent can be done using the the get-cimstance command and the invoke-cimmethod command. 
		
	3.  If it's not possible to use the above two methods, then you can use the generic "for-each" approach. For example,
	    if you do:
		
		  Get-WmiObject -class win32_service | gm
		  
		You will find that win32_service has a method called "change". You will notice that this method 
		actually lets you change multiple settings:
		
		Get-WmiObject -class win32_service | gm | Where-Object -FilterScript {$_.name -eq 'change'} | format-list
		
		Here, specify which setting within "change" that we want to modify, we use a special builtin variable called
		$null, this variable means "keep as is", hence if we want to change the "startpassword" (which is the 8 setting listed above), 
		then you first setup the syntax:
	    
			change($null, $null, $null, $null, $null, $null, $null, $null, "change-me")
		
		we don't have to specify any more nulls after the password, as ps will assume this. 
		
		Now we feed the above into the class (in this case only one instance called "bits"):
		
			Get-WmiObject -class win32_service | where-object {$_.name -eq 'bits'} | foreach-object -process {$_.change($null, $null, $null, $null, $null, $null, $null, $null, "change-me")}
	   
		The "-process" parameter is mandatory, and instructs the scriptblock to operate 
		within the for-eachobject container.
		
		Note: you can use foreach-object all the time instead of invoke-wmimethod, but people still use invoke-wmimethod
		because of habit. E.g to change the Changestartmode property of the "BITS" instance of the 
		win32_class, we do:
		
		Get-WmiObject -class win32_service | Where-Object {$_.name -eq 'bits'} | Invoke-WmiMethod -name changestartmode -ArgumentList "manual"
 
		Whereas, the foreach-object equivalent is:

		
		Get-WmiObject -class win32_service | Where-Object {$_.name -eq 'bits'} | ForEach-Object {$_.changestartmode("automatic")}
 
 
 So far we have covered 3 methods, now here is an example of doing a specific task, using each approaches. In this case
 the specific task is to stop all services that starts with "Bro*" in it's name:
 
Approach 1 - using batch cmdlets
get-service *Bro* | stop-service
 
Approach 2 - Using the foreach-object approach
get-service -name *Bro* | foreach-object -process {$_.stop}	# Here we used an object's "stop" method.
Note: not sure if you are supposed to include round brackets for the method, i.e. $_.stop()

Approach 3 - 
Get-WmiObject -class win32_service | where-object -filterscript {$_.name -like "*Bro*"} | invoke-wmimethod -name stopservice 
Note, get/start/stop-service cmdlets are "frontend" commnads for managing win32_service classes.  
 
Approach 2 and 3 combined
Get-WmiObject -class win32_service | where-object -filterscript {$_.name -like "*Bro*"} | foreach-object -process {$_.stopservice()}
Note, by convention, when using foreach-object, always include the 
closing brackets in the method's name, i.e. "stopservice()".

However, where possible, use the simplest way:
stop-service -name *Bro*

]]></Content>
		<Date><![CDATA[2014-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell - Security Alert]]></Title>
		<Content><![CDATA[Chapter 17 - Security Alert

By default, as a security feature, when you double click on a powershell script, it will not run the script, 
it will instead open the script up in editor mode.

Also by default you cant run the script in ps by just entering the name, you have to always specify the full/relative path.  

There is a setting called ExecutionPolicy which controls whether a machine is allowed to run a ps scripts. You can 
view this here:

Get-ExecutionPolicy

By default this is set to "restricted" which means it won't let you run any ps script. You can change this setting
like this: 

Set-ExecutionPolicy -Executionpolicy {setting}
 
For more info, see:

help Set-ExecutionPolicy -full

If you want to run PS scripts, then microsoft recommends setting the executionpolicy to "RemoteSigned"

]]></Content>
		<Date><![CDATA[2014-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell - Variables and Objects]]></Title>
		<Content><![CDATA[You can store data in the form of a variable and then retrieve that data by calling the variable as and when needed. 

You can do all sorts of things with variables, here's a just a tiny examples.

Here we store a string in a variable:


<pre>PS C:\> $var1 = "CodingBee"
PS C:\> $var1
CodingBee
PS C:\> "Welcome to $var1"
Welcome to CodingBee
PS C:\> $WelcomeMessage = "Welcome to $var1"
PS C:\> $WelcomeMessage
Welcome to CodingBee
</pre>

You can also use variables to do maths:


<pre>PS C:\> $x = 2
PS C:\> $y = 8
PS C:\> $x + $y
10
PS C:\> $sum = $x + $y
PS C:\> $sum
10
PS C:\></pre>



Here are some examples of arrays:

<pre>
$array = 'item1','item2','item3'
PS C:\=> $array
item1
item2
item3

PS C:\=> $array[1]
item2

PS C:\=> $array[0]
item1

PS C:\=> $array[2]
item3

PS C:\=> $array[-1]
item3 
</pre>

In PowerShell, things like strings, integers, and arrays, are not just "data type", are not just data, they are actually "objects".  







Absolutely everything in PS is treated as an object, even a simple string of characters, e.g.:

<pre>
PS C:\> "hello world"
hello world
PS C:\> "hello world" | Get-Member


   TypeName: System.String

Name             MemberType            Definition
----             ----------            ----------
Clone            Method                System.Object Clone(), System.Object ICloneable.Clone()
CompareTo        Method                int CompareTo(System.Object value), int CompareTo(string strB), 
.
.
.
</pre>

This will identify the string belong to object type "System.string".


You can have spaces in the name, but have to use curly brackets:

<pre>PS C:\> ${my variable} = "test123"
PS C:\> ${my variable}
test123
</pre>

$phrase = 'The variable is equal to $var'	#single quotes will treat contents literally
$phrase = "The variable is equal to $var" 	#double quotes will evaluate $var first before variable assignment. 

"`$var - The variable is equal to $var"   # The back-tick forces the "$" character to be treated as a literal character.
										 
In Linux "`n" has a special meaning, it means "new line",

PS C:\> "This is the first line, `nand this is the second line."
This is the first line, 
and this is the second line.

For more info on this and other special characters, see:

help about_Escape_Characters

 
You can return the number of items in an array by invoking the "count" method: 

$array.count	# Not sure where this info is documented because it isn't in the gm. 
3 
 
Similarly you can use other methods on a variable, e.g.:

 
$array[1].ToUpper() 	# to capitalize
ITEM2
$array[1].length		# to return character length 
5
$array[1].replace('2','50')
 
 
You can overwrite items inside an array:

PS C:\> $array
item1
item2
item3
PS C:\> $array[1] = $array[1].replace('2','50')

PS C:\> $array
item1
item50
item3
 
If you want to apply a method on a whole array, in PS v3, you do it like this
$array.toupper
ITEM1
ITEM50
ITEM3


Also when you do:

$array | gm
 
 
You will find that the array has a property called "length", so if we do:

$array | select-object -property length 
 
Note: this will give right-aligned data because the output is numeric. 

You can store whole tabular objects into a variable:

$services = Get-Service

When that is the case, then try:

$services | gm

You will find that the variable is of the same object type as "get-service". This means that "$services" has the same properties, methods,....and etc
as "get-service". As a result a shorthand notation can be used when dealing with object variables like this:

$services.name 				# only works in PSv3. for ps2, you have to do use for-each: $services | foreach-object {write-output $_.name}
 
The above prints out just the name property of each object, i.e. it is equivalent to: 

Get-Service |select-object -property name
 
or 

get-service | foreach-object {write-output $_.name}
 

You can also use a shorthand notation for applying methods to a wmi-object:

 
$objects = Get-WmiObject -Namespace "root\cimv2" -Class win32_service | Where-Object -FilterScript {$_.name -eq "bits"}
$objects.ChangeStartMode('disabled')  	#Where ChangeStartMode is a method.  
 

Note, quotes and square brackets (to specify array item) don't mix well. 

PS C:\> $name= "sher","mirfath","rayyan"
PS C:\> "Hello, my name is $name"
Hello, my name is sher mirfath rayyan	 # Here I want to mention Rayyan only

PS C:\> "Hello, my name is $name[2]"
Hello, my name is sher mirfath rayyan[2]	# This didn't work becuase quotes don't understand square brackets. 
											# The workaround for this is to make ps do a two-pass at the same command, this is done by 
											# introducing round brackets, and the contents of these are evaluated first:

PS C:\> "Hello, my name is $($name[2])"		# Note, $(...), is equivalent to linux's back ticks: `....` 
Hello, my name is rayyan

This $() used above is known as a sub expression. 

 
$name = read-host "type your name"		# This is the linux equivalent to:
										# echo "type your name"
										# read
$name									# echo $1
 
The above is useful for creating an interactive script. 

You can declare data type for a variable like this:

[int]$number = 10			# note, you can use gm to convert that this is a string. 
$number = $number * 100		# this does't output anything
$number
1000

Note: once you have assigned a variable with a data type, it will then always accept that type and will give error message if 
you data of another data type into this variable. 

 


There are a number of other data types that you can use:
[int]
[single] and [double]   - these are to do with floating numbers
[string]
[char]		- exactly one character, e.g. [char]$FavoriteLetter = 'T'
[xml][/xml]		- An xml document. This have be used to ensure that data fed into this data type is valid xml. 
[adsi]			- something to do with Active Directory Service Interface
 
Best practice: You should declare variable type when setting the variable for the first time.   
 
Here are some commands for managing variables:

New-variable	- You don't really need to use this 
set-variable	- You don't really need to use this 
remove-variable - Covered later
get-variable	- You don't really need to use this 
clear-variable  - You don't really need to use this 


You can delete a variable like this:

remove-variable number		# Note, you have to omit the dollar sign. 

]]></Content>
		<Date><![CDATA[2014-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell - Input and Output]]></Title>
		<Content><![CDATA[Chapter 19 - Input and Output


In an earlier chapter we learned about write-host. This bypasses the invisible out-* sequence (as well as anything else in the pipeline)
and sends the output straight to the terminal screen:

get-service | out-default | out-host

write-host redirects the above to:

get-service | write-host

Write-host is mainly used for outputing warning messages, because write-host lets you change the color of your output. 

Get-Service | Write-Host -BackgroundColor black -ForegroundColor red

Hence this is useful for displaying warning messages to the user. Unfortunetly you can use this technique to add color to log files, 
because the message gets outputed to the screen rather than a terminal. 

In reality you will very rarely use the write-host command.

If you want to write confirmation messages, then a better command to use is "write-verbose", e.g.:

Write-Verbose -message "Server connection successful" -Verbose  	

# Note, you have to use the verbose switch, becuase the default is to do it silently. You can change this by changing the following 
variable:

PS C:\> $VerbosePreference
SilentlyContinue
PS C:\> $VerbosePreference = "continue"
PS C:\> $VerbosePreference
Continue
PS C:\> Write-Verbose -message "Server connection successful"  		# notice we dont need to use the verbose switch anymore.   
VERBOSE: Server connection successful

However "write-verbose" is most often written in scripts, and activated during the script's execution, like this:

PS C:\> {\path\to\script} -verbose

This is possible because "-verbose" is actually one of the common parameters. This is a better option then constantly changing the value of $VerbosePreference. 


There is also "write-output". This actually feeds data into the pipeline. In other words, the following 2 commands do the same thing:

PS C:\> write-output "hello world"
hello world
PS C:\> "hello world"			# Here the write-output declaration is invisible (along with out-default and out-host). 
hello world

Note: "write-output" essentially the Linux equivaelent of the "echo" command.

write-output is actually the shells default command. when you tell the shell to do something that isn't a command, then the shell automatically passes 
whatever you typed to write-output (behind the scenes).

There are a few other write-* commands that also behaves like write-host (i.e. also bypasses out-default and out-host, as well as anything
else in the pipeline):



write-warning	# Displays warning message in yellow font with "WARNING:" label preceding it. 
				# It has an associated "$warningpreference" variable setting, that can be set to "continue" or "silentlycontinue"
				# "continue" is the default setting.
				
write-verbose	# Displays "additional info" message in yellow font with "VERBOSE:" label preceding it. 
				# It has an associated "$verbosepreference" variable setting, that can be set to "continue" or "silentlycontinue"
				# "silentlycontinue" is the default setting.
				# You can also enable when running a single script by using the "-verbose" common parameter.
				# This command is useful for out debugging message such as the linux equivalent of "echo line256".

write-debug		# Displays debug message in yellow font with "DEBUG:" label preceding it. 
				# It has an associated "$debugpreference" variable setting, that can be set to "continue" or "silentlycontinue"
				# "silentlycontinue" is the default setting.
				# You can also enable when running a single script by using the "-debug" common parameter.
				# You will discover later that this command is the linux equivalent of the "read" command, 
				# in terms of pausing a script for debugging. 

write-error		# Displays warning message in yellow font with "ERROR:" label preceding it. 
				# It has an associated "$ErrorActionPreference" variable setting, that can be set to "continue" or "silentlycontinue"
				# "continue" is the default setting
				# This also writes an error to the Powershell's error stream. 

There is also another write-* command called "write-progress". This gives a progress bar when running really long scripts, 
you can learn more about it (when the time comes) here:

Here is a sample write-progress script that you can try:


	for ($a=0; $a -lt 10; $a++) {
		Write-Progress -id 1 -Activity "Installing components into Pool D" -PercentComplete $($a*10) -CurrentOperation "copying file xyz" -Status "completed mini task $a"
		Start-Sleep 1
	}
	Write-Progress -id 1 -Activity "Pool D install has completed" -PercentComplete 100 -Status "Everything installed successfully."
	start-sleep 10
	## The following line forces the progress bar to disappear after the 10 second sleep is over.  
	Write-Progress -id 1 -Activity "Installing components into Pool D" -Status "Everything installed successfully." -Completed

For more info, check out:

http://technet.microsoft.com/en-gb/magazine/2008.03.powershell.aspx
http://www.hanselman.com/blog/ProgressBarsInPowerShell.aspx				# This shows how to create a 2 level progress bar. 

]]></Content>
		<Date><![CDATA[2014-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell - Intro to writing PowerShell scripts]]></Title>
		<Content><![CDATA[Chapter 21 - You call this scripting?

In the script panel of PS ISE, you can write a bunch of commands, and then select a secion, right click, and then run that selection. 
 
Here is how you parametise a command:

get-service | out-file -FilePath testfile.txt
$filename = "testfile.txt"
get-content .$filename

Note: "out-file" is the linux equivalent of the ">" piping symbol. If you want to do ">>", then use the append option:
get-service | out-file testfile.txt -append 

An alternative way to do ">>" is by using the add-content command:
write 'testline1' | Out-File -FilePath testfile.txt -Encoding utf8		# Note: you might use to use this encoding option to avoid getting 
																		# japanese/chinese characters appearing.
Add-Content -LiteralPath testfile.txt 'testline2'


In a PS script, you can break a single command line into smaller lines in the following ways:

get-service | 		# By ending a line with a pipe
Select-Object `		# By using back-ticks. This escapes the special end-of-command meaning for the return carriage at the end of the line. (linux uses back-ticks differently)
-First 10

Note: You can use the above technique on the PS command line, this technique can only be used within the powershell script. 


If you want a powershell script to accept parameters (e.g. the environment variable) then you need the "param" construct at the start of the script:

param(
[string] $Environment = 'stress',			# Note each line ends with a comma (except for the last line). 
[string] $Application = 'dropbox',
[switch] $RecommendedSettings = $true		# Note, this doesn't require any quotes since switches cannot accept strings. Also we used "$true" which is a special reserved variable. 
)

In the above example, these are positional paramaters, which have default values if you don't declare them. So if you accepts the defaults 
(by not providing parameters) then you will effectively install dropbox on the stress environment using the recommended settings. 

 
If the script is called "InstallApps.ps1", then you can run the script from the command line in the following ways:

.\InstallApps.ps1 -Environment regression -Application firefox -RecommendedSettings true

.\InstallApps.ps1 -Env regression -App firefox -Recom true		# Here we shortened parameter names

.\InstallApps.ps1 regression firefox true				# Here we are using the idea of positional parameters. 

.\InstallApps.ps1		# Here we are using the default parameter values. Hence will end up installing Dropbox on Stress, using recommended settings.

.\InstallApps.ps1	-app firefox 	# here we are accepting the defaults axcept that we are installing firefox instead of dropbox.  

Note, you have first cd to the correct directory before running the above commands. Alternatively you can specify the full path, e.g.:

C:\Documents and Settings\SChowdhury\Desktop\InstallApps.ps1 -Env regression -App firefox -Recom true

You can add comments in your script like this:

# This is a comment on a single line

<#
This is a 
block of comments. 
#>

You can also create your own custom help pages for your script. This is embeded in your script and has a special syntax. It is usually placed at
the very top of the script, even before the parameter declaration section. (see page 262 for an example of this syntax)

 
You can then view the scripts help like this:

help .\InstallApps.ps1

To learn more about the syntax and structure of a help page so that you can create your own, check out:

help about_comment_based_help


If you run the following commands (one after another) on the command line:

get-service | Select-Object -First 5
get-process | Select-Object -First 5

And then repeat this from within a script. Then the output for get-service is like before, but get-process's output comes out in list format. 

That is because in a script, everything is outputed in a single instance of the pipeline, and the out-host is only able to get formating information once. That's why ite gets it for the first command that it recieves form the pipe. 

As result, out-host doesnt know how to format subsequent outputs so it ends up formating based on the format-list style of formatting. 

Because of this, the best practice is to only develop powershell scripts that only outputs one type of object, since scripts are limited to a single
pipeline. Howeever this causes small problem during script development, that is sometime for debugging and development, you will want to output some strings to check everything is going ok (e.g. echoing line numbers). Luckily you can do this using "write-verbose", this is a special case designed for this purpose. More about write-verbose later.   

When you run a linux shell script, the script's execution takes place in it's own shell. The same is kind of true for powershell scripts. However the 
terminology is a bit different. A scope is kind of container for containing things like aliases, functions, and variables. 

The shell itself is a top-level scope, and hence is called a "global scope". When a script is run, a new scope is created for the script 
and that scope is called the "script scope".  The script scope is a subsidiary of the global scope, and is often referred to as 
the child of the global scope. Whereas the global scope is referred to as the parent scope.  

Functions also get their own "private scope". 

A scope only lasts as long as you need to execute whatever is in the scope. Once a scope vanishes, it takes everything inside it with it. 

In linux, you have to pass all the variable into a script using parameters. If there is a variable in the script that hasn't been defined (although
it has been defined in the shell that is invoking the script), then the script can fail due to the undefined variable. This is not the case in 
powershell. However that is not th case in powershell. If a script requires a variable that hasn't been defined in the script scope, then it will 
start searching for it in it's parent scope, and keep looking upwards throught the scope chain until it finds the parameters value. In other words
a child scope is not encapsulated from it's parent scope. So to avoid confusion on where values are are coming from, you can:

	- Define all variables within the script (excluding script parameters)
	- Ensure all parameters have default values. 
	
With respect to functions, linux and powershell behaves in the same way in the sense that they both look within the script scope. However powershell
will move up to parents if it can't find what it wants in the script scope.   

However, luckily, you can do sourcing in powershell (by placing this line in the powershell script):

. c:\path\to\env\properties\script		# this script contains environment related variable settings.

The above acts like replacing the above line with all the content of the environment script, so that it is within the same script scope. 

]]></Content>
		<Date><![CDATA[2014-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell - Parameterizing your PowerShell scripts]]></Title>
		<Content><![CDATA[Chapter 22

Previously we showed how you can pass parameters into a script using the param construct: 

param(
[string] $Environment = 'stress',			
[string] $Application = 'dropbox',
[switch] $RecommendedSettings = $true		
)

For the rest of this chapter we are going to cover some cool builtin scripting tools that you can use when developing shell scripts. 
For these to work you might need to enable these features by inserting the following line before 
the param section, (but after the help section, if any):

[cmdletbinding()]

for example like this:

[cmdletbinding()]
param(
[string] $Environment = 'stress',			
[string] $Application = 'dropbox',
[switch] $RecommendedSettings = $true		
)


Note, if you insert this, then you always have to include the param construct, if you're script doesn't require any parameters, then simply insert
and empty param construct "param()" 

If you want to make one of these paramaters mandatory (and hence has no default), then do:

param(
[string] $Environment = 'stress',
									# I added blank line for easier readability.
[parameter(mandatory=$true)]
[string] $Application,				# I have now made this mandatory. I did this by removing the default, 
									# and preceding the line with an attribute for this parameter. 

[switch] $RecommendedSettings = $true		
)
 
 
Now when a user runs the above script, they must either specify the "-applicaton" option on the command would. Otherwise they will get prompted
for the value by a pop up window (or in the command line).

You can also give a parameter an additional name, to make it easier for the user:

   
param(
[string] $Environment = 'stress',

[alias('software')]					# here we added an alias to the $application				
[parameter(mandatory=$true)]
[string] $Application,				

[switch] $RecommendedSettings = $true,

[ValidateSet(2,5)]			# Here is another cool feature, where we are saying that $version can only take the values of 2 or 5.
[int] $version = 2		
)
 
As a result, both of the following commands now do the same thing:

.\script.ps1 -application dropbox 
.\script.ps1 -software dropbox 
 
 
Another cool feature is using powershells builtin "-verbose" mode which you can use within a script. Within your script, type:

write-verbose: "Successfully connected to the remote machine." 
 
Then to view this message you simply type "-verbose" on the command line:

.\script.ps1 -application dropbox -verbose

]]></Content>
		<Date><![CDATA[2014-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell - Regular Expressions]]></Title>
		<Content><![CDATA[Note: chapter 23 was skipped, nothing useful

Chapter 24 - regular expressions

You can use an operator called "-match" which pretty much is the linux equivalent of grep. "-match" is referred to as an operator rather than a command. Here are some examples:

PS C:\> "hello" -match "el"
True
PS C:\> "hello" -match "^he"				# Matches pattern at the beginning of the line. Just like linux
True
PS C:\> "hello" -match "lo$"				# Matches pattern at the end of the line. Just like linux
True

PS C:\> "don" -match "d[aeiou]n"
True
PS C:\> "doon" -match "d[aeiou]n"
False
PS C:\> "doon" -match "d[aeiou]{2}n"	# "{2}" means match exactly 2 instances of the preceding character. 
True
PS C:\> "d_m9n" -match "d\w\w\wn"   	# "\w" matches a letter, or number or underscore.
True
PS C:\> "d-%(n" -match "d\w\w\wn"
False
PS C:\> "d-% n" -match "d\W\W\Wn"  		# "\W" Matches everything else that "\w" doesn't match. Even white space. 
True
PS C:\> "d-%2n" -match "d\W\W\Wn" 
False
PS C:\> "d n" -match "d\sn"  			# "\s" matches white(s)pace, e.g. space bar, tab, or carriage return.
True
PS C:\> "don" -match "d\sn"  # "\s"
False
PS C:\> "don" -match "d\Sn"  		# "\S" Matches everything else that "\s" doesn't match. 
True
PS C:\> "d n" -match "d\Sn"  		# "\S"
False
PS C:\> "d!%4Z n" -match "d.....n"  		# A period matches any letter, or number, or special character, or even whitespace!
True

PS C:\> "dns" -match "d[k-p]s"  # matches any letter that falls between k-p. 
True
PS C:\> "dxs" -match "d[k-p]s"   
False
PS C:\> "dxs" -match "d[k-p,w-z]s"  # matches any letter that falls between k-p, or w-z. 
True
PS C:\> "dxs" -match "d[^k-p]s"  	# "^", when used inside square brackets, this means match anything except for letters k-p. 
True
PS C:\> "don" -match "do?n"  		# "?" means matching 0 or 1 instance of the preceding character, which in this case is "o" 
True
PS C:\> "dn" -match "do?n"  
True
PS C:\> "doon" -match "do?n"  
False
PS C:\> "dn" -match "do{0,1}n"		# You can replace "?" with {0,1}
True
PS C:\> "dn" -match "do*n"  # "*" means matching 0 or more instances of the preceding character, which in this case is "o" 
True
PS C:\> "don" -match "do*n"  
True
PS C:\> "dooon" -match "do*n" 
True
PS C:\> "dooon" -match "do{0,}n" 	# You can replace "*" with {0,}
True
PS C:\> "doooon" -match "d[aeiou]+n"   #"+" means matching one or more instance of the previous character
True
PS C:\> "dn" -match "d[aeiou]+n"
False
PS C:\> "doain" -match "d[aeiou]+n"
True
PS C:\> "dotin" -match "d[aeiou]+n"
False
PS C:\> "doain" -match "d[aeiou]{1,}n"	# You can replace "+" with {1,}
True



PS C:\> "dn" -match "d[aeiou]+n"
False
PS C:\> "doain" -match "d[aeiou]+n"
True
PS C:\> "dotin" -match "d[aeiou]+n"
False
PS C:\> "dean" -match "d[aeiou]n"
False
PS C:\> "d9n" -match "d\dn"     		# "\d" means match any digit from 0-9
True
PS C:\> "d9n" -match "d\Dn"    # "\D" matches anything except digits. 
False
PS C:\> "de! *n" -match "d\D{4}n"   
True
PS C:\> "192.168.0.1" -match "\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}"	# Here is how you can grep for ip numbers. 
True

PS C:\> "d13n" -match "d\d+n"     
True
PS C:\> "dhellon" -match "d(hello)+n"	# This matches the block "hello" 1 or more times.  
True
PS C:\> "dhellohellohellon" -match "d(hello)+n"
True
PS C:\> "dhellohelllohellon" -match "d(hello)+n"	# You can also use blocks in the same way with "*", "?" and "{}"
False
PS C:\> "dn" -match "d(hello)+n"
False


help about_regular_expressions		# Here you can find more info about regular expressions. 

Note: all the above are not case sensitive, but you can do case sensitive by using "-cmatch".


Now you understand how regular expressions work, you can then use them in commands that accepts the "-match" operator, e.g.:

PS C:\> Get-Service | Where-Object -FilterScript {$_.Displayname -match "d[aeiou]n"}

Status   Name               DisplayName                           
------   ----               -----------                           
Stopped  MSDTC              Distributed Transaction Coordinator   


#####################################################################################
##### Detour regular expressions - start
#####

There is a special reserved variable, called "$matches" that stores the result of a regex query. 

e.g. 

Lets say we have the following string variable:

$component = "Bulk Update (BU)"

...and we want to pull out whatever is inside the round brackets, in this case we can do something like this:

PS C:\> $component -match '\(.+\)' 
True

However the actual matching portion is automatically stored in "$matches", which is special type of hash table:

PS C:\> $matches

Name                           Value                                                                                                    
----                           -----                                                                                          
0                              (BU)     

now you can pick out the value itself like this:

PS C:\> $matches[0]
(BU)

Also if you are just interested in the letters and not in the round brackets, you can do:

PS C:\> $matches[0].trimstart("(")
BU)

PS C:\> $matches[0].trimend(")")
(BU

Combining both together gives:

PS C:\> ($matches[0].trimstart("(")).trimend(")")
BU

# for more about trim, checkout : http://social.technet.microsoft.com/Forums/windowsserver/en-US/2576a409-6cc2-4772-bfb7-8c0b79b7c015/powershell-what-is-an-equivalent-of-substring-ltrim-rtrim-functions?forum=winserverpowershell

#####
##### Detour regular expressions - end
#####################################################################################
 

However if you want to search through files, then you can use regex in conjunction with the select-string command (along with the "-pattern" option): 

#first I created the file 
PS C:\Documents and Settings\SChowdhury\Desktop> "This is a testfile" > testfile
PS C:\Documents and Settings\SChowdhury\Desktop> "This is another line to experiment with greping" >> testfile
PS C:\Documents and Settings\SChowdhury\Desktop> "How about a third line." >> .\testfile

#Then checked the contents of the file
PS C:\Documents and Settings\SChowdhury\Desktop> Get-Content .\testfile
This is a testfile
This is another line to experiment with greping
How about a third line.

# Now greped the file
PS C:\Documents and Settings\SChowdhury\Desktop> Get-Content .\testfile | Select-String -Pattern "[ml]ine"
This is another line to experiment with greping
How about a third line.

This technique is useful if you want to search through a log file. However windows has lots of log files, and if you don't know which log file 
to look in then can do:

# First here are 2 log files that I created which have different names and stored in different directories:

PS C:\> Get-Content "Documents and Settings\SChowdhury\Desktop\testlogfile.log"
this is a test log file a c
this is a test log file def
this is a test log file abc

PS C:\> Get-Content "Documents and Settings\SChowdhury\My Documents\Downloads\bestlogfile.log"
this is a test log file a!c
this is a test log file def
this is a test log file a9c

# Now do the search to recursively check each directory in the c drive for a file name ending with "logfile.log" and then for each file
# found, return the lines which has a regex match for "a.c":

PS C:\> Get-ChildItem -path c:\ -Filter *logfile.log -Recurse | Select-String -Pattern "a.c"

Documents and Settings\SChowdhury\Desktop\testlogfile.log:1:this is a test log file a c
Documents and Settings\SChowdhury\Desktop\testlogfile.log:3:this is a test log file abc
Documents and Settings\SChowdhury\My Documents\Downloads\bestlogfile.log:1:this is a test log file a!c
Documents and Settings\SChowdhury\My Documents\Downloads\bestlogfile.log:3:this is a test log file a9c

Notice that the output is columns (properties) that are colon delimited, they are:

	1. Full path of the file
	2. Line number where a match has been found
	3. The actual line itself
	
You can format the output by first using get-member to find the property names, and then output the properties that you are interested in using format-table, in my case that is:

PS C:\> Get-ChildItem -path c:\ -Filter *logfile.log -Recurse | Select-String -Pattern "a.c" | Format-Table -Property path,linenumber,line -wrap

Path                                                               LineNumber Line                                                    
----                                                               ---------- ----                                                    
C:\Documents and Settings\SChowdhury\Desktop\testlogfile.                   1 this is a test log file a c                             
log                                                                                                                                                                        
C:\Documents and Settings\SChowdhury\Desktop\testlogfile.                   3 this is a test log file abc                             
log                                                                                                                                                                        
C:\Documents and Settings\SChowdhury\My Documents\Downloa                   1 this is a test log file a!c                             
ds\bestlogfile.log                                                                                                                                                         
C:\Documents and Settings\SChowdhury\My Documents\Downloa                   3 this is a test log file a9c                             
ds\bestlogfile.log                                                                                                                                                         


So far we saw how use regexes in a couple of ways, here is a more complete list:

	- Where-Object -FilterScript (using the "-match" or "-cmatch"operator)
	- switch 	(this is used in scripting, similar to linux's case statement I think)
	- Select-string (using the "-pattern" option)
	

	
	
cd $home 		# this is the linux exquivalent of "cd ~". 	
	























-- detour

-
- get-content textfile.txt # equivalent to linux's "cat" command
-
-- detour

When using get-help, you can use the example switch to just view the examples section of the full man page:

get-help get-eventlog -examples

Here is a quick way to do a regular expressions (regex) on lists:

get-help * | where-object {$_.name -match "about"}

Alse see this example
Get-ChildItem | Where-Object -FilterScript {$_.name -match "fmeEngineConfig_[0-9]{1,}.txt"}
# the regex here has "{1,}" which means find one or more matches of the preceding regex character. SO this will return things like:

Mode LastWriteTime Length Name
---- ------------- ------ ----
-a--- 08/01/2014 11:37 20947 fmeEngineConfig_1.txt
-a--- 08/01/2014 11:37 20947 fmeEngineConfig_10.txt
-a--- 08/01/2014 11:37 20947 fmeEngineConfig_2.txt
-a--- 08/01/2014 11:37 20947 fmeEngineConfig_100.txt

But it will not return things like:
Mode LastWriteTime Length Name
---- ------------- ------ ----
-a--- 08/01/2014 11:37 20947 fmeEngineConfig.txt
-a--- 08/01/2014 11:37 20947 fmeEngineConfig_.txt

Also see this for more examples: http://technet.microsoft.com/en-us/library/ff730947.aspx

Here are the standard regular expression (regex) operators:

? The preceding item is optional and matched at most once.
* The preceding item will be matched zero or more times. Here, the * is not treated like a wildcard.
+ The preceding item will be matched one or more times. NOTE: This doesn't actually work in powershell, so use "{1,}" instead
{n} The preceding item is matched exactly n times.
{n,} The preceding item is matched n or more times.
{,m} The preceding item is matched at most m times.
{n,m} The preceding item is matched at least n times, but not more than m times.

Here, we are applying a filter on the table of contents displayed by the "get-help" command, and filtering based on the "name" column, where there is a pattern match for the word "about". In linux, we can achieve the same result by using the awk command for identifying the correct column, and then use grep.

Hence the "where-object" command acts as a combination of both grep and awk. Here is another way to get the desired effect (using wildcards):

get-help about* # This lists help topics that are not specific to any command.




# [regex] $buildnumber = "_[0-9][0-9][0-9][0-9]$"

# $VersionToBeDeployed = $Version -replace "$buildnumber","" -replace "comp_BLD_",""

# Retrieves full name of comp currently deployed.]]></Content>
		<Date><![CDATA[2014-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell - Additional random tips tricks, and techniques]]></Title>
		<Content><![CDATA[Chapter 25 - Additional random tips tricks, and techniques

In Linux, you have the .profile script in your home directory. This is where you can place various commands and settings that get set up when you 
start a new shell (e.g. set up new aliases too). You can also do the same thing in ps, however you have to take extra care, because ps has two hosting applications that accesses the ps engine, the console host and the ISE host.  



For more info see:

help about_profiles


In ps, there are a number of profile files (some or all of which you might need to create yourself), and these script are run in the following order:

	1. $pshome/profile.ps1  							# This will execute for all users and all hosting application. 
	2.  - $pshome/Microsoft.Powershell_profile.ps1  	# This will execute for all users  but only if they are running the console host. 
		- $pshome/Microsoft.PowershellISE_profile.ps1  	# This will execute for all users but only if they are running the ISE host.
	3. $home/Document/WindowsPowerShell/profile.ps1		# This will execute for the current user only and on all hosts. 
	4. - $home/Document/WindowsPowerShell/Microsoft.Powershell_profile.ps1   # This will execute for the current user only but only for the PS console 
	   - $home/Document/WindowsPowerShell/Microsoft.PowershellISE_profile.ps1   # This will execute for the current user only but only for the PS ISE  
	
If any of the above don't exist then ps will simply skip to the next one. 



Now we take a look at operators, which will make more sense by looking at some examples:


PS C:\> 1001 / 3
333.666666666667
PS C:\> 1001 / 3 -as [int]		# The "-as" operator does it's best to convert one data type to another. 
								# In this case it converts this decimal to an integer by rounding (up or down) to the nearest digit. 
334
PS C:\> $number = 1001 / 3 -as [int]
PS C:\> $number
334

Other available types are:
		[int]
		[xml][/xml]
		[string]
		[datetime]
		[single]
		[double]
		....and others. but these are the main ones. 
		
PS C:\> 1001 / 3 -is [int]		# The "-is" operator is a check to see if an object is a certain type or not.  
False
PS C:\> 1002 / 3
334
PS C:\> 1002 / 3 -is [int]
True	
	

	
PS C:\> "hello world" -replace "hello","goodbye"
goodbye world

The "-replace" operator does the same thing as linux's "sed s/.../.../" command. 

If you want to do this to a whole file (i.e. the (g)lobal option sed s/.../.../g), then do:

PS C:\> Get-Content .\testfile.txt
hello world
also hello to the universe. 
PS C:\> (Get-Content .\testfile.txt) -replace "hello","goodbye"
goodbye world
also goodbye to the universe. 


Now lets look at some operators that are designed for arrays. 

PS C:\> [array]$array = @()  # This creates an empty array

PS C:\> [array]$array = 'one','two','three','four'
PS C:\> $array
one
two
three
four
PS C:\> $array -join "-"	# The "-join" operator joins all the array items into a string: 
one-two-three-four 
PS C:\> $newvar = $array -join "-"
PS C:\> $newvar 
one-two-three-four	

	
PS C:\> $alphabet="a b c d"
PS C:\> $alphabet
a b c d
PS C:\> $newarray = $alphabet -split " "
PS C:\> $newarray
a
b
c
d
$newarray[2]

If you have an array that contains a lot of list items, and you are looking if it contains a particular array item, then you can view them like this:

PS C:\> $collection = 'abcd','efgh','ijkl'
PS C:\> $collection 
abcd
efgh
ijkl
PS C:\> $collection -like "*fg*"		# "-like" will return results and lets you use wildcards. It is similar to "-match"
efgh
PS C:\> $collection -contains "efgh"	# "-contains" is used when you know exactly what you are looking for. It doesn't let you use wildcards. 
True
	
	
# Now we'll take a look at string manipulation. 

If you do a gm on a string, you will find it has a lot of methods. Here are some examples:

PS C:\> $testvar = "1234x678"
PS C:\> $testvar.IndexOf("x")	# The "IndexOf" method tells you the location of a given character.
4								# The position counter starts from 0 and not 1.

PS C:\> $testvar.split("x")		# works similary to the "-split" operator. 
1234
678 

PS C:\> $testvar
1234x678
PS C:\> $testvar.replace('x','z') 	# works similarly to the "-replace" operator. 
1234z678
	
	
PS C:\> $testvar = "   abcDEF "
PS C:\> $testvar.toupper()		# changes whole string to upper case
   ABCDEF
PS C:\> $testvar.tolower()		# changes whole string to lower case
   abcdef
PS C:\> $testvar.trim()			# removes any whitespace. There is also trimend() and trimstart() which involves rempving whitespaces from either side.
abcDEF

("AbC").toupper()			# you can also pass a string directly into a method, rather than storing it in a variable first. 
ABC


 Another useful command is the command that outputs the date:
 
PS C:\> get-date
14 August 2013 08:12:04
	
If you view this command's methods, you will find that you can manipulate the outputting object in the following ways:

(get-date).month			# Notice that to use this method we had to put the main command in brackets.
8	
	
	
$currentDate = get-date			# you can also do the store-object-in-variable-first approach. 
$currentDate.adddays(-90)		# gives you the date that was 90 days ago. 
16 May 2013 08:31:18	

PS C:\> $currentDate.ToShortDateString()		#This method used the machine's current regional settings before determining the appropriate format. 
14/08/2013


ToShortDateString() is a method that is available with all wmiobjects. 
 

a lot of wmi objects have date and time info. However, the date/time format is difficult to read:


 
PS C:\> $os.converttodatetime($os.lastbootuptime)
05 August 2013 08:06:26 

 
 
PS C:\> Get-WmiObject win32_operatingsystem | Select-Object -Property lastbootuptime
lastbootuptime                                                                                                                                                             
--------------                                                                                                                                                             
20130805080626.375000+060 
 
 
Fortunately ALL wmi objects have a ConvertToDateTime method, to transform the date/time into something that's easier to read:

PS C:\> (Get-WmiObject win32_operatingsystem).convertToDateTime((Get-WmiObject win32_operatingsystem).lastbootuptime) 
05 August 2013 08:06:26 

I have rewritten the above command into a slightly more easier to read format: 
PS C:\> $wmiobject = Get-WmiObject win32_operatingsystem 
PS C:\> $wmiobject.converttodatetime($wmiobject.lastbootuptime)
05 August 2013 08:06:26
 


In powershell, many commands have parameters that have default values. For example "get-childitem" has the optional "-path". This is optional 
because it has a default (which turns out to be the current directory). 

In PS you can define your own defaults for any parameter in any command. These defaults are stored in a 
special built-in variable called called $PSDefaultParameterValues. This variable is designed to store a hash table. 

By default this variable is empty when you start a new shell window, but you can fix that by defining the $PSDefaultParameterValues in the profile script.  

For example in PSv3, you can create a credential object that stores your username/password using:

get-credential		# This commands opens up a popup windows prompting for username/password. After that a credential-object is created/. 

This object can then be fed into any commands that has a "-credential" parameter.     


So here is how to get a new default added to the $PSDefaultParameterValues (in this case for the -credential parameter):

$MyLoginDets = get-credential -username admin -message "please enter your password"

Now you need to link (pair) $MyLoginDets with the -credential (using hash table), and then feed that into $PSDefaultParameterValues using the buitin variables's "add" method:

PS C:\>  $psdefaultparametersvlaues		# currently empty
PS C:\>  $psdefaultparametersvlaues.add('*:credential',$logindets)   
PS C:\>  $psdefaultparametersvlaues

Name							Value
----							-----
*:credential					system.management.automation.pscredential

The asterisk means that this is the default value for all commands. However if you only want to use this as the default for "invoke-command", 
then do:

$psdefaultparametersvlaues.add('invoke-command:credential',$logindets)

Also in psv3, for more info check out:

help about_parameters_default_values		



## Playing with script blocks

scripts blocks are basically a block of commands that you can embed within another command. The block is indicated by curly braces {...}.

Note: hash table syntax is @{...}, this is an exception to the above rule since these curly brackets will hash table data and not commands. 

scriptblocks are used in several places:

	- with where-object's -filterscript parameter (in fact it is a mandatory parameter here).
	- with foreach-object's "-process" parameter (in fact it is a mandatory parameter here).
	- Inside of hash tables (e.g. to convert bits to megabytes)
	- Commands that accepts the "-scriptblock" command, e.g. "invoke-command" and "start-job"

for more info, check out:

help about_script_blocks

]]></Content>
		<Date><![CDATA[2014-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell - Using Someone else's script]]></Title>
		<Content><![CDATA[Chapter 26 - Using Someone else's script

for lots of sample powershell scripts, check out this code repository:

www.poshcode.org

Here is the structure of the if-else statement that you can place inside a script:

[cmdletbinding()]
	param(
		[parameter(mandatory=$true)]
		[int] $var
	)

# Here is an if-elseif-else statement. For some reason this is if-conditions are if-else conditions are not really covered in this book
# but is covered in the 2nd book. 
if ($var -gt 2)
{
    Write-Host "The value $var is greater than 2."
}
elseif ($var -eq 2)
{
    Write-Host "The value $var is equal to 2."
}
else
{
    Write-Host "The value $var is less than 2 or was not created or initialized."
}
	

In bash, when you want to check whether an ftp/scp transfer has been successful, then you check the exit code ($?) right after 
running the ftp/scp command. This check is done by placing the exit code within it's own if statement, i.e.:

if [ $? -gt 0 ]; then 
	log "file transfer failed"
	archive fail $file
fi


However, in ps, you do the same thing, but with construct called the try-catch. Covered in the next book.

]]></Content>
		<Date><![CDATA[2014-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell - Scripting overview]]></Title>
		<Content><![CDATA[Chapter 2 and 3 - Powershell scripting overview

You will notice that a ps script's extension is ".ps1" and not. The "1" indicates the version of ps's scripting 
language (which hasn't really change much since ps first came about) as opposed to the version of ps itself. 

You should try using ps ISE for writing/editing scripts rather than notepad++. That's because of the autocomplete feature. 


Chapter 3 - Powershell's scripting language

Everything that's outputted by a script actually runs through a single pipeline. That means that if the script is outputting several types of outputs 
then the subsequent outputs after the first one is outputted in format-list type. 

The following is also sent through a single pipeline:

get-service;get-process 

That's why it is best practice to design ps scripts that only outputs one type of object.



when you store an object like this:
$var = get-process

Then this object is actually storing a collection of objects, since each row is an object. There you can view a specific object like this:

PS C:\> $var[0]
Handles  NPM(K)    PM(K)      WS(K) VM(M)   CPU(s)     Id ProcessName                                                                                                      
-------  ------    -----      ----- -----   ------     -- -----------                                                                                                      
     84       3     1208       3404    32     2.31   3524 alg                                                                                                              

PS C:\> $var[1]
Handles  NPM(K)    PM(K)      WS(K) VM(M)   CPU(s)     Id ProcessName                                                                                                      
-------  ------    -----      ----- -----   ------     -- -----------                                                                                                      
    125       6     8332      10468    73   471.50    476 cagent32                                                                                                         


You can even drill down further to get column entries like this:

PS C:\> $var[0].processname
alg
PS C:\> $var[1].processname
cagent32
PS C:\> $var[0].vm
33480704

In effect, you can think of this variable as an array of objects. 

Now, if you try this:

PS C:\> "The first process is using up $var[0].vm of memory"   	# This will actually fail because the double quotes causes confusion. 

Hence the correct way to do this is by using the $(...) construct like this:

PS C:\> "The first process is using up $($var[0].vm) of memory"
The first process is using up 33480704 of memory

The $(...) is essentially the linux equivalent of the back-ticks: `...`.  Anything inside the $() gets treated as code and is evaluated first 
before the rest of the command is evaluated. 


PS C:\> "The first process is using up $("a lot of") of memory"
The first process is using up a lot of of memory 


You also have "`" for escaping characters:
PS C:\> "Hello my name is: `tSher `nI have `$20 in my wallet"
Hello my name is: 	Sher										# "`t" this create a tab. "`n" indicates new line (i.e. return carriage). 
I have $20 in my wallet											# the "`" used here caused the $ sign to be treated as a literal character. 

Note, in the above, we gave special special meanings to "t" and "n", but took away the special meaning for "$" using the back-tick character.

The linux equivalent to "`" is the "\".

Going back to an earlier example:



PS C:\> $var[1]
Handles  NPM(K)    PM(K)      WS(K) VM(M)   CPU(s)     Id ProcessName                                                                                                      
-------  ------    -----      ----- -----   ------     -- -----------                                                                                                      
    125       6     8332      10468    73   473.58    476 cagent32                                                                                                         
PS C:\> $var[1].processname												# see note 1 
cagent32

# note 1 - The period, "." after a variable name means....."I don't want to access the entire object with this variable, I just want to access 
# just one of it's properties or methods". After the period, provide a property or method name. Note: method names are ALWAYS followed by ().

Some methods accepts arguments, in which case you place them in the round brackets, comma delimited style. Other methods, don't require in which 
case you live the round brackets empty. Hence, here is an example:    

IMPORTANT: As mentioned a few lines above. You can access a variable's (which can also store objects, in fact a variable is an object) method/property by typing a period (.) followed by the properties or methods name. Note, method names are ALWAYS followed by "()", which makes it 
easier distinguish whether you are accessing a property or applying a method. Also when dealing with methods, the "()" can contain parameter values. 

Also note that you cannot use the above (period) technique on a "collection". But the purpose of this technique is to use it on a specific object:   


PS C:\> $myservices = Get-Service		# $myservice is a "collection" type object, essentially it is an array of objects.
PS C:\> $myservices[0]				# now we are looking at specific object (that is in the array)
Status   Name               DisplayName                           
------   ----               -----------                           
Stopped  Alerter            Alerter        

PS C:\> $myservices[0].name			# Now we are accessing a specific property within the object. 
Alerter

Important: here is a really cool thing, a property (e.g. $myservices[0].name) can itself be an object!!! This means you can use/access a property's 
method/property:

$myservices[0].name	| gm		# This will list all the availablee methods and properties for the "name" property  

The above lists a propery called "length" and a method called "substring", which you can try out:

PS C:\> $myservices[0].name.length 
7


$myservices[0].name.substring(2)	# Notice we are accessing a method as indicated by the round brackets. This method removes the first "x"
erter								# (in this case 2) characters and returns the rest. 		

Even the output of a method can be an object in itself:

PS C:\> $myservices[0].name.substring(2).length
5

The above is a really important concept that we will be using regularly!! 


PS C:\> $name | format-list -Property length
Alerter
PS C:\> $name.length			# not sure what is happening here, should display "Alerter"
7
PS C:\> $name.ToUpper()
ALERTER


Round brackets indicates the order of execution:

$name = (get-service)[0].name		# first, the round bracket gets replaced by a collection type object. 
									# Then we pull out the first object, then pull out the name properties of that object.  

get-services -computername (get-content machinenames.txt)	#assume text file contains one pc name per line. 

									
You can pass a condition result directly into an "if" statement

$status=$true		# Note, "$true" is a reserved variable of the object type boolean. Do "$true | gm" to confirm. Same is true for $false.
if ($status)
{
    Write-Host "The status is positive"		#This line will get executed. 
}
else
{
    Write-Host "The status is negative"
}



 
We also have the switch construct (the linux equivalent to this is the "case" statement): 

$status = "hot"
switch -wildcard ($status)			# you need to enable "-wildcard" in order to allow regex based matches. 
{ 
    0 {$status_meaning = 'ok'}
    1 {$status_meaning = 'error'}
    2 {$status_meaning = 'timed-out'}
    "h*" {$status_meaning = 'over-heated'}
    default {$status_meaning = 'Something really bad has gone wrong!!!'}
}
"Hear is the status meaning: $status_meaning"


The above will output:
Hear is the status meaning: over-heated

That's because "hot" is matched by the h* regex, and hence this sets the value for the $status_meaning.


In ps, you can concatenate strings: 

PS C:\> [string]$datetime = get-date
PS C:\> $message = "- INFO: Batch job completed successfully."
PS C:\> $logentry = $datetime + $message			# Here we concatenate two existing string variable to create a new string variable.  
PS C:\> $logentry
08/15/2013 15:48:07- INFO: Batch job completed successfully.
PS C:\> $logentry += " So no need to panic :o)"			# Here we use "+=" to append to an existing variable . 
PS C:\> $logentry
08/15/2013 15:48:07- INFO: Batch job completed successfully. So no need to panic :o)


Here is another switch example:

$servername=DCFILE
$result = ""
switch -wildcard ($servername)
{ 
	"*DC*" 	 {$result += 'Domain Controller'}
	"*FILE*" {$result += 'File Server'}
	"*SQL*"  {$result += 'SQL server'}
	"*EXCH*" {$result += 'exchange server'}
}
$result 

The above will output:

Domain ControllerFile Server


That's because there are 2 regex matches in the switch. and since we used "+=" rather than =, it ended up appending rather than overwriting.
If you want to exit after the first match (which I think happens in linux, but configurable in korn), then simply insert the "break" keyword:



$servername=DCFILE
$result = ""
switch -wildcard ($servername)
{ 
	"*DC*" 	 {
			   $result += 'Domain Controller'
			   break
			 }								# This time will exit switch block as soon stuff inside curly braces is run. 
	"*FILE*" {
			   $result += 'File Server'
			   break
			 }
	"*SQL*"  {
			   $result += 'SQL server'
			   break
			 }
	"*EXCH*" {
			   $result += 'exchange server'
			   break
			 }
}
$result 

The above will now output:

Domain Controller



test-path /path/to/folder		# this command checks if path to folder exists. Will either output true or false. 



Now lets take a look at looping constructs. Here is the construct for the do-while loop:

$counter=0
while ($counter -lt 6)  {
	"Hello counter is now equal to $counter"
	$counter++
}


Here is the while loop:
$counter=0
while ($counter -lt 6)  {
	"Hello counter is now equal to $counter"
	$counter++
}

By the way, you can do the linux equivalent of "&&" like this:

http://stackoverflow.com/questions/3936518/powershell-how-to-check-for-multiple-conditions-folder-existence 



Next we have the foreach loop:

$services = get-service | Select-Object -First 5			
foreach ($ServiceItem in $services) {
    $ServiceItem.name    					# one of the strengths of the foreach construct is that it will go through a table (object) 
											# and process each row (object) in turn.
}


Here is another way to write the above loop,  but this time using the "foreach-object" cmdlet:

get-service | Select-Object -First 5 | ForEach-Object -process {	# "-process" is a mandatory positional parameter, to accept scriptblock that are inside the curly brackets. 
	$_.name															# The $_ syntax represents the current piped in object. 
}


Note, in both foreach and foreach-object loops, you can skip to the next loop, but you use a different trigger word, in foreach, you use the traditional "continue" statement. But in "foreach-object" you use "return", that's because foreach-object is actually a cmdlet. 

The linux equivalent of the foreach loop (or the foreach-object construct) is something like "for file in `ls -l`" 
 
In Powershell, we use the term "enumerate" to refer to going through a list of items one at a time.  
 
 
Here is the for-loop construct (which is used to repeat a code block a set number of times):

for ($i=1; $i -le 5; $i++) {
	"This is line $i"
}
 
The above will output:

This is line 1
This is line 2
This is line 3
This is line 4
This is line 5


 You can replicate the for-loop construct using the foreach construct instead:
 
 1..5 | foreach-object -process {
 "This is line $_"
 }							# Note the "break" statement doesn't work here. It ends up exiting the whole script. Need to 
							# investigate if same also happens in PSv3. 
 
 Notice here that you don't need to set up a counter like $i. However this construct does have a built-in counter called "$_". 
 
 
 There are 2 keywords you can use to control loops:
 
	- Break   		(this is to prematurely exit a loop, or other constructs such as switch-statements or if-conditions)
	- Continue 		(this is to skip to the next iteration of a loop)
 
The "continue" keyword can only be used within a loop, but the "break" statement can be used in other places, e.g. in a switch statement, as shown 
earlier. If "break" is used  
 
 
If the "break" command is placed inside of a script, but outside of a construct (i.e. loops, if-conditions, or switch statements), then it will exit
the whole script when it is executed. The "break" command is the linux equivalent of the "return" command. 

PS also uses the "return" but this will exit the whole script regardless of where you place. In other words, the PS's "return" command is the
linux equivalent of the "exit" command. 

PS also uses the "exit" command. In this case, in PS, the "exit" and "return" both do the same thing, and you can change them interchangeably. However return can output a final string as part of the output. E.g. :

return "goodbye world"		# the goodbye    

However, I believe the best practice should be never use "return" on powershell, and instead just use "break" to exit out of a construct, and "exit"
to exit out of a whole script. 
 

]]></Content>
		<Date><![CDATA[2014-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell - Simple scripts and functions]]></Title>
		<Content><![CDATA[Chapter 4 - Simple scripts and functions

Here is the standard syntax for writing a function in a script:

function hello 
{
    "hello world"
}
hello	# Here I have called the function so that you can see it in action. 

You can also parameterize functions, like this:

function hello 
{
	param(
			[parameter(mandatory=$true)]
			[string]$name 
		 )
    "hello my name is $name"
}
hello	# Here I have called the function so that you can see it in action. 

Notice that scripts and functions uses the same parametization syntax. 

In Linux it is possible to create your own custom commands, which are usually done by:

    1 using aliases  - this option is best for simple commands such as:     alias lsd='ls -d */'
    2 creating a whole script which defines the new custom command. Then update the $PATH variable (ideally in the .profile config time), 
      so that you can call the command (by the script's name) while in any directory, rather than constantly navigating to the script's folder.
      In this scenario, you have a file for each custom command, and hence you can end up with lots of script files. 
    3 You can have one really big script, where each custom command is contained within a function. Then to be able to run each function 
	  as a command, you simply "dot source" script so that the script loads into the current shell. You can then do the dot-sourcing
	  from within the .profile, so it happens on shell startup. 


In PS, you can also do all of the above. 

To create an alias, you use the set-alias command:

PS C:\> set-alias -name list -Value Get-ChildItem 

Note, to delete an alias, you need to use remove-item:

remove-item alias:{alias's name}


In PS, aliases are only limited to simply giving an alternate name to a single command. You cannot use it to store a pre-configured command. e.g. you cannot do:

set-alias FirstService {Get-Service | Select-Object -First 1}		# This will fail and give an error message.



To do something like this, you need to use approach 2 or 3. Approach 2 would be overkill, because it would end up being a script file containing a single line. So approach 3 is best. However you can also place it as a function in the .profile file. Another option is to define the function straight from the command line, using the function command:

PS C:\> function FirstService {Get-Service | Select-Object -First 1}
PS C:\> FirstService
Status   Name               DisplayName                           
------   ----               -----------                           
Stopped  Alerter            Alerter         


Approach 2 also works in PS, but there is no such thing as a $path variable. One way around this is to link up the script file to an alias:

C:\PS>Set-Alias -name {command's name} -value {path\to\file}

Note: I could be wrong about there not being a "$path" variable in ps, that's because see:
PS C:\> (Get-ChildItem Env:\Path | Select-Object -Property 'value') -split ";"


The final option is to create a script that contains lots of functions, and then dot source it just like linux. E.g. assume we have the following 
functions written in a script called "test.ps1", then do:

PS C:\> get-content .\test.ps1
function hello {"hello world"}
function goodbye {"goodbye world"}

Now dot source it:

PS C:\> . .\test.ps1    	# Note, ".\" means current directory, you can also use the full path.  
PS C:\> hello			# now this function runs like a normal command straight from the command line. 
hello world
PS C:\> goodbye
goodbye world

Note, you can even use tab to auto-complete these commands when writing them in the command line. 

To check that the functions has been added into memory do:

PS C:\> Get-Command | Where-Object -FilterScript {$_.name -match "hello"}


So far we have looked at three ways to create custom commands

	1. By creating aliases  (This doesn't really create custom commands, but give nicknames to existing commands, and also can help with approach 2)
	2. Create a standalone script to represent a custom command. 
	3. Create a function within a script to represent a custom command, then dot source the script

There is actually another (much better) way to create custom commands. And that is by creating custom commands in the form of modules. With modules, you can:

	- Define a way to load a script (and it's functions) into memory
	- Allow the functions to run on demand
	- And easily remove all those functions from memory 
	]]></Content>
		<Date><![CDATA[2014-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell - Scope]]></Title>
		<Content><![CDATA[Chapter 5 - Scope

Note: in csharp, even an if-else statement is it's own scope. See if this is also true in powershell. 

Scope is a containerization system designed to stop things from conflicting with each other. 
  
If you define a variable in a script, and then redefine it within a function that resides in the script, then that variable has only been redefined within the function and not in the script itself. 

Sometimes you will want to define a variable in a function and you want it to become available in the parent's scope. This is a useful way to get 
an output of a function to be used as part of the rest of the script, This is done using the new-variable command, here is the script content in this example:


"Current output is $output"
"Current output is $output1"
function hello {
    new-variable -Name output -Value 'Microsoft' -scope 1	# The "-scope" makes $output become available in current scope and one scope above.  
    new-variable -Name output1 -Value 'google'			# The value for "-scope" is zero, which is the current scope only.
    "The value of output within function is $output"
    "The value of output1 within function is $output1"
}
hello
"The value of output within script is $output"
"The value of output1 within script is $output1"


Running the above code will give:


PS C:\> .\test.ps1
initial output is 
initial output is 
The value of output within function is Microsoft
The value of output1 within function is google
The value of output within script is Microsoft		# As you can see "$output" is now present on the script level. 
The value of output1 within script is 

If you want to make a variable available everywhere, i.e. in all scopes then use the "global" value for the "-scope" parameter, i.e.:

new-variable -Name output -Value 'Microsoft' -scope global. 

Even sideway function can end up using this value for $output (by looking into parent(s) scopes), unless $output has already been defined within the function. 

If you want variable to exist just within the current scope, then you can omit defining "-scope", or do:

new-variable -Name output -Value 'Microsoft' -scope 0
new-variable -Name output -Value 'Microsoft' -scope local

A shorthand way to write global parameters, is:

PS C:\> $global:output = "Microsoft"		# This is similar to the linux variable export command.
PS C:\> $output
Microsoft

Note: There is also an "Allscopes", but not sure how this can be useful. 


You can also control how scopes operate using this command:

set-strictmode

This is a switch style command, and there are only 3 modes:

set-strictmode -off				# This is the default, where this feature is disabled. 
set-strictmode -version 2.0	# This mode will cause error messages if you try to use a variable that doesn't exist in current scope or in any 
							  parent scopes. 
set-strictmode -version 1.0	# Same as 2.0, except that it will ignore any undefined variables that are in double quotes. 

The version 2.0 above will be really useful for debugging purposes, and will stop you from running script where you have forgotten to define a 
variable. 

]]></Content>
		<Date><![CDATA[2014-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell - Advanced Functions (part 1)]]></Title>
		<Content><![CDATA[Chapter 6 - skipped, because it is common sense stuff.

Chapter 7 - Advanced functions, part 1

So far we have only looked at basic functions. Now we we will look at how create an advanced functions. Here is the high-level syntax of an advanced function:

function <name> {
	[CmdletBinding()]	# indicates that this is an advanced function.
	param(
						# input parameters go here. 	
	)

	BEGIN {} 
	PROCESS {} 
	END {}

}

You should try to name your functions in the consistent "verb-noun" format.  
 
get-verb	# This simply gives a complete list of microsoft's approved verbs.

BEGIN{} ,	PROCESS{},	and END {} are three named scriptblocks. These are similar to the awk command, and their header footer system. 

When the function is executed, ps runs the BEGIN block first, and the END block last. The PROCESS block:

	- is run once if function is triggered using just it's parameters
	- will run multiple times if function is called with a pipeline input, in which case it will run once for each object. 

If an array of data is fed through, e.g. say we have a computernames passed through as an array, then you can process them in the PROCESS block like this:

BEGIN {"Here is a list"} 
PROCESS {
    foreach ($computer in $computername) {
        "this computer is called: $computer"
    } 
}
END {'List has ended.'}



When using functions, one thing you will be doing a lot of is creating your own custom collection (table). This is done in a 3 step process (within a foreach statement):

	1. Gather each data sources and store them in variable objects.
	2. Define our custom object's new property names and assign values to them. This is done using hash tables. 
	3. Transform the hash-table into an object. 
	

Here is an example of this 3 step process:


Step 1
Lets say we want to create a 3 column (property) collection (note this colleciton will only house one row (object)):

	1. The "version" property from the win32_operatingsystem class
	2. The "servicepackmajorversion" property again from the win32_operatingsystem class. 
	3. The "Serialnumber" property from the win32_BIOS class 
	4. The "model" property from the win32_computersystem class again
	5. The "manufacturer" property from the win32_computersystem class

Step one: Gather the data sources
	
$OperatingSystemInfo = Get-WmiObject -Class win32_operatingsystem   # This object variable will contain values for the "version" and 
																	# "servicepackmajorversion" properties)

$biosInfo = Get-WmiObject -Class win32_bios							# This object variable will contain the value for the "Serialnumber" property.
																	# Since we plan use only value from this class, then we can be just lazy and do:
																	# (Get-WmiObject -Class win32_bios).serialnumber

$computerInfo = Get-WmiObject -Class win32_computersystem			# This object variable will contain values for the "model" and "manufacturer"
																	# properties



Step 2: Now that we have our datasources, we now define the property names for our custom object and assign values to them. This is done using hash tables:

$properties = @{
	'OSVersion'=$OperatingSystemInfo.version;						# Note, i dont think the single quotes are needed here,
	'SPVersion'=$OperatingSystemInfo.servicepackmajorversion;
	'BIOSSerial'=$biosInfo.serialnumber;
	'Manufacturer'=$computerInfo.manufacturer;
	'Model'=$computerInfo.model
}


Note: As indicated above, the general structure of a hashtable is as follows:

$properties = @{
	property's-name=property's-value;
	property's-name=property's-value;
	property's-name=property's-value;
	property's-name=property's-value;
				.
				.
				.
	property's-name=property's-value			# Note: the last line doesn't end with a semi-colon.
}
 

This will create the $properties object variable. The object-type of this variable is "hashtable". If you do gm:

$properties | gm

Then you will find that the it's properties are actually generic hashtable related properties, and doesn't correspond to the properties that have been defined in the above hash table, however the following does still work:

PS C:\> $properties.OSVersion
5.1.2600

PS C:\> $properties.SPVersion
3

PS C:\> $properties.BIOSSerial
HUB7350W1M

PS C:\> $properties.Manufacturer
Hewlett-Packard

PS C:\> $properties.Model
HP Compaq dc7700p Small Form Factor


Next if you want convert your hash-table (which is a bit like a pseudo object), into a proper object, then you need to do step 3.



Step 3 

Transforming a hash table into a real object is done using new-object:

$collection = new-object -typename PSObject -property $properties			#in this instance, this collection only contains one object (row).  

Note: A hashtable (aka associative array) is essentially a two-column table, first column is the "key" and second column is the "value". The above command essentially rearranges this so that each key's name becomes a column name, and each value falles under the corresponding value. 

Here, the "PSObject" typename is specifically designed for this purpose, i.e. convert a hash-table into an object. The resulting object is of the 
object type "PSCustomObject", which is essentially a generic object. 

Also note that "-property" is spefically designed to only accept a hashtable. See help page for new-object, to confirm this. 

Now if you output both the hashtable and custom object:

PS C:\> $properties
Name                           Value
----                           -----
Manufacturer                   Hewlett-Packard 
OSVersion                      5.1.2600       
BIOSSerial                     HUB7350W1M     
Model                          HP Compaq dc7700p Small Form Factor
SPVersion                      3 

PS C:\> $collection
Manufacturer : Hewlett-Packard
OSVersion    : 5.1.2600
BIOSSerial   : HUB7350W1M
Model        : HP Compaq dc7700p Small Form Factor
SPVersion    : 3

You will find both output the same information, but in slightly different format, however if you do:

$collection | gm

Then you will find that this object's properties now correspond with what has defined in the hash-table!!! 




So far we have seen how to create an (collection) object that only contains one (row) object. 

Now if you want to create a proper collection object that houses a group of objects, then you can do it like this:

$bigcollection = @()				# This creates an empty array. At this stage this variable doesn't belong to a typename, which means that 
									# you cannot do "$bigcollection | gm" yet.
									# the syntax for creating an empty array isn't very intuitive, but that's how it is, as described here:
									# http://technet.microsoft.com/en-us/library/ee692797.aspx (see the empty array section)



$bigcollection += $collection		# This essentially adds/appends our custom object to the array. $bigcollection will now take on the typename
									# of the $collectiontype.
									
If you want to delete this collection object, then simply do:

Remove-Variable -Name bigcollection				# Don't forget to omit the "$" symbol. 									












]]></Content>
		<Date><![CDATA[2014-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell - Advanced Functions (part 2)]]></Title>
		<Content><![CDATA[Chapter 8 - Advanced functions, part 2


you will recall that in ps, the output of a whole script is fed through a single pipeline, which in turn is passed through out-host which formats the output before being displayed on screen. Since out-host can only retrieve formatting information of one kind of object per pipeline, it means that it is best practice to follow the output-only-one-kind-of-object per script rule, to avoid incorrectly formatted outputs. However there is an 
exception to this rule, which is "write-verbose". anything outputted by write-verbose has it's own special separate pipeline. This makes write-verbose perfect for outputting debugging messages, e.g.:

write-verbose "line2808" 
   
The above output will not conflict with the the scripts, main output. 

By default write-verbose will not output anything, unless both of the following is true:
	- you type (the common parameter,) "-verbose" as one of the parameters in the script. This will switch-on the write-verbose mode. 
	- You have inserted "[CmdletBinding()]" just before the script's parameter declaration. 
	

Another write-* command that you can use which has it's own special pipeline (and doesn't impact the main output) is:

write-warning "line2808"

However this isn't as good as write-verbose because because you can enable/disable write-warning outputs like you do with write-verbose.  
Also the write-warning command is designed for showing error messages to the end-user, rather than the scripts developer. 

However write-warning is useful if your script fails and terminates early, then you can use 



You can configure scripts to accept input from the pipeline. This is done by applying a config setting to one of the script's or function's parameters:

Get-Content C:\testlist.txt		# Here we have a list of values to be piped in. 
function machines {
    param(
			[parameter(mandatory=$true,valuefrompipeline=$true)]
			[string[]]$computername 
    )
    BEGIN {"start of process"}
    PROCESS {
        foreach ($computer in $computername) {
            "We have a machine called: $computer"
        }    
    }
    END {"end of process"}     
}
	
machines -computername 'one','two','three'		# Here we are feeding a parameter with an array. 

'item1','item2','item3'  | machines				# Here we are piping in an array into the function. 

Get-Content C:\testlist.txt | machines			# Here we are piping in a document's content. 



Sometimes you will want to restrict the size of the array that can be accepted, e.g. a script might take a long time to process an array containing a million items. Here is how this is done:

 

 param(
			[parameter(mandatory=$true,valuefrompipeline=$true)]
			[ValidateCount(1,10)]									# This only allows arrays containing 1 to 10 items. 
			[string[]]$computername 
    )

However this only works if you pass in the data as parameters rather than piping it in. Need to investigate how to restrict piping in as well. 

There are a lot more of these kinds of configurations, and you can find them here:

help about_functions_advanced_parameters


You can also set up a on-off style "switch parameter". It is quite simple to do:

 param(
			[switch]$RunCleanUpScript 
    )

By default the value of this parameter is $false, unless the user runs the script with -RunCleanUpScript, in which case it will switch it on. 

  
If there is a mandatory parameter that a user is meant to define when running a script, then a popup prompt window will appear if they fail to declare it when starting the script. User can then enter the values into this prompt. You can customise the help message that appears with the 
popup message to help the user understand what information that they have to provide:


function machines {
    param(
			[parameter( mandatory=$true,
                        valuefrompipeline=$true,
                        HelpMessage="Please enter pc name or IP address")]		# This is the help message
            [ValidateCount(1,3)]
			[string[]]$computername 
    )
    
    BEGIN {"start of process"}
    
    PROCESS {
        foreach ($computer in $computername) {
            "We have a machine called: $computer"
        }    
    }
    END {"end of process"}     
}  
machine -computername 'a','b','c'
machines				# Prompt window will pop up here, because mandatory parameter "computername" has not been declared here. 

]]></Content>
		<Date><![CDATA[2014-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell - Writing help]]></Title>
		<Content><![CDATA[Chapter 9 - writing help
The general structure for documenting help-info within the script is:

<#
.SYNOPSIS
{enter summary description here}
.DESCRIPTION
{enter more detailed description here}
.PARAMETER {parameter's name}
{enter description of the above parameter}
.PARAMETER {parameter's name}
{enter description of the above parameter}
.PARAMETER {parameter's name}
{enter description of the above parameter}
.EXAMPLE
{enter example here}
.EXAMPLE
{enter example here}
#>

For more info, check out:

help about-comment-based-help


Skipped section 8.2 which is about "xml-based help", which used to provide help info in multiple languages. 


]]></Content>
		<Date><![CDATA[2014-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell - Error Handling]]></Title>
		<Content><![CDATA[In PowerShell there are 2 kinds of errors:
<ul>
	<li><strong>Non-terminating error</strong> - This is any error that doesn't bring PowerShell to a standstill. You can think of it as a minor error.</li>
	<li><strong>Terminating error</strong> (aka "exceptions") - This is an error that ps cannot continue from and everything will stop completely.</li>
</ul>
You can control how powershell behaves when it encounters an error. This is done by changing the value of the following reserved variable:

[powershell]
$ErrorActionPreference
Continue
[/powershell]

This variable can take the following 4 values:
<ul>
	<li><strong>Continue</strong> -  This is the default. This setting tells ps to keep going as far as possible, but show any error messages along the way.</li>
	<li><strong>SilentlyContinue</strong> - As "continue" but suppress displaying any error/warning messages, unless it is a terminating error.</li>
	<li><strong>Stop</strong> - This stops everything when it encounters either types of errors. By default it will display the error message. This is a useful setting to use when debugging a script.</li>
	<li><strong>Inquire</strong> - This lets you decide whether or not to continue.</li>
</ul>
Some people will set this setting to "SilentlyContinue" at the top of the script to hide any non-terminating errors. This is actually very bad practice becuase it suppress error messages for the script's script. The best practice is to either use "continue" or "stop".

Powershell lets you set ErrorActionPreference settings for each individual commands using the "-ErrorAction" parameter which is one of the <a title="PowerShell – Common Paramaters" href="http://codingbee.net/tutorials/powershell/powershell-common-paramaters/">common parameters</a>. For example:

remove-item c:\path\to\nonexistant\file

this parameter takes the same 4 values as given above. This is a much better approach if you want to apply "SilentlyContinue" for a command that you know always gives very minor, trivial, non-terminating errors.

There is also another common parameter, which is called "-ErrorVariable". This is something that you can use to store error output of a command into a variable, e.g.:

[powershell]
remove-item c:\path\to\nonexistant\file -ErrorVariable RemoveItemErrorMessage -ErrorAction SilentlyContinue

$RemoveItemErrorMessage
remove-item : Cannot find path 'C:\path\to\nonexistant\file' because it does not exist.
At line:1 char:1
+ remove-item c:\path\to\nonexistant\file -ErrorVariable RemoveItemErrorMessage -E ...
+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ CategoryInfo : ObjectNotFound: (C:\path\to\nonexistant\file:String) [Remove-Item], ItemNotFoundException
+ FullyQualifiedErrorId : PathNotFound,Microsoft.PowerShell.Commands.RemoveItemCommand
[/powershell]

In the above, I also used ErrorAction:SilentlyContinue to avoid any red writing showing up.

I skipped section 10.4 which is all about the "trap" errorhandling construct, which is something that isn't really used that much any more because it has now been superseded by other constructs, but if want, you can read up about it here: help about_trap.

The most conventional way of doing error handling in powershell is by using the "try...catch...finally" construct. Here is the structure of this construct:

[powershell]
try{ 
    get-wmiobject -class win32_BIOS -computername NotOnline -ErrorVariable wmiObjectErrorMessage -ErrorAction SilentlyContinue &quot;hello world 0&quot; 
} 
catch { 
    &quot;Failure encounter with $computername. The failure message is:&quot; | Out-File -Filepath log.txt -append &quot;wmiObjectErrorMessage&quot; | Out-file -Filepath log.txt -append &quot;hello world 1&quot; 
} 
&quot;hello world 2&quot; 
$wmiObjectErrorMessage 
[/powershell]

The outcome of this will vary depending on what -ErrorAction setting you have chosen:
<ul>
	<li><strong> continue</strong> - This will continue the rest of the try-block, and coninue with rest of the script, which means: - 2 error messages appear on screen. once during try, and once outputting $wmiObjectErrorMessage - Catch block isn't triggered - Both "hello world 0" and "hello world 2" are displayed.</li>
	<li><strong>silentlycontinue</strong> - This gives the same outcome as "continue", except that get-wmiobject doesn't output any error messages.</li>
	<li><strong>stop</strong> - This will stop the try-block midway, and trigger the catch-block. As a result: - Both "hello world 1" and "hello world 2" are displayed. But not "hello world 0" because try-block terminated early. - New entries have been appended to log.txt file - output of $wmiObjectErrorMessage is displayed.</li>
	<li><strong>inquiry</strong> - This opens up a pop-up window, prompting user to click: yes | yes to all | halt command | suspend.</li>
</ul>

In the above example we used "-errorvariable" to pass the error message into the catch construct. However PowerShell automatically passes the "exeptions-object" to the catch-block as "$_", which you can use instead. Note that both of these are actually different (but similar) objects, so they give slightly different outputs. Here are some typical examples of the outputs you can get with $_: 


PS C:\=> $_

Get-WmiObject : The RPC server is unavailable. (Exception from HRESULT: 0x800706BA)
At C:\Scope1.ps1:61 char:18
+ get-wmiobject &lt;&lt;&lt;&lt; -class win32_BIOS -computername NotOnline -ErrorAction stop -ErrorVariable wmiObjectErrorMessage + CategoryInfo : InvalidOperation: (:) [Get-WmiObject], COMException + FullyQualifiedErrorId : GetWMICOMException,Microsoft.PowerShell.Commands.GetWmiObjectCommand PS C:\=> $_.exception
The RPC server is unavailable. (Exception from HRESULT: 0x800706BA)

PS C:\=> $_.exception.message
The RPC server is unavailable. (Exception from HRESULT: 0x800706BA)



Notice that both "$_.exception" and "$_.exception.message" gives the same output. That's because by default, "$_.exception" only outputs the message property.

A try-block can be followed by multiple catch-blocks, each of which can be triggered by a different type of "error object". This is really use way to add logic to your code. For example you if want to try and stop an service, then there could be 3 possible outcomes, one success, and 2 errors. It succeeds, name of service doesn't exist (error1), service already stopped (error2). First let's try to replicate error1:

[powershell]
Try {
    Stop-Service -name &quot;xxxxxx&quot; -ErrorAction Stop
}
Catch [System.exception] {
    &quot;Caught by generic exception&quot;

    $_.exception.gettype().fullname
}
[/powershell]

Note: also check out the "<a href="http://technet.microsoft.com/en-us/library/hh847766.aspx">throw</a>" command. 

This outputs:
[powershell]
Caught by generic exception
Microsoft.PowerShell.Commands.ServiceCommandException
[/powershell]

"System.exception" is a general catch-all exception that can catch all exceptiosn. 

"$_.exception.gettype().fullname" This is a way to identify the error type when you try to stop an service that doesn't exist. With this info now known, we can the generic "System.exception" with the more drilled down option:

[powershell]
Try {
    Stop-Service -name &quot;xxxxxx&quot; -ErrorAction Stop
}
Catch [Microsoft.PowerShell.Commands.ServiceCommandException] {
    &quot;This service doesn't exist.&quot;
}
[/powershell]




You can also add a "finally" block at the end. The finally-block is optional, and it's content will run regardless of whether or not an error occured within the try construct.

For more info, check out:

help about_try_catch_finally]]></Content>
		<Date><![CDATA[2014-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell - Debugging Techniques]]></Title>
		<Content><![CDATA[Chapter 11 - Debugging Techniques

 Nothing new in this chapter, except for how to use a csv object, lets say you have a file called "testcsvfile.csv":
 
PS C:\> Get-Content .\testcsvfile.csv
title1,title2,title3
a,b,c
d,e,f
g,h,i 

Notice that in this csv file, the first row is a bit like a header row. You can treat the header as properties once you have created a property:

$csvobject = Import-Csv .\testcsvfile.csv

PS C:\> $csvobject

title1                 title2                title3                                                  
------                 ------                ------                                                  
a                      b                     c                                                       
d                      e                     f                                                       
g                      h                     i                                                       

Now you can treat this object as any other object:

PS C:\> $csvobject | gm		# tells you the availabe properties which in this case are title1, title2, and title3.

PS C:\> $csvobject | select-object -property title2
title2
------
b
e
h     
 
PS C:\> $csvobject[1]
title1                title2                         title3                                                  
------                ------                         ------                                                  
d                     e                              f 

PS C:\> $csvobject[1].title2
e




One really useful tool for debugging is the ability to pause/start the script midway through the running of it. In linux you can do this using the 
"read" command (although in linux, the main purpose of read is to accept user inputs). However in PS, you do can do this using the "write-debug" command (which also lets you echo something at the same time as pausing). In the script, simply write something like:

write-debug "the variable `$computername should now equal localhost. The actual value is: $computername" -debug

Note: You have to use the -debug switch to actaully get this working. 

As soon as this command is run, a popup window appears prompting user to choose one of the following options: Yes|Yes to All|Halt Command|Suspend. 
The "halt" option will actually terminate the command midway.  

There is a really cool feature that you can do here using the "suspend" option. if you can click this you will notices the your command prompt changes to ">>>". This means that you are in debug mode, and it is a bit like being inside the script/function's scope. This means you can do things like call a variable to see if it's value is what you expected. You can even change the value!!!  

After you have finished, viewing/changing variables, you can then type exit to leave the debug mode, and return to the popup window. Here you can resume the script, or terminate (halt) it. 

So far we have come acros a few write-* commands for error reporting. Each of these are designed to be used in different scenarios:

write-warning	# Displays warning message in yellow. This is mainly for the benefit of end-users when they report issues to developers. 
write-verbose	# Developers use this to monitor progress of scripts. This is well suited for doing linux equivalent of "echo line256"
write-debug		# Developers use this to pause/start scripts to monitor it's progressing and find bugs. It basically can do a better job than the 
				# "write-verbose" command. 
write-error		# Not sure when this should be used. 



The write-debug commands is a good way to pause a script once each it reaches a certain line in the code. However there is a more sophisticated tool that lets you pause a script in the following ways:

	- When you reach a certain line number (which is the same thing as inserting a "write-debug" into the script).
	- Straight after a particular command has been run
	- stopping as soon as a particular variable has been defined
	- stopping as soon as a particular defined variable is being used. 
	- stopping as soon as a particular variable is being either defined or outputted. 

The tool that can do this is called "set-psbreakpoint" command. To use this command, you have to specify the full path of the script that you want to attach the pause (aka breakpoint) to. As soon as that breakpoint has been reached, you automatically go into the debug mode (>>>). To continue
with the rest of the script, simply do "exit".

Here is an example on how to pause the script when it reaches line 12:

PS C:\> Set-PSBreakpoint -Script C:\path\to\script.ps1 -line 12		# Note, this won't execute line 12, but only lines 1-11.

Here is an example on how to pause the script just before it runs a custom function called "machines":

PS C:\> Set-PSBreakpoint -Script C:\path\to\script.ps1 -command machines

If you want to pause the script just before it defines (writes) a particular variable then do:

PS C:\> Set-PSBreakpoint -Script C:\Scope2.ps1 -Variable var2 -Mode write

If you want to pause the script just before each time it uses (reads) a particular variable then do:

PS C:\> Set-PSBreakpoint -Script C:\Scope2.ps1 -Variable var1 -Mode read

If you want to pasu the script just before it defines a particular variable, or each time it uses it, then do:

PS C:\> Set-PSBreakpoint -Script C:\Scope2.ps1 -Variable var1 -Mode readwrite

To view a list of all the psbreakpoints that you have set up, then do:

Get-PSBreakpoint

To remove/delete a breakpoint, then do:

remove-psbreakpoint -id {id number}		# Alternatively you can just close your ps terminal if you want to delete all PSBreakpoints. 


you can also enable/disable breakpoints like this:

Enable-PSBreakpoint -Id {id number}
Disable-PSBreakpoint -Id {id number}

]]></Content>
		<Date><![CDATA[2014-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell - Creating Custom Format Views]]></Title>
		<Content><![CDATA[Chapter 12 - Creating Custom Format Views

In chapter 7 you learn about how to create a custom (collection) object. However PS may not display the object in your preferred format (and order) that you want, since there are no xml-formatting file that tells ps how to format your custom object. Therefore some of your columns may be wider/narrower than you would have preferred, or the ordering of the commands could be wrong. 

A way round this issue is to create your own formatting for your given custom object. The best way to do this is by using an existing xml formatting file as a template:

PS C:\> cd $PSHOME
PS C:\WINDOWS\system32\WindowsPowerShell\v1.0> Get-ChildItem | Where-Object -FilterScript {$_.name -match ".xml$"}

Note: your custom view xml file will need to have the ".ps1xml" extension, which indicates that it is a "Windows Powershell XML Document".

e.g. lets say you have the following function:

function new-hmrcobject {

	$MyHashtable = @{'column1'="aaa";
					 'column2'="bbb";
					 'column3'="ccc"
	}

	$obj = New-Object -TypeName psobject -property $MyHashtable
	$obj.PSObject.TypeNames.Insert(0,'hrmctoolcustomformat')		# Here we attach our object to a custom view called "hrmctoolcustomformat"
	
	$obj
}

Now I create a custom view using the following file as a template:

C:\WINDOWS\system32\WindowsPowerShell\v1.0\DotNetTypes.format.ps1xml

Note: here is a list of custom views that already comes with ps:

Where-Object -FilterScript {$_.name -match ".format.ps1xml"}  	

Mode                LastWriteTime     Length Name
----                -------------     ------ ----
-a---        10/06/2009     21:41      27338 Certificate.format.ps1xml
-a---        10/06/2009     21:41      27106 Diagnostics.Format.ps1xml
-a---        23/07/2012     19:12     144442 DotNetTypes.format.ps1xml
-a---        23/07/2012     19:12      14502 Event.Format.ps1xml
-a---        23/07/2012     19:12      21293 FileSystem.format.ps1xml
-a---        23/07/2012     19:12     287938 Help.format.ps1xml
-a---        23/07/2012     19:12      97880 HelpV3.format.ps1xml
-a---        23/07/2012     19:12     101824 PowerShellCore.format.ps1xml
-a---        10/06/2009     21:41      18612 PowerShellTrace.format.ps1xml
-a---        23/07/2012     19:12      13659 Registry.format.ps1xml
-a---        23/07/2012     19:12      17731 WSMan.Format.ps1xml

Note, a custom view must always end with ".format.ps1xml" 

Will use DotNetTypes.format.ps1xml as a template. Using this file I created a file called "hrmctools.formatps1xml", and it contains the following:

<?xml version="1.0" encoding="utf-8" ?>
<Configuration>
    <ViewDefinitions>     
		<View>
            <Name>hrmctoolcustomformat</Name>		# This must match with the "insert" command above so that object is linked to this format 
            <ViewSelectedBy>
                <TypeName>hrmctoolcustomformat</TypeName> 	# This must match with the "insert" command above so that object is linked to this format
            </ViewSelectedBy>

            <TableControl>
                <TableHeaders>
                    <TableColumnHeader>
                        <Label>columnX</Label>		# This displays a different name rather than defaulting to the property name. 
                        <Width>40</Width>			# This defines the column width. 
                    </TableColumnHeader>
					<TableColumnHeader>
                        <Label>columnY</Label>
                        <Width>15</Width>
                    </TableColumnHeader>
                    <TableColumnHeader>
						<Label>columnZ</Label>
                        <Width>15</Width>
                    </TableColumnHeader>
                </TableHeaders>
                <TableRowEntries>
                    <TableRowEntry>
                        <TableColumnItems>
                            <TableColumnItem>
                                <PropertyName>column1</PropertyName>	# Here you can re-arrange the column ordering. 
                            </TableColumnItem>
                            <TableColumnItem>
                                <PropertyName>column2</PropertyName>
                            </TableColumnItem>
							<TableColumnItem>
                                <PropertyName>column3</PropertyName>
                            </TableColumnItem>
                        </TableColumnItems>
                    </TableRowEntry>
                 </TableRowEntries>
            </TableControl>
        </View>
    </ViewDefinitions>
</Configuration>

Note: In PS, the content of xml files are always CASE SENSITIVE!!! 


Once you have created your own custom view you then need to tell ps to activate it by using (Update-FormatData):

Update-FormatData -PrependPath c:\path\to\xml\file

Once you run the above command, this format should now be loaded into memory. You can check this using "get-formatdata":

get-formatdata



You can then attach it your custom view like this using the "insert" method:

$MyObject.PSObject.TypeNames.Insert(0,'hrmctoolcustomformat')	# Note, this has already been done in the above function.

The first parameter "0" is something that you always type in. You can now confirm that the object has successfully been attached to the custom view, by doing:

$MyObject | gm

Also if you output your object, you should now notice that its appearance should have now changed to those that you defined in the custom view. 

A "Typename" is essentially a name that you give to your object. It tell's PS the type of object that it is. It is possible for a number of commands 
outputting objects of the same type (i.e. they can have the same typename value).

Note: if you lookup gm for $myobject, you will find that "PSObject" is not listed as a property, however it is listed if you use "-force":

$MyObject | Get-Member -Force


]]></Content>
		<Date><![CDATA[2014-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell - Script and manifest modules]]></Title>
		<Content><![CDATA[Chapter 13 - Script and manifest modules

In Chapter 4 you have found a number of ways that you can create a custom command...e.g. by creating custom scripts/functions. E.g. if you want to use a function like a command, then you have to dot-source the script that houses the function. However this is not a good solution from the end user's point of view. 

A better approach is to package your custom commands (in this case they are in the form of functions that are housed in a script) into modules.

First off, in linux, if you want to use a script in the same style as an ordinary command, then you have to place the script them in one of the directories that are defined in the $PATH variable. In Powershell you do the same kind of thing, along with a number of other things.

If you switch to the "env" psdrive:
 
cd env:\
 
Here you will find an item called "PSModulePath"

get-content PSModulePath				# This is the ps equivalent of linux's $PATH variable. It is also semi-colon seperated. 
C:\Documents and Settings\SChowdhury\My Documents\WindowsPowerShell\Modules;C:\WINDOWS\system32\WindowsPowerShell\v1.0\Modules\										

Another way to view this environment's variable:

$env:PSModulePath 

You can then change the value like this:

$env:PSModulePath = "C:\Documents and Settings\SChowdhury\My Documents\WindowsPowerShell\Modules;C:\WINDOWS\system32\WindowsPowerShell\v1.0\Modules\;C:\temp"

However this reverts back to it's original value when you restart the shell. To make this change permenant, you need to do it from the gui:
right-click "mycomputer" and select "properties" | Advanced tab | Environment variables  (This is is a windows xp machine.)

Before you create a module, you should first decide a name for your module, Ideally this should be unique, which you can do by adding a prefix e.g.:

hrmctool

Next, we create a folder by the same name (this folder must be within one of the directories listed in $env:PSModulePath):

PS C:\WINDOWS\system32\WindowsPowerShell\v1.0\Modules> new-item -ItemType directory -Name hrmcTool

Within this newly created folder you need to place your script, this scriptfile needs to:

	- Have the exact same name as the parent folder. (it is also case sensitive)
	- Have the extension "psm1" rather than ".ps1" (which means "windows powershell script module")
	
For the functions (which will become the commands) within the script, it is best practice to:

	- follow the verb-noun name convention, but prefix the noun part with something unique. 
	- prefix the noun with something unique (e.g. hrmc) to avoid any possible conflicts with existing commands with the same names. 


Also, in this folder you can place the xml file that defines your custom view for your function's (custom command's) output. So at this point you should have:

PS C:\Documents and Settings\SChowdhury\My Documents\WindowsPowerShell\Modules\hrmctool> ls

    Directory: C:\Documents and Settings\SChowdhury\My Documents\WindowsPowerShell\Modules\hrmctool

Mode                LastWriteTime     Length Name
----                -------------     ------ ----
-a---        27/08/2013     15:53       1766 hrmctool.psm1
-a---        28/08/2013     08:58       1643 hrmctools.format.ps1xml

However the problem is that the hrmctools.format.ps1xml will not automatically be loaded into memory when you activate the main module.  However this can by overcome by creating a "ModuleManifest" file. This file is generated by running the following command:

New-ModuleManifest -Path .\hrmctool.psd1 `			# This defines the name of the new file. 
-Author 'Sher' `
-CompanyName 'HMRC' `
-Copyright '(c) Sher Chowdhury' `
-Description 'this is a test description' `
-FormatsToProcess .\hmrctool.format.ps1xml `		# This defines the custom view to be loaded into memory as part of this module
-ModuleVersion 1.0 `
-PowerShellVersion 3.0 `
-RootModule .\hmrctool.psm1							# This is the path to the module script itself. 

I have broken down the above command into multiple lines so to make it easier to read. The end result is the creation of the new manifest file:

PS C:\Documents and Settings\SChowdhury\My Documents\WindowsPowerShell\Modules\hmrctool> ls

    Directory: C:\Documents and Settings\SChowdhury\My Documents\WindowsPowerShell\Modules\hmrctool

Mode                LastWriteTime     Length Name
----                -------------     ------ ----
-a---        28/08/2013     10:51       4762 hmrctool.psd1			# This is the new file created by the new-modulemanifest command.  
-a---        27/08/2013     15:53       1766 hmrctool.psm1
-a---        28/08/2013     08:58       1643 hmrctools.format.ps1xml

Now to activate this module along with its custom view, do:

PS C:\> Import-Module -Name hmrctool

Now here are 2 ways to check that the above command has worked 

First check:
PS C:\>  Get-module		# This lists all the modules that are enabled, along with the commands they come with. 

ModuleType Name                                ExportedCommands
---------- ----                                ----------------
Script     hmrctool                            new-hmrcobject
Manifest   Microsoft.PowerShell.Management     {Add-Computer, Add-Content, Checkpoint-Computer, Clear-Content...}
Manifest   Microsoft.PowerShell.Utility        {Add-Member, Add-Type, Clear-Variable, Compare-Object...}


The above indicates that the "new-hmrcobject" can now be used like a command because the module hmrctool has now been enabled.

Second check:
we can simply run the newly enabled function, as if it is a command:

PS C:\> new-hmrcobject
columnX                                                      columnY         columnZ        
-------                                                      -------         -------        
aaa                                                          bbb             ccc   


The above indicates also indicates that our custom view has also been activated. But you can check this uisng "get-formatdata":

PS C:\> Get-FormatData | Where-Object -FilterScript {$_.TypeName -match 'hmrc'}

TypeName                                                   FormatViewDefinition                                     
--------                                                   --------------------                                     
HMRCtoolcustomformat                                       {HMRCtoolcustomformat , TableControl} 
  
Note: The bare minimum required to create a module is create the module's folder, and then place the psm1 file into it. Then you can run the import-module command straight after that. The creation of the psd1 and ps1xml are both optional, but they are highly recommended if you want to create a more mature and well rounded module. 

We can also create default settings for our module, which is a bit like a module's preference settings. For example I have added the following lines
to the module script (for incorporating a log file):


$hmrclogfile='c:/temp/hmrclogfile.txt'	# This define's the new log file. Notice that this isn't defined inside a function. That's because
										# it's likely that other function would want to use the same variable. Also you can't access this variable	
										# from the command line. I.e. This variable is private and is only visible to other components within the   # module script. This means that importing a module is not quite the same thing as dot-sourcing. 
function new-hmrcobject {

	$MyHashtable = @{

		'column1'="aaa";
		'column2'="bbb";
		'column3'="ccc"
	}

	$obj = New-Object -TypeName psobject -property $MyHashtable
	$obj.PSObject.TypeNames.Insert(0,'HMRCtoolcustomformat')
	
	$obj

	"object succesfully outputted" | out-file  -filepath $hmrclogfile -append		# here we make use of this variable. Nothing special here. 
} 
 
export-ModuleMember  -variable hmrclogfile	# This is the important part. This makes this variable visible (and updateable) on the command line. 

export-ModuleMember  -function new-hmrcobject	# By convention if you use the "export-ModuleMember" anywhere in the script (as we have done 
												# just above), all the functions automatically becomes private. You then have to explicitly export # them to be able to use them again.

												
Now when you import the module, you will now find that the following variable now exists:
 
PS C:\> $hmrclogfile
c:/temp/hmrclogfile.txt

This variable actually resides in the "variable" psdrive:

PS C:\> Get-PSDrive | Where-Object -FilterScript {$_.Name -match "variable"}
Name           Used (GB)     Free (GB) Provider      Root                                                    CurrentLocation
----           ---------     --------- --------      ----                                                    ---------------
Variable                               Variable                                                                             
 
PS C:\> cd variable:\
PS Variable:\> ls | Where-Object -FilterScript {$_.Name -match "hmrc"}
Name                           Value                                                                                        
----                           -----                                                                                        
hmrclogfile                    c:/temp/hmrclogfile.txt                                                                      

Now you can simply change this variable, in the same way you would do a linux env variables. 

Also, this variable disappears when you remove the module:

remove-module hmrctool

This essentially means that the whole module is completely self-contained.  

Another way to create a "preference" is to pass the value into the function using parameters, just like we normally do. However some things (like setting the logfile's default path) is more of a preference thing rather than a parameter. To help decide whether somehting should be a parameter, or a preference setting, you should check if it is best to define the variable insided a function, or outsied. If it is defined outsied of all functions (so that all function can make use of it) then it is a preference setting. 
 
]]></Content>
		<Date><![CDATA[2014-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell - Making tools that makes changes]]></Title>
		<Content><![CDATA[Note: skipped ch14 this because it was about db access, using pre-written scripts in .net. Also skipped ch 15 because it is an interlude that didn't cover anythign new

Chapter 16 - Making tools that makes changes

Awhile back you came across "-whatif". You can enable these features in your scripts/functions too by simply updating your "[cmdletbinding()]" with the following:

[cmdletbinding(SupportsShouldProcess=$True)]

By enabling this, you script/function will now have the "-whatif" and "-confirm" options available on the commnand line. 

In the background, what actually happens is that the -whatif requests gets passed down the to the lower level powershell commands who take on the responsibilities of reporting back. 


Now you will find a powershell preference setting called:

PS C:\> $WhatIfPreference
False

If you change this to true:

PS C:\> $WhatIfPreference=$True
PS C:\> $WhatIfPreference
True

Then it means that the "-whatif" option is implicitly always switched-on for every command, in other words it puts the whole powershell into "whatif" mode:

PS C:\> get-Service | Where-Object -FilterScript {$_.name -match "audio"}
Status   Name               DisplayName
------   ----               -----------
Running  AudioSrv           Windows Audio
PS C:\> Stop-Service AudioSrv
What if: Performing operation "Stop-Service" on Target "Windows Audio (AudioSrv)".


Note: Not all commands are whatif compatible, and those that aren't will run as normal. 

If you are in whatif mode and you want to suppress "whatif" when running a command, then you either have to change the preference, or you can do:

PS C:\> Stop-Service AudioSrv -WhatIf:$false		# This will temporarily switch of whatif just for this command only. 

PS C:\> get-Service | Where-Object -FilterScript {$_.name -match "audio"}
Status   Name               DisplayName
------   ----               -----------
Stopped  AudioSrv           Windows Audio 

 
Next we have the following preference setting:

PS C:\> $ConfirmPreference
High

You can think of this as a tolerance level, and this preference has 3 settings:

	- High
	- Medium
	- Low
	
Each ps command has a builtin $ConfirmPreference value, if the command's value is higher then the PS's global value, then the "-confirm" option for that command will automatically be activated (regardless of whether or not you typed in "-confirm" on the command line).

By default, the global setting is set to "High", which means you shouldn't ever get any confirm popup windows. However if you change this to low, and then try to run a command, then the "-confirm" comes into action:

PS C:\> $ConfirmPreference='low'
PS C:\> $ConfirmPreference
Low
PS C:\> Stop-Service AudioSrv

Confirm
Are you sure you want to perform this action?
Performing operation "Stop-Service" on Target "Windows Audio (AudioSrv)".
[Y] Yes  [A] Yes to All  [N] No  [L] No to All  [S] Suspend  [?] Help (default is "Y"):


You can also suppress this auto-confirm behaviour, like this:

Start-Service AudioSrv -Confirm:$false


You can also add a built-in confirmpreference value into your script, which again is done using cmdletbinding:

[cmdletbinding(SupportsShouldProcess=$True,		# This enables the -whatif and -confirm switches
				ConfirmImpact='High')]			# This attaches the builtin "ConfirmImpact" value which can automatically trigger "-confirm"
												# if this value is higher than the global $ConfirmPreference value. 

Note, it is up to you, on  what ConfirmImpact rating you want to give to your scripts/functions, however if $ConfirmPreference is already set to "high", then your scripts/functions ConfirmImpact ratings will never come into play. 

For more info, checkout:

about_Functions_CmdletBindingAttribute


Sometimes you will want to tell your command to only do "-confirm" at a certain part within a function....and then you want the script to respond differently depending on option you pick. In otherwords create something that is the linux equivalent of the "read" command combined with an if-else statement. Here is oneway that this can be done:

$hmrclogfile='c:/temp/hmrclogfile.txt'

function new-hmrcobject {

	[cmdletbinding(ConfirmImpact='High')]		# You need to have this option enabled in order for this work. 

   	param(
	[string[]]$computername 
   	)

	"You are are about to over-write the log file!"
    if ($pscmdlet.ShouldProcess('Are you sure that you want to overwrite the log file?'))		# See note1 below.
		{
        "This is the first line in the logfile" | out-file -filepath $hmrclogfile
		}
    else 
		{
        "exiting because failed to write to the log file."
        return
		}
    "Log file created successfully."

    "Here I am appending another line to the log file." | out-file -filepath $hmrclogfile -append
}

new-hmrcobject -computername test 
 
 
Note1: The cmdletbinding feature comes with a special variable-object called $pscmdlet. This $pscmdlet represent is a variable-object representation of your function (which in this case is "new-hmrcobject"), and you can access the methods of $pscmdlet to control how your functions behaves. In this instance we have accessed the "ShouldProcess" method which results in the "-confirm" window popping. Now depending on which button 
you click, this will either evaluate to True or False....which in turn will trigger either the "if" or "else" part of the conditional block. 
 
]]></Content>
		<Date><![CDATA[2014-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell - Creating a Custom Type Extension]]></Title>
		<Content><![CDATA[Chapter 17 - Creating a Custom Type Extension
 
In an earlier chapter we learnt about how to change the way a function's output looks like. In this chapter we are going learn about how to add our own entries in our custom command's get-member profile. We can only add an entry which belongs to the type-extension family. 
 
When you do:

get-process | gm

YOu will find entries that are properties and methods. However you will also find entries that are of the following type:

	- AliasProperties
	- NoteProperties
	- ScriptProperties
	- ScriptMethods
	- PropertySets
	
These are called "type extensions".

These extensions are added onto each object by a system called the "Powershell's Extensible Type System" (ETS). Why are these extensions added? Each of these extensions have a different purpose:

AliasProperty - This is basically an alias for a given property. 
 
PS C:\> Get-Process | Select-Object -Property handlecount,handles	# This will output two identical columns, since "handles" is an AliasProperty of 
																	# "handlecount". The only difference being the header title. 
 
 
Get-Process | Select-Object -Property processname,name		# Here we get 2 identical columns again, since "name" is an AliasProperty 
															# for "processname" 
 
 
NoteProperty - This is a place where powershell may store internal data about an object.  
 
 
ScriptProperty - This is powershell script that is run to output the script property. This doesn't usually access an external resource, instead it just reformats/reveals  something that is already part of the object. e.g. you could use it to dispaly disk space from mb, to gb.


ScriptMethod - This is the same as ScriptProperty, although it may only take action but not output anything. 


PropertySet - This is a pre-defined collection of properties that has been grouped together. For example:

PS C:\> Get-Process | Select-Object -Property PSResources | Format-Table     # "PSResources" is actually a propertyset. 

We can create custom extensions for our modules in the same style that we did to create the custom view file. I.e. we create an xml file. However, an important thing to remember is that this xml file must have the following extension:

FileName.ps1xml		# Note, it cannot end with "format.ps1xml", because that ending is reserved for custom views. 

All the built-in extension types that ps uses are stored in the following xml file (which is called "types.ps1xml"):

$pshome\types.ps1xml


The general format of this xml file is as follows:

<?xml version="1.0" encoding="utf-8" ?>
<Types>
    
	<Type>											# "Type" will house the info for a single typename. 
        <Name>HMRCtoolcustomformat</Name>					# Here you specify the "Typename" value (which appears on first line of gm).
        <Members>									# "Members" will house the info for one or more type-extensions 	
            <AliasProperty>					# Here we define the first type-extension, which in this case is an alias property. 
                <Name>Name</Name>				# This is the name of this type-extension.
                <ReferencedMemberName>ProcessName</ReferencedMemberName>
            </AliasProperty>
            <AliasProperty>					# Here is the next type-extension, this agian is an alias property
                <Name>Handles</Name>
                <ReferencedMemberName>Handlecount</ReferencedMemberName>
            </AliasProperty>
            <ScriptProperty>				# Here is the third type-extension, this time it is a script-property
                <Name>Path</Name>
                <GetScriptBlock>$this.Mainmodule.FileName</GetScriptBlock>
            </ScriptProperty>
        </Members>
    </Type>
	
	<Type>
       .
	   .
    </Type>
	
	<Type>
       .
	   .
	   .
    </Type>

</Types>

 
Now in my module, created the following extension-type file:

PS C:\Users\SChowdhury\Documents\WindowsPowerShell\Modules\hmrctool> ls

Mode                LastWriteTime     Length Name
----                -------------     ------ ----
-a---        02/09/2013     14:47        376 hmrcextensiontype.ps1xml     	# This is the new file created.
-a---        27/08/2013     17:26       1653 hmrctool.format.ps1xml
-a---        28/08/2013     09:31       5236 hmrctool.psd1
-a---        02/09/2013     14:19        749 hmrctool.psm1
 
 
"hmrcextensiontype.ps1xml" has the following content:

<?xml version="1.0" encoding="utf-8" ?>
<Types>    
	<Type>											 
        <Name>HMRCtoolcustomformat</Name>			
        <Members>									 	
            <ScriptProperty>				
                <Name>GetRidofBinApartment2</Name>					#This will replace all (column 2) instances of letter "b" with the letter "O".
                <GetScriptBlock>$this.column2 -replace 'b','O'</GetScriptBlock>
            </ScriptProperty>
            <ScriptMethod>							# See note below. 
                <Name>CanPing</Name>
                <Script>
                    Test-connection -computername $this.column2 -quiet
                </Script>
            </ScriptMethod>
        </Members>
    </Type>
</Types> 



https://groups.google.com/forum/#!topic/microsoft.public.windows.powershell/r_oUArRP6_8 - explains the difference of $_ and $this
Here is another link:
http://powershell.com/cs/forums/t/11373.aspx 

Detour: Also you can add new type-extension a type-extension straight from the command line, using "add-member":

$object | add-member -MemberType noteproperty `
					 -Name exampleproperty `
					 -Value "hello!"				# Note, i haven't tried this. 


Also checkout:

help about_Automatic_Variables		# This gives info about things like $$, $?



Note: The above script-method will do a ping test for all items in column b....which is handy if column b is a list of computer names. Note, "Test-connection" is the ps equivalent of the ping command. Hence use this command to test if can connect to another machine. 


Once the xml file is ready, you can then load it into memory. For custom views, we did this using "Update-FormatData", however for type-extension, we use the following command instead:

Update-TypeData -PrependPath {filename.ps1xml}

After that you can check if it has worked by running the custom command into gm

PS C:\> new-hmrcobject -computername machine | gm

   TypeName: HMRCtoolcustomformat

Name                  MemberType     Definition                                                               
----                  ----------     ----------                                                               
Equals                Method         bool Equals(System.Object obj)                                           
GetHashCode           Method         int GetHashCode()                                                        
GetType               Method         type GetType()                                                           
ToString              Method         string ToString()                                                        
column1               NoteProperty   System.String column1=machine                                            
column2               NoteProperty   System.String column2=bbb                                                
column3               NoteProperty   System.String column3=ccc                                                
CanPing               ScriptMethod   System.Object CanPing(); 						# This is the new script method.
GetRidofBinApartment2 ScriptProperty System.Object GetRidofBinApartment2 {get=$this.column2 -replace 'b','O';}	# This is the new script property


And, the standard output is:

PS C:\Users\SChowdhury\Documents\WindowsPowerShell\Modules\hmrctool> new-hmrcobject -computername test

columnX                                                      columnY         columnZ
-------                                                      -------         -------
test                                                         bbb             ccc
test                                                         bbb             ccc
test                                                         bbb             ccc

Lets now see if our new script-property works:

PS C:\> new-hmrcobject -computername test | Select-Object -Property column1,column2,column3,GetRidofBinApartment2

column1                            column2                            column3                            GetRidofBinApartment2
-------                            -------                            -------                            ---------------------
test                               bbb                                ccc                                OOO
test                               bbb                                ccc                                OOO
test                               bbb                                ccc                                OOO

Note: this affects each row object. 

Now lets try using the method. 

$tempObj = new-hmrcobject -computername machine			# first store the object in a variable. 

PS C:\Users\SChowdhury\Documents\WindowsPowerShell\Modules\hmrctool> $tempObj.Canping()			# Now apply the method to the (object) variable. 
False
False
False

The above shows "false" because we fed through fake computer names. 

 
So far, we manually loded the type-extension xml file into memory, the better option would be for this to happen automatically when importing the module, and to do this, we use the New-ModuleManifest command again:


New-ModuleManifest -Path .\hrmctool.psd1 `		
-Author 'Sher' `
-CompanyName 'HMRC' `
-Copyright '(c) Sher Chowdhury' `
-Description 'this is a test description' `
-FormatsToProcess .\hmrctool.format.ps1xml `		
-ModuleVersion 1.0 `
-PowerShellVersion 3.0 `
-RootModule .\hmrctool.psm1	`						 
-TypesToProcess '.\hmrcextensiontype.ps1xml'		# Here is the new line for this. 

Note, if you already have a manifest (psd1) file, then you can simply modify this file instead.  
]]></Content>
		<Date><![CDATA[2014-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell - Creating Powershell workflows]]></Title>
		<Content><![CDATA[Chapter 18 - Creating Powershell workflows

Note: workflows is a powershell v3+ only thing. 

When you want to create a custom command, you can do so by either:

	- creating a script to represent that command
	- creating a script that contains functions and each of these function act as a command (either by dot sourcing, or importing module)

However (in ps v3), there is a third way, and that is by creating a "workflow".


When you create a function, you use the following syntax:

function command-name {
							# The code are entered here, e.g. param section, begin/process/end section
}

However when we creat a workflow, we use this syntax: 

workflow command-name {
							# The code is entered here. 
}


For example here is the contents of ps1 script we that we created called "TestWorkFlow.ps1":

PS C:\> Get-Content .\testworkflow.ps1
workflow basic-maths {
    $a=5
    $b=7
    $c= $a * $b
    $c
}
basic-maths

Now if we run this, we get:

PS C:\> .\testworkflow.ps1
35

When you run the above command, you'll notice that there is a fraction of a second delay outputing the result (35), when compared to running the equivalent command in powershell. That's because:

	- functions are processed by the powershell engine, whereas...
	- workflows are processed by the .NET Framework's "Windows Workflow Fondataion (WF)"

As a result, workflows and functions have differing behaviours:

- Both functions and workflows run commands in the same sequence, but workflows include detailed logging/tracking of each command and include the 
  ability to retry steps (if they fail for things like intermitant network issues, maybe useful for developing pandabox/dropbox!)
- Functions do one thing at a time, whereas workflows can do the same thing multiple times (parrallel multitasking)
- Functions start/run/finishes, whereas a workflow can pause/stop/restart
- If you turn off your machine while a function is running, then that function is lost. However a workflow can potentially be recovered and 
  resumed automatically.    

As a result of these distinctions, creating a workflow is a bit like creating a service!!!

Useful link that gives more background info about workflows: http://technet.microsoft.com/en-gb/magazine/jj884462.aspx
Also checkout:
PS C:\> help about_Workflows



Here is a summary of the differences between functions and workflows:

------------------------------------------------------------------------------------------------------------------------------
                         Function 								|                         Workflow
------------------------------------------------------------------------------------------------------------------------------
	Executed by Powershell 										|  	Executed by workflow engine
																|
	Logging and Retry attempts throught complicated coding  	|	Logging and retry attempts part of the workflow engine. 
																|
	Single-action processing									|  	Supports parallelism
																|
	Runs to completion											|  	Can run/pause/restart
																|
	Data loss possible duirng network problems					|  	Data can persist during network problems. 
																|
	Full language set and syntax								|  	Limited language set and syntax
																|
	Run cmdlets													|  	Run activities



The workflow features is available in powershell in the form of a module, called "PSWorkflow". You need to first import this module so that powershell understands what workflows are:


PS C:\> Get-Module PSWorkflow
ModuleType Name                                ExportedCommands
---------- ----                                ----------------
Manifest   PSWorkflow                          {New-PSWorkflowExecutionOption, New-PSWorkflowSession, nwsn}

PS C:\> Import-module  PSWorkflow

After doing this, ps will now understand what workflows are and how to handle them. 

To transform a workflow into a command, you need to add the worflow into a modules script (psm1), i.e. :


Import-Module psworkflow		# include this anyway in case psworkflow hasn't already been imported. 
workflow get-maths {
    $a=5
    $b=7
    $c= $a * $b
    $c
}
export-ModuleMember  -function get-maths 		# This might not be needed if your module doesn't have any preference settings (e.g. setting 
												# location of the log file). But best practice is to always include this. 
												# surprising we use the "-function" parameter because "-workflow" doesn't exist. 


Now you can simply run the workflow like a normal command:

PS C:\> get-maths
35
												

As you will recall, commands and functions have common parameters (see help about_common_parameters). In the same way, workflows comes with their own set of common parameters.

												
One of the really useful parameter, is the "-asjob" switch parameter. This lets you run a workflow-based-command in the background:

PS C:\> get-maths -AsJob

Id     Name            PSJobTypeName   State         HasMoreData     Location             Command                  
--     ----            -------------   -----         -----------     --------             -------                  
35     Job35           PSWorkflowJob   NotStarted    True            localhost            get-maths                

You can then use the -job commandlets to manage this job. e.g:

PS C:\> get-job

Id     Name            PSJobTypeName   State         HasMoreData     Location             Command                  
--     ----            -------------   -----         -----------     --------             -------                  
19     Job19           PSWorkflowJob   Completed     True            localhost            get-maths                
35     Job35           PSWorkflowJob   Completed     True           localhost            get-maths                

PS C:\> Receive-Job -id 35
35

There is also "suspend-job" and "resume-job" to pausing and resuming the workflow. 
 

The "-asjob" is one of the most useful workflow common parameter, but there other useful ones too:

	PSComputername - A list of computers to perform the workflow on. Note, some commands cannot be remoted, e.g. measure-object, see page
					 187 for a complete list of commands that can't be remoted with this parameter. This is mainly for performance reasons,
					 however you can force workflow to run these commands remotely using the "InlineScript{}" block, which is covered later. 
	PSParameterCollection - A list of hash tables that specify different parameter values for for each target machine, so each machine can be 
							treated differently. 
	PSCredential - The username/password to be used to execute the workflow. 
	PSPersist 	- Force the workflow to save (checkpoint) the workflow data and start after executing each step. A bit like log taking. 
		

For a complete list, check out:

help about_WorkflowCommonParameters



Workflows works on the concept of "activities". Each command that runs within a workflow is a standalone "activity". In order for workflows pause/start feature to work, each command has to assume it is running in a completely fresh, brand new environment. Hence, variables created in one command, can't be used in the next (However there are a few exceptions to this rule, e.g. simple variable that holds numbers). 

However you can use the following construct to tell workflows to run a code block as a single activity:

Import-Module psworkflow
workflow get-maths {
    $a=5
    $b=7
    $c= $a * $b						# Simple variables like this will work fine. 
    $c
    #$object = get-service			# this doesn't work becuase outside of inlinescript block.   
    #$object						# Therefore has been commented out. 
	InlineScript {					
		$object = get-service		
        $object							# This works fine. 
	} 
}
export-ModuleMember  -function get-maths


You can suspend (pause) a workflow, by placing a pause command (Suspend-Workflow) inside the workflow itself:


Import-Module psworkflow
workflow get-maths {
    $a=5
    $b=7
    $c= $a * $b
    $c
    Suspend-Workflow					# Here is the pause command. 
	InlineScript {
		$object = get-service
        $object
	} 
}
export-ModuleMember  -function get-maths

The above will cause the job to go into a "job" as soon as it reaches the pause command:

PS C:\> get-maths
35

Id     Name            PSJobTypeName   State         HasMoreData     Location             Command                  
--     ----            -------------   -----         -----------     --------             -------                  
59     Job59           PSWorkflowJob   Suspended     True            localhost            get-maths                


Notice, that the state is "suspended", i.e. that it is paused. To resume, simply do:

resume-job -id 59		# you then have to wait awhile and then do "receive-job -id 59",  in order to get the output.


A while back we covered the invoke-command. One of the things this command lets you do is this:

Invoke-Command -ScriptBlock {block-of-code} -computername {machine1,machine2...etc}

One of the cool thing about invoke-command is that it will run the scriptblock simultaneously across several machines at the same time. This is a 
great way to do things efficiently. However the lines of code inside the scriptblock will be performed sequentially (unless maybe you make use of workflows within the scriptblock). 

In workflows you can control the order in which lines of codes are executed using the following parallel/sequence construct:


Import-Module psworkflow
workflow get-maths {
	parallel {
		
		command1
		command2
		
		sequence{
			commandA
			commandB
		}
	
	
	}
}
export-ModuleMember  -function get-maths
 

In this scenario, the above 4 commands can run in the following possible orders. 


command1	|	command2	|	command1	|	command2	|	commandA	|	commandA
command2	|	command1	|	commandA	|	commandA	|	commandB	|	commandB	
commandA	|	commandA	|	commandB	|	commandB	|	command1	|	command2	
commandB	|	commandB	|	command2	|	command1	|	command2	|	command1	


Notice that commandB always comes after CommandA, and the commandA-B block is competing equally with commands 1 and 2 to be executed first (since the sequence block itself is in the parallel block).

Of the 6 possible orders, anyone is possible. The only thing that determines the order is the amount of time each of the above 4 commands takes to complete. 

The above is useful if you have a set of commands to run and don't care about the order that they are run in. 

Here is another kind of parrellel syntax that you can use in workflows:

workflow test-workflow {
	Foreach -parrallel ($computer in $computername) {
		Do-something -computername $computer
	}
}

This approach does pretty much the same thing as the invoke-command with the "-computername" parameter enabled. 


As mentioned earlier, anything you run in a workflow gets passed to a .net engine which then translates each powershell command into the equivalent .net "activity". However there are some ps commands that don't have a .net equivalent, in these case these commands will fail. This is especially the case for any commands that are part of a module. With module based commands, one way to overcome this is to do:

inlinescript{
	import-module {module's name}
	module's-command
} 

You will often find yourself using "inclinescript" quite regularly when developing workflows.   

page 190 - gives a list of things you can do in functions but can't do in workflows. 

page 191 - gives a list of all built-in commands that don't work in woflows. 


]]></Content>
		<Date><![CDATA[2014-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell - Troubleshooting pipeline input]]></Title>
		<Content><![CDATA[Chapter 19 - Troubleshooting pipeline input

In this chapter we will learn about how to do troubleshooting (in this case relating to pipelines) with the help of a command called "trace-command"

Lets say we have a csv file with only one column, containing the the following rows:

Computername
localhost

Now you get the following output if you do:

PS C:\> Import-Csv -Path .\computers.csv

Computername
------------
localhost


Now let's say we want to get localhost's machine's bios info by piping in "localhost" into the "-computername" parameter of:

Get-WmiObject -ComputerName {piped in data} -Class win32_bios

To achieve this we try:


PS C:\>  Import-Csv -Path .\computers.csv  |  Get-WmiObject -Class win32_bios

Unfortunately this fails to work. That's because if you look in:

help Get-WmiObject -full

You will find that Get-WmiObject command's "-computername" parameter cannot accept pipeline input. 


However it is not a good idea to alway's rely on the help pages to figure out what is wrong with your code. A better/more-sophisticated approach is to use the "trace-command". This is powershell's diagnostic/troubleshooting command. It is used to identify an problems in the code.  

Here is the trace-command's approach to identifying the problem:

PS C:\> Trace-Command -Name ParameterBinding -PSHost -Expression {Import-Csv -Path .\computers.csv  |  Get-WmiObject -Class win32_bios}


	The "-name" parameter is actually radio-button parameter. The available nameradio-options can be listed by running the following command:

	PS C:\> Get-TraceSource

	The -name parameter essentially lets you choose what behaviour of the code you want to analyse. 

	The -PSHost is a switch parameter, which you need to switch on if you want to view the debugging info onscreen. 

	Note, the trace-command tool will actually run the code that is in {...} as part of it's debugging activity. So be careful what you run in this. 

  
The output of the above trace-command is as follows:

#############################################################################################################################################

DEBUG: ParameterBinding Information: 0 : BIND NAMED cmd line args [Import-Csv]          
# To start with, ps tries to bind any parameters (that have been explicitly named) to import-csv command. Which in this case is the "-path" parameter.

DEBUG: ParameterBinding Information: 0 :     BIND arg [.\computers.csv] to parameter [Path] 
# first bind ".\computers.csv" to "-path" parameter. Notice that this line is indented. This means that it is a child element of the previous 
# parent command. This lines wouldn't have appeared if we didn't explicitly declare "-Path" in the code. This line is saying that 
# powershell is about to attempt the value ".\computers.csv" to the parameter "-Path".

DEBUG: ParameterBinding Information: 0 : BIND POSITIONAL cmd line args [Import-Csv]
DEBUG: ParameterBinding Information: 0 :     BIND arg [.\computers.csv] to parameter [Path]
# The above 2 lines didn't appear, but would have if I didn't explicitly declared "-path" in the code. I inserted the above 2 lines 
# to high-light the fact that Parameters are binded in the following order:
#			1. explicitly named parameters
#			2. implicitly (positional) parameters  

DEBUG: ParameterBinding Information: 0 :         COERCE arg to [System.String[]]
# At the moment our value ".\computers.csv" is a string (i.e. System.String). However the -path parameter is designed to only 
# accept arrays (i.e. System.String[]). So the next step is to persuade (i.e. "coerce") our value to change into an array, which is possible if
# powershell creates an array and adding our single value (i.e. ".\computers.csv") to that array.   
   

DEBUG: ParameterBinding Information: 0 :             Trying to convert argument value from System.String to System.String[]
DEBUG: ParameterBinding Information: 0 :             ENCODING arg into collection 
DEBUG: ParameterBinding Information: 0 :             Binding collection parameter Path: argument type [String], parameter type [System.String[]], collection type Array, element type [System.String], coerceElementType
DEBUG: ParameterBinding Information: 0 :             Creating array with element type [System.String] and 1 elements
DEBUG: ParameterBinding Information: 0 :             Argument type String is not IList, treating this as scalar
DEBUG: ParameterBinding Information: 0 :             COERCE arg to [System.String]
# Here we are going to (if required) persuade our value (".\computers.csv") to change into the data type that can be accepted 
# as an entry in the array.

DEBUG: ParameterBinding Information: 0 :                 Parameter and arg types the same, no coercion is needed.
# "string[]" is an array where the entries are supposed to be of the string data type. Since ".\computers.csv" is already a string, it means no
# coercion is required before adding the value to the array. 

DEBUG: ParameterBinding Information: 0 :             Adding scalar element of type String to array position 0
# Here we finally add our value as the first element in the array. 

DEBUG: ParameterBinding Information: 0 :         Executing VALIDATION metadata: [System.Management.Automation.ValidateNotNullOrEmptyAttribute]
# This is a check to see that the value we added to the array is not a null value. 

DEBUG: ParameterBinding Information: 0 :         BIND arg [System.String[]] to param [Path] SUCCESSFUL
# This confirms that the parameter binding has been s. 



DEBUG: ParameterBinding Information: 0 : BIND POSITIONAL cmd line args [Import-Csv]
DEBUG: ParameterBinding Information: 0 : MANDATORY PARAMETER CHECK on cmdlet [Import-Csv]
DEBUG: ParameterBinding Information: 0 : BIND NAMED cmd line args [Get-WmiObject]
DEBUG: ParameterBinding Information: 0 :     BIND arg [win32_bios] to parameter [Class]
DEBUG: ParameterBinding Information: 0 :         COERCE arg to [System.String]
DEBUG: ParameterBinding Information: 0 :             Parameter and arg types the same, no coercion is needed.
DEBUG: ParameterBinding Information: 0 :         BIND arg [win32_bios] to param [Class] SUCCESSFUL
DEBUG: ParameterBinding Information: 0 : BIND POSITIONAL cmd line args [Get-WmiObject]
DEBUG: ParameterBinding Information: 0 : MANDATORY PARAMETER CHECK on cmdlet [Get-WmiObject]
DEBUG: ParameterBinding Information: 0 : CALLING BeginProcessing
DEBUG: ParameterBinding Information: 0 : CALLING BeginProcessing
DEBUG: ParameterBinding Information: 0 : BIND PIPELINE object to parameters: [Get-WmiObject]
DEBUG: ParameterBinding Information: 0 :     PIPELINE object TYPE = [System.Management.Automation.PSCustomObject]
DEBUG: ParameterBinding Information: 0 :     RESTORING pipeline parameter's original values
DEBUG: ParameterBinding Information: 0 : BIND PIPELINE object to parameters: [Out-Default]
DEBUG: ParameterBinding Information: 0 :     PIPELINE object TYPE = [System.Management.Automation.ErrorRecord]
DEBUG: ParameterBinding Information: 0 :     RESTORING pipeline parameter's original values
DEBUG: ParameterBinding Information: 0 :     Parameter [InputObject] PIPELINE INPUT ValueFromPipeline NO COERCION
DEBUG: ParameterBinding Information: 0 :     BIND arg [The input object cannot be bound to any parameters for the command either because the command do
es not take pipeline input or the input and its properties do not match any of the parameters that take pipeline input.] to parameter [InputObject]
DEBUG: ParameterBinding Information: 0 :         BIND arg [The input object cannot be bound to any parameters for the command either because the command does not take pipeline input or the input and its properties do not match any of the parameters that take pipeline input.] to param [InputObject] SUCCESSFUL
DEBUG: ParameterBinding Information: 0 : MANDATORY PARAMETER CHECK on cmdlet [Out-Default]
DEBUG: ParameterBinding Information: 0 : BIND NAMED cmd line args [Out-LineOutput]
DEBUG: ParameterBinding Information: 0 :     BIND arg [Microsoft.PowerShell.Commands.Internal.Format.ConsoleLineOutput] to parameter [LineOutput]
DEBUG: ParameterBinding Information: 0 :         COERCE arg to [System.Object]
DEBUG: ParameterBinding Information: 0 :             Parameter and arg types the same, no coercion is needed.
DEBUG: ParameterBinding Information: 0 :         BIND arg [Microsoft.PowerShell.Commands.Internal.Format.ConsoleLineOutput] to param [LineOutput] SUCCESSFUL
DEBUG: ParameterBinding Information: 0 : BIND POSITIONAL cmd line args [Out-LineOutput]
DEBUG: ParameterBinding Information: 0 : MANDATORY PARAMETER CHECK on cmdlet [Out-LineOutput]
DEBUG: ParameterBinding Information: 0 : CALLING BeginProcessing
DEBUG: ParameterBinding Information: 0 : BIND PIPELINE object to parameters: [Out-LineOutput]
DEBUG: ParameterBinding Information: 0 :     PIPELINE object TYPE = [System.Management.Automation.ErrorRecord]
DEBUG: ParameterBinding Information: 0 :     RESTORING pipeline parameter's original values
DEBUG: ParameterBinding Information: 0 :     Parameter [InputObject] PIPELINE INPUT ValueFromPipeline NO COERCION
DEBUG: ParameterBinding Information: 0 :     BIND arg [The input object cannot be bound to any parameters for the command either because the command does not take pipeline input or the input and its properties do not match any of the parameters that take pipeline input.] to parameter [InputObject]
DEBUG: ParameterBinding Information: 0 :         BIND arg [The input object cannot be bound to any parameters for the command either because the comman
d does not take pipeline input or the input and its properties do not match any of the parameters that take pipeline input.] to param [InputObject] SUCCESSFUL
DEBUG: ParameterBinding Information: 0 : MANDATORY PARAMETER CHECK on cmdlet [out-lineoutput]
DEBUG: ParameterBinding Information: 0 : BIND NAMED cmd line args [Format-Default]
DEBUG: ParameterBinding Information: 0 : BIND POSITIONAL cmd line args [Format-Default]
DEBUG: ParameterBinding Information: 0 : MANDATORY PARAMETER CHECK on cmdlet [Format-Default]
DEBUG: ParameterBinding Information: 0 : CALLING BeginProcessing
DEBUG: ParameterBinding Information: 0 : BIND PIPELINE object to parameters: [Format-Default]
DEBUG: ParameterBinding Information: 0 :     PIPELINE object TYPE = [System.Management.Automation.ErrorRecord]
DEBUG: ParameterBinding Information: 0 :     RESTORING pipeline parameter's original values
DEBUG: ParameterBinding Information: 0 :     Parameter [InputObject] PIPELINE INPUT ValueFromPipeline NO COERCION
DEBUG: ParameterBinding Information: 0 :     BIND arg [The input object cannot be bound to any parameters for the command either because the command does not take pipeline input or the input and its properties do not match any of the parameters that take pipeline input.] to parameter [InputObject]
DEBUG: ParameterBinding Information: 0 :         BIND arg [The input object cannot be bound to any parameters for the command either because the command does not take pipeline input or the input and its properties do not match any of the parameters that take pipeline input.] to param [InputObject] SUCCESSFUL
DEBUG: ParameterBinding Information: 0 : MANDATORY PARAMETER CHECK on cmdlet [format-default]
DEBUG: ParameterBinding Information: 0 : BIND NAMED cmd line args [Set-StrictMode]
DEBUG: ParameterBinding Information: 0 :     BIND arg [1] to parameter [Version]
DEBUG: ParameterBinding Information: 0 :         Executing DATA GENERATION metadata: [Microsoft.PowerShell.Commands.SetStrictModeCommand+ArgumentToVersionTransformationAttribute]
DEBUG: ParameterBinding Information: 0 :             result returned from DATA GENERATION: 1.0
DEBUG: ParameterBinding Information: 0 :         COERCE arg to [System.Version]
DEBUG: ParameterBinding Information: 0 :             Parameter and arg types the same, no coercion is needed.
DEBUG: ParameterBinding Information: 0 :         Executing VALIDATION metadata: [Microsoft.PowerShell.Commands.SetStrictModeCommand+ValidateVersionAttribute]
DEBUG: ParameterBinding Information: 0 :         BIND arg [1.0] to param [Version] SUCCESSFUL
DEBUG: ParameterBinding Information: 0 : BIND POSITIONAL cmd line args [Set-StrictMode]
DEBUG: ParameterBinding Information: 0 : MANDATORY PARAMETER CHECK on cmdlet [Set-StrictMode]
DEBUG: ParameterBinding Information: 0 : CALLING BeginProcessing
DEBUG: ParameterBinding Information: 0 : CALLING EndProcessing
DEBUG: ParameterBinding Information: 0 : BIND NAMED cmd line args [Set-StrictMode]
DEBUG: ParameterBinding Information: 0 :     BIND arg [1] to parameter [Version]
DEBUG: ParameterBinding Information: 0 :         Executing DATA GENERATION metadata: [Microsoft.PowerShell.Commands.SetStrictModeCommand+ArgumentToVersionTransformationAttribute]
DEBUG: ParameterBinding Information: 0 :             result returned from DATA GENERATION: 1.0
DEBUG: ParameterBinding Information: 0 :         COERCE arg to [System.Version]
DEBUG: ParameterBinding Information: 0 :             Parameter and arg types the same, no coercion is needed.
DEBUG: ParameterBinding Information: 0 :         Executing VALIDATION metadata: [Microsoft.PowerShell.Commands.SetStrictModeCommand+ValidateVersionAttribute]
DEBUG: ParameterBinding Information: 0 :         BIND arg [1.0] to param [Version] SUCCESSFUL
DEBUG: ParameterBinding Information: 0 : BIND POSITIONAL cmd line args [Set-StrictMode]
DEBUG: ParameterBinding Information: 0 : MANDATORY PARAMETER CHECK on cmdlet [Set-StrictMode]
DEBUG: ParameterBinding Information: 0 : CALLING BeginProcessing
DEBUG: ParameterBinding Information: 0 : CALLING EndProcessing
DEBUG: ParameterBinding Information: 0 : BIND NAMED cmd line args [Set-StrictMode]
DEBUG: ParameterBinding Information: 0 :     BIND arg [1] to parameter [Version]
DEBUG: ParameterBinding Information: 0 :         Executing DATA GENERATION metadata: [Microsoft.PowerShell.Commands.SetStrictModeCommand+ArgumentToVersionTransformationAttribute]
DEBUG: ParameterBinding Information: 0 :             result returned from DATA GENERATION: 1.0
DEBUG: ParameterBinding Information: 0 :         COERCE arg to [System.Version]
DEBUG: ParameterBinding Information: 0 :             Parameter and arg types the same, no coercion is needed.
DEBUG: ParameterBinding Information: 0 :         Executing VALIDATION metadata: [Microsoft.PowerShell.Commands.SetStrictModeCommand+ValidateVersionAttribute]
DEBUG: ParameterBinding Information: 0 :         BIND arg [1.0] to param [Version] SUCCESSFUL
DEBUG: ParameterBinding Information: 0 : BIND POSITIONAL cmd line args [Set-StrictMode]
DEBUG: ParameterBinding Information: 0 : MANDATORY PARAMETER CHECK on cmdlet [Set-StrictMode]
DEBUG: ParameterBinding Information: 0 : CALLING BeginProcessing
DEBUG: ParameterBinding Information: 0 : CALLING EndProcessing
DEBUG: ParameterBinding Information: 0 : BIND NAMED cmd line args [Set-StrictMode]
DEBUG: ParameterBinding Information: 0 :     BIND arg [1] to parameter [Version]
DEBUG: ParameterBinding Information: 0 :         Executing DATA GENERATION metadata: [Microsoft.PowerShell.Commands.SetStrictModeCommand+ArgumentToVersionTransformationAttribute]
DEBUG: ParameterBinding Information: 0 :             result returned from DATA GENERATION: 1.0
DEBUG: ParameterBinding Information: 0 :         COERCE arg to [System.Version]
DEBUG: ParameterBinding Information: 0 :             Parameter and arg types the same, no coercion is needed.
DEBUG: ParameterBinding Information: 0 :         Executing VALIDATION metadata: [Microsoft.PowerShell.Commands.SetStrictModeCommand+ValidateVersionAttribute]
DEBUG: ParameterBinding Information: 0 :         BIND arg [1.0] to param [Version] SUCCESSFUL
DEBUG: ParameterBinding Information: 0 : BIND POSITIONAL cmd line args [Set-StrictMode]
DEBUG: ParameterBinding Information: 0 : MANDATORY PARAMETER CHECK on cmdlet [Set-StrictMode]
DEBUG: ParameterBinding Information: 0 : CALLING BeginProcessing
DEBUG: ParameterBinding Information: 0 : CALLING EndProcessing
Get-WmiObject : The input object cannot be bound to any parameters for the command either because the command does not take pipeline input or the input and its properties do not match any of the parameters that take pipeline input. At line:1 char:96
+ ... puters.csv  |  Get-WmiObject -Class win32_bios}
+                    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    + CategoryInfo          : InvalidArgument: (@{Computername=localhost}:PSObject) [Get-WmiObject], ParameterBindingException
    + FullyQualifiedErrorId : InputObjectNotBound,Microsoft.PowerShell.Commands.GetWmiObjectCommand
 
DEBUG: ParameterBinding Information: 0 : CALLING EndProcessing
DEBUG: ParameterBinding Information: 0 : CALLING EndProcessing


SMBIOSBIOSVersion : R1120Y6
Manufacturer      : American Megatrends Inc.
Name              : BIOS Date: 09/23/09 11:58:43 Ver: 08.00.10
SerialNumber      : 27524969-5001935
Version           : Sony - 20101011
#################################################################################################################################################




]]></Content>
		<Date><![CDATA[2014-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell - Using object hierarchies for complex output]]></Title>
		<Content><![CDATA[Chapter 20 - Using object hierarchies for complex output

Previously we learned that a (collecion) object can contain a number of other (row) objects, and each row's field contains string values (i.e. a simple one-value object). However it is possible (and quite common) that these fields contain a whole object, rather than just a value. 

This means that you can have a situations where we have an object-within an object, which is embeded within another object, which is embeded within another object......and etc!


To look for these types of occurences, try:

Get-Process | Format-List -Property *       # This will list all properties for all processes. 

Scroll through the output and look for any property's value that is displayed within curly-brackets. In my case, I found one for a process called "jqs":

PS C:\> Get-Process -Name jqs | Format-List -Property *

__NounName                 : Process
Name                       : jqs
Handles                    : 188
VM                         : 38875136
WS                         : 1425408
PM                         : 2662400
NPM                        : 34928
Path                       :
Company                    :
.
.
.
.
StartInfo                  : System.Diagnostics.ProcessStartInfo
StartTime                  :
SynchronizingObject        :
Threads                    : {1344, 1452, 1460, 1472...}				# Here is what we are interested in. 
UserProcessorTime          :
.
.
.


Notice that the "threads" property's value is shown in curly brackets. This is powershell's way of saying that this is actually an object. 

Now if we di this:

PS C:\> Get-Process -Name jqs | Select-Object -Property threads
Threads
-------
{1344, 1452, 1460, 1472...}


You can see that the value is still appearing as indicator that this is an object. So if you want view this value in the form of an object, then you have to use the select-object's "-expandproperty" parameter instead:


PS C:\> Get-Process -Name jqs | Select-Object -expandProperty threads
BasePriority            : 4
CurrentPriority         : 10
Id                      : 1344
.
.
.

By default the info is displayed in list format, so we can show it in table format like this:

PS C:\> Get-Process -Name jqs | Select-Object -expandProperty threads | Format-Table

BasePriorit CurrentPrio          Id IdealProces PriorityBoo PriorityLev PrivilegedP StartAddres StartTime   ThreadState
          y        rity             sor         stEnabled   el          rocessorTim           s
                                                                        e
----------- -----------          -- ----------- ----------- ----------- ----------- ----------- ---------   -----------
          4          10        1344                                                  2088830773                    Wait
          4           5        1452                                                  2088830761                    Wait
          4           5        1460                                                  2088830761                    Wait
          4           4        1472                                                  2088830761                    Wait
          4           4        1608                                                  2088830761                    Wait

Although, this doesn't display the "waitreason" property since terminal screen isn't wide enough, so we do this:

PS C:\> Get-Process -Name jqs | Select-Object -expandProperty threads | Format-Table -Property basepriority,currentprior
ity,id,startaddress,threadstate,waitreason

       BasePriority     CurrentPriority                  Id        StartAddress         ThreadState          WaitReason
       ------------     ---------------                  --        ------------         -----------          ----------
                  4                  10                1344          2088830773                Wait           Executive
                  4                   5                1452          2088830761                Wait         UserRequest
                  4                   5                1460          2088830761                Wait         UserRequest
                  4                   4                1472          2088830761                Wait         UserRequest
                  4                   5                1608          2088830761                Wait         UserRequest

The expandproperty parameter is very useful, and you will find yourself using this quite often. 



You can also create nested-style objects like this as well (which we will cover a little later).

But before we do that, we will first look at a technique called "splatting". Splatting is all about passing parameters into a command in the form 
of a hash table. For example, lets say we have the following command:

PS C:\> Get-WmiObject -computername localhost -Class 'win32_OperatingSystem'

SystemDirectory : C:\Windows\system32
Organization    : Ordnance Survey
BuildNumber     : 7601
RegisteredUser  : ---
SerialNumber    : 00477-001-0000421-84442
Version         : 6.1.7601


Here we have passed 2 parameters into the get-wmiobject parameter, they are  "-computername" and "-win32_OperatingSystem". These 2 parameters can also be fed into the get-wmiobject parameter using a hash table, but first we have to create the hashtable:

$params = @{computername=$computername;
            class='win32_OperatingSystem'            
		}

Now, since this hash table's property names matches those available parameter names of get-wmiobject, it means that the hashtable is now ready to be fed into the get-wmiobject, hence another way to write:

PS C:\> Get-WmiObject -computername localhost -Class 'win32_OperatingSystem'

Is to write:

PS C:\> Get-WmiObject @params

Notice here that we preceded @ rather than $. In Ps, this is way to tell the command (get-wmiobject) to:

     "take whatever characters comes after it as a variable name, and read whatever 
	 is inside the variable as a hashtable, and that the keys are parameter names. 
	 Expand those out, and feed the values from the hashtable to those parameters". 
  
In this context, the "@" is referred to as a splat operator.   	

Here is more info about this: http://technet.microsoft.com/en-us/magazine/gg675931.aspx

Note: although logicial, the following will not work:
PS C:\Users\SChowdhury\Desktop> $computername='localhost'
PS C:\Users\SChowdhury\Desktop> $class='win32_OperatingSystem'
PS C:\Users\SChowdhury\Desktop> Get-WmiObject $computername $class		# Unfortunately this line will generate an error message. 


Now, lets take a look at creating our own nested object. Lets assume that you want to create a collection (object), that lists machine info about each machine. Hence this (collection) object contains row (objects), and each row gives the following info about each machine. The properties for this (collection) object table are:

	- computername  (this will be the parameter that we will pass into the script/function)
	- SPVersion 
	- OSversion 
	- HDD info			# This will become a nested object since each machine can contain 2 or more hdds. 
						# In particular we want the following info:
							# Drive's ID
							# disk size
							# free space


Here is the code that shows how to do the above (i avoided creating it in a function to make it easier to follow):

############################################################### Start script
[cmdletbinding()]	
param(
    [string[]]$computernames 
)

$maintable = @()

foreach ($computername in $computernames) {
   
    # $computername                  # this will be the first property of our main table, which will create later. 

    # first get the "OSversion" and SPVersion info.
    $params = @{
        computername=$computername;
        class='win32_OperatingSystem'            
    }
    
    $os = Get-WmiObject @params
    
        
    # $os.version                    # This will be used later when creating the main table.               
    # $os.ServicePackMajorVersion    # This will be used later when creating the main table.  

    # Now get the hdd info for the given machine so that we can create our hdd collection table. 
    $params = @{
        computername=$computername
        class="win32_logicaldisk"
        filter='drivetype=3'
    }  
    
    $HddInfo = Get-WmiObject @params	# This variable stores all the hdd info for a given machine. 
        
	# Now create the hdd object table

    $HddCollection = @()

    foreach ($Hdd in $HddInfo){

        $HddSubsetInfo = @{
            DiskID=$Hdd.DeviceID;
            DiskSpac=$Hdd.size;
            FreeSpace=$Hdd.freespace
        }
    
    $HddRow = New-Object -TypeName psobject -Property $HddSubsetInfo        # this converts the 2-column (key/value) associative table into an
																			# object, where the key names, becomes the column (property) names.
    $HddCollection += $HddRow
    
    }
    
    
    # Now create the main table which will contain all the info gathered above. 
    
    #$HddCollection

    
    $MaintableAssociativeArray = @{
        computername=$computername;
        OSVersion = $os.version;
        SPVersion = $os.ServicePackMajorVersion;
        HDDInfo   = $HddCollection					# here we are placing a whole (collection) object as a single field.   
    }
    $maintableRow = New-Object -type psobject -Property $MaintableAssoiativeArray

    $maintable += $maintableRow
    
}
$maintable 

############################################################### End script

Now when you run the script, you should get:

PS C:\Users\SChowdhury\Desktop> .\ch20-testfile.ps1 -computernames nd24228, localhost

HDDInfo                              computername                                                    SPVersion OSVersion                          
-------                              ------------                                                    --------- ---------                          
{@{FreeSpace=121001988096; DiskID... nd24228                                                                 3 5.1.2600                           
{@{FreeSpace=5575049216; DiskID=C... localhost                                                               1 6.1.7601                           

As expected, the curly bracket entries in hddinfo indicates that this is a nested object, so we can do this to look inside:


PS C:\Users\SChowdhury\Desktop> .\ch20-testfile.ps1 -computernames nd24228, localhost | Select-Object -ExpandProperty hddinfo -Property computername

                           FreeSpace DiskID                                                           DiskSpac computername                       
                           --------- ------                                                           -------- ------------                       
                        121001644032 C:                                                           160038907904 nd24228                            
                          5574819840 C:                                                            42842714112 localhost                          
                         40796897280 E:                                                            42946523136 localhost                          

						 
Note: I include computername property so that I can see which row relates to which higher level object. 

Another way to do this is by using "format-custom" command. 

PS C:\Users\SChowdhury\Desktop> .\ch20-testfile.ps1 -computernames nd24228, localhost | format-custom -property *

This command will actually expand the entire object and display it in an xml like fashion (aka a tree like fashion)

 
Another way to view the nested object is by using the "powershell's array syntax" like this:

PS C:\Users\SChowdhury\Desktop> $nestedobject = .\ch20-testfile.ps1 -computernames nd24228, localhost
PS C:\Users\SChowdhury\Desktop> $nestedobject

HDDInfo                              computername                                                    SPVersion OSVersion                          
-------                              ------------                                                    --------- ---------                          
{@{FreeSpace=120997146624; DiskID... nd24228                                                                 3 5.1.2600                           
{@{FreeSpace=5574144000; DiskID=C... localhost                                                               1 6.1.7601                           

PS C:\Users\SChowdhury\Desktop> $nestedobject.hddinfo

                                       FreeSpace DiskID                                                                                   DiskSpac
                                       --------- ------                                                                                   --------
                                    120997146624 C:                                                                                   160038907904
                                      5574144000 C:                                                                                    42842714112
                                     40796897280 E:                                                                                    42946523136
 
You can then access, a particular row like this:


PS C:\Users\SChowdhury\Desktop> $nestedobject.hddinfo[1]

                                       FreeSpace DiskID                                                                                   DiskSpac
                                       --------- ------                                                                                   --------
                                      5574144000 C:                                                                                    42842714112

And even do this to access a nested object's property:

PS C:\Users\SChowdhury\Desktop> $nestedobject.hddinfo[1].FreeSpace
5574144000

Another way to access nested object is using the foreach loop (along with the powershell's array syntax), e.g:

foreach ($each_row_in_nested_object  in   $collection.Property.That.Houses.Nested.Object){
	"Here is an object:"
	"$each_row_in_nested_object"
}


To summarise we looked at 4 ways to access nested objects:

	- using select-object, and it's "-expandproperty" parameter
	- using the "format-custom" command
	- Using powershell's array syntax  
	- using foreach loop

]]></Content>
		<Date><![CDATA[2014-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell - Utilising the .NET framework and embeding c# in PowerShell]]></Title>
		<Content><![CDATA[Chapter 21 - Globalizing a function   (skipped because it looks at translating functions to other languages, e.g. french, spanish, etc)

Chapter 22 - Crossing the line: Utilising the .NET framework

.net is all about objects. First what is a class?:

class - this is a hunk of code that act as the template to create an object. You can use a class to create many instances of an object. An "instance". Here is how a class as an example:

########
PS C:\> $source = @"
public class BasicTest
{
    public static int Add(int a, int b)
    {
        return (a + b);
    }

    public int Multiply(int a, int b)						# notice that this is an "int", aka a "non-static method"
    {
        return (a * b);
    }
}
"@
###########

This .net class is stored in a powershell variable called "$source". The class itself is called "BasicTest"

For this to work, you need to define the class in: @"...."@

This class contains 2 "methods" (I think you can think of them as functions), aka known as "members", they are called:
	- Add (which accepts two parameters, 'a', and 'b')
	- Multiply (which accepts two parameters, 'a', and 'b')
	
Now to make ps recognise that $source is a class, you do:

PS C:\> Add-Type -TypeDefinition $source

Once you have done this, a new class by the name of "basictest" should now have been created. You should be able to access the "static" methods of this class, i.e. you can do this:

[BasicTest]::Add(4, 3)

Hence the basic syntax is:

{class's name}::{method's name}(parameter1,parameter2,...parameter-x)

Note: the syntax "::" is specifically used to access a "static" members of the class.

However you can't do this:

[BasicTest]::Multiply(4, 3)			# that's because multiply is an "int" and not a static method. 

If you want to use the "multiply" method, then you first have to create a generic powershell object from this class, i.e., you do this:

$basicTestPowerShell = New-Object BasicTest

PS C:\> $basicTestPowerShell | gm
   TypeName: BasicTest							# Notice what the object type
Name        MemberType Definition                    
----        ---------- ----------                    
Equals      Method     bool Equals(System.Object obj)
GetHashCode Method     int GetHashCode()             
GetType     Method     type GetType()                
Multiply    Method     int Multiply(int a, int b)    	# Notice that we now have the multiply method. But the "add" method is missing
ToString    Method     string ToString()             


Now you can acess the multiply method:

PS C:\> $basicTestPowerShell.multiply(4,3)
12

Note: In a class, you can have one or more static members, but I think you must always have at least (or exactly) 1 nonstatic member. 


You can also use gm list the members of classes, such as the the basictest class that we created using the add-type command:

PS C:\> [BasicTests] 

IsPublic IsSerial Name       BaseType                                                                                                        
-------- -------- ----       --------                                                                                                        
True     False    BasicTests System.Object                                                                                                   

PS C:\> [BasicTests] | gm
   TypeName: System.RuntimeType
Name                           MemberType Definition     
----                           ---------- ----------                         
Clone                          Method     System.Object Clone()                     
Equals                         Method     bool Equals(System.Object obj), bool Equals(type o) 
FindInterfaces                 Method     type[] FindInterfaces(System.Reflection.TypeFilter filter, System.Object 
.
.
.
etc.


PS C:\> [BasicTest] | gm -Static						# "-static" switch will only list the static methods. 
   TypeName: BasicTest
Name            MemberType Definition                                                         
----            ---------- ----------                                                         
Add             Method     static int Add(int a, int b)                                       
Equals          Method     static bool Equals(System.Object objA, System.Object objB)         
ReferenceEquals Method     static bool ReferenceEquals(System.Object objA, System.Object objB)




Classes interact with eachother



In the .net framework we have the following terminologies:

class: 	    This is basically a hunk of code that handles some specific set of related tasks. E.g. the "System.math" class provides 
			capabilities for doing arithmetics. Most classes have a multipart names like this which are seperated by periods. In the case of
			"system.math", it means that the "math" class belongs to the "system" namespace. i.e. 
			
Assemblies 	: (see def for class above) Assemblies are actually the physical files that contains compiled codel. If the essambly contains re-usable 
			 code, then they are dll files, if they are standalone apps, then they are exe files. 
			
namespace:	This is an "organisational" element for logically grouping classes together. Hence each class belongs to a namespace. 
			Analogy, a football player belongs to a team. Confusingly, classes are grouped together in another way using "assemblies". There is no
			strict relationship between classes and namespaces, i.e. an assembly can contain multiple namespaces, or just a subset of a namespace.  

instance:	this is like creating a new instance of a class. It is a bit like using a Microsoft word template to create a new doucment. 

###################### Detour start
Namespace and assemplies are quite similar, but the key difference is that namespace is the logically way to group classes together, whereas assemblies are the physical ways to group classes together. It is however common to have an entire namespace stored in the assembly that has the same name. But this is not always the case. 

To view a list of list of currently loaded assemblies, you have to do this with a .net class, (since a ps command isn't available to do this):

PS C:\> [System.AppDomain]::CurrentDomain.GetAssemblies()     # "GetAssemblies()" is a static method, hence this works. covered later. 

GAC    Version        Location                                                             
---    -------        --------                                                              
True   v2.0.50727     C:\WINDOWS\Microsoft.NET\Framework\v2.0.50727\mscorlib.dll 
False  v2.0.50727     C:\WINDOWS\system32\WindowsPowerShell\v1.0\powershell_ise.exe         
True   v2.0.50727     C:\WINDOWS\assembly\GAC_MSIL\System.Management.Automation\1.0.0.0__31bf3856ad364e35\System.Management.Automation.dll
.
.
.
etc

Now to add a new assembly to the current session, do:

PS C:\>  Add-Type -AssemblyName {assembly name}

Unfortunately there is no command for viewing all the available (loaded+nonloaded) assemblies. 

####################### Detour end



E.g. here's how we create a an instance of the system.string class:

$var = "hello"

$var | gm | Select-Object -First 1
   TypeName: System.String
Name  MemberType Definition                                             
----  ---------- ----------                                             
Clone Method     System.Object Clone(), System.Object ICloneable.Clone()


In fact, ALL objects outputed by powershell commands, are actually instances of .net classes!

Hence, the get-process command produces an instance of the System.Diagnostics.Process class. 

	
Each class comes with a collection of:

	- Properties
	- Methods
	- Events

These things are collectively called "members"...hence this is what the "get-member" commands naming comes from. 


All instances have access to these members. 


many classes

	


Useful link:
http://blogs.technet.com/b/heyscriptingguy/archive/2013/01/19/weekend-scripter-basics-of-net-framework-language.aspx 

add-type   	# this cmdlet lets you define a .NET Framework class in your Windows PowerShell session. 
			# You can then instantiate objects (by using the New-Object cmdlet) and use the objects, just as you would use any .NET Framework object.
			# see help pages for more info, especially for the first example. 




############ Detour - start

add-type   	# this cmdlet lets you define a .NET Framework class in your Windows PowerShell session. 
			# You can then instantiate objects (by using the New-Object cmdlet) and use the objects, just as you would use any .NET Framework object.
			# see help pages for more info, especially for the first example. 


############ Detour - end]]></Content>
		<Date><![CDATA[2014-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell -  Creating a GUI tool]]></Title>
		<Content><![CDATA[Chapter 23 - Creating a GUI tool, Part 1: the GUI


You can create a gui interface in Powershell by making using of the .net library. In .net there are actually 2 GUI systems available:

  - Windows Forms (aka WinForms)
  - Windows Presentation Framework (aka WPF) - this is newer, more modern system, but hasn't been introduced to replace winforms

Out of the 2 we will be using winforms, because there is no drag-and-drop powershell IDE available for building forms using WPF, however there 
is one for winforms, called Sapien Powershell Sudios 2012:

http://www.sapien.com/software/powershell_studio
 
For the time being, this only supports winforms and not wpf:

http://www.sapien.com/blog/2010/10/18/does-primalforms-use-wpf/
https://www.sapien.com/forums/viewtopic.php?f=8&t=6628

To summarise, powershell studio is the only software of its kind that lets you create a powershell gui using a gui. 

## Winforms crash course - start ##
Winforms is based on the concept of forms (aka windows). Each windows/form can house a number of items, such as:

	- checkboxes
	- text boxes
	- buttons

Each of these items are called "Controls". 

Each of these "controls" have a variety of "properties" which are mainly used to customise the various aspects of that control, including:
	- appearance (e.g colour, font type ...etc )
	- behaviour (e.g. if check box, then what is the default state, checked or unchecked, also is it enabled or greyed out)
	- layout (where is it placed, in the context of the main window)
	

Controls are basically a kind of object, a bit like powershell objects. Hence a control's properties are a bit like a powershell object's properties. 

However each "control" also have something called "events" (indicated by the lightning icon). Events are a bit like a status of an object, e.g. the status of a service can either be 
stopped or running. However a better description of an event is that an event is essentially a notification to the user. 

E.g. Lets say we a have a control that is a checkbox. If that checkbox is unticked and you tick on it, then that checkbox "notifies" you that it
has been activate by displaying a tick. These ticked/unticked statuses are referred to as "events".  

When each event is activated....then this triggers something called "event handlers". Event handlers are essentially empty containers where 
you can add your powershell code in (or maybe other e.g. c#). This is the feature that makes your gui forms interactive. 

In powershell studio, if you right click a control, and select add event, then you will end up adding an event handler to the code, this event handler has the following syntax:

	<control's name>_<event's name>={
		# here you enter what you want to happen as soon as the event is triggered 
	}

e.g. for button, it could be something like:

	$button1_Click={
		#here is what needs to be done. 
		$testvar = 5
	
	}

For more sophisticated controls, e.g. a treeview (which is a collapsible, hierarchial checkbox tree), the event-handler's syntax could look slightly different:

	<control's name>_<event's name>=[a.name.of.a.handler]{
		# here you enter what you want to happen as soon as the event is triggered 
	}

e.g. :

	$treeview1_AfterSelect=[System.Windows.Forms.TreeViewEventHandler]{
		# here you write your code, e.g.:
		$testvar = 5
	}

  
  
When doing debugging, there are a few useful techniques:
	- write-host (if you want to output a simple variale), note you can also do: write-host ($object | gm) ..... but that isn't so good because
	  it outputs in a really long string. instead, a better option is to use:
	- write-debug (covered earlier)

 
 
For more reference info about winforms, checkout:

http://msdn.microsoft.com/en-us/library/cc656767


### Winforms crashcourse - end  ###


Note, it is possible to create a ps gui by handwriting all the code, however this is very time consuming and it is much 
faster to have the powershell studio IDE software autogenerate the code for you. 


  
A tool can be made up of several forms. And each form is represented by a function that resides in the root parent script. If the 
forms name is something like:

results.pff

then the correspoding function name conforms to the following format:

call-results_pff		# essentially, it adds the "call-" prefix and replaces the period, "." with a underscore, "_"

You can then place this in the button-click of one form to trigger this form. 
]]></Content>
		<Date><![CDATA[2014-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell -  Web automation]]></Title>
		<Content><![CDATA[Note: You should use selenium rather than powershell for web autoamation, that's because it is easier. 

This script automatically logs into hotmail:

############################################## Start script



$ie = new-object -com "InternetExplorer.Application"

$ie.Visible = $true

$ie.Navigate("https://login.live.com/login.srf?wa=wsignin1.0&rpsnv=11&ct=1348676581&rver=6.1.6206.0&wp=MBI&wreply=http:%2F%2Fmail.live.com%2Fdefault.aspx&lc=2057&id=64855&mkt=en-gb&cbcxt=mai&snsc=1")

while ($ie.Busy -eq $true)

{

      Start-Sleep -Milliseconds 1000;

}

$ie.Document.getElementById("login").value ="ihkhan@hotmail.co.uk"

$ie.Document.getElementById("passwd").value ="2222"

sleep -Seconds 5

$ie.Document.getElementById("SI").click()

sleep -Seconds 2

$ie.Document.getElementById("msgChkAll").click() 
 

while ($ie.Busy -eq $true)

{

      Start-Sleep -Milliseconds 1000;
}


############################################## end script

-------------------------------------- End Detour

I think the following link helps to explain how the above script works:
http://msdn.microsoft.com/en-us/magazine/cc337896.aspx

Here is an alternative approach using a new psv3 command called invoke-webrequest
http://www.powershellcookbook.com/recipe/vODQ/script-a-web-application-session


At work, we interact with web pages using a lot of .net components. However psv3 introduced the "invoke-webrequest" which I think gives an easier approach
to automate web tasks. ]]></Content>
		<Date><![CDATA[2014-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell -  Working with xml (via DOM and XPATH)]]></Title>
		<Content><![CDATA[Let's say you have the following xml file:

<pre>PS C:\> $myxmldata = Get-Content C:\temp\credentials.xml
PS C:\> $myxmldata
<?xml version="1.0" encoding="UTF-8"?>
<scom>
  <username>codingbee</username>
  <password>mysecret</password>
</scom>
PS C:\></pre>



if you have xml data in an xml file, then you load it into powershell like this:
<pre>
PS C:\> &#091;xml&#093;$myxmldata = Get-Content C:\temp\credentials.xml
PS C:\> $myxmldata

xml                                                         scom
---                                                         ----
version="1.0" encoding="UTF-8"                              scom


PS C:\>
</pre>

Note: don't use "import-clixml" (which we covered earlier) because that can only accept xml files that were generated by powershell in the first place (using "export-clixml").

You can create an "xml object variable" from an xml file like this:



<pre>&#091;xml&#093;$myxmldata = Get-Content .\samplexml.xml</pre>

&nbsp;

Once this is done, you can then navigate/view/browse/retrieve-values from $myxmldata just like any ordinary hierarchiacal object, which contains lots of nested objects.

Note: xml tags are actually case sensitive. e.g. :

########## detour - start

PS C:\=> $myxmldata xml parameters --- ---------- version="1.0" parameters

&nbsp;

# Here, the "xml" column actually represents the first xml tag in the xml code, along with it's "version" attribute. It is a single-tag, # similar to the like of &lt;br /=> and &lt;hr /=> that you would find in html # The second "parameters" column has a value of the same name. This is powershell's way of telling you that "parameters" is a an outer tag # that contains more nested tags....hence we can drill donw into it, like this: PS C:\=> $myxmldata.parameters envname : cde9 # This is actually an attribute for the "parameters" tag. stage : 5 # This is actually an attribute for the "parameters" tag. version : 1.0 # This is actually an attribute for the "parameters" tag. modifiedDate : 1382431774679 # This is actually an attribute for the "parameters" tag. createdDate : 1379082771334 # This is actually an attribute for the "parameters" tag. modifiedBy : OS\SChowdhury # This is actually an attribute for the "parameters" tag. active : true # This is actually an attribute for the "parameters" tag. servers : servers # Ps's way of indicating that an element contains nested elements is showing a parameter has the same value as it's name. schema : schema # Here is another elements containing nested elements. environment : environment # Here is another elements containing nested elements. source : source # Here is another elements containing nested elements. # You can identify inner tags from the attributes, by the fact that inner tags parameter name and value are the same. The values are not in # curly braces because in this case there are just one instances of each. E.g. there is only one opening an closing tag for the "servers" tag. PS C:\=> $myxmldata.parameters.schema dbservicename : ARCACDE9.WORLD sid : ARCACDE9 modifiedDate : 1363690045613 modifiedBy : OS\chowdhury services : services entry : {entry, entry, entry, entry...} # here is another nested object, containing multiple entries of the same type. ########## detour -end This approach of navigating an xml file is called the "XML DOM" approach: http://www.w3schools.com/dom/default.asp Another approach to querying xml data is by using "xpath". XPath is a syntax for defining parts of an XML document. Note: the main difference between xpath and xml-dom, is that xpath only reads data, whereas xml-dom can be used to edit xml data: http://forums.codeguru.com/showthread.php?469432-How-to-run-a-very-long-SQL-statement http://stackoverflow.com/questions/16671642/dom-vs-xpath-difference You can find out more about xpath here: http://www.w3schools.com/xpath/xpath_intro.asp ######## detour - start xpath crash course: In XPath, there are seven kinds of nodes: - element - attribute - text - namespace - processing-instruction - comment - nodes ######## detour - end There is a special cmdlet in powershell, that can handle xpath notation, it is: select-xml ---------------------------------------------------------------------------------------------------------------------------------------- Special Chapter - working with xml data using the xpath syntax http://www.w3schools.com/xpath/xpath_intro.asp ֘Path is a syntax for defining parts of an XML document ֘Path uses a path-like (e.g. c:/users/desktops) syntax to drill down into xml data to get to the part you want. ֘Path contains a library of standard functions In xpath, there are several types of nodes (aka items): 1. Element - This is a pair of tags, with all the content inside it. 2. Attribute 3. Text 4. Namespace 5. Processing-instruction 6. Comment 7. Document E.g. : &lt;?xml version="1.0" encoding="ISO-8859-1"?=> &lt;bookstore=> # This is an "element" node. Since this particular element houses all the other elements, it is aka as the "root element" &lt;book=> &lt;title lang="en"=>Harry Potter&lt;/title=> # here, the 'lang="en"' is an attribute node. &lt;author=>J K. Rowling&lt;/author=> # Here is another element node. &lt;year=>2005&lt;/year=> &lt;price=>29.99&lt;/price=> # the "29.99" is called an "atomic value", since it has no children. "en" above is also known as an atomic value. &lt;/book=> &lt;/bookstore=> Now lets assume we have an xml file called "books.xml", which contain's the following xml code: &lt;?xml version="1.0" encoding="ISO-8859-1"?=> &lt;bookstore=> &lt;book=> &lt;title lang="eng"=>Harry Potter&lt;/title=> &lt;price=>29.99&lt;/price=> &lt;/book=> &lt;book=> &lt;title lang="eng"=>Learning XML&lt;/title=> &lt;price=>39.95&lt;/price=> &lt;/book=> &lt;otherbooks=> &lt;usedbooks=> &lt;book=> &lt;title lang="eng"=>Romeo and Juliet&lt;/title=> &lt;price=>5.00&lt;/price=> &lt;/book=> &lt;/usedbooks=> &lt;ebook=> &lt;book=> &lt;title lang="eng"=>Lord of the Rings&lt;/title=> &lt;price=>100.00&lt;/price=> &lt;/book=> &lt;/ebook=> &lt;/otherbooks=> &lt;/bookstore=> First we load this file into an xml-based variable object (called booksxml): PS C:\=>



$booksxml = Get-Content .\bookstore.xml

To best learn the xpath syntax, let's try out the following examples:

eg1:
Here's how to access the root element:

PS object method:

PS C:\=> $booksXML

xml bookstore
--- ---------
version="1.0" encoding="ISO-8859-1" bookstore

XPATH method:

PS C:\=> $booksXML.SelectNodes("/") # note, here we used to "selectnodes" method to pass the xpath string "/"

xml bookstore
--- ---------
version="1.0" encoding="ISO-8859-1" bookstore

eg2:
Here's how to access the content of the bookstore element:

PS object method:

PS C:\=> $booksxml.bookstore

book usedbooks ebook
---- --------- -----
{book, book} usedbooks ebook

XPATH method:

PS C:\=> $booksXML.SelectNodes("bookstore") # note, here we used to selectnodes method to pass the xpath string "bookstore"

book usedbooks ebook
---- --------- -----
{book, book} usedbooks ebook

eg3:
Display all book elements throughout the xml file:

PS object method: Note sure how to do this here.

XPATH method:

PS C:\=> $booksXML.SelectNodes("//book") # The "//" means display all matching elements no matter where they are in the document.
# hence in this case we want to display all the "books" element.

title price
----- -----
title 29.99
title 39.95
title 5.00
title 100.00

eg4:
Display all "book" elements that are childrens of the "otherbooks" element:

PS object method: Not sure how to do this here.

XPATH method:

PS C:\=> $booksXML.SelectNodes("/bookstore/otherbooks//book")

title price
----- -----
title 5.00
title 100.00

eg5:
Get all values for the attribute "lang":

PS object method: Not sure how to do this here.

XPATH method:

PS C:\=> $booksXML.SelectNodes("//@lang")

#text
-----
eng
eng
eng
eng

eg6:
of the 2 books, that are direct childs of the bookstore element, display the first book:

PS object method: Not sure how to do this here. Nearest thing I could do is:

PS C:\=> $booksXML.bookstore.book | Where-Object -FilterScript {$_.price -match "29.99"} # this is not good approach, because you have
# declare "29.99"

XPATh method:

PS C:\=> $booksXML.SelectNodes("/bookstore/book[1]")

title price
----- -----
title 29.99

You can find more example notations here:

http://www.w3schools.com/xpath/xpath_syntax.asp]]></Content>
		<Date><![CDATA[2014-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell -  Working withed zipped files/folders]]></Title>
		<Content><![CDATA[Special Chapter - unzipping zipped files using Powershell

Note: one of the easiest way to worked with compressed tar files is by using the pscx module:

http://pscx.codeplex.com/


######## detour - start
FOund this bit of code on the internet:

function Extract-Zip
{
	param([string]$zipfilename, [string] $destination)

	if(test-path($zipfilename))
	{	
		$shellApplication = new-object -com shell.application
		$zipPackage = $shellApplication.NameSpace($zipfilename)
		$destinationFolder = $shellApplication.NameSpace($destination)
		$destinationFolder.CopyHere($zipPackage.Items())
	}
}

extract-zip filename.zip unzippedfilename
######## detour - end 


######## Another detrou - start
# Code this code from:
# http://sushihangover.blogspot.co.uk/2013/03/powershell-gzip-gz-compress-and.html#!/2013/03/powershell-gzip-gz-compress-and.html
# Got the initialize-file (and touch-file) function from:
# https://github.com/sushihangover


Set-StrictMode זersion latest
function Initialize-File{
    <#
        .NOTES
            Copyright 2013 Robert Nees
            Licensed under the Apache License, Version 2.0 (the "License");
            http://sushihangover.blogspot.com
        .SYNOPSIS
            touch-file -- change file access and modification times
        .DESCRIPTION
         The touch utility sets the modification and access times of files.  If any file does not exist, 
         it is created with default permissions. (see examples)
     
            -a (AccessTime) Change just the access time of the file.
            -c (Create) Do not create the file if it does not exist.  The touch utility does not treat 
                this as an error.  No error messages are displayed and the exit value is not affected.
            -f (Force) Attempt to force the update, even if the file permissions do not currently permit 
                it. FYI: Only valid on file creation!
            -m (ModificationTime) Change just the modification time of the file.
            -n (CreationTime) Change just the creation time of the file (when it was 'n'ew).
            -r (Replace) Use the access and modifications times from the specified file instead of the 
                current time of day.
            -t (Time) Change the access and modification times to the specified time instead of the 
                current time of day.  The argument is of the form of a .Net DateTime string
        .EXAMPLE
            TODO : Add Examples
        .LINK
            http://sushihangover.blogspot.com
    #>
    [cmdletbinding(SupportsShouldProcess=$True)]
    Param(
        [Parameter(Mandatory=$true,ValueFromPipeline=$True)][String]$FileName,
        [Parameter(Mandatory=$false)][Alias('r')][String]$Replace = "",
        [Parameter(Mandatory=$false)][Alias('t')][String]$Time = "",
        [Parameter(Mandatory=$false)][Alias('c')][Switch]$Create,
        [Parameter(Mandatory=$false)][Alias('a')][Switch]$AccessTime,
        [Parameter(Mandatory=$false)][Alias('m')][Switch]$ModificationTime,
        [Parameter(Mandatory=$false)][Alias('n')][Switch]$CreationTime,
        [Parameter(Mandatory=$false)][Alias('f')][Switch]$Force
    )
    begin {
        function Update-FileSystemInfo([System.IO.FileSystemInfo]$fsInfo) {
            if ($Time -ne $null) {
                $fsInfo.CreationTime = $CurrentDateTime
                $fsInfo.LastWriteTime = $CurrentDateTime
                $fsInfo.LastAccessTime = $CurrentDateTime
            } else {
                if ($AccessTime.IsPresent) {
                    $fsInfo.LastAccessTime = $CurrentDateTime
                }
                if ($ModificationTime.IsPresent) {
                    $fsInfo.LastWriteTime = $CurrentDateTime
                }
                if ($CreationTime.IsPresent) {
                    $fsInfo.CreationTime = $CurrentDateTime
                }
            }
        }
   
        function Touch-NewFile {
            [cmdletbinding(SupportsShouldProcess=$True)]
            Param(
                [Parameter(Mandatory=$True)][String]$FileName
            )
            if ($Force.IsPresent ) {
                Set-Content -Path ($FileName) -Value ($null) -Force
            } else {
                Set-Content -Path ($FileName) -Value ($null)
            }
            $fsInfo = new-object System.IO.FileInfo($FileName)
            return $fsInfo
        }

        if ($Replace -ne "") {
            try {
                $replaceInfo = Get-ChildItem $Replace
                $CurrentDateTime = $replaceInfo.CreationTime
            } catch {
                return
            }
        } else {
            if ($Time -ne "") {
                $CurrentDateTime = [DateTime]::Parse($Time)
            } else {
                $CurrentDateTime = Get-Date            
            }
        }
    }
    process {
        if ($pscmdlet.ShouldProcess($FileName)) {
            if (test-path $FileName) {
                $fsInfo = Get-ChildItem $FileName
                Update-FileSystemInfo($fsInfo)
            }
            else {
                if (!$Create.IsPresent) {
                    $fsInfo = Touch-NewFile($FileName)
                    $fsInfo = Get-ChildItem $FileName
                    Update-FileSystemInfo($fsInfo)
                }
            }
        }
        $fsInfo = $null
    }
    end {
    }
}

# Initialize-File naming sucks for the 'touch' command but makes sense in the 
# verb list and passes loading without errors, but lets alias to 'touch-file', ok!
# Not setting alias to 'touch' to avoid 'hidding' your cygwn touch.exe, etc..., do 
# that in your profile if you are not using another version of Touch on your system.
Set-Alias Touch-File Initialize-File -Scope Global
 
 
 <#
.NOTES
    Copyright 2013 Robert Nees
    Licensed under the Apache License, Version 2.0 (the "License");
.SYNOPSIS
    GZip Compress and DeCompress
.DESCRIPTION
    A 8k buffered GZip (.gz) Compress and DeCompress functions that support pipelined input
.LINK
    http://sushihangover.blogspot.com
.LINK
    https://github.com/sushihangover
#>
function Compress-GZip {
    <#
    .NOTES
        Copyright 2013 Robert Nees
        Licensed under the Apache License, Version 2.0 (the "License");
    .SYNOPSIS
        GZip Compress (.gz)
    .DESCRIPTION
        A buffered GZip (.gz) Compress function that support pipelined input
    .Example
        ls .\NotCompressFile.xml | Compress-GZip -Verbose -WhatIf
    .Example
        Compress-GZip -FullName NotCompressFile.xml -NewName Compressed.xml.funkyextension
    .LINK
        http://sushihangover.blogspot.com
    .LINK
        https://github.com/sushihangover
    #>
    [cmdletbinding(SupportsShouldProcess=$True,ConfirmImpact="Low")]
    param (
        [Alias("PSPath")][parameter(mandatory=$true,ValueFromPipeline=$true,ValueFromPipelineByPropertyName=$true)][string]$FullName,
        [Alias("NewName")][parameter(mandatory=$false,ValueFromPipeline=$false,ValueFromPipelineByPropertyName=$true)][string]$GZipPath,
        [parameter(mandatory=$false)][switch]$Force
    )
    Process {
        $_BufferSize = 1024 * 8
        if (Test-Path -Path $FullName -PathType Leaf) {
            Write-Verbose "Reading from: $FullName"
            if ($GZipPath.Length -eq 0) {
                $tmpPath = ls -Path $FullName
                $GZipPath = Join-Path -Path ($tmpPath.DirectoryName) -ChildPath ($tmpPath.Name + '.gz')
            }
            if (Test-Path -Path $GZipPath -PathType Leaf -IsValid) {
                Write-Verbose "Compressing to: $GZipPath"
            } else {
                Write-Error -Message "$FullName is not a valid path/file"
                return
            }
        } else {
            Write-Error -Message "$GZipPath does not exist"
            return
        }
        if (Test-Path -Path $GZipPath -PathType Leaf) {
            If ($Force.IsPresent) {
                if ($pscmdlet.ShouldProcess("Overwrite Existing File @ $GZipPath")) {
                    Touch-File $GZipPath
                }
            }
        } else {
            if ($pscmdlet.ShouldProcess("Create new Compressed File @ $GZipPath")) {
                Touch-File $GZipPath
            }
        }
        if ($pscmdlet.ShouldProcess("Creating Compress File @ $GZipPath")) {
            Write-Verbose "Opening streams and file to save compressed version to..."
            $input = New-Object System.IO.FileStream (ls -path $FullName).FullName, ([IO.FileMode]::Open), ([IO.FileAccess]::Read), ([IO.FileShare]::Read);
            $output = New-Object System.IO.FileStream (ls -path $GZipPath).FullName, ([IO.FileMode]::Create), ([IO.FileAccess]::Write), ([IO.FileShare]::None)
            $gzipStream = New-Object System.IO.Compression.GzipStream $output, ([IO.Compression.CompressionMode]::Compress)
            try {
                $buffer = New-Object byte[]($_BufferSize);
                while ($true) {
                    $read = $input.Read($buffer, 0, ($_BufferSize))
                    if ($read -le 0) {
                        break;
                    }
                    $gzipStream.Write($buffer, 0, $read)
                }
            }
            finally {
                Write-Verbose "Closing streams and newly compressed file"
                $gzipStream.Close();
                $output.Close();
                $input.Close();
            }
        }
    }
}
function Expand-GZip {
    <#
    .NOTES
        Copyright 2013 Robert Nees
        Licensed under the Apache License, Version 2.0 (the "License");
    .SYNOPSIS
        GZip Decompress (.gz)
    .DESCRIPTION
        A buffered GZip (.gz) Decompress function that support pipelined input
    .Example
        ls .\RegionName.cs.gz | Expand-GZip -Verbose -WhatIf
    .Example
        Expand-GZip -FullName CompressFile.xml.gz -NewName NotCompressed.xml
    .LINK
        http://sushihangover.blogspot.com
    .LINK
        https://github.com/sushihangover
    #>
    [cmdletbinding(SupportsShouldProcess=$True,ConfirmImpact="Low")]
    param (
        [Alias("PSPath")][parameter(mandatory=$true,ValueFromPipeline=$true,ValueFromPipelineByPropertyName=$true)][string]$FullName,
        [Alias("NewName")][parameter(mandatory=$false,ValueFromPipeline=$false,ValueFromPipelineByPropertyName=$true)][string]$GZipPath = $null,
        [parameter(mandatory=$false)][switch]$Force
    )
    Process {
        if (Test-Path -Path $FullName -PathType Leaf) {
            Write-Verbose "Reading from: $FullName"
            if ($GZipPath.Length -eq 0) {
                $tmpPath = ls -Path $FullName
                $GZipPath = Join-Path -Path ($tmpPath.DirectoryName) -ChildPath ($tmpPath.BaseName)
            }
            if (Test-Path -Path $GZipPath -PathType Leaf -IsValid) {
                Write-Verbose "Decompressing to: $GZipPath"
            } else {
                Write-Error -Message "$GZipPath is not a valid path/file"
                return
            }
        } else {
            Write-Error -Message "$FullName does not exist"
            return
        }
        if (Test-Path -Path $GZipPath -PathType Leaf) {
            If ($Force.IsPresent) {
                if ($pscmdlet.ShouldProcess("Overwrite Existing File @ $GZipPath")) {
                    Touch-File $GZipPath
                }
            }
        } else {
            if ($pscmdlet.ShouldProcess("Create new decompressed File @ $GZipPath")) {
                Touch-File $GZipPath
            }
        }
        if ($pscmdlet.ShouldProcess("Creating Decompressed File @ $GZipPath")) {
            Write-Verbose "Opening streams and file to save compressed version to..."
            $input = New-Object System.IO.FileStream (ls -path $FullName).FullName, ([IO.FileMode]::Open), ([IO.FileAccess]::Read), ([IO.FileShare]::Read);
            $output = New-Object System.IO.FileStream (ls -path $GZipPath).FullName, ([IO.FileMode]::Create), ([IO.FileAccess]::Write), ([IO.FileShare]::None)
            $gzipStream = New-Object System.IO.Compression.GzipStream $input, ([IO.Compression.CompressionMode]::Decompress)
            try {
                $buffer = New-Object byte[](1024);
                while ($true) {
                    $read = $gzipStream.Read($buffer, 0, 1024)
                    if ($read -le 0) {
                        break;
                    }
                    $output.Write($buffer, 0, $read)
                }
            }
            finally {
                Write-Verbose "Closing streams and newly decompressed file"
                $gzipStream.Close();
                $output.Close();
                $input.Close();
            }
        }
    }
}
Expand-GZip -FullName vmlin258_17_out.tar.gz -NewName vmlin258_17_out.tar
######## another dtoru - end


The best way to to see this in action by following the following example:

In this example we look in a folder, and in this folder we get a list of all the zip files, on which we 

Get-ChildItem C:\path\to\folder\containing\one\or\more\zip\files\*.zip | ForEach-Object {
    
    $FileName = $_.Name				# this houses the name of the zip file. 
    $FullZippedFileName = "$TargetFolder\$component\$FileName"    # this again is the zipfile's name but also includes the full path. 
    $TargetFolderForUnzippedFiles = "$TargetFolder\$component"    # this is where the unzipped contain will get sent to. 
        
    $shell_app=new-object -com shell.application		# here we create a generic shell.application object. need to search for "shell.applicaton"
														# in powershell books. 
    
    $zip_file = $shell_app.namespace("$FullZippedFileName")             # Not sure what happens here, but I think we are telling the unzipping object
																		# where the zipfile is so that it can locate it. 
																		
    $destination = $shell_app.namespace($TargetFolderForUnzippedFiles)	# here we are saying where to store the extracted files too. 
	
    $destination.Copyhere($zip_file.items())							# this initiates the actual unzipping process. 

}


Also see:

http://serverfault.com/questions/18872/how-to-zip-unzip-files-in-powershell

Here is also a very useful script that you can use to pull out certain files out of the zip file, based on their file/folder name:

http://gallery.technet.microsoft.com/scriptcenter/PowerShell-Get-Specific-9b35352f

I used this script to pull out sql scripts out of release folder versions, when they were missing from my chosen component version folder. 


]]></Content>
		<Date><![CDATA[2014-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell -  Run sql queries (using sqlplus)]]></Title>
		<Content><![CDATA[<h2>What is SQLPLUS?</h2>

If you have an Oracle database and you want to run sql queries/scripts against it, then you can use "sqlplus", which is Oracle's own command line utility for running sql scripts including those in in Oracle's powerful <a href="http://amzn.to/1PmFTq3">Oracle PL/SQL Programming PL/SQL scripting language</a>. You can install sqlplus onto your machine by installing the <a href="http://www.oracle.com/technetwork/topics/winsoft-085727.html">oracle client</a>. Sqlplus also comes included with <a href="http://www.oracle.com/technetwork/developer-tools/sql-developer/downloads/index.html">Oracle SQL Developer</a>.

Once you have sqlplus installed, you then need to add a valid tns entry to the <a href="http://www.orafaq.com/wiki/Tnsnames.ora">tns file</a>. 

sqlplus was originally designed to be called using the <a href="http://en.wikipedia.org/wiki/Cmd.exe">old legacy cmd</a>. However it works just as well using Powershell (although  sqlplus won't run properly if run from inside the Powershell ISE).

Powershell users can use sqlplus in three main ways:
<ul>
	<li>Start an interactive sqlplus session from the powershell command line</li>
	<li>Run a sql script from inside a powershell script</li>
	<li>Embed sql code directly into powershell scripts</li>
</ul>
&nbsp;
<h2>Start an interactive sqlplus session from the powershell command line</h2>
To do this, you simply open up a powershell terminal and then do:

<pre>
sqlplus username/password@TnsAlias
</pre>

This puts you into an interactive sqlplus session where you can start running sql statements. You can exit the session by typing "exit".

&nbsp;

This is done in the same way in CMD as well.

You can also run sql scripts as well, but for this to work you first need to navigate to the directory that contains the sql scripts. Once your powershell session is in that directory, you then enter the sqlplus session. After that you can run the script like this:

<pre>
SQL> @DBscriptName.sql;
</pre>
This approach is great if you want to run a db script/query manually but if you want to automate this then you need to look at the next two approaches:

<h2>Run a SQL script from inside a powershell script</h2>
<pre>
sqlplus username/password@TnsAlias 'c:\path\to\DBscript.sql' | out-file 'c:\temp\sql-output.txt'
</pre>

Here we chose to capture the sqlplus's output into a file, sql-output.txt. However although we could have also captured it into a powershell variable as well, like this:

<pre>
$output = sqlplus username/password@TnsAlias 'c:\path\to\DBscript.sql'
</pre>

This output might not quite look like the way you want it. Luckily there are a bunch of <a href="http://docs.oracle.com/cd/B19306_01/server.102/b14357/ch6.htm">sqlplus formatting</a> statements that you can insert at the top of your DBscript.sql to fix this.

If you want the output to be read/manipulated by Powershell (either manually or via a powershell script), then you may want to remove as much sqlplus formatting as possible, in which case here are a couple of useful formatting options that could be worth applying:

"set heading off" - this <a href="http://docs.oracle.com/cd/B19306_01/server.102/b14357/ch12040.htm#i2699001">switches-off column titles</a>.
"set newpage none" - This <a href="http://docs.oracle.com/cd/B19306_01/server.102/b14357/ch12040.htm#i2678637">removes any whitespacing</a> at the begining of the output.
"SET FEEDBACK OFF" - this <a href="http://docs.oracle.com/cd/B19306_01/server.102/b14357/ch12040.htm#i2698970">suppresses displaying the a count of the number of rows selected</a>, i.e. it wont show "xxx rows selected"
"SET PAGESIZE o" - I have used this so to further neutralize sqlplus formatting.

&nbsp;

Also note, the captured content will also contain the usual "you have successfully connected to Oracle db" message, which gets outputted as soon as a new sqlplus session is created. If you want to suppress this message, then simply use the "-silent" command. e.g.

<pre>
sqlplus username/password@TnsAlias '@c:\path\to\DBscript.sql' -silent | out-file 'c:\temp\sql-output.txt' </pre>

<h3>Running a sql script from the powershell command line</h3>
Let's say we have a simple sql script called, c:\temp\simplequery.sql, and it's content is:

<pre>
set NewPage none
set heading off
SELECT username FROM dba_users;
exit
</pre>

This is a simple sql script that should output a list of database user accounts (i.e. schemas in an oracle db). After that, you can then simply call this sql script like this:

<pre>
PS C:\> $username = "codingbee"
PS C:\> $password = "liverpool"
PS C:\> $tnsalias = "xyz"
PS C:\> $script = "c:\temp\simplequery.sql"
PS C:\> $outputfile = "C:\temp\activeusers.txt"
PS C:\> sqlplus -silent $username/$password@$tnsalias "$script" | Out-File $outputfile
</pre>

As you can see, it is quite straightforward to write the above commands into a powershell script too, which will require a sql script as an input for the powershell script to run.  

<h3>Run sql queries from inside a powershell script</h3>

However, a really cool thing you can also do, is embed sql queries directly into a powershell script. This can be useful, if you want to run a really simple sql query, in which case having it in it's own sql file would be overkill. 

You do this by piping the sql query in, with the help of the "here-strings". A here-string is basically a technique for creating a (multiline) string variable. Here is the syntax you use to create it:


<pre>PS C:\> $MultilineVar = @"
>> line number one
>> line number two
>> "@
>>
PS C:\> $MultilineVar
line number one
line number two
PS C:\>
</pre>


In our case, all we do is somewhere inside our powershell script, we use the here-string to create a multi-line string variable containing the sql code, Then pipe this multiline variable into the sqlplus command, and finally capture the output, either by piping it to "out-file" or in a variable, e.g.:

<pre>	$sqlQuery = @"
		set NewPage none
		set heading off
		set feedback off
		SELECT username FROM dba_users;
		exit
	"@

	$username = "CodingBee"
	$password = "Liverpool"
	$tnsalias = "simple.world"
	$outputfile = "C:\Temp\activeusers.txt"      # this is only required for option 1

        # You can now do one of 2 things.

        # Option 1 - piping the sql output into a file. 
	$sqlQuery | sqlplus -silent $username/$password@$tnsalias | Out-File $outputfile

        # Option 2 - capture the sql output into a variable. 
	$sqlOutput = $sqlQuery | sqlplus -silent $username/$password@$tnsalias 


</pre>
You can put the above in a ps1 file and it will run fine.

<strong>
See also</strong>


<a href="http://docs.oracle.com/cd/E11882_01/server.112/e16604/toc.htm">
sqlplus user's guide</a>



[post-content id=5393]]]></Content>
		<Date><![CDATA[2014-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell -  exiting out of outerloop]]></Title>
		<Content><![CDATA[Special Chapter - exiting out of outerloop

when exiting out of an outer loop, you are effecting exiting out from all the inner loops. 

You do this by labeling your loops like this:

$list = 1,2,3,4,5

# here we use the ":{loops label}" syntax to give a loop a name, then refer to it when we do continue/break
:outerloop foreach ($i in $list) {
  for ($j = 0; $j -lt 5; $j++) {
    if ($j -eq 2) {
      continue outerloop  	# notice here that we are specifying which loop to apply continue to.  
    }
    Write-Host "outer loop counter = $i, inner loop counter = $j"
  }
}

For more info, see:

http://www.out-web.net/?p=785

]]></Content>
		<Date><![CDATA[2014-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell -  find and replace text in a file]]></Title>
		<Content><![CDATA[Special Chapter - find and replace text in a file

Here is an example:


$filescontent = Get-Content c:\path\to\file

$filescontent -replace "foo", "bar" | Set-Content c:\path\to\file

If you don't want to overwrite, then you can do:

copy-item -path "c:\path\to\file" -destination "c:\path\to\orig-file"

$filescontent -replace "foo", "bar" | Set-Content c:\path\to\file

]]></Content>
		<Date><![CDATA[2014-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell - Web services, WSDL, and SOAP]]></Title>
		<Content><![CDATA[A WSDL (Web Services Description Language) is a url web page that describes a web service. WSDL documents the web service in the form of an xml, here is an example of a WSDL:

 http://www.webservicex.net/globalweather.asmx?wsdl
 
The wsdl contains 4 main element tags:

<ul>
	<li><strong>portType</strong> - this element can be thought of as housing a collection of operations elements (aka functions). Hence each portype element in a wsdl 
is a logical grouping for a collection of functions. the portype's name is given as an attribute.</li>
	<li><strong>operation</strong> - these elements are child elements of the porttype element. Their "name" attribute is equivalent to a function's name. Each operation can 
contain upto 2 child elements "input" and "output". 
<ul>
	<li><strong>input</strong> - this is basically a function's input parameter</li>
	<li><strong>output</strong> - this is basically parameter.</li>
</ul>
The input and output elements have an attribute called "message" the value of this attribute is actually a pointer to another element called: 
</li>
	<li><strong>message</strong> - this has a "name" attribute that matches the value of the output/input element's "message" attribute's value. 

The "message" element has a child element called "part"</li>
	<li><strong>part</strong> - this explains the data type that the parameter (or output) can be of and also attaches a name (using attributes again) to the parameter/output. 

the word "part" is used because each functions "whole message" is a combination of both the input and output.</li>
</ul>



So far, we know that a porttype (aka function library) house a collection of operations (aka functions). Each functions input(aka parameter)/outputs is defined in the message-element, this include the parameter's name, and the output variable's name, since I think the output is given in the form of a variable that stores the output. 

However we haven't look at how you can access a function (e.g. do you use http, soap, etc). This information is stored in an element called <strong>binding</strong>. A binding element should be available for each port type, since all the operations (functions) in a porttype uses the same network-mechanisn. 

Each binding element has 2 attributes:

<ol>
	<li><strong>name</strong> - this helps to refer to a bind in particular</li>
	<li><strong>type</strong> - this is the name of the porttype that this binding is associated to.</li>
</ol>


Eaching binding element contains the following child elements:

<ul>
	<li><strong>soap:binding</strong> - this is actually a one line element (e.g. like html's <br />). (Note, this can also be "http:binding", if http is used instead of SOAP.) It has the following attributes:</li>
<ul>
	<li><strong>style</strong> - This defines the soap protocol to use. It is either equal to "rpc" or "document". In nearly all cases it is "document", which means that we use the http protocol to deliver the soap "envelope". I think this is optional</li>
	<li><strong>transport</strong> - this is pointer to the document that defines the protocol to be used, this should be equal to "http://schemas.xmlsoap.org/soap/http" . Note, in the case of "http:binding", we have the "verb" attribute instead of "transport", and this is equal to either "get" or "post".  </li>
</ul>


	<li><strong>wsdl:operation</strong>: There are several of these. Each one is a reference to each function that are in the given porttype. Here "http://www.w3schools.com/webservices/ws_wsdl_binding.asp" it shows that an operation doesn't have a name, this is a typo because it should have a name that correspond to the given porttype's operation's name attribute's value. Each operation here, has these elements:</li>
	<li>Each "wsdl:operation" (aside from the name attribute, which matches the operation attribute's name) have the following child elements:
</li>
<ul>
	<li><strong>Soap:operation</strong> - This has 2 attributes:</li>
<ul>
	<li><strong>soapaction</strong> - This I think is the url location to where to send the SOAP message to. 
</li>

</ul>

	<li><strong>wsdl:input</strong> - This I think is the encoding to use to send the entire SOAP message</li>
	<li><strong>wsdl:output</strong> - this I think is the encoding to use to read the SOAP message received from the web service. </li>
</ul>



</ul>

Hence without the binding element, it won't be able to know where to send/recieve soap-messages. 

<h2>Here's an Example</h2>

If you analyse the following wsdl:

http://www.webservicex.net/globalweather.asmx?wsdl

You will find that it contains 3 porttype elements, and each porttype contains 2 operations. So across all 3 porttypes there are 6 operations (functions). If you look closely you will discover that each porttype contains the functions called:

<ul>
	<li>1st porttype (GlobalWeatherSoap) has the following functions
<ul>
	<li>GetWeather</li>
	<li>GetCitiesByCountry</li>
</ul>


</li>
	<li>2nd porttype (GlobalWeatherHttpGet) has the following functions


<ul>
	<li>GetWeather</li>
	<li>GetCitiesByCountry</li>
</ul>


</li>
	<li>3nd porttype (GlobalWeatherHttpPost) has the following functions

<ul>
	<li>GetWeather</li>
	<li>GetCitiesByCountry</li>
</ul>

</li>

</ul>




	
So why do we have 3 porttypbes with 3 identical pair of functions? That's because this wsdl can accept 3 types of communications, SOAP, HTTPGet, and httpPost. In most case you will just use the SOAP porttype, but web designers could also use the other 2.  
	
Since each function has an input and output, it means that each function is accompanied by 2 message elements (one for each part), and since we have 6 functions, it means that we have a total of 12 (6*2) message elements to cover off all the functions. 

Each porttype's communication details (e.g. for putty connection, the communication details are, hostname, protocol, e.g. ssh, port number,..etc) are covered in the "binding" elements.

Since there are 3 porttypes, there should be 3 binding elements. However there is 4 (2 of which releates to SOAP). I think this could be for load balancing purposes, but not sure.   

Now in powershell, if we do the following:

<pre>
PS C:\> $url = "http://www.webservicex.net/globalweather.asmx?wsdl"
PS C:\> $webservicex = New-WebServiceProxy -Uri $url -namespace WebServiceProxy -Class GlobalWeatherSoap
</pre>

Note: I think the -class declaration is optional because I think the New-WebServiceProxy will default to the SOAP protocol anyway. 

This results in the "$webservicex" object. You can think of this object as our personal messenger that will send and receive data from the webservicex.net's globalweather service. If you do:

<pre>
PS C:\> $webservicex | gm


   TypeName: WebServiceProxy.GlobalWeather

Name                                 MemberType Definition
----                                 ---------- ----------
Disposed                             Event      System.EventHandler Disposed(System.Object, System.EventArgs)
GetCitiesByCountryCompleted          Event      WebServiceProxy.GetCitiesByCountryCompletedEventHandler GetCitiesByC...
GetWeatherCompleted                  Event      WebServiceProxy.GetWeatherCompletedEventHandler GetWeatherCompleted(...
Abort                                Method     void Abort()
BeginGetCitiesByCountry              Method     System.IAsyncResult BeginGetCitiesByCountry(string CountryName, Syst...
BeginGetWeather                      Method     System.IAsyncResult BeginGetWeather(string CityName, string CountryN...
CancelAsync                          Method     void CancelAsync(System.Object userState)
CreateObjRef                         Method     System.Runtime.Remoting.ObjRef CreateObjRef(type requestedType)
Discover                             Method     void Discover()
Dispose                              Method     void Dispose(), void IDisposable.Dispose()
EndGetCitiesByCountry                Method     string EndGetCitiesByCountry(System.IAsyncResult asyncResult)
EndGetWeather                        Method     string EndGetWeather(System.IAsyncResult asyncResult)
Equals                               Method     bool Equals(System.Object obj)
<strong>GetCitiesByCountry                   Method     string GetCitiesByCountry(string CountryName)</strong>
<strong>GetCitiesByCountryAsync              Method     void GetCitiesByCountryAsync(string CountryName), void GetCitiesByCo...</strong>
GetHashCode                          Method     int GetHashCode()
GetLifetimeService                   Method     System.Object GetLifetimeService()
GetType                              Method     type GetType()
GetWeather                           Method     string GetWeather(string CityName, string CountryName)
GetWeatherAsync                      Method     void GetWeatherAsync(string CityName, string CountryName), void GetW...
InitializeLifetimeService            Method     System.Object InitializeLifetimeService()
ToString                             Method     string ToString()
AllowAutoRedirect                    Property   bool AllowAutoRedirect {get;set;}
ClientCertificates                   Property   System.Security.Cryptography.X509Certificates.X509CertificateCollect...
ConnectionGroupName                  Property   string ConnectionGroupName {get;set;}
Container                            Property   System.ComponentModel.IContainer Container {get;}
CookieContainer                      Property   System.Net.CookieContainer CookieContainer {get;set;}
Credentials                          Property   System.Net.ICredentials Credentials {get;set;}
EnableDecompression                  Property   bool EnableDecompression {get;set;}
PreAuthenticate                      Property   bool PreAuthenticate {get;set;}
Proxy                                Property   System.Net.IWebProxy Proxy {get;set;}
RequestEncoding                      Property   System.Text.Encoding RequestEncoding {get;set;}
Site                                 Property   System.ComponentModel.ISite Site {get;set;}
SoapVersion                          Property   System.Web.Services.Protocols.SoapProtocolVersion SoapVersion {get;s...
Timeout                              Property   int Timeout {get;set;}
UnsafeAuthenticatedConnectionSharing Property   bool UnsafeAuthenticatedConnectionSharing {get;set;}
Url                                  Property   string Url {get;set;}
UseDefaultCredentials                Property   bool UseDefaultCredentials {get;set;}
UserAgent                            Property   string UserAgent {get;set;}
</pre>

You will find that that the "GetWeather" and "GetCitiesByCountry" appear as object methods. Note, anything that have "string" method can be called on.

This means that you can now do this:

<pre>
PS C:\> [XML]$AustralianCities = $webservicex.GetCitiesByCountry("Australia")
</pre>
 
Here we are basically instructing the "$webservicex" object (messenger) to go to the webservicex.net and request for a list of all the cities in Australia, which we have requested by using the GetCitiesByCountry, and passing the parameter "Australia" into it. The web service recieves the request and gives a response, this response is an output that is in SOAP (xml) format. Since the output is in xml, we can't output it directly into the powershell terminal, instead we have to capture it in the a preformatted variable object.

We can then pull the info out of this variable instead, just like we do for any other object. E.g.:

<pre>
PS C:\> $australiancities 

NewDataSet                                                                                                                     
----------                                                                                                                     
NewDataSet                                                                                                                     

PS C:\> $australiancities.NewDataSet

Table                                                                                                                          
-----                                                                                                                          
{Table, Table, Table, Table...}                                                                                                

PS C:\> $australiancities.NewDataSet.Table

Country   City
-------   ----
Australia Archerfield Aerodrome
Australia Amberley Aerodrome
Australia Alice Springs Aerodrome
Australia Brisbane Airport M. O
Australia Coolangatta Airport Aws
Australia Cairns Airport
Australia Charleville Airport
Australia Gladstone
Australia Longreach Airport
Australia Mount Isa Amo
Australia Mackay Mo
Australia Oakey Aerodrome
Australia Proserpine Airport
Australia Rockhampton Airport
Australia Broome Airport
Australia Townsville Amo
Australia Weipa City
Australia Gove Airport
Australia Tennant Creek Airport
Australia Yulara Aws
Australia Albury Airport
Australia Devonport East
Australia Goldstream Aws
Australia East Sale Aerodrome
Australia Hobart Airport
Australia Launceston Airport
Australia Laverton Aerodrome
Australia Moorabbin Airport Aws
Australia Mount Gambier Aerodrome
Australia Mildura Airport
Australia Melbourne Airport
Australia Macquarie Island
Australia Wynyard West
Australia Adelaide Airport
Australia Albany Airport
Australia Broken Hill Patton Street
Australia Ceduna Airport
Australia Derby
Australia Darwin Airport
Australia Bullsbrook Pearce Amo
Australia Edinburgh M. O.
Australia Forrest Airport
Australia Geraldton Airport
Australia Kalgoorlie Boulder Amo
Australia Kununurra Kununurra Aws
Australia Leigh Creek Airport
Australia Learmonth Airport
Australia Meekatharra Airport
Australia Port Hedland Pardoo
Australia Parafield Airport
Australia Belmont Perth Airport
Australia Katherine Aerodrome
Australia Woomera Aerodrome
Australia Bankstown Airport Aws
Australia Canberra
Australia Coffs Harbour Mo
Australia Cooma
Australia Camden Airport
Australia Dubbo
Australia Norfolk Island Airport
Australia Nowra Ran Air Station
Australia Richmond Aus-Afb
Australia Sydney Airport
Australia Tamworth Airport
Australia Wagga Airport
Australia Williamtown Aerodrome
</pre>



When we send a request to a web service, we can send it in many forms, e.g. using html's "post" or "get". However the most powerful and popular approach is to send the request in the form of xml? The structure of this xml (i.e. schema), is that of a schema called "SOAP"

You can find the structure of SOAP here:

http://www.w3schools.com/webservices/ws_soap_syntax.asp

Here is a diagram of the structure:

http://en.wikipedia.org/wiki/SOAP

In the case of the above powershell example, the object we created called $webservicex will automatically generate the SOAP request xml behind the scenes and send it to the webservicex.net web service. e.g. when we run:

<pre>
PS C:\> [xml][/xml]$AustralianCities = $webservicex.GetCitiesByCountry("Australia")
</pre>

then the GetCitiesByCountry method for the $webservicex will generate the following xml (which I got from soapui):

[code language="xml"]
&lt;soapenv:Envelope xmlns:soapenv=&quot;http://schemas.xmlsoap.org/soap/envelope/&quot; xmlns:web=&quot;http://www.webserviceX.NET&quot;&gt;
   &lt;soapenv:Header/&gt;
   &lt;soapenv:Body&gt;
      &lt;web:GetCitiesByCountry&gt;
         &lt;!--Optional:--&gt;
         &lt;web:CountryName&gt;Australia&lt;/web:CountryName&gt;
      &lt;/web:GetCitiesByCountry&gt;
   &lt;/soapenv:Body&gt;
&lt;/soapenv:Envelope&gt;
[/code]

And the method then sends this xml (SOAP) request to the webservice. After a few moments, the web service returns the following soap (xml) response:

[code language="xml"]
&lt;soap:Envelope xmlns:soap=&quot;http://schemas.xmlsoap.org/soap/envelope/&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:xsd=&quot;http://www.w3.org/2001/XMLSchema&quot;&gt;
   &lt;soap:Body&gt;
      &lt;GetCitiesByCountryResponse xmlns=&quot;http://www.webserviceX.NET&quot;&gt;
         &lt;GetCitiesByCountryResult&gt;&lt;![CDATA[&lt;NewDataSet&gt;
  &lt;Table&gt;
    &lt;Country&gt;Australia&lt;/Country&gt;
    &lt;City&gt;Archerfield Aerodrome&lt;/City&gt;
  &lt;/Table&gt;
  &lt;Table&gt;
    &lt;Country&gt;Australia&lt;/Country&gt;
    &lt;City&gt;Amberley Aerodrome&lt;/City&gt;
  &lt;/Table&gt;
  &lt;Table&gt;
    &lt;Country&gt;Australia&lt;/Country&gt;
    &lt;City&gt;Alice Springs Aerodrome&lt;/City&gt;
  &lt;/Table&gt;
  &lt;Table&gt;
    &lt;Country&gt;Australia&lt;/Country&gt;
    &lt;City&gt;Brisbane Airport M. O&lt;/City&gt;
  &lt;/Table&gt;
  .
  .
  .
  .
  &lt;Table&gt;
    &lt;Country&gt;Australia&lt;/Country&gt;
    &lt;City&gt;Tamworth Airport&lt;/City&gt;
  &lt;/Table&gt;
  &lt;Table&gt;
    &lt;Country&gt;Australia&lt;/Country&gt;
    &lt;City&gt;Wagga Airport&lt;/City&gt;
  &lt;/Table&gt;
  &lt;Table&gt;
    &lt;Country&gt;Australia&lt;/Country&gt;
    &lt;City&gt;Williamtown Aerodrome&lt;/City&gt;
  &lt;/Table&gt;
&lt;/NewDataSet&gt;]]&gt;&lt;/GetCitiesByCountryResult&gt;
      &lt;/GetCitiesByCountryResponse&gt;
   &lt;/soap:Body&gt;
&lt;/soap:Envelope&gt; 
[/code]

To summarise, A SOAP message is an ordinary XML document. The main elements of an a soap's xml request can be found here:

http://en.wikipedia.org/wiki/SOAP#SOAP_Building_Blocks

http://www.w3schools.com/webservices/default.asp]]></Content>
		<Date><![CDATA[2014-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell - Miscellaneous]]></Title>
		<Content><![CDATA[



This is how you run bat scripts in ps:
& '.\mybatscript.cmd'
But this will also work:
.\mybatscript.cmd

Also you must create your cmd (bat) file using notepad, (and not notepad++), as advised here:
http://smallbusiness.chron.com/write-cmd-script-53226.html



http://searchwindowsserver.techtarget.com/tip/Top-25-Windows-PowerShell-commands-for-administrators
 
 
To open up the explorer window in the current directory, simply do:

explorer . 
 
 
 
 
The start-process command is used to start up gui based apps, e.g. :
 
Start-Process firefox
Start-Process notepad 
 
 
on windows xp, do start->run, then type "eventvwr"....good place for troubleshooting. 


Putty LINK - shortened to PLINK --- http://en.wikipedia.org/wiki/Plink




 
get-module -listavailable # this lists all the available modules
According to this link --- http://blog.richprescott.com/2012/02/powershell-training-back-to-basics.html --- it
says that if you want to access a given module, then you first have to install the actual applicaiton first, then 
refresh the module list by running get-module. Once they appear in the list, you then import them using the import-module command. 

Note: Some of the vendors are still providing their cmdlets in snap-ins, which is legacy for PowerShell v1.0. 


Automate web page interactions with powershell: 
https://www.google.co.uk/search?q=powershel+interact+with+web+pagesl&ie=utf-8&oe=utf-8&rls=org.mozilla:en-GB:official&client=firefox-a&gws_rd=cr
http://msdn.microsoft.com/en-us/magazine/cc337896.aspx
https://www.google.co.uk/search?q=powershel+interact+with+web+pagesl&ie=utf-8&oe=utf-8&rls=org.mozilla:en-GB:official&client=firefox-a&gws_rd=cr#client=firefox-a&rls=org.mozilla:en-GB%3Aofficial&sclient=psy-ab&q=powershell+read+a+web+page&oq=powershell+read+a+web+page&gs_l=serp.3..0i22i30l4.350311.354256.0.354465.15.13.0.2.2.0.165.1094.9j4.13.0....0...1c.1.21.psy-ab.L0xgP4I4eHI&pbx=1&bav=on.2,or.r_qf.&bvm=bv.49478099,d.d2k&fp=94de025e9fbfd016&biw=1280&bih=856 
http://www.google.co.uk/search?q=internet+explorer+powershell&sourceid=ie7&rls=com.microsoft:en-gb:IE-SearchBox&ie=&oe=&gws_rd=cr&ei=6nMLUqS-JY-V7AaTq4GICg
http://social.technet.microsoft.com/Forums/scriptcenter/en-US/9d003109-0903-414b-a635-ca2a9c485381/open-internet-explorer-8-and-its-tabs-using-powershell

For Bash, I think you could do similar things with curl and:
https://www.google.co.uk/search?q=bash+interact+with+web+pages&ie=utf-8&oe=utf-8&rls=org.mozilla:en-GB:official&client=firefox-a&gws_rd=cr#client=firefox-a&hs=U0X&rls=org.mozilla:en-GB%3Aofficial&sclient=psy-ab&q=use+bash+to+click+a+web+page+link+&oq=use+bash+to+click+a+web+page+link+&gs_l=serp.3...99346.111783.1.112059.41.32.6.3.5.2.108.2270.25j7.32.0....0...1c.1.21.psy-ab.Xe353crwGmI&pbx=1&bav=on.2,or.r_qf.&bvm=bv.49478099,d.d2k&fp=94de025e9fbfd016&biw=1280&bih=856
http://www.unix.com/shell-programming-scripting/125885-filling-out-web-form-script.html


]]></Content>
		<Date><![CDATA[2014-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[puppet - rough notes]]></Title>
		<Content><![CDATA[https://puppetlabs.com/blog/part-top-questions-on-puppet-and-windows

&nbsp;

http://puppetlabs.com/blog/part-2-top-questions-on-puppet-and-windows

&nbsp;

&nbsp;]]></Content>
		<Date><![CDATA[2014-05-22]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - What is Puppet]]></Title>
		<Content><![CDATA[Puppet is a solution that lets you control whats installed/configured on multiple machines from a single central machine. This single central machine is referred to as the "Puppet Master", and the machines it controls are called "Puppet Agents" (aka nodes or slaves).

To follow along with this tutorial series, you first need to set up your own puppet sandpit environment, this should comprise of 2 or more machines, one of which acts as the puppet master, and the rest act as puppet agents. <a title="Puppet – Install and set-up the Puppet Master and it’s Agents" href="http://codingbee.net/tutorials/puppet/puppet-crash-course-setting-masteragent-relationship/">Installing and setting up the Puppet Master and it's Agents</a> is covered in the next lesson.

In a puppetised environment, each agent checks-in with the puppet-master to see if it is set up the way it's supposed to. If the answer is yes then nothing happens, if not, then the puppet master tells the agent what it should look like, then the node makes changes to itself accordingly to reflect this.

The way this works is that on the puppet master you can define a "Desired State" for each of your nodes.  This desired state is declared in a syntax similar to a hash table's syntax.

If a node doesn't resemble a desired state, then we say that a "drift" has occurred.  A drift is identified every 30 minutes, where the following happens:
<ol>
	<li>The node sends it current setup info (referred to as "FACTS") to the Puppet master.</li>
	<li>The Puppet master uses the facts to compile a "catalog". This catalog contains detailed data about how the node should be configured.</li>
	<li>The Puppet master sends the catalog back to the node.</li>
	<li>The node enforces the changes as described in the the catalog</li>
	<li>The node then sents a "Report" back to the puppet master. You can view these reports and integrate them with other systems.</li>
</ol>
In the next lesson we'll walk through how to set up your own puppet environment which you can then experiment on to learn puppet.




Useful Links
<a href="http://docs.puppetlabs.com/learning/ral.html">
http://docs.puppetlabs.com/learning/ral.html</a> - really good intro to puppet

&nbsp;

<a href="https://docs.puppetlabs.com/puppet/latest/reference/index.html">https://docs.puppetlabs.com/puppet/latest/reference/index.html </a>- this is a the full puppet reference manual.

&nbsp;

http://www.planetpuppet.org/ - really good puppet blog aggregator. ]]></Content>
		<Date><![CDATA[2014-05-25]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Resources]]></Title>
		<Content><![CDATA[Resources are the fundamental unit for modelling your system configurations. They are essentially building blocks, like legos. Each resource describe a particular thing, e.g. a service that must be running, or a package that must be installed.

The block of Puppet code that defines a resource is called a <strong>resource declaration.</strong> This block of code is written in a language called the "Declarative Modelling Language" (aka DML). Here is an example of what a resource declaration looks like:

<pre>
user { 'gary':
ensure => present,
uid    => '101',
shell  => '/bin/bash',
home   => '/home/Gary',
}
</pre>
The above is an example of a "resource declaration" for a resource of the type "user". and it is made up of 4 parts:
<ol>
	<li><strong>Resource Type</strong> - which in this case is "<a href="https://docs.puppetlabs.com/references/latest/type.html#user">user</a>"</li>
	<li><strong>Resource Title</strong> - which in this case is "gary"</li>
	<li><strong>Attributes</strong> - these are the resource's properties, e.g. ensure, uid, gid,...etc</li>
	<li><strong>Values</strong> - this are the values that correspond to each porperties</li>
</ol>
Note: Each resource declaration is self contained and you can pick and choose which resources you want as a way to define what a target node should look like. The ordering of the resources doesn't matter, although you can add dependencies by using <a href="https://docs.puppetlabs.com/references/latest/metaparameter.html">metaparameters</a>.

<strong>Resource Types</strong>

There are various types of resources, and the above resource is an example of a "<a href="https://docs.puppetlabs.com/references/latest/type.html#user">user</a>" resource type. There are actually a lot of other types of resources. You can view a list of the available resource types by using puppet's "describe" sub-command along with it's "--list" option:
<pre>[root@puppetmaster ~]# puppet describe --list
These are the types known to puppet:
augeas          - Apply a change or an array of changes to the  ...
computer        - Computer object management using DirectorySer ...
cron            - Installs and manages cron jobs
exec            - Executes external commands
file            - Manages files, including their content, owner ...
filebucket      - A repository for storing and retrieving file  ...
group           - Manage groups
host            - Installs and manages host entries
interface       - This represents a router or switch interface
k5login         - Manage the `.k5login` file for a user
macauthorization - Manage the Mac OS X authorization database
mailalias       - .. no documentation ..
maillist        - Manage email lists
mcx             - MCX object management using DirectoryService  ...
mount           - Manages mounted filesystems, including puttin ...
nagios_command  - The Nagios type command
nagios_contact  - The Nagios type contact
nagios_contactgroup - The Nagios type contactgroup
nagios_host     - The Nagios type host
nagios_hostdependency - The Nagios type hostdependency
nagios_hostescalation - The Nagios type hostescalation
nagios_hostextinfo - The Nagios type hostextinfo
nagios_hostgroup - The Nagios type hostgroup
nagios_service  - The Nagios type service
nagios_servicedependency - The Nagios type servicedependency
nagios_serviceescalation - The Nagios type serviceescalation
nagios_serviceextinfo - The Nagios type serviceextinfo
nagios_servicegroup - The Nagios type servicegroup
nagios_timeperiod - The Nagios type timeperiod
notify          - .. no documentation ..
package         - Manage packages
resources       - This is a metatype that can manage other reso ...
router          - .. no documentation ..
schedule        - Define schedules for Puppet
scheduled_task  - Installs and manages Windows Scheduled Tasks
selboolean      - Manages SELinux booleans on systems with SELi ...
selmodule       - Manages loading and unloading of SELinux poli ...
service         - Manage running services
ssh_authorized_key - Manages SSH authorized keys
sshkey          - Installs and manages ssh host keys
stage           - A resource type for creating new run stages
tidy            - Remove unwanted files based on specific crite ...
user            - Manage users
vlan            - .. no documentation ..
whit            - Whits are internal artifacts of Puppet's curr ...
yumrepo         - The client-side description of a yum reposito ...
zfs             - Manage zfs
zone            - Manages Solaris zones
zpool           - Manage zpools
[root@puppetmaster ~]#
</pre>
Out of this list, the 8 types that are used most often are:

The 8 core resource types that are used most often are:
<ul>
	<li>notify</li>
	<li>file</li>
	<li>package</li>
	<li>cron</li>
	<li>user</li>
	<li>service</li>
	<li>exec</li>
	<li>group</li>
</ul>
<strong>Resource Title</strong>
In the above example, we have named the resource "gary". This is the unique title for this user resource type. You can't have another user-resource of the same name because that would cause a conflict.

You can use the "resource" command to view a list of all the resources that are of the type "user":

[bash]
[root@puppetmaster ~]# puppet resource user
user { 'abrt':
  ensure           =&gt; 'present',
  gid              =&gt; '173',
  home             =&gt; '/etc/abrt',
  password         =&gt; '!!',
  password_max_age =&gt; '-1',
  password_min_age =&gt; '-1',
  shell            =&gt; '/sbin/nologin',
  uid              =&gt; '173',
}
user { 'adm':
  ensure           =&gt; 'present',
  comment          =&gt; 'adm',
  gid              =&gt; '4',
  groups           =&gt; ['sys', 'adm'],
  home             =&gt; '/var/adm',
  password         =&gt; '*',
  password_max_age =&gt; '99999',
  password_min_age =&gt; '0',
  shell            =&gt; '/sbin/nologin',
  uid              =&gt; '3',
}
user { 'apache':
  ensure           =&gt; 'present',
  comment          =&gt; 'Apache',
  gid              =&gt; '48',
  home             =&gt; '/var/www',
  password         =&gt; '!!',
  password_max_age =&gt; '-1',
  password_min_age =&gt; '-1',
  shell            =&gt; '/sbin/nologin',
  uid              =&gt; '48',
}
.
.
.
.
...etc.
 [/bash]

If you want to list the resource of a particular user (e.g. a user resource with the title "apach"), then do:

[bash]
[root@puppetmaster ~]# puppet resource user apache
user { 'apache':
  ensure           =&gt; 'present',
  comment          =&gt; 'Apache',
  gid              =&gt; '48',
  home             =&gt; '/var/www',
  password         =&gt; '!!',
  password_max_age =&gt; '-1',
  password_min_age =&gt; '-1',
  shell            =&gt; '/sbin/nologin',
  uid              =&gt; '48',
}
 [/bash]

<strong>Attributes and Values</strong>
The main body of the resource is made up of attribute-value pairs. Here you can specify values for a given resource's properties.

Each resource type has it's own set of attributes that you can configure. You can use the "describe" subcommand to to find out more about a particular resource-type along with a list of all it's available attributes. For example if you want to view more info about the user-resource-type along with all it's configurable attributes, then do:
<pre>[root@puppetmaster ~]# puppet describe user

user
====
Manage users.  This type is mostly built to manage system
users, so it is lacking some features useful for managing normal
users.
This resource type uses the prescribed native tools for creating
groups and generally uses POSIX APIs for retrieving information
about them.  It does not directly modify `/etc/passwd` or anything.
**Autorequires:** If Puppet is managing the user's primary group (as
provided in the `gid` attribute), the user resource will autorequire
that group. If Puppet is managing any role accounts corresponding to the
user's roles, the user resource will autorequire those role accounts.


Parameters
----------

- **allowdupe**
    Whether to allow duplicate UIDs. Defaults to `false`.
    Valid values are `true`, `false`, `yes`, `no`.

- **attribute_membership**
    Whether specified attribute value pairs should be treated as the
    **complete list** (`inclusive`) or the **minimum list** (`minimum`) of
    attribute/value pairs for the user. Defaults to `minimum`.
    Valid values are `inclusive`, `minimum`.

- **attributes**
    Specify AIX attributes for the user in an array of attribute = value
    pairs.
Requires features manages_aix_lam.

- **auth_membership**
    Whether specified auths should be considered the **complete list**
    (`inclusive`) or the **minimum list** (`minimum`) of auths the user
    has. Defaults to `minimum`.
Valid values are `inclusive`, `minimum`.

- **auths**
    The auths the user has.  Multiple auths should be
    specified as an array.
Requires features manages_solaris_rbac.

- **comment**
    A description of the user.  Generally the user's full name.

- **ensure**
    The basic state that the object should be in.
    Valid values are `present`, `absent`, `role`.

- **expiry**
    The expiry date for this user. Must be provided in
    a zero-padded YYYY-MM-DD format --- e.g. 2010-02-19.
    If you want to make sure the user account does never
    expire, you can pass the special value `absent`.
    Valid values are `absent`. Values can match `/^\d{4}-\d{2}-\d{2}$/`.
    Requires features manages_expiry.

- **forcelocal**
    Forces the mangement of local accounts when accounts are also
    being managed by some other NSS
    Valid values are `true`, `false`, `yes`, `no`.
    Requires features libuser.

- **gid**
    The user's primary group.  Can be specified numerically or by name.
    This attribute is not supported on Windows systems; use the `groups`
    attribute instead. (On Windows, designating a primary group is only
    meaningful for domain accounts, which Puppet does not currently manage.)

- **groups**
    The groups to which the user belongs.  The primary group should
    not be listed, and groups should be identified by name rather than by
    GID.  Multiple groups should be specified as an array.

- **home**
    The home directory of the user.  The directory must be created
    separately and is not currently checked for existence.

- **ia_load_module**
    The name of the I&amp;A module to use to manage this user.
    Requires features manages_aix_lam.

- **iterations**
    This is the number of iterations of a chained computation of the
    password hash (http://en.wikipedia.org/wiki/PBKDF2).  This parameter
    is used in OS X. This field is required for managing passwords on OS X
    &gt;= 10.8.
Requires features manages_password_salt.

- **key_membership**
    Whether specified key/value pairs should be considered the
    **complete list** (`inclusive`) or the **minimum list** (`minimum`) of
    the user's attributes. Defaults to `minimum`.
    Valid values are `inclusive`, `minimum`.

- **keys**
    Specify user attributes in an array of key = value pairs.
    Requires features manages_solaris_rbac.

- **managehome**
    Whether to manage the home directory when managing the user.
    This will create the home directory when `ensure =&gt; present`, and
    delete the home directory when `ensure =&gt; absent`. Defaults to `false`.
    Valid values are `true`, `false`, `yes`, `no`.

- **membership**
    Whether specified groups should be considered the **complete list**
    (`inclusive`) or the **minimum list** (`minimum`) of groups to which
    the user belongs. Defaults to `minimum`.
    Valid values are `inclusive`, `minimum`.

- **name**
    The user name. While naming limitations vary by operating system,
    it is advisable to restrict names to the lowest common denominator,
    which is a maximum of 8 characters beginning with a letter.
    Note that Puppet considers user names to be case-sensitive, regardless
    of the platform's own rules; be sure to always use the same case when
    referring to a given user.

- **password**
    The user's password, in whatever encrypted format the local
    system requires.
    * Most modern Unix-like systems use salted SHA1 password hashes. You can
    use
      Puppet's built-in `sha1` function to generate a hash from a password.
    * Mac OS X 10.5 and 10.6 also use salted SHA1 hashes.
    * Mac OS X 10.7 (Lion) uses salted SHA512 hashes. The Puppet Labs
    [stdlib][]
      module contains a `str2saltedsha512` function which can generate
    password
      hashes for Lion.
    * Mac OS X 10.8 and higher use salted SHA512 PBKDF2 hashes. When
      managing passwords on these systems the salt and iterations properties
      need to be specified as well as the password.
    * Windows passwords can only be managed in cleartext, as there is no
    Windows API
      for setting the password hash.
    [stdlib]: https://github.com/puppetlabs/puppetlabs-stdlib/
    Be sure to enclose any value that includes a dollar sign ($) in single
    quotes (') to avoid accidental variable interpolation.
    Requires features manages_passwords.

- **password_max_age**
    The maximum number of days a password may be used before it must be
    changed.
Requires features manages_password_age.

- **password_min_age**
    The minimum number of days a password must be used before it may be
    changed.
Requires features manages_password_age.

- **profile_membership**
    Whether specified roles should be treated as the **complete list**
    (`inclusive`) or the **minimum list** (`minimum`) of roles
    of which the user is a member. Defaults to `minimum`.
    Valid values are `inclusive`, `minimum`.

- **profiles**
    The profiles the user has.  Multiple profiles should be
    specified as an array.
Requires features manages_solaris_rbac.

- **project**
    The name of the project associated with a user.
    Requires features manages_solaris_rbac.

- **purge_ssh_keys**
    Purge ssh keys authorized for the user
    if they are not managed via ssh_authorized_keys. When true,
    looks for keys in .ssh/authorized_keys in the user's home
    directory. Possible values are true, false, or an array of
    paths to file to search for authorized keys. If a path starts
    with ~ or %h, this token is replaced with the user's home directory.
    Valid values are `true`, `false`.

- **role_membership**
    Whether specified roles should be considered the **complete list**
    (`inclusive`) or the **minimum list** (`minimum`) of roles the user
    has. Defaults to `minimum`.
Valid values are `inclusive`, `minimum`.

- **roles**
    The roles the user has.  Multiple roles should be
    specified as an array.
Requires features manages_solaris_rbac.

- **salt**
    This is the 32 byte salt used to generate the PBKDF2 password used in
    OS X. This field is required for managing passwords on OS X &gt;= 10.8.
    Requires features manages_password_salt.

- **shell**
    The user's login shell.  The shell must exist and be
    executable.
    This attribute cannot be managed on Windows systems.
    Requires features manages_shell.

- **system**
    Whether the user is a system user, according to the OS's criteria;
    on most platforms, a UID less than or equal to 500 indicates a system
    user. Defaults to `false`.
    Valid values are `true`, `false`, `yes`, `no`.

- **uid**
    The user ID; must be specified numerically. If no user ID is
    specified when creating a new user, then one will be chosen
    automatically. This will likely result in the same user having
    different UIDs on different systems, which is not recommended. This is
    especially noteworthy when managing the same user on both Darwin and
    other platforms, since Puppet does UID generation on Darwin, but
    the underlying tools do so on other platforms.
    On Windows, this property is read-only and will return the user's
    security identifier (SID).

Providers
---------
    aix, directoryservice, hpuxuseradd, ldap, pw, user_role_add, useradd,
    windows_adsi
[root@puppetmaster ~]#

</pre>
<strong>Resource Abstraction Layer (RAL)</strong>

The RAL is essentially the name for the following core concepts that fundamentally explains how puppet operates:
<ol>
	<li>Resource - All resource belong to a pre-defined resource type. It's a bit like object-oriented-programming concept where an object is an instance class. However in this case, it is a resource that's an instance of a resource type.</li>
	<li>Abstraction - resources are described independently from the target operating system, i.e. a resource gives enough info to tell you what needs to exist on a puppet agent, regardless of whether the puppet agent is a windows or linux machine. It's then left to puppet to work out how to implement the resource for the given operating, and oyu don't have to worry about how puppet does this behind the scenes</li>
	<li>Layer - It is possible to that you can define an entire machine's setup and configuration in terms of a collection of resources, and you can view/manage all these resource via Puppet's CLI interface Layer.</li>
</ol>
In order for the Abstraction part of the RAL model to work, Puppet separates out the resource from its implementation. The platform-specific-implementation exists in the form of "Providers", you have multiple providers to cover all windows/Linux platforms for each resource-type's attribute. You can use the "describe" subcommand along with it's "--providers" option to view a list of providers for each attribute for a given resource type. For example, to view all the providers for the user resource type, you do:
<pre><code>
</code>[root@puppetmaster ~]# puppet describe user --providers

user
====
Manage users.  This type is mostly built to manage system
users, so it is lacking some features useful for managing normal
users.
This resource type uses the prescribed native tools for creating
groups and generally uses POSIX APIs for retrieving information
about them.  It does not directly modify `/etc/passwd` or anything.
**Autorequires:** If Puppet is managing the user's primary group (as
provided in the `gid` attribute), the user resource will autorequire
that group. If Puppet is managing any role accounts corresponding to the
user's roles, the user resource will autorequire those role accounts.


Parameters
----------

- **allowdupe**
    Whether to allow duplicate UIDs. Defaults to `false`.
    Valid values are `true`, `false`, `yes`, `no`.

- **attribute_membership**
    Whether specified attribute value pairs should be treated as the
    **complete list** (`inclusive`) or the **minimum list** (`minimum`) of
    attribute/value pairs for the user. Defaults to `minimum`.
    Valid values are `inclusive`, `minimum`.

- **attributes**
    Specify AIX attributes for the user in an array of attribute = value
    pairs.
Requires features manages_aix_lam.

- **auth_membership**
    Whether specified auths should be considered the **complete list**
    (`inclusive`) or the **minimum list** (`minimum`) of auths the user
    has. Defaults to `minimum`.
Valid values are `inclusive`, `minimum`.

- **auths**
    The auths the user has.  Multiple auths should be
    specified as an array.
Requires features manages_solaris_rbac.

- **comment**
    A description of the user.  Generally the user's full name.

- **ensure**
    The basic state that the object should be in.
    Valid values are `present`, `absent`, `role`.

- **expiry**
    The expiry date for this user. Must be provided in
    a zero-padded YYYY-MM-DD format --- e.g. 2010-02-19.
    If you want to make sure the user account does never
    expire, you can pass the special value `absent`.
    Valid values are `absent`. Values can match `/^\d{4}-\d{2}-\d{2}$/`.
    Requires features manages_expiry.

- **forcelocal**
    Forces the mangement of local accounts when accounts are also
    being managed by some other NSS
    Valid values are `true`, `false`, `yes`, `no`.
    Requires features libuser.

- **gid**
    The user's primary group.  Can be specified numerically or by name.
    This attribute is not supported on Windows systems; use the `groups`
    attribute instead. (On Windows, designating a primary group is only
    meaningful for domain accounts, which Puppet does not currently manage.)

- **groups**
    The groups to which the user belongs.  The primary group should
    not be listed, and groups should be identified by name rather than by
    GID.  Multiple groups should be specified as an array.

- **home**
    The home directory of the user.  The directory must be created
    separately and is not currently checked for existence.

- **ia_load_module**
    The name of the I&amp;A module to use to manage this user.
    Requires features manages_aix_lam.

- **iterations**
    This is the number of iterations of a chained computation of the
    password hash (http://en.wikipedia.org/wiki/PBKDF2).  This parameter
    is used in OS X. This field is required for managing passwords on OS X
    &gt;= 10.8.
Requires features manages_password_salt.

- **key_membership**
    Whether specified key/value pairs should be considered the
    **complete list** (`inclusive`) or the **minimum list** (`minimum`) of
    the user's attributes. Defaults to `minimum`.
    Valid values are `inclusive`, `minimum`.

- **keys**
    Specify user attributes in an array of key = value pairs.
    Requires features manages_solaris_rbac.

- **managehome**
    Whether to manage the home directory when managing the user.
    This will create the home directory when `ensure =&gt; present`, and
    delete the home directory when `ensure =&gt; absent`. Defaults to `false`.
    Valid values are `true`, `false`, `yes`, `no`.

- **membership**
    Whether specified groups should be considered the **complete list**
    (`inclusive`) or the **minimum list** (`minimum`) of groups to which
    the user belongs. Defaults to `minimum`.
    Valid values are `inclusive`, `minimum`.

- **name**
    The user name. While naming limitations vary by operating system,
    it is advisable to restrict names to the lowest common denominator,
    which is a maximum of 8 characters beginning with a letter.
    Note that Puppet considers user names to be case-sensitive, regardless
    of the platform's own rules; be sure to always use the same case when
    referring to a given user.

- **password**
    The user's password, in whatever encrypted format the local
    system requires.
    * Most modern Unix-like systems use salted SHA1 password hashes. You can
    use
      Puppet's built-in `sha1` function to generate a hash from a password.
    * Mac OS X 10.5 and 10.6 also use salted SHA1 hashes.
    * Mac OS X 10.7 (Lion) uses salted SHA512 hashes. The Puppet Labs
    [stdlib][]
      module contains a `str2saltedsha512` function which can generate
    password
      hashes for Lion.
    * Mac OS X 10.8 and higher use salted SHA512 PBKDF2 hashes. When
      managing passwords on these systems the salt and iterations properties
      need to be specified as well as the password.
    * Windows passwords can only be managed in cleartext, as there is no
    Windows API
      for setting the password hash.
    [stdlib]: https://github.com/puppetlabs/puppetlabs-stdlib/
    Be sure to enclose any value that includes a dollar sign ($) in single
    quotes (') to avoid accidental variable interpolation.
    Requires features manages_passwords.

- **password_max_age**
    The maximum number of days a password may be used before it must be
    changed.
Requires features manages_password_age.

- **password_min_age**
    The minimum number of days a password must be used before it may be
    changed.
Requires features manages_password_age.

- **profile_membership**
    Whether specified roles should be treated as the **complete list**
    (`inclusive`) or the **minimum list** (`minimum`) of roles
    of which the user is a member. Defaults to `minimum`.
    Valid values are `inclusive`, `minimum`.

- **profiles**
    The profiles the user has.  Multiple profiles should be
    specified as an array.
Requires features manages_solaris_rbac.

- **project**
    The name of the project associated with a user.
    Requires features manages_solaris_rbac.

- **purge_ssh_keys**
    Purge ssh keys authorized for the user
    if they are not managed via ssh_authorized_keys. When true,
    looks for keys in .ssh/authorized_keys in the user's home
    directory. Possible values are true, false, or an array of
    paths to file to search for authorized keys. If a path starts
    with ~ or %h, this token is replaced with the user's home directory.
    Valid values are `true`, `false`.

- **role_membership**
    Whether specified roles should be considered the **complete list**
    (`inclusive`) or the **minimum list** (`minimum`) of roles the user
    has. Defaults to `minimum`.
Valid values are `inclusive`, `minimum`.

- **roles**
    The roles the user has.  Multiple roles should be
    specified as an array.
Requires features manages_solaris_rbac.

- **salt**
    This is the 32 byte salt used to generate the PBKDF2 password used in
    OS X. This field is required for managing passwords on OS X &gt;= 10.8.
    Requires features manages_password_salt.

- **shell**
    The user's login shell.  The shell must exist and be
    executable.
    This attribute cannot be managed on Windows systems.
    Requires features manages_shell.

- **system**
    Whether the user is a system user, according to the OS's criteria;
    on most platforms, a UID less than or equal to 500 indicates a system
    user. Defaults to `false`.
    Valid values are `true`, `false`, `yes`, `no`.

- **uid**
    The user ID; must be specified numerically. If no user ID is
    specified when creating a new user, then one will be chosen
    automatically. This will likely result in the same user having
    different UIDs on different systems, which is not recommended. This is
    especially noteworthy when managing the same user on both Darwin and
    other platforms, since Puppet does UID generation on Darwin, but
    the underlying tools do so on other platforms.
    On Windows, this property is read-only and will return the user's
    security identifier (SID).

Providers
---------

- **aix**
    User management for AIX.
    * Required binaries: `/bin/chpasswd`, `/usr/bin/chuser`,
    `/usr/bin/mkuser`, `/usr/sbin/lsgroup`, `/usr/sbin/lsuser`,
    `/usr/sbin/rmuser`.
    * Default for `operatingsystem` == `aix`.
    * Supported features: `manages_aix_lam`, `manages_expiry`,
    `manages_homedir`, `manages_password_age`, `manages_passwords`,
    `manages_shell`.

- **directoryservice**
    User management on OS X.
    * Required binaries: `/usr/bin/dscacheutil`, `/usr/bin/dscl`,
    `/usr/bin/dsimport`, `/usr/bin/plutil`, `/usr/bin/uuidgen`.
    * Default for `operatingsystem` == `darwin`.
    * Supported features: `manages_password_salt`, `manages_passwords`,
    `manages_shell`.

- **hpuxuseradd**
    User management for HP-UX. This provider uses the undocumented `-F`
    switch to HP-UX's special `usermod` binary to work around the fact that
    its standard `usermod` cannot make changes while the user is logged in.
    * Required binaries: `/usr/sam/lbin/useradd.sam`,
    `/usr/sam/lbin/userdel.sam`, `/usr/sam/lbin/usermod.sam`.
    * Default for `operatingsystem` == `hp-ux`.
    * Supported features: `allows_duplicates`, `manages_homedir`,
    `manages_passwords`.

- **ldap**
    User management via LDAP.
    This provider requires that you have valid values for all of the
    LDAP-related settings in `puppet.conf`, including `ldapbase`.  You will
    almost definitely need settings for `ldapuser` and `ldappassword` in
    order
    for your clients to write to LDAP.
    Note that this provider will automatically generate a UID for you if
    you do not specify one, but it is a potentially expensive operation,
    as it iterates across all existing users to pick the appropriate next
    one.
* Supported features: `manages_passwords`, `manages_shell`.

- **pw**
    User management via `pw` on FreeBSD and DragonFly BSD.
    * Required binaries: `pw`.
    * Default for `operatingsystem` == `freebsd, dragonfly`.
    * Supported features: `allows_duplicates`, `manages_expiry`,
    `manages_homedir`, `manages_passwords`, `manages_shell`.

- **user_role_add**
    User and role management on Solaris, via `useradd` and `roleadd`.
    * Required binaries: `passwd`, `roleadd`, `roledel`, `rolemod`,
    `useradd`, `userdel`, `usermod`.
    * Default for `osfamily` == `solaris`.
    * Supported features: `allows_duplicates`, `manages_homedir`,
    `manages_password_age`, `manages_passwords`, `manages_solaris_rbac`.

- **useradd**
    User management via `useradd` and its ilk.  Note that you will need to
    install Ruby's shadow password library (often known as `ruby-libshadow`)
    if you wish to manage user passwords.
    * Required binaries: `chage`, `luseradd`, `useradd`, `userdel`,
    `usermod`.
    * Supported features: `allows_duplicates`, `libuser`, `manages_expiry`,
    `manages_homedir`, `manages_password_age`, `manages_passwords`,
    `manages_shell`, `system_users`.

- **windows_adsi**
    Local user management for Windows.
    * Default for `operatingsystem` == `windows`.
    * Supported features: `manages_homedir`, `manages_passwords`.
[root@puppetmaster ~]#
 
</pre>
You don't need to know much about providers at this stage as they are mainly about the internal workings of puppet.
<h2>Apply a resource to a machine</h2>
After writing a resource, then next thing we would want to do is apply the resource to a machine, so that it's state is changed accordingly.

For the purpose of this demo, we are going to apply the resource locally. We'll cover how to apply the resource to a remote puppet agent later.

&nbsp;

So going back to our previous resource example:

<pre>
user { 'gary':
ensure => present,
uid    => '101',
shell  => '/bin/bash',
home   => '/home/Gary',
}
</pre>

One way to do this, is from the CLI. You can do this by rewriting the resource into a single command, and then passing it into the "resource" subcommand, here's the syntax for this:
<pre>
puppet resource user gary ensure=present uid='101' shell='/bin/bash' home='/home/Gary'
</pre>

Before we do this, let's first let's check that the user "gary" doesn't already exist by grepping the /etc/passwd file:

<pre>
[root@puppetmaster ~]# cat /etc/passwd | grep "gary"
[root@puppetmaster ~]#                                
</pre>

Nothing has returned which means that the "gary" user doesn't exist yet. Now let's activate the resource:


<pre>
[root@puppetmaster ~]# puppet resource user gary ensure=present uid='101' shell='/bin/bash' home='/home/Gary'
Notice: /User[gary]/ensure: created
user { 'gary':
  ensure => 'present',
  home   => '/home/Gary',
  shell  => '/bin/bash',
  uid    => '101',
}
[root@puppetmaster ~]#
</pre>

As you can see, puppet outputs the results of running the command, and on this occasion it indicates that the "gary" user now exists. We can confirm this by checking the /etc/passwd file again:

&nbsp;

[bash][root@puppetmaster ~]# cat /etc/passwd | grep &quot;gary&quot;
gary:x:101:501::/home/Gary:/bin/bash
[root@puppetmaster ~]# [/bash]

We can also confirm via puppet using the "resource" sub-command as demonstrated earlier:

[bash] [root@puppetmaster ~]# puppet resource user gary
user { 'gary':
  ensure           =&gt; 'present',
  gid              =&gt; '501',
  home             =&gt; '/home/Gary',
  password         =&gt; '!!',
  password_max_age =&gt; '99999',
  password_min_age =&gt; '0',
  shell            =&gt; '/bin/bash',
  uid              =&gt; '101',
}
[root@puppetmaster ~]#

[/bash]

If you want to undo the changes made by this resource, then you can simply run the above subcommand again, but this time with the absent attribute set to "ensure=absent", i.e.:

<pre>
[root@puppetmaster ~]# <code class="bash comments">puppet resource user gary ensure=present uid='101' shell='/bin/bash' home='/home/Gary'</code>
Notice: /User[gary]/ensure: removed
user { 'gary':
  ensure => 'absent',
}
</pre>


In fact you can just do:

<pre>
[root@puppetmaster ~]# puppet resource user gary ensure=absent
Notice: /User[gary]/ensure: removed
user { 'gary':
  ensure => 'absent',
}
</pre>

Activating resource via the cli as just demonstrated, is actually rarely done in practice. But it can be useful on the odd occasions, e.g. troubleshooting. 

&nbsp;

The proper way to specify a resource is by writing the resource (in the form of a resource declaration) in a file and then instruct puppet to load that file. That's what we'll do in the next chapter.

&nbsp;

info:
See here for <a href="http://docs.puppetlabs.com/learning/ral.html">Resources</a>.]]></Content>
		<Date><![CDATA[2014-05-25]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Classes]]></Title>
		<Content><![CDATA[In Puppet we use "classes" to reduce code repetition. 

For example lets say we have:


<pre>
[root@puppetmaster manifests]# cat site.pp

node 'PuppetAgent1' {

  user { 'homer':
    ensure => present,
    uid    => '101',
    shell  => '/bin/bash',
    home   => '/home/homer',
  }
}

node 'PuppetAgent2' {

  user { 'homer':
    ensure => present,
    uid    => '101',
    shell  => '/bin/bash',
    home   => '/home/homer',
  }
}

</pre>

Here we have 2 agents where the the same user accounts needs to exist called "homer". As you can see we repeated the same resource twice. One way to avoid this is by combining the 2 nodes, like this:

<pre>
[root@puppetmaster manifests]# cat site.pp

node 'PuppetAgent1','PuppetAgent2' {

  user { 'homer':
    ensure => present,
    uid    => '101',
    shell  => '/bin/bash',
    home   => '/home/homer',
  }
}

</pre>

However merging nodes like this isn't the best option because it's likely that one agent needs a different combination of resources than another, while only sharing the odd few resources. 

Hence a better option is rewriting the resource into a class, using the class-syntax, and then adding the class to each node using the "include" keyword, i.e.

<pre>
class homer_simpson {

  user { 'homer':
    ensure => present,
    uid    => '101',
    shell  => '/bin/bash',
    home   => '/home/homer',
  }

}


node 'PuppetAgent1' {
  include homer_simpson
}

node 'PuppetAgent2' {
  include homer_simpson
}

</pre>

Note: the class's names must always be in lower-case only. Hence something like "Homer_Simpson" won't work. 


In the above example we used the "<a href="https://docs.puppetlabs.com/puppet/latest/reference/lang_classes.html#declaring-classes">include-like</a>" syntax to add the class to a node definition (aka declare a class). There is another way to declare a class, which is by using the <a href="https://docs.puppetlabs.com/puppet/latest/reference/lang_classes.html#resource-like-behavior">resource-like</a> syntax: 

<pre>
class homer_simpson {

  user { 'homer':
    ensure => present,
    uid    => '101',
    shell  => '/bin/bash',
    home   => '/home/homer',
  }

}

node 'PuppetAgent1' {
  class {homer_simpson:}
}

node 'PuppetAgent2' {
  class {homer_simpson:}
}

</pre>

Notice how the class syntax basically looks like you are defining a new resource of the type "class". At first sight both syntax appears to do the same thing, however as you progress through this course you'll discover that each syntax has it's own features. Hence the syntax you pic depends on the circumstance. For instance the resource-like syntax let's you pass class parameters to the class (which we'll cover in the next tutorial), whereas the include-like syntax can also do this but through a utility called Hiera. 


See also:
https://docs.puppetlabs.com/puppet/latest/reference/lang_classes.fin

]]></Content>
		<Date><![CDATA[2014-05-28]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell - Common Paramaters]]></Title>
		<Content><![CDATA[Every command in powershell, has a set of "common parameters" (i.e. options). This is indicated in all the help pages, in the syntax section...which alway ends with:

[]

This family of paramaters is made up of the following parameters:
- Debug (db)
- ErrorAction(ea)
- ErrorVariable(ev)
- OutVariable(ov)
- OutBuffer(ob)
- Verbose(vb)
- WarningAction(wa)
- WarningVariable(wv)

For more info, check out:

help about_CommonParameters # "help" is a function of get-help, and does the same thing.
]]></Content>
		<Date><![CDATA[2014-06-01]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell - Use CredSSP to run commands remotely with fewer issues]]></Title>
		<Content><![CDATA[The "invoke-command" cmdlet is specifically used to send commands to remote windows machines (aka servers), using your local workstation (aka client).

In order to do this, run the following command on both client and servers machines, if you haven't already done so:


<pre>enable-psremoting -force
</pre>
This commands tells the machine that it can now run an commands it receives from somewhere.

Once you have done this, you can then use the client machine to send commands to be run on other machines, using invoke-command:

<pre>
Invoke-Command -ComputerName RemoteServerName -ScriptBlock {
   get-childitem
   get-service
}
</pre>

When you run this, you should get a prompt to enter valid username and password, in order for the invoke-command to first authenticate against the server, before the server can accept the commands and run them. However this can be a problem if you want to run this script from inside an powershell-script, which has to be run non-interactively. The answer to this problem is to use the invoke-command cmdlet's "-credential" parameter. You need to feed in a "pscredential" object into the credential parameter in order for this to work. pscredentials are basically an object that stores your username, and an encrypted version of your password. You can create this object like this:

<pre>
# Set up credentials.
$password =  ConvertTo-SecureString -String $env:Password -AsPlainText -Force
$cred = New-Object System.Management.Automation.PSCredential $env:Username,$password
</pre>

After that, you can then use your credential like a door-key like this:

<pre>
Invoke-Command -ComputerName RemoteServerName -credential $cred -ScriptBlock {
   get-service
}
</pre>

This time, invoke-command will run non-iteractively, and won't prompt you for username/password. That means that you can now use invoke-command inside your powershell scripts, as long as you use the -credential option.

One thing you might want to do is have your pscredential always available to you in the form of a <a title="PowerShell – Environment Variables" href="http://codingbee.net/tutorials/powershell/powershell-environment-variables/">PowerShell Environment Variable</a> (look up the section about export-clixml).

However invoke-command by default runs commands with a low privilege level, so some commands may fail to run, unless you run the commands by logging into the server directly. Here is an example of a command that will fail:

<pre>
# Set up credentials.
$password =  ConvertTo-SecureString -String $env:Password -AsPlainText -Force
$cred = New-Object System.Management.Automation.PSCredential $env:Username,$password 

Invoke-Command -ComputerName RemoteServerName -Credential $cred -ScriptBlock {
   get-childitem \\path\to\another\remove\server\within\the\same\domain
}
</pre>

This will fail because you are hopping from machine1 to machine2, and then hopping from machine2 to machine3. This is something that isn't allowed with lowered privileges. The answer to this problem is to use "credssp". Using invoke-command along with "CredSSP" will really help avoid various privilege related issues:

<pre>
PS C:\WINDOWS\system32=> Get-WSManCredSSP

The machine is not configured to allow delegating fresh credentials.
This computer is configured to receive credentials from a remote client computer.
</pre>


<pre>
Enable-WSManCredSSP -Role Client -DelegateComputer {domain-name}
</pre>

However I noticed that didn't work, instead I had to use the fully qualified machine name, which is:

<pre>
{$env:COMPUTERNAME}.{$env:USERDNSDOMAIN}
</pre>

The Enable-WSManCredSSP cmdlet needs to be run on the central machine that will be controlling all the other machines. Hence you only need to run this command on the one machine. After running this command you might need to wait a few minutes before you do anything else. This command could also stop the winRM service running, so might need to enable it again by running the enable-psremoting command again. The output should be:

<pre>
Enable-WSManCredSSP -Role Client -DelegateComputer {domain-name}

cfg         : http://schemas.microsoft.com/wbem/wsman/1/config/client/auth
lang        : en-US
Basic       : Basic
Digest      : Digest
Kerberos    : true
Negotiate   : true
Certificate : true
CredSSP     : CredSSP
</pre>

Also you can now confirm this as follows:

<pre>
PS Env:\=> get-WSManCredSSP
The machine is configured to allow delegating fresh credentials to the following target(s): wsman/{domain-name}
This computer is configured to receive credentials from a remote client computer.
</pre>

&nbsp;

On all the other machines that you want to control, you need to access the windows desktops in turn, open up powershell from the start menue, and then run the following:

<pre>
PS C:\Windows\system32=> Enable-WSManCredSSP -Role Server

cfg               : http://schemas.microsoft.com/wbem/wsman/1/config/service/auth
lang              : en-US
Basic             : false
Kerberos          : true
Negotiate         : true
Certificate       : false
CredSSP           : true
CbtHardeningLevel : Relaxed

</pre>

After that, you should see:

<pre>
PS C:\Windows\system32=> Get-WSManCredSSP
The machine is not configured to allow delegating fresh credentials.
This computer is configured to receive credentials from a remote client computer.
</pre>

Now you can run the invoke-command inside a ps1 script with the credssp mode switched on (using the authentication parameter):

<pre>
# Set up credentials.
$password =  ConvertTo-SecureString -String $env:Password -AsPlainText -Force
$cred = New-Object System.Management.Automation.PSCredential $env:Username,$password 

Invoke-Command -ComputerName MachineName.domain.com -Authentication CredSSP  -Credential $cred -ScriptBlock {
   get-childitem \\path\to\remove\server\within\the\same\domain
}
</pre>

Note: When connecting to a remote machine using invoke-command and credssp enabled, you also need to specify the full machine name.

Another way to test, is by creating an interactive session like this:

<pre>
Enter-PSSession -ComputerName machine.domain.com -Credential $cred -Authentication Credssp
</pre>

Note, in the above examples we configured remote machines manually by via remote desktoping. However it is possible to do everything from the remote command line using the <a title="PowerShell – Using psexec to automate UI tasks on remote machines" href="http://codingbee.net/tutorials/powershell/powershell-using-psexec-automate-ui-tasks-remote-machines/">psexec command-line utility</a>.

After you have done everything above, you migth still have issues connecting to the target machine, and you might get a error message like this:

&nbsp;

<a href="http://codingbee.net/wp-content/uploads/2014/06/credsspError.png"><img class="alignnone size-full wp-image-1103" src="http://codingbee.net/wp-content/uploads/2014/06/credsspError.png" alt="credsspError" width="844" height="232" /></a>

This error message is quite informative, and can help to fix the issue, so if you get this message, you need to do the following on the client machine:

&nbsp;
<ol>
	<li>start | run "gpedit.msc"</li>
	<li>drill down to Computer-configuration | administrative templates | system | credential delegation</li>
	<li>double click on "Allow Delgating Fresh Credentials with NTLM-only server authentication"</li>
	<li>Enable this option</li>
	<li>click on the show button</li>
	<li>add in "WSMAN/*.domain.co.uk"</li>
</ol>
&nbsp;

&nbsp;

Useful links:
http://social.technet.microsoft.com/Forums/windowsserver/en-US/e2b59f03-b49f-4432-ba7b-86cd74e1d392/remote-execution-of-a-script-with-getchild-item-cannot-find-path?forum=ITCG

http://dustinhatch.tumblr.com/post/24589312635/enable-powershell-remoting-with-credssp-using-group

http://technet.microsoft.com/en-us/library/hh849871.aspx

http://technet.microsoft.com/en-us/library/hh849876.aspx

http://stackoverflow.com/questions/2248093/run-command-as-administrator-in-powershell-script-uac

http://stackoverflow.com/questions/2248093/run-command-as-administrator-in-powershell-script-uac

http://blogs.technet.com/b/heyscriptingguy/archive/2011/05/11/check-for-admin-credentials-in-a-powershell-script.aspx (this is useful)

http://blogs.msdn.com/b/virtual_pc_guy/archive/2010/09/23/a-self-elevating-powershell-script.aspx

http://www.ravichaganti.com/blog/?p=1230

http://stackoverflow.com/questions/2248093/run-command-as-administrator-in-powershell-script-uac

&nbsp;

http://www.thomas-franke.net/powershell-manage-passwords-credentials/    (this tells you how to store the pscredential object as an xml file)

http://searchwindowsserver.techtarget.com/answer/Why-does-a-PowerShell-command-run-fine-locally-but-not-remotely

&nbsp;

http://powershell.org/wp/ebooks/

&nbsp;

Also do some research on:

http://technet.microsoft.com/en-us/library/cc754243.aspx]]></Content>
		<Date><![CDATA[2014-06-04]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell - Uninstall a program]]></Title>
		<Content><![CDATA[Using the gui, you can uninstall a program by going to control-panel -&gt; add/remove program.

You can also do this via powershell:

http://stackoverflow.com/questions/113542/how-can-i-uninstall-an-application-using-powershell

http://blogs.technet.com/b/heyscriptingguy/archive/2011/12/14/use-powershell-to-find-and-uninstall-software.aspx

&nbsp;

&nbsp;

Note: the above is quite slow, here is a faster way:

&nbsp;

function Uninstall ($featureName)
{
$uninstall32 = gci "HKLM:\SOFTWARE\Wow6432Node\Microsoft\Windows\CurrentVersion\Uninstall" | foreach { gp $_.PSPath } | ? { $_ -match $featureName } | select UninstallString
$uninstall64 = gci "HKLM:\SOFTWARE\Microsoft\Windows\CurrentVersion\Uninstall" | foreach { gp $_.PSPath } | ? { $_ -match $featureName } | select UninstallString

if ($uninstall64) {
$uninstall64 = $uninstall64.UninstallString -Replace "msiexec.exe","" -Replace "/I","" -Replace "/X",""
$uninstall64 = $uninstall64.Trim()
Write "Uninstalling..."
start-process "msiexec.exe" -arg "/X $uninstall64 /qb" -Wait}
if ($uninstall32) {
$uninstall32 = $uninstall32.UninstallString -Replace "msiexec.exe","" -Replace "/I","" -Replace "/X",""
$uninstall32 = $uninstall32.Trim()
Write "Uninstalling..."
start-process "msiexec.exe" -arg "/X $uninstall32 /qb" -Wait}
}

&nbsp;

Uninstall -featureName "{name of msi}"]]></Content>
		<Date><![CDATA[2014-06-04]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[private]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell - Environment Variables]]></Title>
		<Content><![CDATA[to set an environment variable, you can do this

[powershell]

[environment]::SetEnvironmentVariable(&quot;TestVar&quot;, &quot;a simple string&quot;, &quot;Machine&quot;)

[/powershell]

Note, I think you can set the value at sessions/user/machine level. In the above case I have set them to the machine level.

Now to retrieve an environmenment from somewhare else, e.g. from another script, do this:

[powershell]

$TestVar = [System.Environment]::GetEnvironmentVariable(&quot;TestVar&quot;,&quot;Machine&quot;)
&quot;this variable contains: $TestVar&quot;

[/powershell]

You can only store strings as environment variables. That means that you can't store object-variables as environment variables.

However you can output an object-variable to an xml file, and then import it back to an object variable again using import-Clixml and export-Clixml. With this approach all you need to is keep track of where you stored the outputted xml file, so that you can import as and when need.

A common use for this is to store your "pscredential" object

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

useful links:

http://technet.microsoft.com/en-us/library/ff730964.aspx (ready the part about "GetEnvironmentVariable")

&nbsp;

http://www.thomas-franke.net/powershell-manage-passwords-credentials/]]></Content>
		<Date><![CDATA[2014-06-05]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Jenkins & PowerShell - Create Jenkins environment variables during a job run]]></Title>
		<Content><![CDATA[In <a href="http://www.amazon.co.uk/gp/product/1784390089/ref=as_li_qf_sp_asin_il_tl?ie=UTF8&camp=1634&creative=6738&creativeASIN=1784390089&linkCode=as2&tag=codi0f-21">Jenkins</a> there are a set of default <a href="https://wiki.jenkins-ci.org/display/JENKINS/Building+a+software+project#Buildingasoftwareproject-JenkinsSetEnvironmentVariables">jenkins environment variables</a> that you can call from within a Powershell Build step. You can add a powershell script into a job's build step with the used of the <a href="https://wiki.jenkins-ci.org/display/JENKINS/PowerShell+Plugin">Jenkins Powershell plugin</a>. The Powershell build step can then access a jenkins job's build parameter, under the $env scope, that is::


<pre>
 if ($env:JenkinsParameter -eq $null){
        "ERROR: Cannot find value for 'JenkinsParameter'"
        exit 1
    }
 $JenkinsParameter = $env:JenkinsParameter
</pre>


You can also access <a href="https://wiki.jenkins-ci.org/display/JENKINS/Building+a+software+project#Buildingasoftwareproject-JenkinsSetEnvironmentVariables">Jenkins environment variables</a> in the same way, E.g. if you want to access the "Job_name" envrinment variable, then you do:


<pre>$JOB_NAME= $env:JOB_NAME
write-host $JOB_NAME</pre>


Also in your powershell script you can create <a href="http://codingbee.net/tutorials/powershell/powershell-environment-variables/">environment variables</a> that can be accessible by any latter powershell build steps. Also if you set the environment at the "machine" scope than these variables will be available by powershell build steps in different jenkins jobs. But that's not good practice because you want your Jenkins jobs self contained.  

However "powershell environment variables" are not accessible by any other types of build step (e.g. a python build step). You can only pass them from one powershell build step to any other successive powershell build steps that are within the same Job. 

The only way round this issue to create a variable as "Jenkins environment variable" (aka a jenkins job parameter that gets set midway through the job's run). This is possible with the help of the <a href="https://wiki.jenkins-ci.org/display/JENKINS/EnvInject+Plugin">EnvInject plugin</a>.

It is actually really easy to do. The way it works is that your powershell build step actually outputs all the variables to a file using the key=value syntax, e.g. here we created a file falled "env.properties":

<pre>
PS C:\> "message=HelloWorld" | Out-File env.properties -Encoding ASCII
</pre>

Straight after the powershell build step we add a new "inject" build step:

&nbsp;

&nbsp;

<a href="http://codingbee.net/wp-content/uploads/2014/06/Jenkins-inject-plugin.png"><img class="alignnone size-full wp-image-1609" src="http://codingbee.net/wp-content/uploads/2014/06/Jenkins-inject-plugin.png" alt="Jenkins-inject-plugin" width="648" height="218" /></a>

&nbsp;

Then do:

&nbsp;

<a href="http://codingbee.net/wp-content/uploads/2014/06/Jenkins-inject-plugin2.png"><img class="alignnone size-full wp-image-1610" src="http://codingbee.net/wp-content/uploads/2014/06/Jenkins-inject-plugin2.png" alt="Jenkins-inject-plugin2" width="581" height="268" /></a>

&nbsp;

Finally you can append another powershell build step and call this new jenkins environment variable like this:

&nbsp;

<pre>
write-host "variables have been injected"
write-host "the message is $env:message"
</pre>

Note: This, this technique of using envinject plugin isn't limited to powershell, the equivalent should work with other languages e.g. with python and ruby. 

Note: you can also pass the variables stored in env.properties to other jobs by triggering them from the current job. You can do because these types of build steps natively supports a properties file, e.g. :

<a href="http://codingbee.net/wp-content/uploads/2014/06/buildstep-trigger-anotherjob.png"><img src="http://codingbee.net/wp-content/uploads/2014/06/buildstep-trigger-anotherjob.png" alt="buildstep-trigger-anotherjob" width="717" height="511" class="alignnone size-full wp-image-1613" /></a>

&nbsp;
here's an example of a post build step triggering another jenkins job:

<a href="http://codingbee.net/wp-content/uploads/2014/06/jenkins-postbuildstep.png"><img src="http://codingbee.net/wp-content/uploads/2014/06/jenkins-postbuildstep.png" alt="jenkins-postbuildstep" width="1029" height="628" class="alignnone size-full wp-image-1614" /></a>
&nbsp;


Hence this env.properties file has two possible uses, it creates new environment variables midway through current jobs (using the EnvInject) plugin, and it can be used as an input file for triggering other jenkins jobs. 


<strong>Warning</strong>: You might have problems getting your powershell script working becuase of "second-hop"/remoting related issues. The way I have managed to fix this is by not using the jenkins powershell plugin. Instead I ran the powershell script inside an ordinary "execute windows batch command build step":

<a href="http://codingbee.net/wp-content/uploads/2014/06/windows-batch-command-to-run-powershell-script-in-jenkins.png" rel="attachment wp-att-6400"><img src="http://codingbee.net/wp-content/uploads/2014/06/windows-batch-command-to-run-powershell-script-in-jenkins.png" alt="windows-batch-command-to-run-powershell-script-in-jenkins" width="947" height="269" class="alignnone size-full wp-image-6400" /></a>  

Note, to do the above, you first need to save your powershell code into a .ps1 file and add it to your code repo, in order to make it available via your jenkins workspace.  ]]></Content>
		<Date><![CDATA[2014-06-05]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Jenkins|PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Jenkins]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell - Remote Deskop Protocol (RDP)]]></Title>
		<Content><![CDATA[&nbsp;

&nbsp;

http://blogs.technet.com/b/ptsblog/archive/2011/12/09/extending-remote-desktop-services-using-powershell.aspx

windows 2008 has this :)

&nbsp;

http://technet.microsoft.com/en-us/library/jj215451.aspx

This only seems to be available on windows server 2012 :(]]></Content>
		<Date><![CDATA[2014-06-06]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell - Using psexec to automate UI tasks on remote machines]]></Title>
		<Content><![CDATA[When you use PowerShell native remoting commands, e.g. invoke-command, the remote server acts as a terminal server, and doesn't have an active UI session. This means that you can't do something like this:

[powershell]
powershell -computername {name} -scriptblock {notepad.exe}
[/powershell]

Officially there are no powershell cmdlets that you can use to remotely control the gui.

Instead you have to use another microsoft owned utility called "psexec". This is a standalone exe that you can download for free. However psexec comes as part of a bundle of other exe. This bundle is called <a href="http://technet.microsoft.com/en-gb/sysinternals/bb896649.aspx">pstools</a>.

So all you need to do is
<ol>
	<li>download pstools (which is a zip file),</li>
	<li>unblock it (right click on the zip file | properties | unblock button) if necessary,</li>
	<li>then just extract the psexec.exe</li>
	<li>Place the file in a memorable place, preferably in the one of the following directories: $env:path.split(";")</li>
</ol>
&nbsp;

Now from the cmd or powershell, you can do:

[cmd]
& psexec.exe \\{machine-name}  -u {domain}\{username} -p {username} -w c:\temp -d -h -i 2 notepad.exe
[/cmd]

Note: the ampersand is a way to tell powershell to pass the rest of the line to cmd. However I think you can leave out the ampersand and will still work in Powershell.  

One of the first things you may need to do is:

[powershell]
psexec.exe \\$servername -u os\$username -p $password -w c:\temp -d -h  powershell.exe &quot;Enable-PSRemoting -Force&quot;
[/powershell]

You can run the above from within a powershell terminal and don't even need to start the command with an ampersand. Also the variables are automatically first evaluated before psexec.exe takes over :)

You can find the remote session id here:

[powershell]
qwinsta /SERVER:{server-name}   
[/powershell]

This outputs a table in the form of a really long string, which is tricky to deal with, but you can retrieve the session id, using: 

[powershell]
$DirtyQwinstaArray = (qwinsta /SERVER:{machine-name}| Where-Object -FilterScript {$_ -match &quot;{username}&quot;}) -split (&quot; &quot;)
# &quot;DirtyQwinstaArray&quot; contains array items that are just whitespace, these needs to be cleaned up. 

$CleanQwinstaArray = @()
$DirtyQwinstaArray | ForEach-Object {

    if ($_ -eq $DirtyQwinstaArray[2]){
       # &quot;null found&quot;
    }
    else {
       $CleanQwinstaArray += $_
    }
}

$RDPsessionID = $CleanQwinstaArray[2]
[/powershell]


However if you don't have an active session, then I think you can set one via a powershell script using "cmdkey" and "mstsc":

http://stackoverflow.com/questions/11296819/run-mstsc-exe-with-specified-username-and-password

http://technet.microsoft.com/en-us/library/cc754243.aspx


usefule links:

http://stackoverflow.com/questions/17563505/enable-psremoting-remotely

http://forum.sysinternals.com/psexec-i-does-not-interact-with-remote-desktop_topic14473.html

http://www.reddit.com/r/PowerShell/comments/1khn9o/a_fun_script_for_friday_make_your_friends/

&nbsp;]]></Content>
		<Date><![CDATA[2014-06-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Python - Installing and Getting Started]]></Title>
		<Content><![CDATA[There are 2 versions of python v2.x.x and version v3.x.x.x

v3 is actually a complete rewrite of v2 and will eventually replace v2. v2 is therefore legacy and you should avoid using it if possible.

&nbsp;

&nbsp;

Most linux distro comes with python as standard.

Python works example the same way on all linux and windows platforms.

To install python on windows, you need to download the appropriate msi installer which you can find on the <a href="https://www.python.org/downloads/">official python website</a> website. Download the 6b-bit msi windows installer if you have a 64-bit windows OS.

during the install process, it will create a new python folder directly under the c:\ rather than in the program folders, this is so that python can circumvent some windows based limitations.

Make a note of where python will be installed, you need to check that this folder exists after the install completes, and within this folder you should find a "Scripts", for example you should have folders like:

&nbsp;

C:\Python34

C:\Python34\Scripts

One you have made a note of these two folders, you need to add them to your machine's environment variables (using the gui). You go to:

Control Panel | System | Advance systems settings | advanced (tab) | Environment Variable (button)

Here select the path variable, and append the above two folder paths with semicolon.

After that you should be able to open powershell and type "python" to enter the python terminal....which displays three arrows.

Useful links:

&nbsp;

The standard library - https://docs.python.org/3/library/

&nbsp;

&nbsp;]]></Content>
		<Date><![CDATA[2014-06-14]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[python]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Python]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Python - Hello world]]></Title>
		<Content><![CDATA[Everytime you run a python command, python does does the following steps in sequence:
<ol>
	<li>Read</li>
	<li>Evaluate</li>
	<li>Print      - if anything needs printing.</li>
	<li>Loopback   - go back to the command line</li>
</ol>
The above is called "repl" for short.


&nbsp;

To print anything, you need to use the "print", which in Python 3, is a function.

[python]
=>=>=> print(&quot;hello world!&quot;)
hello world!
[/python]


Some maths are also really easy to do:

[python]
=>=>=> 5    # this is a comment.
5
=>=>=> 2
2
=>=>=> 2 + 5
7
=>=>=> x = 20
=>=>=> x*5      # note you can leave out white spaces.
100
=>=>=> _  - 1   # &quot;_&quot; is a builtin variable that holds the last thing that has been outputted. 
99
=>=>=>
[/python]

Note: The "_" only works on the command line, but not in scripts. 

With the maths you can see repl in action. ]]></Content>
		<Date><![CDATA[2014-06-14]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[python]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Python]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Python - The For-Loop]]></Title>
		<Content><![CDATA[Here is a simple for loop which we'll demonstrate using the <a href="https://docs.python.org/release/1.5.1p1/tut/range.html">range function</a>.

In Pyth+on, a block of code is indicated by white space indents rather than encasing it brackets.

[python]
=>=>=> for i in range(5):
...     x = i * 10      # note the 4 space indent
...     print(x)         
...
0
10
20
30
40
=>=>=>
[/python]

The ":" colon indicates the start of the new block.

There are a couple of conventions regarding white spaces:
<ol>
	<li>indents should be 4 spaces long</li>
	<li>always use space rather than tab to create the whitespace. Also never mix tabs with spaces.</li>
</ol>
These conventions are documeted in Python Enhancement Proposals <a href="http://legacy.python.org/dev/peps/">peps</a>, in particular <a href="http://legacy.python.org/dev/peps/pep-0008/">pep 8</a>.

&nbsp;

Also long lines can be broken over multiple lines by wrapping expressions in parentheses.]]></Content>
		<Date><![CDATA[2014-06-14]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[python]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Python]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Python - The Standard Library]]></Title>
		<Content><![CDATA[Python comes with a large number of <a href="https://docs.python.org/3/library/functions.html">builtin functions</a>. However these alone only provides a limited functionality of python. The true power of python comes from the python's standard library. 

Python comes included with an extensive library of modules. modules extends python's features, in the same way as installing an app on smart phone, will extend what the smart phone can do. However unlike smart phone which requires downloading followed by installing and activating, you don't need to download python modules since they are already in the standard library which comes with python as standard. The only thing you need to do is activate the modules you want to using in your current python session, or inside your python script.

You use the "import" keyword to activate/access a module, like this:

import {module_name}

You can find a <a href="https://docs.python.org/3/library/"> list of all modules in the Python Standard Library</a> on the official website.

For example, if you want to do square roots of a number, then you need to use the "<a href="https://docs.python.org/3/library/math.html">math</a>" module:

[python]
=>=>=> import math
=>=>=> math.sqrt(81)
9.0
=>=>=> math.sqrt(81) * math.sqrt(25)
45.0
=>=>=>
[/python]

If you are familiar with oop languages, e.g. c#, then you can think of "math" as a class, and "sqrt" is a static-method of the class. Reminder, static-methods are method that you use without having to instantiate an object of that class. Hence the "." notation is the standar oop 

You can view help info for a module using the help() funtion:

[python]
=>=>=> help(math)
Help on built-in module math:

NAME
    math

DESCRIPTION
    This module is always available.  It provides access to the
    mathematical functions defined by the C standard.

FUNCTIONS
    acos(...)
        acos(x)

        Return the arc cosine (measured in radians) of x.

    acosh(...)
        acosh(x)

        Return the hyperbolic arc cosine (measured in radians) of x.

    asin(...)
        asin(x)

        Return the arc sine (measured in radians) of x.

    asinh(...)
        asinh(x)

        Return the hyperbolic arc sine (measured in radians) of x.

    atan(...)
        atan(x)

        Return the arc tangent (measured in radians) of x.

    atan2(...)
        atan2(y, x)

        Return the arc tangent (measured in radians) of y/x.
        Unlike atan(y/x), the signs of both x and y are considered.
[/python]

&nbsp;

If you want to view help for a specific method, rather than the whole module, then you can simply do:
&nbsp;
[python]
=>=>=> help(math.sqrt)
Help on built-in function sqrt in module math:

sqrt(...)
    sqrt(x)

    Return the square root of x.

=>=>=>
[/python]
&nbsp;

If you want, you can import method directly into the current workspace, which would allow you to use the method without needing to use it's fully drilled down name:

[python]
=>=>=> sqrt(81)                        # this will output an error message.
Traceback (most recent call last):
  File &quot;&lt;stdin=>&quot;, line 1, in &lt;module=>
NameError: name 'sqrt' is not defined
=>=>=> from math import sqrt
=>=>=> sqrt(81)
9.0
=>=>=>
[/python]

However this approach can be risky in case you already have a sqrt function in the current namespace, in which case there could be a conflict. If there is a namespace clash but you still want to impart a method into the namespace directly, then you can give the method you want to name an alternative name (a bit like an alias) during the import process, e.g. :

[python]
=>=>=> from math import sqrt as SquareRoot
=>=>=> SquareRoot(81)
9.0
=>=>=> 
[/python]

Note: everything in python is case sensitive. 


&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;]]></Content>
		<Date><![CDATA[2014-06-14]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[python]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Python]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Python - Data Types]]></Title>
		<Content><![CDATA[Python comes with the following built-in data types:

&nbsp;
<ol>
	<li>integer</li>
	<li>floating points</li>
	<li>null object</li>
	<li>boolean (true/false)</li>
</ol>


You can find the type of variable using the "type()" <a href="https://docs.python.org/3/library/functions.html">builtin function</a>:

[python]
=>=>=> x = 10
=>=>=> type(x)
&lt;class 'int'=>
=>=>=> y = 10.2345
=>=>=> type(y)
&lt;class 'float'=>
=>=>=> z = &quot;hello world&quot;
=>=>=> type(z)
&lt;class 'str'=>
=>=>=>
[/python]

As you can, python automatically recognised that automatically assigns a type, after it evaluates it. 



]]></Content>
		<Date><![CDATA[2014-06-14]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[python]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Python]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Python - Navigation directories]]></Title>
		<Content><![CDATA[If you want to do pwd, cd, and ls, then you need to import the os module first:


[python]
import os
[/python]


Now we have the following functions:

[python]
=>=>=> os.getcwd()         # equivalent to pwd
'C:\\users\\mir'
=>=>=> os.chdir(&quot;C:/&quot;)     # equivalent to cd
=>=>=> os.listdir()        # equivalent to ls
['$AVG', '$Recycle.Bin', 'Documents and Settings', 'hiberfil.sys', 'inetpub', 'Intel', 'NV
IDIA', 'pagefile.sys', 'PerfLogs', 'Prey', 'Program Files', 'Program Files (x86)', 'Progra
mData', 'Python34', 'Recovery', 'System Volume Information', 'Temp', 'Users', 'Windows']
=>=>=>
[/python]


]]></Content>
		<Date><![CDATA[2014-06-15]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[python]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Python]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Python - run python scripts]]></Title>
		<Content><![CDATA[You have to do this from the powershell command line:

&nbsp;
<ol>
	<li>user powershell to cd to the directory containin the py file.</li>
	<li>from the powershell command line type "python {script-name}.py"</li>
</ol>
&nbsp;

to load From within python, check out:

&nbsp;

http://stackoverflow.com/questions/436198/what-is-an-alternative-to-execfile-in-python-3-0

This link says you need to navigate to the python script directory using powershell, start terminal session, then do:
<pre class="lang-py prettyprint prettyprinted"><code><span class="kwd">exec</span><span class="pun">(</span><span class="pln">open</span><span class="pun">(</span><span class="str">"./pythonscript.py"</span><span class="pun">).</span><span class="pln">read</span><span class="pun">())</span></code></pre>]]></Content>
		<Date><![CDATA[2014-06-15]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[python]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Python]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Python - Using the builtin software repo, Pip]]></Title>
		<Content><![CDATA[<code>Pip is the python equivalent of "linux redhat's yum"
</code>

&nbsp;

It comes included with python 3.4.

&nbsp;

it is something that you run from the powershell command line rather than within python terminal itself, e.g.



[powershell]
PS C:\Users\Mir\Desktop=> pip install -U selenium
Downloading/unpacking selenium
  Running setup.py (path:C:\Users\Mir\AppData\Local\Temp\pip_build_Mir\selenium\setup.py)
egg_info for package selenium

Installing collected packages: selenium
  Running setup.py install for selenium

Successfully installed selenium
Cleaning up...
PS C:\Users\Mir\Desktop=>
[/powershell]
&nbsp;
useful links:
http://stackoverflow.com/questions/8548030/syntax-error-on-install-when-doing-pip-install-for-python]]></Content>
		<Date><![CDATA[2014-06-15]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[python]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Python]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Maven - Intro]]></Title>
		<Content><![CDATA[Maven is basically a "build tool" that:
<ul>
	<li>always produces once artifact...e.g. a jar file, or a war file or a zip file</li>
	<li>manages dependencies (the main reason for using maven)</li>
</ul>
&nbsp;

Maven key selling point:

&nbsp;
<ul>
	<li>Maven lets you recreate your build for "any environment", and not needing to change settings for each environment. e.g. you can deploy to windows or linux environments.</li>
</ul>
&nbsp;

&nbsp;

&nbsp;

With maven, you have to organize all your code files in a certain folder structure, and the folders need to have certain names, this includes:

&nbsp;


1.0 src  (top level folder)

2.0 src\main  (second level folder)

3.0 src\main\java   (third level folder, this is where your source code will live, in form of filename.java)

&nbsp;

Since we are using the maven  folder structure, maven will know that it needs to compile any classes in src/main/java folder.


<strong>The Pom.xml file</strong>

The core part to Maven is the pom.xml. You need to create and place the pom.xml file In the top level folder (i.e. in the src folder), you need to create a pom.xml file (maven looks for this file when it's building the application). "POM" stands for project object model. This xml file is a bit rigid, like a template, and you need to fill out the mandatory fields.

<pre>
<project>
    <groupId>net.codingbee</groupId>
    <artifactId>HelloWorld</artifactId>
    <version>1.2.0</version>
    <modelversion>4.0.0</modelversion>
    <packaging>jar</packaging>
</project>
</pre>

Here is a summary of what the various tags means:

project - this is the top level of the xml  which houses the object's model.

groupid - this is the name of the company that owns the application. It is usually the company's website name, in reverse order.

artifactId -this is usually the name of the application, which in turn is the name of the project.

version - the version of the app

modelversion - the version of pom xml structure. This is not likely to change that much.

packaging - this tells maven to what the end product packaging type is. For java, it is usually jar/war/ear files, and for windows it can be msi files.

&nbsp;






&nbsp;

After you have created the project with some code in it, you can then open up powershell in admin mode, then navigate to the folder that contains the pom.xml file. then type:

maven clean

The first time you run this command, it will download lots of stuff which is a one off task. The main purpose of "clean" is to prepare the environment.

Then run:

mvn compile

This command will also download other stuff, but this again is a one time thing. The compile command will create a "target" folder which sits alongside the src folder.

Under the target folder, you'll find the "classes" folder. Inside this, you'll find your compiled code. for java, you should have a file with the "class" extension e.g.  "filename.class". You can then run this class like from the command line like this:

java helloworld

Note: you have to be in that directory and you can omit the "class" extension.

You might have a bunch of class, files that you need to "package" into a single file, in Java these packaged files can be ".jar" files. You can then instruct this like this:

mvn package

The jar file can be found in the target folder. the jar file's name has the following format:

{artifactid}-{version}.{packaging}

&nbsp;

<strong> Other languages</strong>

When you are working in a java project, then your main source folder is:

src\main\java

However if you are working on an language type, e.g. groovy, then you can configure maven to look at the following instead:

src\main\groovy

&nbsp;

<strong>Testing</strong>

You can place automated unit tests here:

/src/test/java

Note, this folder is only for automated unit tests, and not system/blackbox testing.

&nbsp;

<strong> More about the target folder</strong>

This is where everything gets compiled to and packaged. So in this folder you'll find:

"classes" folder - this contains the compiled code

projectname-version.jar - this is the packaged artifact.

"test-classes" folder - this contains the compiled automated unit tests.

"maven-archiver" folder - not sure

surefire folder.

&nbsp;

<strong>More about the pom file</strong>

The pom.xml file is made up 4 parts:
<ol>
	<li><strong>Project Information</strong>
<ol>
	<li>groupid</li>
	<li>artifactid</li>
	<li>version</li>
	<li>packaging - e.g. jar</li>
</ol>
</li>
	<li><strong>Dependencies</strong> - artifacts we want to use inside our application.
<ol>
	<li>Direct dependencies used in our application</li>
</ol>
</li>
	<li><strong>Build</strong> - this is about:
<ol>
	<li>what plugins we want to use.</li>
	<li>Directory structure info  (in case you wan to override the maven default structure)</li>
</ol>
</li>
	<li>Repositories - location where we downlaod artifacts from. Initially be default maven downloads from the "central maven repo", e.g. if you want to use a custom-made bespoke artifact.</li>
</ol>
&nbsp;

<strong>Quick overview of dependencies</strong>
<ul>
	<li>These are what we want to use in our application.</li>
	<li>dependencies are imported by their naming convention, this means you need to know the artifactid, groupid, and version</li>
	<li>adding dependencies are quite simple.</li>
</ul>
Before you can add a dependency you need to know 3 things, the dependencies:
<ol>
	<li>groupid</li>
	<li>artifactid</li>
	<li>version</li>
</ol>
Once you know all this, you then add the dependencies at the bottom of the pom file (after the project section), like this:

&nbsp;

[xml]
&lt;dependencies=>
    &lt;dependencies=>
        &lt;groupid=>commons-lang&lt;/groupid=>
        &lt;artifactid=>commons-lang&lt;/artifactid=>
        &lt;version=>2.1&lt;/version=>
    &lt;/dependencies=>
&lt;/dependencies=>
[/xml]

<strong>
Maven has 5 core commands</strong>
<ol>
	<li><strong>Clean</strong> - this deletes any target folder as well as any generated files (e.g. exe)</li>
	<li><strong>Compile</strong> - this compiles the source code, generate files, creates the target folder and populates it.</li>
	<li><strong>package</strong> - by default this will run the "compile" command first before it starts to package it. It also runs any unit tests.</li>
	<li><strong>install</strong> - this runs the "package" command and then copy/paste it in your local repo. It doesn't actually install the artifact on a target machine. The default local repo is located at "c:\users\username\.m2\repository"</li>
	<li><strong>Deploy</strong> - this runs the "install" command, to save packaged artifact in your local repository, after that the deploy command copy&paste it to the central corporate repo. the corporate repo can be as simple as fileshare server, or something else.</li>
</ol>
&nbsp;

If you want to run the "clean" command followed by the "package" command, then you can simply do:

<pre>
mvn clean package
</pre>

<strong>Using the "Install" command to save to a local repo</strong>

As mentioned earlier, when you run the install command, your packaged artifact gets copied from the target folder to to your local repo, which by default is located at:

c:\users\username\.m2\repository

In this folder the exact location where the packaged artifact stored is based on the following convention:

c:\users\username\.m2\repository\{groupid}\{artifactid}\{version} 

Also note, that the groupid is broken down further based on the dot notation. For example in the above helloWorld pom.xml, we would have:

c:\users\username\.m2\repository\net\codingbee\HelloWorld\1.2.0

Directly under this directory you should find the file:

HelloWorld-1.2.0.jar

Hence the fully path to the jar file in the local repo is: 


c:\users\username\.m2\repository\net\codingbee\HelloWorld\1.2.0\HelloWorld-1.2.0.jar


<strong>Over-riding the default conventions</strong>
Here's a reminder of how our example pom.xml file looks:

<pre>
<project>
    <groupId>net.codingbee</groupId>
    <artifactId>HelloWorld</artifactId>
    <version>1.2.0</version>
    <modelversion>4.0.0</modelversion>
    <packaging>jar</packaging>
    <dependencies>
        <dependency>
            <groupid>commons-lang</groupid>
            <artifactid>commons-lang</artifactid>
            <version>2.1</version>
        </dependency>
    </dependencies>
</project>
</pre>

So far we have looked at two key sections of the pom file.

The first section is the domestics, and give the general info, and it is made up of the tags groupid, artifactid, version, modelversion, packaging. This section isn't enclosed in it's own tags but nonetheless these tags makes up section in it's own right. This section is also mandatory. 

The second section that we looked at is "dependencies". This section is optional, and is only used as and when needed. 

However there is another section that you can add after the dependencies section, and that is the "build" section. This section is used if you want over-ride some of the maven defaults. For instance, in the above example if you run the "mvn clean package" command, you will end up with the packaged artifact with the filename of:

HelloWorld-1.2.0.jar
 
This is the default naming convention applied by Maven, however if you actually want your packaged file to be called "HelloWorld-beta.jar", then you can do this by introducing the build section and adding the "finalname" entry in the build section:


<pre>
<project>
    <groupId>net.codingbee</groupId>
    <artifactId>HelloWorld</artifactId>
    <version>1.2.0</version>
    <modelversion>4.0.0</modelversion>
    <packaging>jar</packaging>
    <dependencies>
        <dependency>
            <groupid>commons-lang</groupid>
            <artifactid>commons-lang</artifactid>
            <version>2.1</version>
        </dependency>
    </dependencies>
    <build>
        <finalname>HelloWorld-beta</finalname>
    </build>
</project>
</pre>




&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;]]></Content>
		<Date><![CDATA[2014-06-18]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Maven]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials]]></Categories>
	</post>
	<post>
		<Title><![CDATA[vagrant vs docker vs vmware]]></Title>
		<Content><![CDATA[http://blogs.isostech.com/technology/vagrant-virtualbox-vs-vmware-fusion/

&nbsp;

http://www.scriptrock.com/articles/docker-vs-vagrant

&nbsp;

&nbsp;]]></Content>
		<Date><![CDATA[2014-06-25]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell - Detect what version of .NET is installed]]></Title>
		<Content><![CDATA[http://stackoverflow.com/questions/199080/how-to-detect-what-net-framework-versions-and-service-packs-are-installed

&nbsp;

The tale given in the first answer in the above stackoverflow questions shows what to look for. Then do:

&nbsp;

[powershell]
cd HKLM:\SOFTWARE\Microsoft\NET Framework Setup\NDP
ls
[/powershell]

This should give output that looks like:

<pre>
cd HKLM:\SOFTWARE\Microsoft\NET Framework Setup\NDP
ls

SKC  VC Name                           Property
---  -- ----                           --------
  1   0 CDF                            {}
 23   6 v2.0.50727                     {Install, Version, Increment, SP...}
  2   5 v3.0                           {Version, CBS, Increment, Install...}
  1   5 v3.5                           {Version, CBS, Install, InstallPath...}
  2   0 v4                             {}
  1   1 v4.0                           {(default)}
</pre>

now let's take a look at .NET 3.5 in particular:
<pre>
PS C:\> get-childitem -path "HKLM:\SOFTWARE\Microsoft\NET Framework Setup\NDP" | Where-Object -FilterScript {$_.name -match "v3.5"}


    Hive: HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\NET Framework Setup\NDP


SKC  VC Name                           Property
---  -- ----                           --------
  1   5 v3.5                           {Version, CBS, Install, InstallPath...}
</pre>

Now if we drill down the property field we get:

<pre>
PS C:\> (get-childitem -path "HKLM:\SOFTWARE\Microsoft\NET Framework Setup\NDP" | Where-Object -FilterScript {$_.name -m
atch "v3.5"} ).property
Version
CBS
Install
InstallPath
SP
</pre>

The above actually outputs the field as strings, and not in a hashtable format that we are expecting. If we look at the same info via regedit, we see:

<a href="http://codingbee.net/wp-content/uploads/2014/07/regedit.png"><img src="http://codingbee.net/wp-content/uploads/2014/07/regedit.png" alt="regedit" width="864" height="468" class="alignnone size-full wp-image-1179" /></a>

Here, we want to view the highlighted parts, to view them we use the "get-itemproperty" cmdlet, like this:

<pre>
PS C:\> get-childitem -path "HKLM:\SOFTWARE\Microsoft\NET Framework Setup\NDP" | Where-Object -FilterScript {$_.name -ma
tch "v3.5"} | Get-ItemProperty


PSPath       : Microsoft.PowerShell.Core\Registry::HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\NET Framework Setup\NDP\v3.5
PSParentPath : Microsoft.PowerShell.Core\Registry::HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\NET Framework Setup\NDP
PSChildName  : v3.5
PSProvider   : Microsoft.PowerShell.Core\Registry
Version      : 3.5.30729.5420
CBS          : 1
Install      : 1
InstallPath  : C:\Windows\Microsoft.NET\Framework64\v3.5\
SP           : 1
</pre>

&nbsp;

&nbsp;]]></Content>
		<Date><![CDATA[2014-07-01]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Powershell - Accessing the registry]]></Title>
		<Content><![CDATA[There are 2 main cmdlets you use to identify and navigate the registry (instead of using regedit). 

First of you navigate the registry in the same style as navigating a folder structure, i.e. you navigate it using cd and get-childitem. But the registry doesn't reside inside the c:\ drive, instead it resides in it's own drive called registry:


<pre>
PS C:\> Get-PSProvider

Name                 Capabilities                                      Drives
----                 ------------                                      ------
WSMan                Credentials                                       {WSMan}
Alias                ShouldProcess                                     {Alias}
Environment          ShouldProcess                                     {Env}
FileSystem           Filter, ShouldProcess                             {C, E, A, D}
Function             ShouldProcess                                     {Function}
Registry             ShouldProcess, Transactions                       {HKLM, HKCU}
Variable             ShouldProcess                                     {Variable}
Certificate          ShouldProcess                                     {cert}
</pre>

Here you should see that that all the registry info is stored in 2 drives, HKLM and HKCU. 

The other important cmdlet that you also need to use is the get-itemproperty cmdlet. Here's an example on how to <a href="http://codingbee.net/tutorials/powershell/powershell-detect-version-net-installed/" title="PowerShell – Detect what version of .NET is installed">explore the registry to find the version of .net installed</a>.

 ]]></Content>
		<Date><![CDATA[2014-07-01]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Maven - Dependencies]]></Title>
		<Content><![CDATA[]]></Content>
		<Date><![CDATA[2014-07-02]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Maven]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Chromecast - Access another countries Netflix]]></Title>
		<Content><![CDATA[Take the following instructions:

https://gist.github.com/epeli/9789586     (i only created the script not the dnsmasq)




http://asuswrt.lostrealm.ca/download

https://www.youtube.com/watch?v=qcDHzNefxw0

http://asuswrt.lostrealm.ca/screenshots


http://asuswrt.lostrealm.ca/sites/default/files/pictures/dhcp.png
you can access above screenshot here:
http://<router ip address>/Advanced_DHCP_Content.asp]]></Content>
		<Date><![CDATA[2014-07-13]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[chromecast]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell - IIS automation]]></Title>
		<Content><![CDATA[You can view the IIS gui conole by running "Inetmgr" on the commandline.

&nbsp;

&nbsp;

To automate IIS you first need to ensure you have the "webadministration":

&nbsp;

[powershell]
PS C:\Windows\system32&gt; Get-Module -ListAvailable

    Directory: C:\Windows\system32\WindowsPowerShell\v1.0\Modules

ModuleType Version    Name                                ExportedCommands
---------- -------    ----                                ----------------
Manifest   1.0.0.0    ADRMS                               {Update-ADRMS, Uninstall-ADRMS, Install-ADRMS}
Manifest   1.0.0.0    AppLocker                           {Set-AppLockerPolicy, Get-AppLockerPolicy, Test-AppLockerP...
Manifest   1.0        BestPractices                       {Get-BpaModel, Invoke-BpaModel, Get-BpaResult, Set-BpaResult}
Manifest   1.0.0.0    BitsTransfer                        {Add-BitsFile, Remove-BitsTransfer, Complete-BitsTransfer,...
Manifest   1.0.0.0    CimCmdlets                          {Get-CimAssociatedInstance, Get-CimClass, Get-CimInstance,...
Script     1.0.0.0    ISE                                 {New-IseSnippet, Import-IseSnippet, Get-IseSnippet}
Manifest   3.0.0.0    Microsoft.PowerShell.Diagnostics    {Get-WinEvent, Get-Counter, Import-Counter, Export-Counter...
Manifest   3.0.0.0    Microsoft.PowerShell.Host           {Start-Transcript, Stop-Transcript}
Manifest   3.1.0.0    Microsoft.PowerShell.Management     {Add-Content, Clear-Content, Clear-ItemProperty, Join-Path...
Manifest   3.0.0.0    Microsoft.PowerShell.Security       {Get-Acl, Set-Acl, Get-PfxCertificate, Get-Credential...}
Manifest   3.1.0.0    Microsoft.PowerShell.Utility        {Format-List, Format-Custom, Format-Table, Format-Wide...}
Manifest   3.0.0.0    Microsoft.WSMan.Management          {Disable-WSManCredSSP, Enable-WSManCredSSP, Get-WSManCredS...
Binary     1.0        PSDesiredStateConfiguration         {Set-DscLocalConfigurationManager, Start-DscConfiguration,...
Script     1.0.0.0    PSDiagnostics                       {Disable-PSTrace, Disable-PSWSManCombinedTrace, Disable-WS...
Binary     1.1.0.0    PSScheduledJob                      {New-JobTrigger, Add-JobTrigger, Remove-JobTrigger, Get-Jo...
Manifest   2.0.0.0    PSWorkflow                          {New-PSWorkflowExecutionOption, New-PSWorkflowSession, nwsn}
Manifest   1.0.0.0    PSWorkflowUtility                   Invoke-AsWorkflow
Manifest   1.0.0.0    ServerManager                       {Get-WindowsFeature, Add-WindowsFeature, Remove-WindowsFea...
Manifest   1.0.0.0    TroubleshootingPack                 {Get-TroubleshootingPack, Invoke-TroubleshootingPack}
Manifest   1.0.0.0    WebAdministration                   {Start-WebCommitDelay, Stop-WebCommitDelay, Get-WebConfigu...

    Directory: C:\Program Files\System Center Operations Manager\Agent\PowerShell

ModuleType Version    Name                                ExportedCommands
---------- -------    ----                                ----------------
Binary     0.0.0.0    Microsoft.MonitoringAgent.PowerS... {Checkpoint-WebApplicationMonitoring, Get-WebApplicationMo...

PS C:\Windows\system32&gt;
[/powershell]

&nbsp;

If you have windows 2008 with IIS 7.5 installed, then you should see a module called "WebAdministration". Next do:

&nbsp;

[powershell]
Import-Module -Name WebAdministration
[/powershell]

After that you should have a bunch of IIS related commands ready for use:

[powershell]
PS C:\Windows\system32&gt; Get-Command -Module webadministration

CommandType     Name                                               ModuleName
-----------     ----                                               ----------
Alias           Begin-WebCommitDelay                               webadministration
Alias           End-WebCommitDelay                                 webadministration
Function        IIS:                                               webadministration
Cmdlet          Add-WebConfiguration                               webadministration
Cmdlet          Add-WebConfigurationLock                           webadministration
Cmdlet          Add-WebConfigurationProperty                       webadministration
Cmdlet          Backup-WebConfiguration                            webadministration
Cmdlet          Clear-WebConfiguration                             webadministration
Cmdlet          Clear-WebRequestTracingSettings                    webadministration
Cmdlet          ConvertTo-WebApplication                           webadministration
Cmdlet          Disable-WebGlobalModule                            webadministration
.
.
.
.
[/powershell]

Along with this you should have the IIS drive available too:

[powershell] PS C:\Windows\system32&gt; psdrive Name Used (GB) Free (GB) Provider Root CurrentLocation ---- --------- --------- -------- ---- --------------- A FileSystem A:\ Alias Alias C 30.80 19.10 FileSystem C:\ Windows\system32 Cert Certificate \ D FileSystem D:\ Env Environment Function Function HKCU Registry HKEY_CURRENT_USER HKLM Registry HKEY_LOCAL_MACHINE IIS WebAdminis... \\OSVM1043 Variable Variable WSMan WSMan [/powershell]

<h2> Create a Virtual Directory</h2>
You need to use the New-WebVirtualDirectory command:

&nbsp;

http://www.jasonholden.com/blog/index.cfm/2014/1/7/Use-PowerShell-to-add-IIS-virtual-directory-with-Login-Connect-As

&nbsp;

Here is an example command:

New-WebVirtualDirectory -Name "xxx"  -site "Default Web Site" -PhysicalPath 'C:\Program Files\path\to\folder'

&nbsp;

Name - This is the name as you want it to when viewing it in the Inetmgr's tree structure.

Sites - on the inetmgr gui, it is the parent too the name, within the tree.

physicalpath - it is the full path to the folder where the folder should reside. For consistency it might be a good idea to mirror the folder's name with the "name"

&nbsp;

&nbsp;

&nbsp;

<strong>Resources</strong>

http://stackoverflow.com/questions/5963647/webadministration-powershell-module-not-found-on-windows-server-data-center

&nbsp;

http://www.iis.net/learn/manage/powershell/powershell-snap-in-creating-web-sites-web-applications-virtual-directories-and-application-pools

&nbsp;

http://stackoverflow.com/questions/14236406/how-to-enable-a-windows-feature-via-powershell

&nbsp;

&nbsp;

After this, you should see IIS when you do:

get-psprovider

Also you should see the following new commands:

Get-Command -Module webadministration

&nbsp;

&nbsp;

&nbsp;]]></Content>
		<Date><![CDATA[2014-07-16]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Linux - Give your Linux machine a hostname and domain-name]]></Title>
		<Content><![CDATA[When you build a brand new linux machine, you machine will not have a name. As a result, your router (i.e. network) will instead refer to your machine by it's mac-address:

<a href="http://codingbee.net/wp-content/uploads/2014/07/2hGFpvg.png"><img class="alignnone size-full wp-image-1231" src="http://codingbee.net/wp-content/uploads/2014/07/2hGFpvg.png" alt="" width="343" height="344" /></a>

Now you can assign a name for your hostname using the hostname command, like this:

[bash]
[root@localhost ~]# hostname PuppetMaster.MayfieldRoad.net
[root@localhost ~]# hostname
PuppetMaster
[/bash]

Here, I've called my machine "PuppetMaster", which is the hostname. I also set the domain name as "MayfieldRoad.net",

which you can confirm like this:

[bash]
[sher@PuppetMaster ~]$ hostname --domain
MayfieldRoad.net
[/bash]

Note, the above won't work, unless you have set "MayfieldRoad.net" as your domain on your router. You can set the domain up in your router, by logging into your router's web console, and change the dhcp setting, e.g.:

<a href="http://codingbee.net/wp-content/uploads/2014/07/Ra2mb9x.png"><img class="alignnone size-full wp-image-1239" src="http://codingbee.net/wp-content/uploads/2014/07/Ra2mb9x.png" alt="" width="1079" height="620" /></a>

to check if that is already done, simply open up a browser, and go to:

RT-N66U.MayfieldRoad.net

Note, in this case the "RT-N66U" is the model name of my Asus router. You can also confirm this by telneting into the router, using putty and the router's ip. and then viewing the /etc/hosts files.

Unfortunately the setting the hostname (and domain) using the "hostname" command won't persist after a reboot, to make it permanent, you need to use the "nmtui" bios-like-gui command instead:

<a href="http://codingbee.net/wp-content/uploads/2014/07/dZ1wLte.png"><img class="alignnone size-full wp-image-1240" src="http://codingbee.net/wp-content/uploads/2014/07/dZ1wLte.png" alt="" width="350" height="327" /></a>

Note: the nmtui command is new addition to centos/rhel 7. For older versions you need to edit the following file:

<em>/etc/sysconfig/network</em>

Note: If the machine you are working on is a vm clone, then the <a title="VirtualBox – cloning RHEL/Cento checklist" href="http://codingbee.net/tutorials/virtualbox/virtualbox-cloning-rhelcento-checklist/">mac address is hardcoded in the config files</a>, which needs to be fixed.

&nbsp;

After that you can check this has been set by doing:

[sher@PuppetMaster ~]$ hostnamectl status
Static hostname: PuppetMaster.mayfieldroad.net
Icon name: computer
Chassis: n/a
Machine ID: 7adea129229342bb9f7735b5dfd42050
Boot ID: db90bac6e96148bfac35b43ca5ccde59
Operating System: CentOS Linux 7 (Core)
CPE OS Name: cpe:/o:centos:centos:7
Kernel: Linux 3.10.0-123.4.2.el7.x86_64
Architecture: x86_64

Another better check is trying to see if you can connect to the machine using it's hostname.domainname rather than it's IP address.

Reference:
http://www.itzgeek.com/how-tos/linux/centos-how-tos/change-hostname-in-centos-7-rhel-7.html#axzz381VBEDUs]]></Content>
		<Date><![CDATA[2014-07-20]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Redhat|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Enable-psremoting while Virtualbox is installed]]></Title>
		<Content><![CDATA[If you have virtualbox installed on your machine, then as a side effect, you'll discover that the enable-psremoting no longer works. That's because as part of the vb install, it created a "public network" adapter as a byproduct, which blocks enable-psremoting from working. This is actually a bug relating to virtualbox and can be fixed by adding the following entry in regedit:

<a href="http://codingbee.net/wp-content/uploads/2014/07/tEp6nCh.png"><img class="alignnone size-full wp-image-1252" src="http://codingbee.net/wp-content/uploads/2014/07/tEp6nCh.png" alt="" width="1171" height="528" /></a>

&nbsp;

1. Open regedit and navigate to <em>HKLM\SYSTEM\CurrentControlSet\Control\Class\{4D36E972-E325-11CE-BFC1-08002BE10318}</em>

2. Browse through the subkeys (named 0000, 0001, etc) until you find the subkey containing the virtualbox network adapter, this is the one where the “<em>DriverDesc</em>” key has “<em>VirtualBox Host-Only Ethernet Adapter</em>” as value.

3. Add a new DWORD value with a name of “<em>*NdisDeviceType</em>” and a value of “<em>1</em>”

&nbsp;

&nbsp;

http://www.brokenwire.net/bw/Various/120/fix-virtualbox-causes-unidentified-network-on-vista-and-windows-7]]></Content>
		<Date><![CDATA[2014-07-27]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell|VirtualBox]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[virtualbox command lines]]></Title>
		<Content><![CDATA[virtualbox comes with it's own command line utility:

https://www.virtualbox.org/manual/ch08.html#vboxmanage-list

to make use of this you first have to cd to the vboxmanage.exe which is in the virtualbox program file's folder:


<pre>C:\Program Files\Oracle\VirtualBox</pre>

here are some examples:

<strong>Start a vm</strong>
use the following:

.\VBoxManage.exe startvm {vm's name}

Note: the vm's name is case sensitive. I think.

e.g.:

[powershell]
PS C:\Program Files\Oracle\VirtualBox=> .\VBoxManage.exe startvm Centos7
Waiting for VM &quot;Centos7&quot; to power on...
VM &quot;Centos7&quot; has been successfully started.
[/powershell]

<strong>List running vms</strong>

This is done like this:

[powershell]
PS C:\Program Files\Oracle\VirtualBox=> .\VBoxManage.exe list runningvms
&quot;Centos7&quot; {6cfe3d82-b4ae-46f1-9188-d5e6a603474e}
PS C:\Program Files\Oracle\VirtualBox=>
[/powershell]


<strong>Shutdown a VM</strong>

To shutdown a vm, do:

[powershell]
PS C:\Program Files\Oracle\VirtualBox=> .\VBoxManage.exe controlvm Centos7 savestate
0%...10%...20%...30%...40%...50%...60%...70%...80%...90%...100%
PS C:\Program Files\Oracle\VirtualBox=>
[/powershell]]]></Content>
		<Date><![CDATA[2014-07-27]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[VirtualBox]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>VirtualBox]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Install/set-up the Puppet Master and it's Agents]]></Title>
		<Content><![CDATA[This is a walkthrough on how to set up your very own full puppet master-agent environment.

&nbsp;
<h2>
Pre-requisites</h2>
Before your start, you need the following:
<ul>
	<li>At least 2 machines. The first machine will be set up as the puppet master, and the other machine will be set up as a puppet agent. The puppet agent can be either a windows or linux machine. It's recommended that these machines are set up as virtual machines (and we recommend using virtualbox for this) rather than physcal machines. That's because with virtualbox you can quickly clone machines, and take snapshots for easy rollback, if needed.</li>
	<li>Turn master/agent machines into <a title="Linux – NTP Server" href="http://codingbee.net/tutorials/linux/red-hat/linux-ntp-server/">ntp clients</a> so that there internal clocks are synced</li>
	<li><a title="Linux – Give your Linux machine a hostname and domain-name" href="http://codingbee.net/tutorials/linux/red-hat/linux-give-linux-machine-name/"><a href="http://codingbee.net/uncategorized/linux-give-your-linux-machine-a-hostname-and-domain-name/">set hostnames and domains for each machine</a></a>....also recommended to set up ddns if you have a dynamic ip address.</li>
	<li>Both machines are on the same LAN (or have static WAN IPs). You can check this by ensuring that you can ssh from one machine to another using the fqdn, i.e. you can do "ssh {username}@{fqdn}".</li>
</ul>
&nbsp;

For the purpose of this guide, I'm going to use 2 machines which are both running on Centos 6.5 (you can <a href="http://wiki.centos.org/Download">download centos 6.5 here</a>). The first is going to act as my puppet master and it's fqdn is:

<pre>
puppetmaster.codingbee.dyndns.org</pre>

In the remainder of this article, I will be referring to this machine as the puppetmaster.

The second is going be my puppet agent, and it's fqdn is:

puppetagent1.codingbee.dyndns.org

In the remainder of this article, I will be referring to this machine as the puppetagent.

&nbsp;
<h2>Part A - Setting up the Puppet Master</h2>
All the steps in part-A are performed on the puppet master only.
<h3>Step 1 - install "puppetlabs"</h3>
This install software will also deploy the yum repo configs:

For Centos/RHEL 6.5 do:

<pre>rpm -ivh http://yum.puppetlabs.com/puppetlabs-release-el-6.noarch.rpm
</pre>

For Centos/RHEL 7, do:

<pre>rpm -ivh http://yum.puppetlabs.com/puppetlabs-release-el-7.noarch.rpm
</pre>

This command will end up installing puppetlabs package.

<pre>
rpm -qa | grep puppet
</pre>

This packages comes with yum configuration, i.e. a .repo file, so that you can then use the yum command to perform that actual puppet install:

<pre>
[root@localhost yum.repos.d]# pwd
/etc/yum.repos.d
[root@localhost yum.repos.d]# ls -l
total 20
-rw-r--r--. 1 root root 1612 Jul 15 15:18 CentOS-Base.repo
-rw-r--r--. 1 root root  640 Jul 15 15:18 CentOS-Debuginfo.repo
-rw-r--r--. 1 root root 1331 Jul 15 15:18 CentOS-Sources.repo
-rw-r--r--. 1 root root  156 Jul 15 15:18 CentOS-Vault.repo
-rw-r--r--. 1 root root 1250 Jan 22 22:03 puppetlabs.repo    # this is the new file created by the rpm command above
</pre>

<h3>Step 2 - Use yum to finish off installing all puppet components</h3>
After that, to install puppet on a linux server that is going to become the master, do:


<pre>yum install puppet puppet-server facter
</pre>

In the above we installed 3 packages:
<ul>
	<li><strong> puppet-server </strong>- This is the core puppet master component.</li>
	<li><strong> puppet </strong>- This is the puppet-agent component. You don't really need to install this on the puppet master, but in this case I have so that the puppet master server can also act as a puppet agent at the same time. This would mean that the puppet master can be used to make changes to itself</li>
	<li><strong> facter</strong> - This contains the system inventory tool Facter. Facter gathers information, or facts, about your hosts that are used to help customize your Puppet configuration.</li>
</ul>
You can check, if the install has been successful by doing:

<pre>[root@puppetmaster ~]# rpm -qa | grep puppet
puppet-server-3.6.2-1.el6.noarch
puppetlabs-release-6-10.noarch        # this was installed as part of the rpm install done earlier.
puppet-3.6.2-1.el6.noarch
[root@puppetmaster ~]# rpm -qa | grep facter
facter-2.1.0-1.el6.x86_64

</pre>


<h3>Step 3 - Define the "server" parameter in the puppet.conf file</h3>
At this stage, we’re only going to add one entry, server, to the puppet.conf file. The "server" option specifies the (host)name of the Puppet master. We’ll add the server value to the [main]section (if the section doesn’t already exist in your file, then create it). It should like this:
<pre>[root@puppetmaster ~]# cat /etc/puppet/puppet.conf
[main]
    # The Puppet log directory.
    # The default value is '$vardir/log'.
    logdir = /var/log/puppet

    # Where Puppet PID files are kept.
    # The default value is '$vardir/run'.
    rundir = /var/run/puppet

    # Where SSL certificates are kept.
    # The default value is '$confdir/ssl'.
    ssldir = $vardir/ssl

    server=puppetmaster.codingbee.dyndns.org
[agent]
    # The file in which puppetd stores a list of the classes
    # associated with the retrieved configuratiion.  Can be loaded in
    # the separate ``puppet`` executable using the ``--loadclasses``
    # option.
    # The default value is '$confdir/classes.txt'.
    classfile = $vardir/classes.txt

    # Where puppetd caches the local configuration.  An
    # extension indicating the cache format is added automatically.
    # The default value is '$confdir/localconfig'.
    localconfig = $vardir/localconfig
</pre>
In my case the puppetmaster's fqdn is "puppetmaster.codingbee.dyndns.org", as shown above.
<h3>Step 4 - update hosts file (this could be optional)</h3>
This is what I added, but not sure if this is correct or necessary:
<pre>[root@puppetmaster ~]# cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
{eth0 ip address} puppetmaster.codingbee.dyndns.org puppetmaster
</pre>
&nbsp;
<h3>step 5 - create the site.pp file</h3>
The following site.pp file needs to exist:

[bash]
[root@puppetmaster manifests]# ls -l /etc/puppet/manifests/site.pp
-rw-r--r--. 1 root root 0 Aug  4 17:53 /etc/puppet/manifests/site.pp
[/bash]

If not then create it along with any folders. At this stage this file can be left as an empty file.
<h3>step 6 - update iptables</h3>
You need to open port 8140 because puppetmaster uses this port to listen in on. There are 2 approaches that could be used to open this:

<strong>Approach 1 - use the iptables command</strong>
Note, this didn't work for me, so I used approach 2 instead. Also I don't this approach will survive a reboot.

In this approach you simply run the following command:

[bash]
$ iptables -A INPUT -p tcp -m state --state NEW --dport 8140 -j ACCEPT [/bash]

In my case it didnt work, it inserted the rule after the "REJECT". I think you can overcome this by <a href="http://serverfault.com/questions/374993/how-do-you-prepend-an-iptables-rather-than-append">prepending the rule by using the I switch, instead of the A switch</a>. i.e. run the following instead:

[bash]
$ iptables -I INPUT -p tcp -m state --state NEW --dport 8140 -j ACCEPT [/bash]

After running this, try:

[bash]
iptables -L
[/bash]

The new rule should be higher than the REJECT statement.

<strong>Approach 2 - update the /etc/sysconfig/iptables file directly</strong>

insert the "8140" line so that it looks like this:
<pre>[root@puppetmaster /]# cat /etc/sysconfig/iptables
# Firewall configuration written by system-config-firewall
# Manual customization of this file is not recommended.
*filter
:INPUT ACCEPT [0:0]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [0:0]
-A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT
-A INPUT -p icmp -j ACCEPT
-A INPUT -i lo -j ACCEPT
-A INPUT -p udp -m state --state NEW --dport 23 -j ACCEPT
-A INPUT -p tcp -m state --state NEW --dport 23 -j ACCEPT
-A INPUT -p tcp -m state --state NEW --dport 8140 -j ACCEPT
-A INPUT -m state --state NEW -m tcp -p tcp --dport 22 -j ACCEPT
-A INPUT -j REJECT --reject-with icmp-host-prohibited
-A FORWARD -j REJECT --reject-with icmp-host-prohibited
COMMIT
</pre>
Then restart iptables (only do this if you followed approach 2):

[bash]
[root@puppetmaster ~]# service iptables restart
iptables: Setting chains to policy ACCEPT: filter          [  OK  ]
iptables: Flushing firewall rules:                         [  OK  ]
iptables: Unloading modules:                               [  OK  ]
iptables: Applying firewall rules:                         [  OK  ]

[/bash]

<h3>Step 7 - Start the puppet master service (aka puppetmaster)</h3>
Before you start the puppetmaster service, first check if you have any files here:

[bash]
[root@puppetmaster ~]# ls -l /var/lib/puppet/ssl/
ls: cannot access /var/lib/puppet/ssl/: No such file or directory
[root@puppetmaster ~]#
 [/bash]

Notice that even the ssl folder doesn't exist yet.

&nbsp;

Now run:

[bash]
[root@puppetmaster ~]# service puppetmaster status
puppet is stopped
[root@puppetmaster ~]# service puppetmaster start
Starting puppetmaster:                                     [  OK  ]
[root@puppetmaster ~]# service puppetmaster status
puppet (pid  2032) is running...
[root@puppetmaster ~]#
 [/bash]

This will have generated the files in the ssl folder now:

&nbsp;

[bash]
[root@puppetmaster ~]# ls -l /var/lib/puppet/ssl/
total 28
drwxr-xr-x. 5 puppet puppet 4096 Aug  4 18:38 ca
drwxr-xr-x. 2 puppet puppet 4096 Aug  4 18:38 certificate_requests
drwxr-xr-x. 2 puppet puppet 4096 Aug  4 18:38 certs
-rw-r--r--. 1 puppet puppet  987 Aug  4 18:38 crl.pem
drwxr-x---. 2 puppet puppet 4096 Aug  4 18:38 private
drwxr-x---. 2 puppet puppet 4096 Aug  4 18:38 private_keys
drwxr-xr-x. 2 puppet puppet 4096 Aug  4 18:38 public_keys
 [/bash]

The presence of these files indicates that the puppetmaster server has started successfully.
<h3>step 8 - ensure puppetmaster starts at boot time</h3>

[bash]
[root@puppetmaster /]# chkconfig | grep puppetmaster
puppetmaster    0:off   1:off   2:off   3:off   4:off   5:off   6:off
[root@puppetmaster /]# chkconfig puppetmaster on
[root@puppetmaster /]# chkconfig | grep puppetmaster
puppetmaster    0:off   1:off   2:on    3:on    4:on    5:on    6:off
[root@puppetmaster /]#
 [/bash]

That's it!
<h2><span style="color: #993300;">Part B - Setting up the Puppet Agent</span></h2>
<h3>Step 1 - install "puppetlabs"</h3>
This install software will also deploy the yum repo configs:

&nbsp;

[bash]
# note it is different for rhel 6 and rhel 7.

rpm -ivh http://yum.puppetlabs.com/puppetlabs-release-el-7.noarch.rpm   # for centos/rhel 7
rpm -ivh http://yum.puppetlabs.com/puppetlabs-release-el-6.noarch.rpm   # for centos/rhel 6
[/bash]

<h3>Step 2 - Use yum to finish off installing all yum components</h3>
After that, to install puppet on a linux server that is going to become the master, do:

[bash]

yum install puppet facter

[/bash]

<h3>Step 3 - updated /etc/hosts</h3>
Not sure if this is required, but I added the follwoign line:
<pre>192.168.1.74 puppetmaster.codingbee.dyndns.org puppetmaster
</pre>
<h4>Step 4 - Update IP tables</h4>
insert the "8139" line so that it looks like this:
<pre>[root@puppetmaster /]# cat /etc/sysconfig/iptables
# Firewall configuration written by system-config-firewall
# Manual customization of this file is not recommended.
*filter
:INPUT ACCEPT [0:0]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [0:0]
-A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT
-A INPUT -p icmp -j ACCEPT
-A INPUT -i lo -j ACCEPT
-A INPUT -p udp -m state --state NEW --dport 23 -j ACCEPT
-A INPUT -p tcp -m state --state NEW --dport 23 -j ACCEPT
-A INPUT -p tcp -m state --state NEW --dport 8139 -j ACCEPT
-A INPUT -m state --state NEW -m tcp -p tcp --dport 22 -j ACCEPT
-A INPUT -j REJECT --reject-with icmp-host-prohibited
-A FORWARD -j REJECT --reject-with icmp-host-prohibited
COMMIT
</pre>
Then restart iptables (only do this if you followed approach 2):

[bash]
[root@puppetmaster ~]# service iptables restart
iptables: Setting chains to policy ACCEPT: filter          [  OK  ]
iptables: Flushing firewall rules:                         [  OK  ]
iptables: Unloading modules:                               [  OK  ]
iptables: Applying firewall rules:                         [  OK  ]

[/bash]

That's it!
<h2><span style="color: #993300;">Part C - Authenticate the puppet agent with it's puppet master</span></h2>
In Part C we are going alternate running commands on both the puppet-master and the puppet-agent.

<strong>Step 1 - Submit new digital certificate to puppet master (task performed on the puppet-agent)</strong>
Log into the puppet agent and run the following:

[bash]
[root@puppetagent1 ~]# puppet agent --test --server=puppetmaster.codingbee.dyndns.org    # only run this command once.
Info: Caching certificate for ca
Info: csr_attributes file loading from /etc/puppet/csr_attributes.yaml
Info: Creating a new SSL certificate request for puppetagent1.codingbee.dyndns.org
Info: Certificate Request fingerprint (SHA256): 6D:08:DE:7F:43:D0:91:2C:10:44:F3:DE:0D:AF:C5:28:85:E7:8B:9D:9A:A8:6E:26:96:4F:53:CD:26:BA:E6:57
Info: Caching certificate for ca
Exiting; no certificate found and waitforcert is disabled   # note this error message is expected.
[/bash]

(see page 22 of puppet pro book)

The "Exiting; no certificate found and waitforcert is disabled" message above means that the puppet agent has sent a digital certificate to the puppetmaster to sign.

In this case this certificate has the id (aka fingerprint):
<pre>6D:08:DE:7F:43:D0:91:2C:10:44:F3:DE:0D:AF:C5:28:85:E7:8B:9D:9A:A8:6E:26:96:4F:53:CD:26:BA:E6:57</pre>
&nbsp;

<strong>step 2 - check if puppetmaster has recieved the certificate (task performed on the puppet-master)</strong>

Now run the following on  the puppet master to check if it has receieved the certificate:

&nbsp;

&nbsp;

[bash]
puppet# puppet cert list
 &quot;node1.pro-puppet.com&quot; (SHA256) 6F:0D:41:14:BD:2D:FC:CE:1C:DC:11:1E:26:07:4C:08:D0:C7:E8:62:A5:33:E3
 :4B:8B:C6:28:C5:C8:88:1C:C8

 
[/bash]

&nbsp;

<strong>step 3 - sign the certificate and return it to puppet agent. </strong> <strong>(task performed on the puppet-master)</strong>

now sign this certificate like this:

&nbsp;

&nbsp;

[bash]
[root@puppetmaster examples]# puppet cert sign puppetagent2.codingbee.dyndns.org
Notice: Signed certificate request for puppetagent2.codingbee.dyndns.org
Notice: Removing file Puppet::SSL::CertificateRequest puppetagent2.codingbee.dyndns.org at '/var/lib/puppet/ssl/ca/requests/puppetagent2.codingbee.dyndns.org.pem'
[root@puppetmaster examples]#
[/bash]

&nbsp;

Alternatively, instead of waiting, you can restart the....not sure.
<strong>Step 4 - See if the agent can finally communicate with the master (task performed on the puppet agent)</strong>

[bash]
[root@puppetagent1 ~]# puppet agent --test --server=puppetmaster.codingbee.dyndns.org    # only run this command once.
Info: Retrieving plugin
Info: Caching catalog for node1.pro-puppet.com
Info: Applying configuration version '1365655737
Notice: Finished catalog run in 0.13 seconds
[/bash]

If you get the message like above then it means everything has been done successfully!!!

&nbsp;

If you get a "Error 400 on SERVER" message then don't panic, see pro puppet book, ch1, page 24

&nbsp;

Puppetlabs also provide <a href="https://docs.puppetlabs.com/guides/install_puppet/pre_install.html">puppet open source install instructions</a>.



&nbsp;

Useful info:

Puppet makes use of the following ports:
<table class="wikitable sortable jquery-tablesorter">
<tbody>
<tr style="background: lightsteelblue;">
<td>8139</td>
<td>TCP</td>
<td></td>
<td><a title="Puppet (software)" href="http://en.wikipedia.org/wiki/Puppet_%28software%29">Puppet (software)</a> Client agent</td>
<td>Unofficial</td>
</tr>
<tr style="background: lightsteelblue;">
<td>8140</td>
<td>TCP</td>
<td></td>
<td><a title="Puppet (software)" href="http://en.wikipedia.org/wiki/Puppet_%28software%29">Puppet (software)</a> Master server</td>
<td>Unofficial</td>
</tr>
</tbody>
</table>
That is why these ports were opened as part of the install/setup process above.

&nbsp;

http://xmodulo.com/2014/08/install-puppet-server-client-centos-rhel.html

&nbsp;
https://docs.puppetlabs.com/pe/latest/agent_cert_regen.html
&nbsp;
https://docs.puppetlabs.com/learning/agentprep.html
https://docs.puppetlabs.com/learning/agent_master_basic.html
&nbsp;
https://docs.puppetlabs.com/puppet/#installing-and-configuring

https://docs.puppetlabs.com/guides/install_puppet/pre_install.html (guide for installing open source puppet.)]]></Content>
		<Date><![CDATA[2014-08-03]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[linux - telnet]]></Title>
		<Content><![CDATA[telnet comes in 2 parts,

&nbsp;

- telnet (client)

- telnet-server (daemon, that will by default listen on port 23)

&nbsp;

http://www.unixmen.com/how-to-install-telnet-in-centos-rhel-scientific-linux-6-4/

&nbsp;

Telnet is also really useful to check if a certain port on a remote machine is open, here's the format:

<pre>
$ telnet {ip number} {port number}
</pre>

For example, to check port 22, we do:

<pre>
telnet 192.168.52.201 22
Trying 192.168.52.201...
Connected to home-directories.
Escape character is '^]'.
SSH-2.0-OpenSSH_6.6.1
</pre>

This shows port 22 is open, whereas if we check a port that is closed, e.g. in my case 21, then we get:

<pre>
[root@openldap-client01 ~]# telnet 192.168.52.201 21
Trying 192.168.52.201...
telnet: connect to address 192.168.52.201: Connection refused
</pre>



&nbsp;

also install nmap:

&nbsp;

yum install nmap

&nbsp;

after that you can check if telnet daemon is listenning on the correct port:

[bash][root@puppetagent1 ~]# nmap localhost

Starting Nmap 5.51 ( http://nmap.org ) at 2014-08-04 02:08 BST
Nmap scan report for localhost (127.0.0.1)
Host is up (0.000019s latency).
Other addresses for localhost (not scanned): 127.0.0.1
Not shown: 995 closed ports
PORT    STATE SERVICE
22/tcp  open  ssh
23/tcp  open  telnet
25/tcp  open  smtp
111/tcp open  rpcbind
631/tcp open  ipp [/bash]

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;]]></Content>
		<Date><![CDATA[2014-08-04]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[telnet]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - subcommands]]></Title>
		<Content><![CDATA[As a linux user, you should be familiar with the regular linux commands such as cd, pwd, ls, cp, mv,...etc. 

However the Puppet software introduces it's own set of commands that you can also run on the linux terminal. For example, node, master and config. However if you try to run them, you get:

<pre>
[root@puppetmaster ~]# node
-bash: node: command not found
[root@puppetmaster ~]# master
-bash: master: command not found
[root@puppetmaster ~]# config
-bash: config: command not found
[root@puppetmaster ~]#
</pre>

Bash hasn't appeared to have recognised these commands and that's because these puppet commands are actually sub-commands of the over-arching "puppet" command. And to run these sub-command, you need to prefix "puppet" i.e. for the node subcommand we do:

<pre>
[root@puppetmaster ~]# puppet node
Error: 'node' has no default action.  See `puppet help node`.
[root@puppetmaster ~]#
</pre>

This time, it looks like bash has recognised the node command, but it failed for a different reason, which is that the node command requires further input (arguments/options) in order for it to run. We'll cover those later. 

If you just run "puppet" on it's own, you'll get a prompt to run the "help" subcommand, so if you follow this advice and run "puppet help", then you'll get a list of all the available puppet subcommands, along with a short description:

<pre>
[root@puppetmaster ~]# puppeet
-bash: puppeet: command not found
[root@puppetmaster ~]# puppet
See 'puppet help' for help on available puppet subcommands
[root@puppetmaster ~]# puppet help

Usage: puppet <subcommand> [options] <action> [options]

Available subcommands:

  agent             The puppet agent daemon
  apply             Apply Puppet manifests locally
  ca                Local Puppet Certificate Authority management.
  catalog           Compile, save, view, and convert catalogs.
  cert              Manage certificates and requests
  certificate       Provide access to the CA for certificate management.
  certificate_request  Manage certificate requests.
  certificate_revocation_list  Manage the list of revoked certificates.
  config            Interact with Puppet's settings.
  describe          Display help about resource types
  device            Manage remote network devices
  doc               Generate Puppet documentation and references
  facts             Retrieve and store facts.
  file              Retrieve and store files in a filebucket
  filebucket        Store and retrieve files in a filebucket
  help              Display Puppet help.
  inspect           Send an inspection report
  instrumentation_data  Manage instrumentation listener accumulated data.
  instrumentation_listener  Manage instrumentation listeners.
  instrumentation_probe  Manage instrumentation probes.
  key               Create, save, and remove certificate keys.
  kick              Remotely control puppet agent
  man               Display Puppet manual pages.
  master            The puppet master daemon
  module            Creates, installs and searches for modules on the Puppet Forge.
  node              View and manage node definitions.
  parser            Interact directly with the parser.
  plugin            Interact with the Puppet plugin system.
  queue             Deprecated queuing daemon for asynchronous storeconfigs
  report            Create, display, and submit reports.
  resource          The resource abstraction layer shell
  resource_type     View classes, defined resource types, and nodes from all manifests.
  secret_agent      Mimics puppet agent.
  status            View puppet server status.

See 'puppet help <subcommand> <action>' for help on a specific subcommand action.
See 'puppet help <subcommand>' for help on a specific subcommand.
Puppet v3.6.2

</pre>



One of the main reasons that puppet has adopted this "puppet {subcommand}" system rather than having each command as a standalone, is so to avoid possible conflict's with Linux's native commands. For example, one of puppet's subcommand is called "man" (see in the list above). If puppet's man command was a standalone alone command (i.e. didn't require the "puppet " prefix) then Linux would get confused on whether you are calling the native man command, or Puppet's man command. Therefore the "puppet {subcommand}" actually offers greater clarity, but at the small price of having to type out "puppet " every time you want to call one of puppet's subcommands. 

We will cover more about each of these commands as we go through this course. 


Reference

<a href="https://docs.puppetlabs.com/puppet/3.7/reference/services_commands.html#full-list-of-subcommands ">Full List of subcommands</a>]]></Content>
		<Date><![CDATA[2014-08-06]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Manifests]]></Title>
		<Content><![CDATA[In Puppet, it is intended that resources are written into files using the resource declaration syntax. In Puppet these files are referred to as "Manifests" and they must end with the ".pp" extension.

&nbsp;

For example let's say we have a couple of resources declared in a manifest file called "CatAndMouse.pp":

<pre>
[root@puppetmaster ~]# pwd
/root
[root@puppetmaster ~]# cat CatAndMouse.pp
user { 'Tom':
  ensure => 'present',
  home   => '/home/Tom',
  shell  => '/bin/bash',
  uid    => '101',
}

user { 'Jerry':
  ensure => 'present',
  home   => '/home/Jerry',
  shell  => '/bin/bash',
  uid    => '102',
}
[root@puppetmaster ~]#
</pre>

Note: The resource's title and the attribute names should always be lowercase.

&nbsp;

We are now going to load this file so that users Tom &amp; Jerry exists on the local machine (where they currently don't exist). To load these into puppet we use the "apply" subcommand, like this:

<pre>
[root@puppetmaster examples]# puppet apply /root/examples/CatAndMouse.pp
Notice: Compiled catalog for puppetmaster.codingbee.dyndns.org in environment production in 0.12 seconds
Notice: /Stage[main]/Main/User[Tom]/ensure: created
Notice: /Stage[main]/Main/User[Jerry]/ensure: created
Notice: Finished catalog run in 0.14 seconds
[root@puppetmaster examples]#
</pre>

Users Tom &amp; Jerry now exists.

&nbsp;

Now if we tried to apply the same manifest again:

<pre>
[root@puppetmaster examples]# puppet apply /root/examples/CatAndMouse.pp
Notice: Compiled catalog for puppetmaster.codingbee.dyndns.org in environment production in 0.12 seconds
Notice: Finished catalog run in 0.04 seconds
[root@puppetmaster examples]#
</pre>

This time nothing happened, that's because the machine is already at it's desired state, i.e. both users exist.

There's a few things you need to remember about puppet:
<ul>
	<li>It only Enforces attributes that are explicitly specified. When we omit attributes, Puppet doesn’t manage them, as a result these attributes take on whatever value that linux initially assigns to it, or whatever value you assign to it. You can change these values and Puppet will not change it</li>
	<li>Each resource type has an attribute whose value is automatically set to match the resource's title. This type of attribute is referred to as "namevars" and they are a little special because Puppet will actively enforce this even if they are not explicitly specified. For that reason, a namevars attribute's value is unique and can be used to identify a resource. Note, a very small number of resource types don't have a namevars attribute.</li>
	<li>If a user makes a change that results in drift, then puppet will undo those changes on the next run, which by default should be within 30 mins.</li>
	<li>Most resource types have an attribute called "ensure", this attribute controls whether a resource exists or not, and if it does exist, then in what form.</li>
</ul>
&nbsp;

Here's another example, but this time we have a manifest called (DummyFile.pp) and it has a resource to create a dummy file:

<pre>
[root@puppetmaster ~]# cat /root/examples/DummyFile.pp
file {'testfile.txt':
        ensure  => present,
        path    => '/tmp/testfile.txt',
        mode    => 0777,
        content => "Hello World \n",
}
notify {"INFO: This is a customised log message":}

[root@puppetmaster examples]# puppet apply DummyFile.pp
Notice: Compiled catalog for puppetmaster.codingbee.dyndns.org in environment production in 0.07 seconds
Notice: INFO This is a custom message
Notice: /Stage[main]/Main/Notify[INFO This is a custom message]/message: defined 'message' as 'INFO This is a custom message'
Notice: Finished catalog run in 0.10 seconds
[root@puppetmaster examples]#
</pre>

Notice here that we used a customised log message using the "notify" statement.
&nbsp;

Now let's confirm that this file now exists:

<pre>
[root@puppetmaster examples]# ls -l /tmp/testfile.txt
-rwxrwxrwx. 1 root root 13 Aug  6 23:20 /tmp/testfile.txt
[root@puppetmaster examples]# cat /tmp/testfile.txt
Hello World
[root@puppetmaster examples]#
</pre>

Now lets try changing the file's permission to 755.:

<pre>
[root@puppetmaster examples]# chmod 755 /tmp/testfile.txt
[root@puppetmaster examples]# ls -l /tmp/testfile.txt
-rwxr-xr-x. 1 root root 13 Aug  6 23:20 /tmp/testfile.txt
[root@puppetmaster examples]#
</pre>

<pre>
[root@puppetmaster examples]# puppet apply DummyFile.pp
Notice: Compiled catalog for puppetmaster.codingbee.dyndns.org in environment production in 0.07 seconds
Notice: /Stage[main]/Main/File[testfile.txt]/mode: mode changed '0755' to '0777'
Notice: INFO This is a custom message
Notice: /Stage[main]/Main/Notify[INFO This is a custom message]/message: defined 'message' as 'INFO This is a custom message'
Notice: Finished catalog run in 0.09 seconds
[root@puppetmaster examples]# ls -l /tmp/testfile.txt
-rwxrwxrwx. 1 root root 13 Aug  6 23:20 /tmp/testfile.txt
[root@puppetmaster examples]#
</pre>

Here you'll notice that puppet has re-enforced the desired permission configuration again. 

In the case of the "file" resource type, the "path" is actually the file resource type's namevar. This means that  the following:

<pre>
[root@puppetmaster ~]# cat /root/examples/DummyFile.pp
file {'/tmp/testfile.txt':
        ensure  => present,
        path    => '/tmp/testfile.txt',
        mode    => 0777,
        content => "Hello World \n",
}
</pre>

Can be simplified by removing the "path" attribute and it's value, i.e. we have:

<pre>
[root@puppetmaster ~]# cat /root/examples/DummyFile.pp
file {'/tmp/testfile.txt':
        ensure  => present,
        mode    => 0777,
        content => "Hello World \n",
}
</pre>

....An the overall result remains the same. 
&nbsp;


In this lesson we applied resource to the local machine. 



Useful links:

https://docs.puppetlabs.com/learning/manifests.html

https://docs.puppetlabs.com/puppet/latest/reference/dirs_manifest.html

&nbsp;

]]></Content>
		<Date><![CDATA[2014-08-08]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Issue catalog to puppet agents (using node definitions)]]></Title>
		<Content><![CDATA[So far we have looked at how to:
<ol>
	<li>how to write a resource in the form of a resource declaration</li>
	<li>store the resource in a manifest file (aka a puppet program)</li>
	<li>Apply the resource to the local machine.</li>
</ol>
&nbsp;

We did all of this on the puppet master and we so far ignored that we also had 2 puppet agents to play with.

However the primary reason for the Puppet master/agent architecture is so that all the puppet agents are managed centrally via the puppet master. To do this we need to use the site.pp file:

<pre>
/etc/puppet/manifests/site.pp
</pre>

Note: this file only resides in the puppet master. 

The puppet master always reads one special manifest, called the “site manifest” or site.pp. It always reads this to compile a catalog, which it sends back to the agent. 

Once the agent receives the catalog, it then applies the resources that are defined in the catalogue.

This means that you can define resources in the site.pp and it will in turn get sent to the puppet agents. We can apply resources to a particalur agent by using the <a href="https://docs.puppetlabs.com/puppet/latest/reference/lang_node_definitions.html">node definitions</a> syntax. 

The first time you open this file, you will find this to be empty. Lets say you enter the following 3 resources:

<pre>
[root@puppetmaster manifests]# cat site.pp
# This will be applied to all agents
file {'AgentTestfileALL.txt':
    ensure  => present,
    path    => '/tmp/AgentTestfileALL.txt',
    mode    => 0777,
    content => "Hello ALL agents \n",
}

node 'PuppetAgent1' {

        file {'AgentTestfile1.txt':
            ensure  => present,
                path    => '/tmp/AgentTestfile1.txt',
                mode    => 0777,
                content => "Hello agent 1 \n",
        }
}

node 'PuppetAgent2' {

        file {'AgentTestfile2.txt':
            ensure  => present,
                path    => '/tmp/AgentTestfile2.txt',
                mode    => 0777,
                content => "Hello agent 2 \n",
        }
}
</pre>


This manifest indicates that the first resource should get applied to all agent's whereas the 2nd and 3rd are encased within a node definition. 


&nbsp;
Now for each of my agent's I'll manually trigger an update, for puppetagent1, we have:

<pre>
[root@puppetagent1 ssl]# puppet agent --test
Info: Caching certificate for puppetagent1.codingbee.dyndns.org
Info: Caching certificate_revocation_list for ca
Info: Caching certificate for puppetagent1.codingbee.dyndns.org
Info: Retrieving pluginfacts
Info: Retrieving plugin
Info: Caching catalog for puppetagent1.codingbee.dyndns.org
Info: Applying configuration version '1407806502'
Notice: /Stage[main]/Main/Node[puppetagent1]/File[AgentTestfile1.txt]/ensure: created
Notice: /Stage[main]/Main/File[AgentTestfileALL.txt]/ensure: created
Notice: Finished catalog run in 0.08 seconds
[root@puppetagent1 ssl]#
</pre>

And for Puppetagent2, we have:

<pre>
[root@puppetagent2 sysconfig]# puppet agent --test
Info: Caching certificate for puppetagent2.codingbee.dyndns.org
Info: Caching certificate_revocation_list for ca
Info: Caching certificate for puppetagent2.codingbee.dyndns.org
Info: Retrieving pluginfacts
Info: Retrieving plugin
Info: Caching catalog for puppetagent2.codingbee.dyndns.org
Info: Applying configuration version '1407806502'
Notice: /Stage[main]/Main/Node[puppetagent2]/File[AgentTestfile2.txt]/ensure: created
Notice: /Stage[main]/Main/File[AgentTestfileALL.txt]/ensure: created
Info: Creating state file /var/lib/puppet/state/state.yaml
Notice: Finished catalog run in 0.10 seconds
[root@puppetagent2 sysconfig]# 
</pre>


As you can see, this approach is going to result in a really long site.pp file. The first probelm 
&nbsp;
]]></Content>
		<Date><![CDATA[2014-08-12]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Regenerating certificates]]></Title>
		<Content><![CDATA[https://docs.puppetlabs.com/puppet/latest/reference/ssl_regenerate_certificates.html
on agent do:

<pre>
puppet config print ssldir   # this should output something like:
</pre>

We'll assume for the rest of this article that the above outputs:

<pre>
/var/lib/puppet/ssl 
</pre>

In this directory, go view what files it has:


<pre>

[root@puppetmaster ssl]# find . -name puppetmaster*
./public_keys/puppetmaster.*.pem
./certificate_requests/puppetmaster.*.pem
./certs/puppetmaster.*.pem
./private_keys/puppetmaster.*.pem
./ca/signed/puppetmaster.*.pem
[root@puppetmaster ssl]# 
</pre>

Run:

<pre>
puppet cert clean {agent's fqdn}         # note the "-all" option doesn't work. 
</pre>

this should then result in:

<pre>
[root@puppetmaster ssl]# find . -name puppetmaster*
[root@puppetmaster ssl]# 

</pre>

As you can see, nothing found this time. 

Now on the agent do:

<pre>
puppet agent -t --trace --debug
</pre>

This command will outpupwdt a message like:

<pre>
Exiting; no certificate found and waitforcert is disabled
</pre>

Now on the puppetmaster, you should the following files have been regenerated:

<pre>
[root@puppetmaster ssl]# pwd
/var/lib/puppet/ssl
[root@puppetmaster ssl]# find . -name puppetmaster*
./public_keys/puppetmaster.ordsvy.gov.uk.pem
./certificate_requests/puppetmaster.fqdn.pem
./private_keys/puppetmaster.fqdn.pem
./ca/requests/puppetmaster.fqdn.pem
[root@puppetmaster ssl]# 

</pre>

Now ensure that we sign. 

To sign this certificate do:

<pre>
puppet cert sign {agent's fqdn} 
</pre>


This should now result in:


<pre>
[root@puppetmaster ssl]# pwd
/var/lib/puppet/ssl
[root@puppetmaster ssl]# find . -name puppetmaster*
./public_keys/puppetmaster.ordsvy.gov.uk.pem
./certificate_requests/puppetmaster.ordsvy.gov.uk.pem
./private_keys/puppetmaster.ordsvy.gov.uk.pem
./ca/signed/puppetmaster.ordsvy.gov.uk.pem
[root@puppetmaster ssl]# 

</pre>



https://docs.puppetlabs.com/references/3.5.1/man/cert.html




on master do:

<pre>puppet cert clean {agent's fqdn}</pre>

then you can redo <a href="http://codingbee.net/tutorials/puppet/puppet-crash-course-setting-masteragent-relationship/" title="Puppet – Install/set-up the Puppet Master and it’s Agents">part c</a>]]></Content>
		<Date><![CDATA[2014-08-12]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Ordering your resources]]></Title>
		<Content><![CDATA[In Puppet, the resources in a manifest can potentially be applied in any order, rather than in the order it has been specified in the manifest. One way round this is to insert the following:

ordering = manifest 

in the puppet.conf file. This is a good idea to do. On top of that you can override this with the use of special attributes called <strong>metaparameters </strong>which you can embed within your resource declarations.  


&nbsp;

The 4 main metaparameters for controlling the order that resources are:

<ul>
	<li><code>before</code></li>
	<li><code>require</code></li>
	<li><code>notify</code></li>
	<li><code>subscribe</code></li>
</ul>

Note: there is a also a resource type called "notify". But here we are referring to the notify metaparameter which is a different thing. 

These 4 are similar and can be paired up like this:
<ul>
	<li>before and require</li>
	<li>notifiy and subscribe</li>
</ul>
&nbsp;
<h2>"Before" and "Require"</h2>
Here is the "before" attribute in action:

<pre> 
file {'/tmp/test1':
      ensure  =&gt; present,
      content =&gt; "Hi.",
 }

 notify {'/tmp/test1 has already been synced.':
      <strong style="color:red">require =&gt; File['/tmp/test1'], </strong>                  # Note, you must capitalise, i.e. "File" and not "file"
 }
</pre>
The above says that the "notify" resource requires the "/tmp/test1" resource to be applied first before it itself can be applied.

The above can also be rewritten using the "before" metaparameter to:

&nbsp;
<pre> file {'/tmp/test1':
      ensure  =&gt; present,
      content =&gt; "Hi.",
      <strong style="color:red">before  =&gt; Notify['/tmp/test1 has already been synced.'],</strong> # Again "Notify" and not "notify"

 } 

 notify {'/tmp/test1 has already been synced.':}

</pre>
&nbsp;

Here we are saying that the '/tmp/test1' file resource has to be applied before the notify resource.

As you can see we have done some capitalisation. The rule is that you <em>only</em> type in lowercase (when writing the resrouce's type) when <em>declaring a new resource.</em> Any other situation will always call for writing in uppercase.

&nbsp;

&nbsp;
<h2>"Subscribe" and "Notify"</h2>
These are similar to require/before, in terms of ordering your resources. However it also has an extra "notification" feature which is used to "refresh" a resource.

This is useful if you make changes to a config file and then want to restart (aka refresh) the corresponding service so that it loads the new settings into memory.

In this situation puppet is smart enough to do resource refresh. E.g. if you have declared a service resource's "ensure" attribute set as "running" then if that resource get's applied by subscribe/notify, then it will restart itself. Here's an example:

&nbsp;
<pre>
file { '/etc/ssh/sshd_config':
      ensure =&gt; file,
      mode   =&gt; 600,
      source =&gt; 'puppet:///modules/ssh/sshd_config',
    }
service { 'sshd':
      ensure    =&gt; running,
      enable    =&gt; true,
      subscribe =&gt; File['/etc/ssh/sshd_config'],
    }
</pre>
In this example, the sshd service will be restarted if Puppet has to edit its config file, called "sshd_config".

Here we used the "subscribe" attribute. But we can also achieve the same outcome using the notify attribute:

&nbsp;
<pre>file { '/etc/ssh/sshd_config':
      ensure =&gt; file,
      mode   =&gt; 600,
      source =&gt; 'puppet:///modules/ssh/sshd_config',
      <strong style="color:red">notify =&gt; Service['sshd'],</strong>
    }
service { 'sshd':
      ensure    =&gt; running,
      enable    =&gt; true,

    }
</pre>
&nbsp;

&nbsp;
<h2>Chaining Arrows</h2>
Both the before+after and notify+subscribe relationships can be written in short-hand form with the help of the "chaining arrows" syntax. Chaining arrows also makes your code easier to read.

&nbsp;

So far we have looked at 2 ways to represent the same ordering. One using the "Before" parameter and the other using the "after" parameter. However there are 2 more ways, but this time using chaining arrows "-&gt;"

&nbsp;
<pre>
 file {'/tmp/test1':
      ensure  => present,
      content => "Hi.",
 }

 notify {'after':
      message => '/tmp/test1 has already been synced.',
 }
 
 File['/tmp/test1'] -> Notify['after']           # here we used "->" to show the order. 
</pre>
This is a lot easier to to read, and doesn't require you to user the before/after metaparameters. Another even simpler way to write the above is:

<pre> 
file {'/tmp/test1':
      ensure  =&gt; present,
      content =&gt; "Hi.",
}
 <strong style="color:red">-&gt;</strong>
notify {'after':
      message =&gt; '/tmp/test1 has already been synced.',
}
</pre>
&nbsp;

We can do the same thing with subscribe/notify, but using "~&gt;" instead of "-&gt;".


<h2>A Common Scenario</h2>
&nbsp;
You often find yourself performing a the following sequence of tasks:

<ul>
	<li>install package</li>
	<li>update config files</li>
	<li>start service</li>
</ul>


In thi this situation you use the subscribe/notify to help sequence these tasks. 

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;
<h2 id="autorequire">Autorequire</h2>
Sometimes puppet is smart enough to realise an order even if it hasn't been explicitly specified. for example you have two resources, one of them creates a folder, and the other creates a file within that folder. In this situation puppet is smart enough to apply the folder resource followed by the file resource.

However it is best practice to not rely on autorequire and just specify the order explicitly.

&nbsp;

&nbsp;
<h2></h2>
&nbsp;

https://docs.puppetlabs.com/learning/ordering.html

&nbsp;

https://docs.puppetlabs.com/puppet/latest/reference/lang_relationships.html]]></Content>
		<Date><![CDATA[2014-08-13]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Variables, If-statements, and Environment variables]]></Title>
		<Content><![CDATA[https://docs.puppetlabs.com/learning/variables.html

&nbsp;

https://docs.puppetlabs.com/puppet/latest/reference/lang_variables.html#facts-and-built-in-variables

&nbsp;

https://docs.puppetlabs.com/puppet/latest/reference/lang_facts_and_builtin_vars.html

&nbsp;

&nbsp;

Complete list of environment variables (aka "FACTS"): <a href="https://docs.puppetlabs.com/facter/latest/core_facts.html">https://docs.puppetlabs.com/facter/latest/core_facts.html</a>

"facter" is a standalone package. This means that it isn't a puppet subcommand but is actually a command in it's own right.

If you you:

&nbsp;

[bash]# facter[/bash]

then it will output a list of all environment variable (similar to linux's "env" command).

&nbsp;

&nbsp;

In the selectors example it showed:

&nbsp;
<pre>
$apache = $operatingsystem ? {
      centos                => 'httpd',
      redhat                => 'httpd',
      /(?i)(ubuntu|debian)/ => 'apache2',
      default               => undef,
    }
</pre>
Here we are trying to determine what value that "$apache" can take. This depends on the value of the $operatingsystem (which is a builtin variable, aka fact).

&nbsp;

The third line shows "/...../". This is actually a regex. The  "?i" is acutally a regex switch that <a href="http://www.regular-expressions.info/modifiers.html">enables case-sensitivity</a>, a bit like the "-i" switch for bash's grep command.

&nbsp;

&nbsp;]]></Content>
		<Date><![CDATA[2014-08-13]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Regular expressions]]></Title>
		<Content><![CDATA[https://docs.puppetlabs.com/puppet/latest/reference/lang_datatypes.html#regular-expressions

&nbsp;

&nbsp;

In bash you encase regular expressions in double-quotes. However in puppet you encase them in forward slashes. I.e. this is wrong:

&nbsp;

"regex"

&nbsp;

but this is correct:

&nbsp;

/regex/

This is the <a href="http://en.wikipedia.org/wiki/Regular_expression#Syntax">perl syntax of regex</a>]]></Content>
		<Date><![CDATA[2014-08-13]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Using double colon (::) notation]]></Title>
		<Content><![CDATA[Every variable/class has 2 names, a shortname and a Fully qualified name.

&nbsp;

the "::" drills down.

&nbsp;

https://docs.puppetlabs.com/learning/variables.html#variables

&nbsp;

&nbsp;

&nbsp;

https://docs.puppetlabs.com/learning/modules1.html

&nbsp;

&nbsp;

&nbsp;]]></Content>
		<Date><![CDATA[2014-08-13]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[SCCM - Roles]]></Title>
		<Content><![CDATA[SCCM has  19 "<a href="http://eskonr.com/2012/01/configmgrsccm-2012-site-system-roles/">roles</a>" available:

<strong>Application Catalog Web service point</strong> :A site system role that serves as application catalog web services point

<strong>Application Catalog Website</strong> <strong>Point</strong>:A site system role that serves as an application catalog website point

<span style="color: #008000;">(The above 2 Application Catalog gives the client-machine users the ability to install their own software via a website, a bit like an "App Portal/Online-shop, where user search and select the software they want and add it to their "basket", they then "checkout", after that the software get's installed in the background while the user is still using the client machine. An email is then issued to the user once the install is complete and the software is ready to use.)</span>

<strong>Asset Intelligence Synchnozation Point :</strong>A site system role that connects to system center online to download asset intelligence catalog information and upload uncategorized titles that can be considered for future inclusion in the catalog

<strong>Component Server</strong> :Any server that runs the SMS Executive service. A server that runs Configuration Manager services. When you install all the site system roles except for the distribution point role, Configuration Manager automatically installs the component server.

<strong>Distribution Point  <span style="color: #008000;">(commonly used)</span></strong> :A configuration manager role that stages packages for Distribution to clients. <span style="color: #008000;">This is essentially a file-server. </span> <span style="color: #008000;">This is useful if you want to deploy a really big package (e.g. ms office 2012) to hundreds of clients. you can set up multiple Distribution Point servers for clients to download from, rather than a single DP server which could get overwhelmed. </span>

<strong>End Point Protection Point</strong> :A site system role that serves as end point protection

<strong>Enrollment Point :</strong>A site system role that enables enrollment for mobile devices and AMT functionality

<strong>Enrollment Proxy Point:</strong>A site system role that communicates with mobile devices during enrollment

<strong>Fallback Status Point:</strong>A site system role that receives messages from configmgr clients that can not communicate with their management point

<strong>Management Point <span style="color: #008000;">(commonly used) </span></strong>: A site system role that replies to configuration Manager Clients requests and accepts management data from configuration manager clients. <span style="color: #008000;">This roles makes it possible for all your clients to communicate with your sccm infrastructure. It is where clients download their policy, deploy agents to clients. </span>

<span style="color: #008000;">Mobile Device Management Point</span>

<strong>Out of band service point : </strong>A site system role that provisions and configures Intel AMT-based computers for out of band management.

<strong>Reporting services Point<span style="color: #008000;"> (commonly used)</span></strong> :A site system role that provides integration with SQL server reporting services to create and manager reports for configuration manager

<strong>Site Database Server :</strong>A site system role that runs Microsoft SQL server and hosts the configuration Manager Site Database

<strong>Site Server <span style="color: #008000;">(commonly used)</span></strong>: A computer on which you run the Configuration Manager setup program and which provides the core functionality for the site. <span style="color: #008000;">This software that runs on this machine requires a db to work, which is provided by machine that performs the "Site Database Server" role. Hence "Site Server" and "Site Database Server" goes hand in hand. </span>

<strong>Site System <strong><span style="color: #008000;">(commonly used)</span></strong></strong>: A server or server Share that hosts one or more site system roles for Configuration Manager site. <span style="color: #008000;">Any machine that performs one of these 19 roles is described as a "site server"</span>

<strong>Software update point :</strong>A site system role that runs Microsoft widows server update services and allows configuration manager to use the WSUS catalog to scan configuration manager clients for software updates

<strong>State migration Point :</strong>A site system role that store user state and settings migrated during the Operating system deployment

<strong>System health Validator Point</strong> <span style="color: #008000;">(rarely used)</span>:settings for all system health validator point in their site such as statement of health validation and configuration manager health state references.

&nbsp;

&nbsp;]]></Content>
		<Date><![CDATA[2014-08-16]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[SCCM]]></Tags>
		<Status><![CDATA[draft]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[WIX - What is Windows Installer XML]]></Title>
		<Content><![CDATA[The Windows Installer XML is  a powerful utility for packaging a softare into an MSI file.

&nbsp;

If you have a software that you want make to available to others, so that they can install it on their own Windows Machine, then Microsoft recommends that you first package up your software into a single MSI file before distributing it.

Windows Installer XML (WIX) is a free open-source tool that lets you package your software into an MSI file. To create an MSI using wix, all you essentially need to do is create an xml file then feed that xml file into WIX along with all your software's source/binary files. WIX will then handle the rest and generate the MSI file for you.

Once the MSI file has been created, you can then install the software by:
<ol>
	<li>Double clicking on the msi file -   an install wizard pop-up will appear prompting the user for more info</li>
	<li>Run msi file from the command line - this method is often used if you want to install the software silently. With this option you can provide all the info the windows installer needs in a single command and thereby suppressing any install wizard windows popping up.  Hence this approach is really useful if you want to automate software installations.</li>
</ol>
WIX is used from the command line only. However WIX is also integrated in a lot of big name IDE's such as Visual Studio.

Here are some of benefits of packaging your software into an MSI file:
<ul>
	<li>All your software files are can be packaged into one convenient .msi file.</li>
	<li>Your software will automatically be registered with Progam and Features</li>
	<li>Windows can easily uninstall your software should the user select the uninstall option in Program and Features</li>
	<li>If any of your software's file are accidently removed, then this can be fixed by right-clicking on the MSI file and select repair.</li>
	<li>You can create msi file for different versions of your software and the msi package can detect which version has been installed.</li>
	<li>you can create patches to update only specific areas of your application.</li>
	<li>If something goes wrong during the isntall process, then the windows installer can easily roll back to a previous state.</li>
	<li>You can create Install Wizard pop windows to guide user's through the install process.</li>
</ul>
All of these benefits comes included as part of offering your software in the form of an MSI package. There are lots of <a href="http://en.wikipedia.org/wiki/Windows_Installer#Tools">MSI creation tools</a> out there, but WIX has emerged as one of the market leaders in this space.

&nbsp;

Before you can use WIX, you first have to <a href="http://wixtoolset.org/">install wix</a>.

&nbsp;

After the install, the wix install folder should contain the following exe-based commandline utilities:

[powershell]
PS C:\Program Files (x86)\WiX Toolset v3.8\bin&amp;gt; ls | where-Object -f {$_.name -match &amp;quot;.exe
$&amp;quot;}

    Directory: C:\Program Files (x86)\WiX Toolset v3.8\bin

Mode                LastWriteTime     Length Name
----                -------------     ------ ----
-a---        28/11/2013     05:13      28672 candle.exe
-a---        28/11/2013     05:14      28672 dark.exe
-a---        28/11/2013     05:14      28672 heat.exe
-a---        28/11/2013     05:14      24576 insignia.exe
-a---        28/11/2013     05:13      32768 light.exe
-a---        28/11/2013     05:14      28672 lit.exe
-a---        28/11/2013     05:16      32768 lux.exe
-a---        28/11/2013     05:14      28672 melt.exe
-a---        28/11/2013     05:16      28672 nit.exe
-a---        28/11/2013     05:14      32768 pyro.exe
-a---        28/11/2013     05:14      49152 shine.exe
-a---        28/11/2013     05:14      28672 smoke.exe
-a---        28/11/2013     05:14     109056 ThmViewer.exe
-a---        28/11/2013     05:14      32768 torch.exe
-a---        28/11/2013     05:14      86016 WixCop.exe
[/powershell]

You can read up about these <a href="http://wixtoolset.org/documentation/manual/v3/overview/">wix tools</a>.

When creating an xml, you need to provide guid values. An easy way to do this is just use <a href="http://wixtoolset.org/documentation/manual/v3/howtos/general/generate_guids.html">the online guid generation tool</a>. This page also says wix can autogenerate this using *, this might be the better approach in fast moving continuous integration. Although when you do official release then you should generate the guid using the online tool and then hard code them in.

Here's what a typical main wix source xml file looks like (note this file has the extensiont ".wxs")

[xml]
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;Wix xmlns=&quot;http://schemas.microsoft.com/wix/2006/wi&quot;&gt;
    &lt;Product Id=&quot;3E786878-358D-43AD-82D1-1435ADF9F6EA&quot; Name=&quot;Awesome Software&quot; Language=&quot;1033&quot; Version=&quot;1.0.0.0&quot; Manufacturer=&quot;Awesome Company&quot; UpgradeCode=&quot;B414C827-8D81-4B4A-B3B6-338C06DE3A11&quot;&gt;
        &lt;Package InstallerVersion=&quot;301&quot; Compressed=&quot;yes&quot; InstallScope=&quot;perMachine&quot; Manufacturer=&quot;Awesome Company&quot; Description=&quot;Installs Awesome Software&quot; Keywords=&quot;Practice,Installer,MSI&quot; Comments=&quot;(c) 2012 Awesome Company&quot; /&gt;
        &lt;MediaTemplate EmbedCab=&quot;yes&quot; /&gt;
        &lt;!--Directory structure--&gt;

        &lt;Directory Id=&quot;TARGETDIR&quot; Name=&quot;SourceDir&quot;&gt;    &lt;!-- This if hardcoded in: http://stackoverflow.com/questions/1641094/in-wix-files-what-does-name-sourcedir-refer-to --&gt;
            &lt;Directory Id=&quot;ProgramFilesFolder&quot;&gt;    &lt;!--&quot;ProgramsFilesFolder&quot; is a built-in property: http://msdn.microsoft.com/en-us/library/aa370905%28v=vs.85%29.aspx#system_folder_properties -- also you can specify whether it is 64bit or x86, from the command line using the candle.exe's &quot;arch&quot; switch --&gt;
                &lt;Directory Id=&quot;MyProgramDir&quot; Name=&quot;Awesome Software&quot; /&gt; &lt;!-this &quot;directory&quot; element will create a new folder called &quot;Awesome Software&quot;, see: http://wixtoolset.org/documentation/manual/v3/xsd/wix/directory.html---&gt;
                &lt;Directory Id=&quot;ProgramMenuFolder&quot;&gt;
                    &lt;Directory Id=&quot;MyShortcutsDir&quot; Name=&quot;Awesome Software&quot; /&gt;
                &lt;/Directory&gt;
            &lt;/Directory&gt;
        &lt;/Directory&gt;
        &lt;!--Components--&gt;
        &lt;DirectoryRef Id=&quot;MyProgramDir&quot;&gt;
            &lt;Component Id=&quot;CMP_InstallMeTXT&quot; Guid=&quot;E8A58B7B-F031-4548-9BDD-7A6796C8460D&quot;&gt;
                &lt;File Id=&quot;FILE_InstallMeTXT&quot; Source=&quot;InstallMe.txt&quot; KeyPath=&quot;yes&quot; /&gt;
            &lt;/Component&gt;
        &lt;/DirectoryRef&gt;
        &lt;!--Start Menu Shortcuts--&gt;
        &lt;DirectoryRef Id=&quot;MyShortcutsDir&quot;&gt;
            &lt;Component Id=&quot;CMP_DocumentationShortcut&quot; Guid=&quot;33741C82-30BF-41AF-8246-44A5DCFCF953&quot;&gt;
                &lt;Shortcut Id=&quot;DocumentationStartMenuShortcut&quot; Name=&quot;Awesome Software Documentation&quot; Description=&quot;Read Awesome Software Documentation&quot; Target=&quot;[MyProgramDir]InstallMe.txt&quot; /&gt;
                &lt;Shortcut Id=&quot;UninstallShortcut&quot; Name=&quot;Uninstall Awesome Software&quot; Description=&quot;Uninstalls Awesome Software&quot; Target=&quot;[System64Folder]msiexec.exe&quot; Arguments=&quot;/x [ProductCode]&quot; /&gt;
                &lt;RemoveFolder Id=&quot;RemoveMyShortcutsDir&quot; On=&quot;uninstall&quot; /&gt;
                &lt;RegistryValue Root=&quot;HKCU&quot; Key=&quot;Software\Microsoft\AwesomeSoftware&quot; Name=&quot;installed&quot; Type=&quot;integer&quot; Value=&quot;1&quot; KeyPath=&quot;yes&quot; /&gt;
            &lt;/Component&gt;
        &lt;/DirectoryRef&gt;
        &lt;!--Features--&gt;
        &lt;Feature Id=&quot;ProductFeature&quot; Title=&quot;Main Product&quot; Level=&quot;1&quot;&gt; &lt;!-- Note all components must have a corresponding feature --&gt;
            &lt;ComponentRef Id=&quot;CMP_InstallMeTXT&quot; /&gt;
            &lt;ComponentRef Id=&quot;CMP_DocumentationShortcut&quot; /&gt;
        &lt;/Feature&gt;
    &lt;/Product&gt;
&lt;/Wix&gt;
[/xml]

All these elements, e.g. "directory element" are described in the manual. Just do word find (ctrl+f) on this page and search for "wix schema". the childs of this node covers all this.

Also the <a href="http://wixtoolset.org/documentation/manual/v3/">official wix documentation</a> has lots of hand info.

Once you have created wxs file, place this in the same folder as your source code. Open up powershell and cd to the location of your wxs file. Then run the following command:

[powershell]

PS C:\Users\SChowdhury\Desktop\DummyApp&gt; ls

    Directory: C:\Users\SChowdhury\Desktop\DummyApp

Mode                LastWriteTime     Length Name
----                -------------     ------ ----
-a---        26/08/2014     08:56         26 DummyDLLFile.txt
-a---        26/08/2014     09:15       1155 Product.wxs

PS C:\Users\SChowdhury\Desktop\DummyApp&gt; &amp; 'C:\Program Files (x86)\WiX Toolset v3.8\bin\candle.exe' .\Product.wxs
Windows Installer XML Toolset Compiler version 3.8.1128.0
Copyright (c) Outercurve Foundation. All rights reserved.

Product.wxs
PS C:\Users\SChowdhury\Desktop\DummyApp&gt; ls

    Directory: C:\Users\SChowdhury\Desktop\DummyApp

Mode                LastWriteTime     Length Name
----                -------------     ------ ----
-a---        26/08/2014     08:56         26 DummyDLLFile.txt
-a---        26/08/2014     12:01       6205 Product.wixobj      # new file created
-a---        26/08/2014     09:15       1155 Product.wxs

PS C:\Users\SChowdhury\Desktop\DummyApp&gt;
[/powershell]

The candle.exe will even create the wixobj file even if it encounters minor errors. If you want candle.exe to stop as soon as it encounters any small error's then use the "pedantic" option:

[powershell]
PS C:\Users\SChowdhury\Desktop\DummyApp&gt; &amp; 'C:\Program Files (x86)\WiX Toolset v3.8\bin\candle.exe' .\Product.wxs -pedantic
Windows Installer XML Toolset Compiler version 3.8.1128.0
Copyright (c) Outercurve Foundation. All rights reserved.

Product.wxs
C:\Users\SChowdhury\Desktop\DummyApp\Product.wxs(3) : error CNDL0087 : The Product/@Id attribute's value, '774cdf65-1c3a-4650-8868-b4c090e74c5a', is a mixed-case guid.  All letters in a guid value should be uppercase.
C:\Users\SChowdhury\Desktop\DummyApp\Product.wxs(3) : error CNDL0087 : The Product/@UpgradeCode attribute's value, '478ebf08-ca88-4e3b-9460-2db6f6584e6e', is a mixed-case guid.  All letters in a guid value should be uppercase.
PS C:\Users\SChowdhury\Desktop\DummyApp&gt;
[/powershell]

This time no files have been generated.
http://msdn.microsoft.com/en-us/magazine/cc163456.aspx

http://en.wikipedia.org/wiki/List_of_build_automation_software
&nbsp;

&nbsp;

http://wixtoolset.org/

&nbsp;

http://wix.tramontana.co.hu/tutorial

&nbsp;

http://en.wikipedia.org/wiki/Windows_Installer

&nbsp;

Puppet's "<a href="https://docs.puppetlabs.com/references/latest/type.html#package">package</a>" resource type also handle msi natively.

Windows Installer documentation: http://msdn.microsoft.com/en-us/library/cc185688

Alternatives to WIX - http://en.wikipedia.org/wiki/List_of_installation_software]]></Content>
		<Date><![CDATA[2014-08-25]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[msi|wix]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>WIX]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Modules]]></Title>
		<Content><![CDATA[In the previous tutorial, we saw that a module is essentially folder, where it's structure and contents follows a set of rules. If you follow all these rules, then puppet will "autoload" the module, and hence automtically make the class residing in the init.pp file already available to be called from the site.pp file. 

We only created a barebone module, but a proper module can have other folders and files as you add more features to your module. (for example called "ntp") has the following following structure: 
<pre>
/etc/puppet/module/        # all modules are stored in this directory (which is called the <a href="https://docs.puppetlabs.com/learning/modules1.html#the-modulepath">modulepath</a>)
                |
                +-ntp/     # Rule: the module's main folder must be named after the module itself.  
                   |
                   +-files/      # This contains a bunch of static filee, which can be downloaded by puppet agents.
                   --templates/  # This contains templates that are used by a module’s manifest.
                   --lib/        # This contains plugins like custom facts and custom resource types.
                   --tests/      # This contains examples showing how to declare the module’s classes and defined types.
                   --spec/       # This contains spec tests for any plugins in the lib folder.
                   --manifests/  # this houses all your manifests. You can have any number of manifest files.  
                        |
                        +- init.pp   # rule: all modules should have a manifest called init.pp
                             |
                             +- class ntp   # rule: all init.pp must have only one class, and this class 
                                                 # must be named after the module itself. 
                        +- manifest1.pp
                        +- manifest2.pp
                        +- manifest3.pp  # ....and etc. You need to follow some extra conventions 
                                             # when dealing with other manifests. Cover below. 

 
 
</pre>



Therefore to follow this structure in our "user_account" example, we need to start by creating the other folders:

<pre>
[root@puppetmaster user_account]# pwd
/etc/puppet/modules/user_account
[root@puppetmaster user_account]# ls
manifests
[root@puppetmaster user_account]# mkdir files templates lib spec
[root@puppetmaster user_account]# cd ..
[root@puppetmaster modules]# pwd
/etc/puppet/modules
[root@puppetmaster modules]# tree .
.
└── user_account
    ├── files
    ├── lib
    ├── manifests
    │   └── init.pp
    ├── spec
    └── templates

6 directories, 1 file
[root@puppetmaster modules]#
</pre> 

We will cover about what goes into the other folders later. 

However for now the key thing to remember that several rules/conventions needs to be followed in order to avoid confusion and for puppet to properly autoload the module:

<ul>
	<li>Rule - The init.pp must contain a single class definition, the class's name must be the same as the module's name. </li>

	<li>Best Practice - Each manifest in a module should contain only one class.</li>
	<li>Rule - aside from the init.pp manifest, which is special, all other manifests in the module must be named after the class's name that it contains. E.g. a manifest called "group_account.pp" must contain a class called "group_account"</li>
	<li>rule - when calling another class in the module, i.e. not the one in init.pp, then you need to use the class's FQDN. </li>
	<li>Rule - Modules have to be stored in one of the approved directories, as specified by Puppet's "modulepath" setting. You can view the approved direcotories by using "puppet config print modulepath" or a better yet, get the <a href="https://docs.puppetlabs.com/learning/modules1.html#aside-configprint">setting the puppet master will use</a>. </li>

</ul>


When creating a new module, you have to create all the folders/files from scratch, and then leave the folders as placeholders for later use, if you need them. It can be a bit tedious/error-prone to create all these folders manually. Luckily Puppet can generate this for you using the following the <code>puppet module generate username-modulename</code> command. This command will prompt you for some information (in order to create the metadata.json file) in which case you can accept the defaults for now:

<pre>
[root@puppetmaster modules]# puppet module generate codingbee-dummymodule

We need to create a metadata.json file for this module.  Please answer the
following questions; if the question is not applicable to this module, feel free
to leave it blank.

Puppet uses Semantic Versioning (semver.org) to version modules.
What version is this module?  [0.1.0]
--> 

Who wrote this module?  [codingbee]
--> 

What license does this module code fall under?  [Apache 2.0]
--> 

How would you describe this module in a single sentence?
--> 

Where is this module's source code repository?
--> 

Where can others go to learn more about this module?
--> 

Where can others go to file issues about this module?
--> 

----------------------------------------
{
  "name": "codingbee-dummymodule",
  "version": "0.1.0",
  "author": "codingbee",
  "summary": null,
  "license": "Apache 2.0",
  "source": "",
  "project_page": null,
  "issues_url": null,
  "dependencies": [
    {"name":"puppetlabs-stdlib","version_requirement":">= 1.0.0"}
  ]
}
----------------------------------------

About to generate this metadata; continue? [n/Y]
--> y

Notice: Generating module at /etc/puppet/modules/codingbee-dummymodule...
Notice: Populating templates...
Finished; module generated in codingbee-dummymodule.
codingbee-dummymodule/tests
codingbee-dummymodule/tests/init.pp
codingbee-dummymodule/spec
codingbee-dummymodule/spec/spec_helper.rb
codingbee-dummymodule/spec/classes
codingbee-dummymodule/spec/classes/init_spec.rb
codingbee-dummymodule/README.md
codingbee-dummymodule/Rakefile
codingbee-dummymodule/manifests
codingbee-dummymodule/manifests/init.pp
codingbee-dummymodule/metadata.json
codingbee-dummymodule/Gemfile
[root@puppetmaster modules]#

</pre>

Note: you have to run this command while in the module's folder. 
Note: you have to pass the module's name as a "authorname-modulename" construct. This is so that you can ensure your module has a unique name if you decide to share your module on <a href="https://forge.puppetlabs.com/">puppetforge</a>. Hence your puppetforge account's username is what you start your module's name with. 

After that it creates the following:



<pre>
[root@puppetmaster modules]# cd codingbee-dummymodule/
[root@puppetmaster codingbee-dummymodule]# tree -a
.
├── Gemfile
├── manifests
│   └── init.pp
├── metadata.json
├── Rakefile
├── README.md
├── spec
│   ├── classes
│   │   └── init_spec.rb
│   └── spec_helper.rb
└── tests
    └── init.pp

4 directories, 8 files
[root@puppetmaster codingbee-dummymodule]# 
</pre>






<strong>See Also:</strong>
https://docs.puppetlabs.com/learning/modules1.html

https://docs.puppetlabs.com/puppet/latest/reference/lang_namespaces.html

https://docs.puppetlabs.com/learning/modules1.html



<a href="http://docs.puppetlabs.com/puppet/latest/reference/modules_fundamentals.html">Module Fundamentals</a>

http://puppetlabs.com/blog/best-practices-building-puppet-modules

]]></Content>
		<Date><![CDATA[2014-08-22]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[WIX - A "Hello World" example (via the command line)]]></Title>
		<Content><![CDATA[Creating an msi, you need 3 things

&nbsp;

&nbsp;

I am now going to create a very simple MSI that will install a software that I'm going to call "<strong>CodingBee</strong>". This MSI will:
<ol>
	<li>Create a folder called "<strong>CodingBee</strong>" which will be under "c:\program files" folder</li>
	<li>Place a single text file in this folder, called <strong>HelloWorld.txt. </strong>This file only contains 1 line which says "Hello World"</li>
</ol>
&nbsp;

Here's my approach:

&nbsp;
<ol>
	<li>Create a folder called "CodingBeeFolder" in a convienent location (in my case I've created on my deskop)</li>
	<li>Create the <strong>HelloWorld.txt</strong> file and save it inside  the CodingBeeFolder folder.</li>
	<li>Create the wxs file, which I have called CodingBeeXML.wxs, and in the file, I inserted the following xml:</li>
	<li>Open up a powershell terminal and cd into CodingBeeFolder.  At this stage if you view this folder's content, you should see:</li>
	<li>Now, while you are still in the CodingBeeFolder, run the following command:</li>
	<li>Check that running this command has resulted in the "Product.wixobj" file being generated. the wixobj an intermediary file that get's produced as part of the MSI file generating process.</li>
	<li>Run the following (light.exe) command:</li>
	<li>This should have now generated the</li>
</ol>]]></Content>
		<Date><![CDATA[2014-08-26]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[msi|wix]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>WIX]]></Categories>
	</post>
	<post>
		<Title><![CDATA[WIX - How to use WIX Extensions]]></Title>
		<Content><![CDATA[In the wxs file, you have bunch of elements out of the box, e.g. component, directory,....etc. However there are other elements which you can use, by loading in some extensions. You have to load the extensions in when running the light.exe command, using the -ext option. A bunch of extensions comes included with wix but not loaded in. You can find them here:

[powershell]

PS C:\Program Files (x86)\WiX Toolset v3.8\bin&gt; ls | Where-Object -f {$_.name -match &quot;Extension.dll$&quot;}


    Directory: C:\Program Files (x86)\WiX Toolset v3.8\bin


Mode                LastWriteTime     Length Name
----                -------------     ------ ----
-a---        28/11/2013     05:14     266240 WixBalExtension.dll
-a---        28/11/2013     05:15     466944 WixComPlusExtension.dll
-a---        28/11/2013     05:15      98304 WixDependencyExtension.dll
-a---        28/11/2013     05:15      28672 WixDifxAppExtension.dll
-a---        28/11/2013     05:15      61440 WixDirectXExtension.dll
-a---        28/11/2013     05:15      98304 WixFirewallExtension.dll
-a---        28/11/2013     05:16      90112 WixGamingExtension.dll
-a---        28/11/2013     05:16     405504 WixIIsExtension.dll
-a---        28/11/2013     05:16     163840 WixLuxExtension.dll
-a---        28/11/2013     05:16     139264 WixMsmqExtension.dll
-a---        28/11/2013     05:16     290816 WixNetFxExtension.dll
-a---        28/11/2013     05:16      36864 WixPSExtension.dll
-a---        28/11/2013     05:16     270336 WixSqlExtension.dll
-a---        28/11/2013     05:16      49152 WixTagExtension.dll
-a---        28/11/2013     05:16    3637248 WixUIExtension.dll
-a---        28/11/2013     05:16     729088 WixUtilExtension.dll
-a---        28/11/2013     05:16     937984 WixVSExtension.dll

[/powershell]


However there are others that are availabe but in the form of an extension that needs to be loaded in.

&nbsp;

http://wixtoolset.org/documentation/manual/v3/howtos/general/extension_usage_introduction.html

http://wixtoolset.org/documentation/manual/v3/xsd/iis/webvirtualdir.html

&nbsp;

https://www.google.co.uk/search?q=wix+iis&amp;ie=utf-8&amp;oe=utf-8&amp;aq=t&amp;rls=org.mozilla:en-GB:official&amp;client=firefox-a&amp;gfe_rd=cr&amp;ei=g7D8U93lGpeCbK70gJgB#q=wix+virtual+directory&amp;rls=org.mozilla:en-GB:official&amp;safe=off

&nbsp;

http://wixtoolset.org/documentation/manual/v3/wixui/wixui_dialog_library.html

&nbsp;

http://wixtoolset.org/documentation/manual/v3/xsd/iis/webvirtualdir.html

&nbsp;

&nbsp;]]></Content>
		<Date><![CDATA[2014-08-27]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[msi|wix]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>WIX]]></Categories>
	</post>
	<post>
		<Title><![CDATA[WIX - Create virtual directory]]></Title>
		<Content><![CDATA[<ol>
	<li>first you include the &lt;Wix xmlns="http://schemas.microsoft.com/wix/2006/wi"<strong> xmlns:iis="http://schemas.microsoft.com/wix/IIsExtension"</strong>&gt; at the start of the xml file</li>
	<li>The virtualdirectory needs to point to a real life folder (aka physical path). To do this you first add compoenent inside the directory-element that represents the physical path.</li>
	<li>Insite this you add 2 elements. &lt;createFolder /&gt; and &lt;iis:WebVirtualDir Id="VDir" WebSite="DefaultWebSite" Alias="{this is the folder's name as it appears in inetmgr}"  Directory="{name of physical path's folder}"&gt;</li>
	<li>Just before feature's element, enter:&lt;iis:WebSite Id='DefaultWebSite'
Description='Default Web Site'
Directory='{name of physical path's folder}'&gt;
&lt;!-- This element has to be here or WiX does not compile. It's ignored in this case. --&gt;
&lt;iis:WebAddress Id="AllUnassigned" Port="80" /&gt;
&lt;/iis:WebSite&gt;</li>
	<li>On the command line, cd to the folder contain the wxs file, then use the iis extension:
&amp; 'C:\Program Files (x86)\WiX Toolset v3.8\bin\light.exe' .\product.wixobj -ext 'C:\Program Files (x86)\WiX Toolset v3.8\bin\WixIIsExtension.dll'</li>
</ol>
&nbsp;

&nbsp;

http://www.wintellect.com/blogs/jrobbins/install-a-new-virtual-directory-to-default-web-site-with-wix]]></Content>
		<Date><![CDATA[2014-08-28]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[msi|wix]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>WIX]]></Categories>
	</post>
	<post>
		<Title><![CDATA[WIX - Stop and start service]]></Title>
		<Content><![CDATA[https://www.google.co.uk/search?hl=en-GB&amp;ie=UTF-8&amp;source=android-browser&amp;q=wix+stop+service&amp;gfe_rd=cr&amp;ei=K5r_U-gQrqjzB53zgvgN&amp;gws_rd=ssl]]></Content>
		<Date><![CDATA[2014-08-29]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[msi|wix]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>WIX]]></Categories>
	</post>
	<post>
		<Title><![CDATA[WIX - Embed powershell into an MSI file]]></Title>
		<Content><![CDATA[You can embed a single powershell command into an wxs file like this:

&nbsp;

<pre>
<wix ....>
    <product .....>
        <CustomAction
	    Id="EXE_RunPowershell" 
	    Directory="DIR_v1.0"
	    ExeCommand="[DIR_v1.0]powershell.exe &quot;ls | out-file c:\temp\\powerdummy.txt&quot; "     
	    Execute="deferred" 
	    Return="check" />

            <InstallExecuteSequence>
	        <Custom Action="EXE_RunPowershell" After="InstallFiles">
		    NOT Installed AND NOT PATCH
		</Custom>
	    </InstallExecuteSequence>
    <product>
</wix>
</pre>

&nbsp;
Note, if DIR_v1.0 path contains spaces, e.g. "program files", then you need to use "&qoute;"
&nbsp;

Here's another example, but this time using installutil.exe as an example:

<pre>
<CustomAction
			Id="EXE_InstallUtil" 
			Directory="DIR_v2"
			ExeCommand="[DIR_v2]InstallUtil.exe &quot;[DIR_QueueManagementService]dummy.exe&quot;"
			Execute="deferred" 
			Return="check"/>

		<InstallExecuteSequence>
			<Custom Action="EXE_InstallUtil" After="InstallFiles">
				NOT Installed AND NOT PATCH
			</Custom>
		</InstallExecuteSequence>


		<CustomAction
			Id="EXE_U_InstallUtil" 
			Directory="DIR_v2"
			ExeCommand="[DIR_v2]InstallUtil.exe /u &quot;[DIR_QueueManagementService]dummy.exe&quot;"
			Execute="deferred" 
			Return="check"/>

		<InstallExecuteSequence>
			<Custom Action="EXE_U_InstallUtil" After="RemoveFiles">
				Installed
			</Custom>
		</InstallExecuteSequence>
</pre>

Any folder reference's e.g. "DIR_v2" within square-brackets automatically get's expanded. 

https://www.google.co.uk/search?hl=en-GB&amp;ie=UTF-8&amp;source=android-browser&amp;q=embed+powershell+wix&amp;gfe_rd=cr&amp;ei=JHj_U7_nNPHH8geBxoHYBQ&amp;gws_rd=ssl#hl=en-GB&amp;q=embed+powershell+wix


http://stackoverflow.com/questions/2313545/execute-command-line-in-wix-script 

http://stackoverflow.com/questions/15472793/custom-action-on-install-only ]]></Content>
		<Date><![CDATA[2014-08-29]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[msi|wix]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>WIX]]></Categories>
	</post>
	<post>
		<Title><![CDATA[WIX - Edit file during install]]></Title>
		<Content><![CDATA[https://www.google.co.uk/search?hl=en-GB&amp;ie=UTF-8&amp;source=android-browser&amp;q=puppet+double+colon&amp;gfe_rd=cr&amp;ei=tJv_U7a3BOSq8wfGo4DoAg&amp;gws_rd=ssl#hl=en-GB&amp;q=wix+edit+file+during+install


http://wix.tramontana.co.hu/tutorial/com-expression-syntax-miscellanea/xml]]></Content>
		<Date><![CDATA[2014-08-29]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[msi|wix]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>WIX]]></Categories>
	</post>
	<post>
		<Title><![CDATA[WIX - The Installation Sequence]]></Title>
		<Content><![CDATA[there are two tables in the MSI database, that contain the order in which installation events should occur, they are:
<ul>
	<li>InstallUISequence
<a href="http://codingbee.net/wp-content/uploads/2014/08/InstallExecuteSequence.png"><img class="alignnone size-full wp-image-1529" src="http://codingbee.net/wp-content/uploads/2014/08/InstallExecuteSequence.png" alt="InstallExecuteSequence" width="839" height="706" /></a> <a href="http://codingbee.net/wp-content/uploads/2014/08/InstallUISequence.png"><img class="alignnone size-full wp-image-1530" src="http://codingbee.net/wp-content/uploads/2014/08/InstallUISequence.png" alt="InstallUISequence" width="736" height="705" /></a></li>
	<li>InstallExecuteSequence
<a href="http://codingbee.net/wp-content/uploads/2014/08/InstallExecuteSequence.png"><img class="alignnone size-full wp-image-1529" src="http://codingbee.net/wp-content/uploads/2014/08/InstallExecuteSequence.png" alt="InstallExecuteSequence" width="839" height="706" /></a></li>
</ul>
&nbsp;
<h2>InstallUISequence</h2>
This covers the first half of the install process. during this period there are no acual changes applied to the machine. Instead it is the msi ask for input from the user, and checks whether the machine meets it's pre-requisites (e.g. correct version of .net installed)

there are a number of standard actions (i.e. non-custom-actions) that can be added to this table

&nbsp;
<ul>
	<li>FindRelatedProducts</li>
	<li>AppSearch</li>
	<li>LaunchConditions</li>
	<li>ValidateProductID</li>
	<li>CostInitialize</li>
	<li>FileCost</li>
	<li>CostFinalize</li>
	<li>MigrateFeatureStates</li>
	<li>ExecuteAction</li>
</ul>
this side is run under the msi initiating user's privelege (aka client side)

&nbsp;
<h2>InstallExecuteSequence</h2>
This is the 2-half ot the install process and happens after  InstallUISequence has completed. This is where the actual install happens and includes tasks like:
<ul>
	<li>copying files to target directories</li>
	<li>updating the registry</li>
	<li>add a new entry to "Program and Features"</li>
</ul>
This sequence is run using the localsystem user's privilege (aka server side). Here are the standard actions that takes place here:
<ul>
	<li>InstallValidate - double checks that there is enough disk space, and relevant files have not been locked by running processes</li>
	<li>InstallInitialize  - a bit like taking a snapshot, so that rollbacks can happen easily. This prevents any partial installs.</li>
	<li>ProcessComponents</li>
	<li>UnpublishFeatures - only occurs when uninstalling an msi. It is ignored during the install.</li>
	<li>RemoveRegistryValues - only occurs when uninstalling an msi. It is ignored during the install.</li>
	<li>RemoveShortcuts - only occurs when uninstalling an msi. It is ignored during the install.</li>
	<li>RemoveFiles - only occurs when uninstalling an msi. It is ignored during the install.</li>
	<li>InstallFiles -</li>
	<li>CreateShortcuts</li>
	<li>WriteRegistryValues</li>
	<li>RegisterUser</li>
	<li>RegisterProduct - registers your product with Programs and Features and stores a copy of the MSI package in the Windows Installer Cache, found at %WINDIR%\Installer.</li>
	<li>PublishFeatures</li>
	<li>PublishProduct</li>
	<li>InstallFinalize</li>
</ul>
(note , it might also do the actions of the first-half, if for any reason they failed to happen during the 1st half)]]></Content>
		<Date><![CDATA[2014-08-29]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[msi|wix]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>WIX]]></Categories>
	</post>
	<post>
		<Title><![CDATA[WIX - Custom Action ]]></Title>
		<Content><![CDATA[There are different types of custom actions:

&nbsp;

http://msdn.microsoft.com/en-us/library/aa372048%28v=vs.85%29.aspx

&nbsp;

&nbsp;]]></Content>
		<Date><![CDATA[2014-09-03]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[msi|wix]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>WIX]]></Categories>
	</post>
	<post>
		<Title><![CDATA[WIX - edit xml file with the help of xpath notation]]></Title>
		<Content><![CDATA[We can apply changes to an xml file, we can use the xmlfile element which is part of the Util Extension:

http://wixtoolset.org/documentation/manual/v3/xsd/util/xmlfile.html


The following copies the file across and makes a change to an element's value:

<DirectoryRef Id="DIR_folder">
    <Component Id="CMP_Web.config" Guid="*">
        <File Id="FILE_Web.config" Source="FGWCF.Host\Web.config" KeyPath="yes" />
	    <util:XmlFile 
	        Id='XmlSettings1' 
		File='[DIR_folder]Web.config'    
		Action='setValue'
		Value='woooohooooooooooooooooooooooooooooooooooooooooo'
		ElementPath="//path/to/node[\[]@name='connection.connection_string'[\]]" 
		Sequence='1' />	
    </Component>
</DirectoryRef>  

Note: It singles out en element that has an attribute called "name", and has value "connection.connection_string". 

Note: The "[\[]" is an escaped opening square bracket, also "[\]]" is an escaped closing square bracket. That's because in wix, square brackets have a special meaning, i.e. it indicates a variable. However we are using the square brackets as part of xpath notation, that's why they needed to be escaped. 

http://wix.tramontana.co.hu/tutorial/com-expression-syntax-miscellanea/xml

http://wixtoolset.org/documentation/manual/v3/xsd/util/xmlfile.html]]></Content>
		<Date><![CDATA[2014-09-02]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[wix]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>WIX]]></Categories>
	</post>
	<post>
		<Title><![CDATA[vsphere - puppet and powershell, and Powercli]]></Title>
		<Content><![CDATA[https://www.google.com/search?hl=en-GB&ie=UTF-8&source=android-browser&q=powershell+vsphere&gfe_rd=cr&ei=FyQPVKX_Je3H8gft84CwAg&gws_rd=ssl

https://www.google.co.uk/search?hl=en-GB&ie=UTF-8&source=android-browser&q=puppet+vsphere&gfe_rd=cr&ei=SiQPVMjDIu3H8gft84CwAg&gws_rd=ssl

https://www.google.co.uk/search?q=vsphere+command+line&ie=utf-8&oe=utf-8&aq=t&rls=org.mozilla:en-US:official&client=firefox-a&channel=sb&gfe_rd=cr&ei=PGQPVMfrBO3H8gft84CwAg

]]></Content>
		<Date><![CDATA[2014-09-09]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[powercli]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Cucumber - getting started]]></Title>
		<Content><![CDATA[Cucumber runs on top of "ruby on rails" (ror). To install ror, go to:

http://railsinstaller.org/en

Now set the path variable:
in cmd you do:
path=%path%;C:\RailsInstaller\Ruby1.9.3;C:\RailsInstaller\Ruby1.9.3\bin

in powershell you do:
$env:path="$env:path;C:\RailsInstaller\Ruby1.9.3;C:\RailsInstaller\Ruby1.9.3\bin"


Now we create a folder to house all our rails projects e.g.:

mkdir RailsProjects

Now within this directory you create your first project:

PS C:\Users\Mir\RailsProjects> rails new cucumberproject1

This will create a new folder by this name and within this folder it will contain the project files. 

next, cd into the project and install the relevent gems using the gemfile.conf file, insert the following at the bottom of the file:

<pre>
group :CucumberGems do
	gem 'cucumber-rails'
	gem 'database_cleaner'
end
</pre>

Next, while in the project folder, run:

install bundler    


this will read the gemfile and download the specified gems. note you need internet connection for this because it downloads gems from the internet and installs them. 


you can also do:

update bundler 

if you made a change to gemfile and want to do an update. 


Next you need to install cucumber, while in the project folder, do:

rails generate cucumber:install

this command will create a new "features" folder, this is where all your cucumber files goes in. 




]]></Content>
		<Date><![CDATA[2014-09-19]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[cucumber]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[WIX - MSI silent install]]></Title>
		<Content><![CDATA[some msi files can generate response files aka answer files, when you do a manual gui-wizard based install. You can then use that file to automate any future msi installs:

https://www.google.co.uk/search?q=msi+answer+file&ie=utf-8&oe=utf-8&aq=t&rls=org.mozilla:en-GB:official&client=firefox-a&channel=fflb&gfe_rd=cr&ei=w6E_VJQh1MfyB5GfgDg

http://superuser.com/questions/246639/how-do-i-automate-an-msi-installs]]></Content>
		<Date><![CDATA[2014-10-16]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[wix]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>WIX]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - webrick vs passeger web server]]></Title>
		<Content><![CDATA[puppetmaster requires a web server software to run on. By default puppetmaster comes with webrick pre-installed. However you can change this to anything you want. webrick has big drawback that it isn't scalable. 

Puppet enterprise on the other hand is scalable and that is because it comes pre-installed with "passenger" instead of "webrick". Passenger is actually optimised for scalability. 

https://www.google.co.uk/search?q=passenger+web+server&ie=utf-8&oe=utf-8&aq=t&rls=org.mozilla:en-GB:official&client=firefox-a&channel=sb&gfe_rd=cr&ei=7qY_VOC_Ks-q8wfC_oKACw

https://www.phusionpassenger.com/

http://en.wikipedia.org/wiki/WEBrick

https://docs.puppetlabs.com/guides/passenger.html]]></Content>
		<Date><![CDATA[2014-10-16]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Roles and Profiles]]></Title>
		<Content><![CDATA[roles and profiles can basically be thought of a a egyptian pyramid structure. 

The pyramid is broken into 3 levels

- top tier = roles
- middle = profiles
- bottom tier = puppet modules and puppet module libraries. 

The goal is to only assign one (role) class per vm. 




http://puppetlabs.com/presentations/designing-puppet-rolesprofiles-pattern

http://rnelson0.com/puppet-for-vsphere-admins/

https://puppetlabs.com/presentations/designing-puppet-rolesprofiles-pattern


http://puppetlabs.com/blog/best-practices-building-puppet-modules

http://puppetlunch.com/puppet/roles-and-profiles.html]]></Content>
		<Date><![CDATA[2014-10-17]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Vagrant - What is Vagrant]]></Title>
		<Content><![CDATA[Sometimes you'll want to try things out on your machine but you're reluctant to do so because it might end up breaking your workstation. 

One solution is to use virtualbox to create a virtual machine that runs inside your workstation. You can then do your experiments inside your vm, and if you end up breaking it, then you can simply recreate the vm. 

There are a couple of downsides with this approach: 

<ul>
	<li>Creating a vm involves performing a number of repetitive tasks to get the vm configured to how it was before. Constantly doing these tasks can get tedious.</li>
	<li>When creating a VM you need to specify quite a lot of settings, so you need to remember them, or have them written down somewhere.</li>
	<li>Sharing a vm with colleagues is not that easy.</li>
	<li>If you work in a team, then each member might setup their vm differently to each other. This means that any products you develop in your vm, might not work in someone else's vm</li>

</ul>

  

That's where vagrant comes to the rescue. Vagrant is a command-line utility that acts as a wrapper around virtualbox and can rapidly automate the VM creation process. 

]]></Content>
		<Date><![CDATA[2014-10-18]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[vagrant|VirtualBox]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Vagrant]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Powershell - Make a permanent change to the "path" environment variable]]></Title>
		<Content><![CDATA[A common task you often need to do is append a new path to the path environment variable:

$ENV:PATH


The first way is simply to do:


$ENV:PATH="$ENV:PATH;c:\path\to\folder"

But this change isn't permenantly, $env:path will default back to what it was before as soon as you close your powershell terminal and reopen it again. That's because you have applied the change at the session level and not at the source level (which is the registry level). To view the global value of $env:path, do:

Get-ItemProperty -Path 'Registry::HKEY_LOCAL_MACHINE\System\CurrentControlSet\Control\Session Manager\Environment' -Name PATH 

or, more specifically:

(Get-ItemProperty -Path 'Registry::HKEY_LOCAL_MACHINE\System\CurrentControlSet\Control\Session Manager\Environment' -Name PATH).path

Now to change this, first we capture the original path that needs to be modified:

$oldpath = (Get-ItemProperty -Path 'Registry::HKEY_LOCAL_MACHINE\System\CurrentControlSet\Control\Session Manager\Environment' -Name PATH).path

Now we define what the new path should look like, in this case we are appending a new folder:
$newpath = "$oldpath;c:\path\to\folder"

Note: Be sure that the $newpath looks how you want it to look, if not then you could damage your OS. 

Now apply the new value:

Set-ItemProperty -Path 'Registry::HKEY_LOCAL_MACHINE\System\CurrentControlSet\Control\Session Manager\Environment' -Name PATH –Value $newPath

Now do one final check that it looks how you expect it:

Get-ItemProperty -Path 'Registry::HKEY_LOCAL_MACHINE\System\CurrentControlSet\Contro
l\Session Manager\Environment' -Name PATH).Path

You can now restart your powershell terminal (or even reboot machine) and see that it doesn't rollback to it's old value again. Note the ordering of the paths may change so that it's in alphabetical order, so make sure you check the whole line, to make it easier, you can split the output into rows by using the semi-colon as a delimeter:

($env:path).split(";")





http://blogs.technet.com/b/heyscriptingguy/archive/2011/07/23/use-powershell-to-modify-your-environmental-path.aspx]]></Content>
		<Date><![CDATA[2014-10-18]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA["environment variable"|"path environment variable"|PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Modules]]></Title>
		<Content><![CDATA[Modules are similar to classes, but with one key difference, modules can't be instantiated. 

modules can hold any combination of the following:

- methods  (which I guess will in this case be static methods) 
- classes 
- constants
- other modules


You drill down using the scope operator (::)


You can thing of modules as containers for organising all your code. In c# we use something similar, called namespaces. ]]></Content>
		<Date><![CDATA[2014-10-24]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - The "Initialize" method]]></Title>
		<Content><![CDATA[This is the <a href="http://codingbee.net/tutorials/c/c-instantiate-object-set-properties-simultaneously-using-constructors-approach/">c# equivalent of the constructur method</a>. 

In c# you don't have to explicitly define the constructor class, because it by default already implicitly specified in every class that you define, however in Ruby you do have to explicitly define it.

Also you invoke the initialize method by calling a special reserved method called "new"


Reference:
http://stackoverflow.com/questions/10383535/in-ruby-whats-the-relationship-between-new-and-initialize-how-to-return-n

 

]]></Content>
		<Date><![CDATA[2014-10-24]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Methods]]></Title>
		<Content><![CDATA[In ruby, methods works the same way as c# methods do. 

you can have static and instance methods. 

you define methods using the "def" keyword and construct:


<pre>
def MethodName (OptionalParameters)
   puts OptionalParameter 
end

</pre>

for example if we have:

<pre>
$ cat testscript.rb
#!/usr/bin/env ruby

def WelcomeMessage (a_message)
  puts a_message
end

a_string = "Hello World!"
WelcomeMessage a_string
</pre>

Then we will end up with:

<pre>
$ ./testscript.rb
Hello World!
</pre>

Here, the "puts" command is the ruby equivalent of bash's "echo" command, or powershell's "write-host" command. 

There are a number of other <a href="http://www.tutorialspoint.com/ruby/ruby_builtin_functions.htm">ruby commands</a> that you can use straight from the command line. These commands are available because the <a href="http://www.ruby-doc.org/core-2.1.4/Kernel.html">kernel module</a> is always automatically loaded.  


For example here I have created an .rb file called helloworld2.rb with the following 3 methods:

[ruby]
def simple_welcome
  puts &quot;Hello&quot;
end

puts &quot;the simple_welcome method outputs:&quot;
simple_welcome                            # notice how we call this method like a normal command. 


def normal_welcome (name)

  # If you want to insert a variable inside double-quotes, then you need to
  # encase it in #{...} syntax
  puts &quot;Hello #{name}&quot;
end

puts &quot;the normal_welcome method outputs&quot;

normal_welcome (&quot;CodingBee&quot;)

def complex_welcome (name1, name2, name3)
  puts &quot;Hello #{name1}, #{name2}, and #{name3}&quot;
end

puts &quot;The complex_welcome method outputs&quot;
complex_welcome(&quot;Tom&quot;, &quot;Jerry&quot;, &quot;CodingBee&quot;)   # Note for some reason you have any whitespace before opening bracket.
[/ruby]


The above code outputs:

<pre>
C:\RailsInstaller\Ruby1.9.3\bin\ruby.exe -e $stdout.sync=true;$stderr.sync=true;load($0=ARGV.shift) C:/Users/Mir/RailsProjects/spec2xmlproj/helloworld2.rb
the simple_welcome method outputs
Hello
the normal_welcome method outputs
Hello CodingBee
The complex_welcome method outputs
Hello Tom, Jerry, and CodingBee
</pre>


Any variables set in a method will be confined to that method's scope. Also a method can't access any variables outside it's scope. e.g.:


<pre>
website_name = "codingbee"
puts website_name

def normal_welcome
  # puts website_name           # this will output an error message, hence commented out.  
  website_name = "google"
  puts website_name
end

normal_welcome

puts website_name
</pre>

This outputs:

<pre>
PS C:\Temp\irb> ruby .\scope.rb
codingbee
google
codingbee
PS C:\Temp\irb>
</pre>

If you want a variable accessible/settable everywhere, then you need to create a "global variable". global variables are prefixed with the dollar symbol, e.g.:

[ruby]
$website_name = 'codingbee'
puts $website_name

def normal_welcome

  puts $website_name
  $website_name = 'google'
end

normal_welcome

puts $website_name
[/ruby]


The above outputs:

<pre>
$ ./testscript.rb
codingbee
codingbee
google
</pre>


Note: if you a method to output an object instead of using "puts" output, then simply specify the object as the last line in the method. ]]></Content>
		<Date><![CDATA[2014-10-30]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Variables]]></Title>
		<Content><![CDATA[Convention: You create variables in all lower case and with underscores, e.g.


<pre>
# This is wrong:
TestVariable = 7

# This is correct:
test_variable = 7
</pre>


To create a null (aka nil) variable, you do:
<pre>
irb(main):333:0* test_variable = nil
=> nil
irb(main):334:0> test_variable.class
=> NilClass

# The "nil?" is actually a name of an instance variable that simply returns a true/false boolean. 
irb(main):335:0> test_variable.nil?
=> true
irb(main):336:0>
</pre>

The question mark doesn't have a special meaning here, it is just part of the method's name:

<pre>
irb(main):337:0> test_variable.public_methods.sort
=> [:!, :!=, :!~, :&, :<=>, :==, :===, :=~, :^, :__id__, :__send__, :class, :clone, :define_singleton_method, :display, :doublingnumber, :dup, :enum_for, :eql?, :equal?, :extend, :freeze, :frozen?, :hash, :initialize_clone, :initialize_dup, :inspect, :instance_eval, :instance_exec, :instance_of?,:instance_variable_defined?, :instance_variable_get, :instance_variable_set, :instance_variables, :is_a?, :kind_of?, :method, :methods, <strong>:nil?</strong>, :object_id, :private_methods, :protected_methods, :public_method, :public_methods, :public_send, :rationalize, :respond_to?, :respond_to_missing?, :send, :singleton_class, :singleton_methods, :taint, :tainted?, :tap, :to_a, :to_c, :to_enum, :to_f, :to_i, :to_r, :to_s, :trust, :untaint, :untrust, :untrusted?, :|]
irb(main):338:0>
</pre>

Method naming convention: methods ending with a "?" usually means it will do a true/false test. methods ending with a "!" usually means that it will make some kind of permenant change to the object. 

E.g., the string class has two methods called "upcase" and "upcase!", let's try them out:

<pre>
irb(main):348:0> a_string.public_methods.sort
=> [:!, :!=, :!~, :%, :*, :+, :<, :<<, :<=, :<=>, :==, :===, :=~, :>, :>=, :[], :[]=, :__id__, :__send__, :ascii_only?, :between?, :bytes, :bytesize, :byteslice, :capitalize, :capitalize!, :casecmp, :center, :chars, :chomp, :chomp!, :chop, :chop!, :chr, :class, :clear,:clone, :codepoints, :concat, :count, :crypt, :define_singleton_method, :delete, :delete!, :display, :doublingnumber, :downcase, :downcase!, :dump, :dup, :each_byte, :each_char, :each_codepoint, :each_line, :empty?, :encode, :encode!, :encoding, :end_with?, :enum_for, :eql?, :equal?, :extend, :force_encoding, :freeze, :frozen?, :getbyte, :gsub, :gsub!, :hash
, :hex, :include?, :index, :initialize_clone, :initialize_dup, :insert, :inspect, :instance_eval, :instance_exec, :instance_of?, :instance_variable_defined?, :instance_variable_get, :instance_variable_set, :instance_variables, :intern, :is_a?, :kind_of?, :length, :lines, :ljust, :lstrip, :lstrip!, :match, :method, :methods, :next, :next!, :nil?, :object_id,:oct, :ord, :partition, :prepend, :private_methods, :protected_methods, :public_method, :public_methods, :public_send, :replace, :respond_to?, :respond_to_missing?, :reverse, :reverse!, :rindex, :rjust, :rpartition, :rstrip, :rstrip!, :scan, :send, :setbyte, :singleton_class, :singleton_methods, :size, :slice, :slice!, :split, :squeeze, :squeeze!, :start_wit
h?, :strip, :strip!, :sub, :sub!, :succ, :succ!, :sum, :swapcase, :swapcase!, :taint, :tainted?, :tap, :to_c, :to_enum, :to_f, :to_i, :to_r, :to_s, :to_str, :to_sym, :tr, :tr!, :tr_s, :tr_s!, :trust, :unpack, :untaint, :untrust, :untrusted?, <strong>:upcase, :upcase!</strong>, :upto, :valid_encoding?]
irb(main):349:0> a_string.upcase
=> "HELLO WORLD"            # Displays modified output with changing the original
irb(main):350:0> a_string
=> "hello world"
irb(main):351:0> a_string.upcase!   # This time the change is permanent
=> "HELLO WORLD"
irb(main):352:0> a_string
=> "HELLO WORLD"
irb(main):353:0>
</pre>


In Ruby, variables are essentially labels to an object:


<pre>
irb(main):001:0> a = "abc"
=> "abc"
irb(main):002:0> b =a
=> "abc"
irb(main):003:0> b
=> "abc"
irb(main):004:0> a.upcase
=> "ABC"
irb(main):005:0> a
=> "abc"
irb(main):006:0> a.upcase!
=> "ABC"
irb(main):007:0> a
=> "ABC"
irb(main):008:0> b
=> "ABC"
</pre>

The reason that ruby takes this approach is so that it makes more efficient use of memory. To prove that both a and b are 2 doors to enter the same room, do:

<pre>
irb(main):009:0> a.object_id
=> 19946700
irb(main):010:0> b.object_id
=> 19946700
irb(main):011:0>
</pre>


Note, the same isn't true for integers, because integers doesn't use up as much memory as a string can. This is similar to <a href="http://codingbee.net/tutorials/c/c-variables-types/" title="c# –  What are Variables Types">c# value types and reference types</a>.

If you want b to be seperate from a, the you can do:

<pre>
b = a.clone
</pre>



There are actually different types of variables:
<ul>
	<li>local variables</li>

	<li>global variables</li>
	<li>instance variables</li>
</ul>

The examples shown above are only examples of local variables. 



http://www.tutorialspoint.com/ruby/ruby_variables.htm

http://en.wikibooks.org/wiki/Ruby_Programming/Syntax/Variables_and_Constants 


  ]]></Content>
		<Date><![CDATA[2014-10-30]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Storing data inside an Object (aka Instance Variables)]]></Title>
		<Content><![CDATA[In OOP, encapsulation is really important in order have class be properly self contained. For that reason, when you create a new instance of a class, then any variables that are stored inside it are private, and can't be accessed directly. 

The only way thing that is public, are the methods defined in the class. Hence an indirect way to get/set an instance variable is by creating a pair of methods for each variable. One method (aka set method) is used to set the variable (hence requires an input parameter, but doesn't return a value), and another method (aka get method) is used to retrieve the variable's value (hence this method has return value, but doesn't required an input parameter).

This <a href="http://codingbee.net/tutorials/c/c-storing-data-inside-objects-using-properties/">ecapsulation concept works in c# too</a>. 

Instance Variables are defined using the "@" sybmol, e.g.

<pre>
@test_var = "hello world"
</pre>

We have three possible scenarios, when setting/getting objects:
- you want to set only, and not get (in which case you only need to define a set-method, aka it is write only variable)
- you want to get only and not set (in which case you need to define a get-method, aka it s a read-only variable)
- you want to both set and get (in which case you need to define both a get-method and set-method, aka it is a variable with read-write priveleges)

For each variable, it can get a bit tedious constantly writing respective get and set methods. This will also add a lot more clutter in your code. Fortunately ruby provides a special shorthand notation that greatly simplify this, here is an example

<pre>
# Here we are defining a class called "Employee"
class Employee
  attr_accessor :name     # Notice that we used the special "attr_accessor"  
end

employee1 = Employee.new
employee1.name = "John"      # here we are setting the instance variable
puts employee1.name           # here we are getting the instance variable
</pre>

Note: here we are using a <a href="http://codingbee.net/tutorials/ruby/ruby-symbols/" title="Ruby – Symbols">symbol</a> called ":name". Symbols starts with a colon. We will cover symbols a bit more further down. 

As you can see the "attr_accessor" gives both get and set privileges. If you just want to set a value then use "attr_writer" instead. Also if you just want to read a value then just use "attr_reader". 

these accessors are just special notation that simplifies the code writing, the essentially just create the necessary get/set methods behind the scenes. 


You can also define multiple instance variables like this:


<pre>
class Employee
  attr_accessor :first_name, :surname, :job_title
end


employee1 = Employee.new
employee1.first_name = "John"              # essentially here we are calling a 
employee1.surname = "Smith"                # bunch of hidden set methods. 
employee1.job_title = "Level 2 Manager" 

puts employee1.first_name        # essentially here we are calling a 
puts employee1.surname           # bunch of hidden get methods. 
puts employee1.job_title 
</pre> 

This outputs:

<pre>
PS C:\Temp\irb> ruby .\employeeclass.rb
John
Smith
Level 2 Manager
PS C:\Temp\irb>
</pre>
 
Note, attr_accessor, reader_accessor, and writer_accessor, are used in conjunction with instance variables only. They can't be used for other variable types such as local or global variables. 

Also remember that it is possible to have an instance variable and local variable of the same name in the same class. This can create a bit of confusion when we call a variable. The way around this is to use the "self" keyword when we are calling an instance variable:


<pre>
class Employee
  attr_accessor :first_name, :surname, :job_title
  

  def DummyMethod
    first_name = "Jane"                
    puts first_name               # this outputs the local variable. 
    puts self.first_name          # this outputs the instance variable. 
  end
end


employee1 = Employee.new
employee1.first_name = "John"
employee1.surname = "Smith"
employee1.job_title = "Level 2 Manager" 

employee1.DummyMethod
</pre>

Behind the scenes, the "self" keyword is calling the accessor method. 

This outputs:

<pre>
PS C:\Temp\irb> ruby employeeclass.rb
Jane
John
</pre>


Finally let's see what happens behind the scenes when we are using <a href="http://codingbee.net/tutorials/ruby/ruby-symbols/" title="Ruby – Symbols">symbols </a>and attr_accessor.  Let's say we have:

<pre>


class Employee
  attr_accessor :age     # Notice that we used the special "attr_accessor"  
end

john = Employee.new
john.age = 25      # here we are setting the instance variable
puts john.age           # here we are getting the instance variable
</pre>

In the background, i.e. behind the scenes, this get's expanded to:

<pre>
class Employee
  def set_age (years)
    @age = years
  end

  def get_age
    @age
  end

end

john = Employee.new
john.set_age(25)      # here we are setting the instance variable
puts john.get_age           # here we are getting the instance variable

</pre>

As you can see using symbols (and in this instance, accessors) can really make things shorter and simpler syntactly. 


]]></Content>
		<Date><![CDATA[2014-10-30]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Introduction]]></Title>
		<Content><![CDATA[Ruby is a really useful language that is worth learning about, especially if want to master any of the following technologies that are built with Ruby:

- Puppet (Configuration Management)
- Cucumber (Automated System testing framework)
- Rspec (Unit test framework) 

<h2>Installing Ruby on windows</h2>

There are a few ways to install ruby on a windows machine. One option is just to install Ruby using the <a href="http://rubyinstaller.org/">ruby installer</a>. However I have often had issues when I wanted to do certain things which required a rails installation. I often found that installing rails separately from Ruby often causes all kinds of issues. That's why I have discovered that it makes like much easier if you install ruby and rails at the same time.  

This is possible thanks to the <a href="http://codingbee.net/tutorials/rails/rails-installing-rails-windows-machine/" title="Rails – Installing Rails on a Windows Machine">rails installer</a>. 

Ruby comes as part of Rails, so once you have installed rails, you can start using ruby as a standalone, and only use rails as and when you want to.  


Useful resources:

<a href="http://learncodethehardway.org/">http://learncodethehardway.org/
</a><a href="http://learnrubythehardway.org/book/">http://learnrubythehardway.org/book/</a>

<a href="https://www.ruby-lang.org/en/documentation/" title="https://www.ruby-lang.org/en/documentation/"></a>

<a href="http://en.wikibooks.org/wiki/Ruby_Programming" title="http://en.wikibooks.org/wiki/Ruby_Programming"></a>

<a href="https://rubymonk.com/">https://rubymonk.com/</a>]]></Content>
		<Date><![CDATA[2014-11-01]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - The Interactive Ruby Shell (IRB)]]></Title>
		<Content><![CDATA[Most things in ruby are objects. Even classes themselves are objects!

You can run ruby commands straight from the command line. First open up Powershell and then run "irb" to enter the interactive ruby shell

<pre>
<span style="font-size:10px">PS C:\> irb
irb(main):007:0> a_string = "Hello World!"
=> "Hello World!"
irb(main):008:0> a_string.class  # We used a method called "class" to output data type.
=> String
irb(main):009:0> an_integer = 7   # here we create a new variable called "an_integer"
=> 7
irb(main):010:0> an_integer.class
=> Fixnum
irb(main):011:0> a_big_integer = 76464685413123454835455225
=> 76464685413123454835455225
irb(main):012:0> a_big_integer.class
=> Bignum
irb(main):013:0>
</span>
</pre>


As you can see "a_string" variable is actually an instance of the <a href="http://www.ruby-doc.org/core-2.1.4/String.html">"String" class</a>.

In the above example we only tried applying only one of the string class's <strong>instance method</strong>, called "class". The "class" method outputs the name of the class that this object has been instantiated from. 

If you want to see what other instance methods are available that you can apply to this type of object, then you can do this by using another <strong>instance method</strong> called "public_methods":

<pre>
irb(main):001:0> a_string = "hello world"
=> "hello world"
irb(main):002:0> a_string.public_methods
=> [:<=>, :==, :===, :eql?, :hash, :casecmp, :+, :*, :%, :[], :[]=, :insert, :length, :size, :bytesize, :empty?, :=~, :match, :succ, :succ!, :next, :next!, :upto, :index, :rindex,:replace, :clear, :chr, :getbyte, :setbyte, :byteslice, :to_i, :to_f, :to_s, :to_str, :inspect, :dump, :upcase, :downcase, :capitalize, :swapcase, :upcase!, :downcase!, :capitalize!, :swapcase!, :hex, :oct, :split, :lines, 
.
.
...etc
</pre>

Use the "sort" method to make it easier to read, by sorting output into alpahbetical order:

<pre>
irb(main):074:0* a_string.public_methods.sort
=> [:!, :!=, :!~, :%, :*, :+, :<, :<<, :<=, :<=>, :==, :===, :=~, :>, :>=, :[], :[]=, :__id__, :__send__, :ascii_only?, :between?, :bytes, :bytesize, :byteslice, :capitalize, :capitalize!, :casecmp, :center, :chars, :chomp, :chomp!, :chop, :chop!, :chr, :class, :clear,:clone, :codepoints, :concat, :count, :crypt, :define_singleton_method, :delete, :delete!, :display, :downcase, :downcase!, :dump, 
.
.
... etc
</pre>

To make the above even more easier to read, then prefix the "puts" command:


<pre>
irb(main):007:0> <strong>puts</strong> a_string.public_methods.sort
!
!=
!~
%
*
+
<
<<
<=
<=>
==
===
=~
>
>=
[]
[]=
__id__
__send__
ascii_only?
between?
bytes
bytesize
byteslice
capitalize
capitalize!
casecmp
center
chars
chomp
chomp!
chop
chop!
.
.
...etc.
</pre>

Note: the "public_methods" method is similar to Powershell's "get-member" command. 

When you call the "class" method, the irb not only outputs the class's name that the object has been instantiated from, but output's the object itself. <strong>That's becuase classes are actually objects themselves!</strong>. Since classes are also objects, it means objects (that represent classes) has it's own set of methods:

<pre>
irb(main):031:0* a_string = "hello world"
=> "hello world"
irb(main):032:0> a_string.class
=> String
irb(main):026:0* puts a_string.class.public_methods.sort
!
!=
!~
<
<=
<=>
==
===
=~
>
>=
__id__
__send__
allocate
ancestors
autoload
autoload?
class
class_eval
class_exec
class_variable_defined?
.
.
...etc.
</pre>
 
Now to find out what class the "String" class has been instantiated from (i.e. what is the String class's parent class), this time you use the "superclass" method (for some reason using the class method again doesn't work):

<pre>
irb(main):255:0> a_string.class.superclass
=> Object
</pre>


Once again, "Object" is not only a class, but also an object in it's own right. Therefore let's see what methods can be applied to this "class-object":


<pre>
irb(main):037:0* puts a_string.class.superclass.public_methods.sort
!
!=
!~
<
<=
<=>
==
===
=~
>
>=
__id__
__send__
allocate
ancestors
autoload
autoload?
class
class_eval
class_exec
.
.
...etc
</pre>



Now let's see which class the "Object" instance has been instantiated from: 


<pre>
irb(main):257:0> a_string.class.superclass.superclass
=> BasicObject
</pre>

Once again, "Object" is not only a class, but also an object in it's own right. Therefore let's see what methods can be applied to this "class-object":

<pre>
puts a_string.class.superclass.superclass.public_methods.sort
!
!=
!~
<
<=
<=>
==
===
=~
>
>=
__id__
__send__
allocate
ancestors
autoload
autoload?
class
class_eval
class_exec
class_variable_defined?
class_variable_get
.
.
.
=> nil
</pre>

Note, nil is  the top level class, that all other class-objects are derived from. 

<pre>
irb(main):217:0* puts a_string.class.superclass.superclass.superclass.public_methods.sort
!
!=
!~
&
<=>
==
===
=~
^
__id__
__send__
class
clone
define_singleton_method
display
.
.
.
=> nil
</pre>


You don't need to create a variable before inspecting it's methods, you can apply methods to any objects directly:

<pre>
irb(main):298:0* "hello world".upcase
=> "HELLO WORLD"
irb(main):299:0> "hello world".class
=> String
</pre>

You can also define your methods straight in the command line, for example here is a method: 

<pre>
irb(main):300:0> def doublingnumber(val); val * 2; end
=> nil
irb(main):301:0> doublingnumber(6)
=> 12
irb(main):302:0>

# Notice that it also doubles strings in an odd way:
irb(main):304:0> doublingnumber("hello")
=> "hellohello"

# Also here's what happens when you pass in an array:
irb(main):305:0> doublingnumber([1,2,3])
=> [1, 2, 3, 1, 2, 3]

# If you want to retrieve the last output from the doublingnumber method, then 
# you can simply use the following shortcut:

irb(main):315:0> doublingnumber(6)
=> 12
irb(main):316:0> _              # Notice that we simply used a single underscore!
=> 12
</pre>

 ]]></Content>
		<Date><![CDATA[2014-11-01]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - If-Else Statements and other conditional statements]]></Title>
		<Content><![CDATA[Here's a simply if-else statement:

[ruby]
a_number = 15

if a_number &gt; 10
  puts &quot;this is a big  number&quot;
else
  puts &quot;this is a small number&quot;
end

# the above outputs: &quot;this is a big  number&quot;
[/ruby]

Another way to write the above is to capture the if-else statements output into a variable like this:


[ruby]
a_number = 15

result = if a_number &gt; 10
  &quot;this is a big  number&quot;
else
  &quot;this is a small number&quot;
end

puts result

# the above outputs: &quot;this is a big  number&quot;

[/ruby]


]]></Content>
		<Date><![CDATA[2014-11-01]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Operators]]></Title>
		<Content><![CDATA[<strong>Comparison operators</strong> 
>
>=
<
<=
==
!=

<strong>mathematical operators</strong>
+
-
*
/
%

<strong>Logical operators
</strong>!  (can also use "not")
&& (can also use "and")
|| (can also use "or")




<strong>Bitwise Intger Operators (not sure how these are used)
</strong>&
|
^
~
<<
>>


<strong>Assignment Operators
</strong>

+=  e.g. <strong>a+=10</strong>   is shorthand for a=a+10   This is used for setting up a counter. 
-=  e.g. <strong>a-=10</strong> is shorthand for a=a-10    Also used fo setting up a counter. 
*=  e.g. a*=10 is shorthand for a=a*10
/=  e.g. a/=10 is shorthand for a=a/10
%=  not sure about this
**= not sure about this
&= 
|=
^=
>>=






]]></Content>
		<Date><![CDATA[2014-11-01]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Prompt for user input]]></Title>
		<Content><![CDATA[In Linux you can prompt a user provide input by using the "read" command. You can do the same thing with Powershell by using the read-host command. 

Similarly you can do the same thing in Ruby using the "gets" command:


[ruby]
print &quot;Please enter your name: &quot;
name = gets
print &quot;Welcome #{name.strip}!!!&quot;
[/ruby]

Note: I used the strip method to remove any whitespacing. 

The above should output the following (assuming you enter "CodingBee" when prompted):

<pre>
C:\RailsInstaller\Ruby1.9.3\bin\ruby.exe -e $stdout.sync=true;$stderr.sync=true;load($0=ARGV.shift) C:/Users/Mir/RailsProjects/spec2xmlproj/helloworld2.rb
Please enter your name: Sher
Welcome Sher!!!
Process finished with exit code 0
</pre>

]]></Content>
		<Date><![CDATA[2014-11-01]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[WIX - Find version info for installed products using regedit]]></Title>
		<Content><![CDATA[open up regedit, and then navigate to:

HKEY_LOCAL_MACHINE/SOFTWARE/Microsoft/Windows/CurrentVersion/Installer/UserData/Products

Under this folder you will find a list of GUIDs, of which only of them is the one you want. Therefore:

While you have "Products" select, got to edit->find

Then type the name of the software you want to find more info about. 

This will open up th corresponding guid that you are interested in. 
]]></Content>
		<Date><![CDATA[2014-11-05]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[msi|wix]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>WIX]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Use backticks and system to run Bash or Powershell commands]]></Title>
		<Content><![CDATA[In ruby you can embed system commands (e.g. bash or Powershell) inside your rb scripts. 

The commands that are available depends on whether you are working with a Windows or Linux Operating System.  

<h2>Windows Operating System</h2>
Here's a script:


<pre>
irb(main):030:0> `time /t`
=> "12:10\n"                   # Notice that the new line (\n) formatting doesn't get interpreted. 

# There is also the %x(...) which is an alternative to the backtick notation:
irb(main):002:0> %x(time /t)   
=> "14:00\n"

</pre>

We also have "system" which is slightly different to backtick, in the sense that it returns a boolean ("true" if command as 0-exit-status, and "nil" for all other exit status). However this command also outputs the actual commands output to the standard output. 
<pre>
irb(main):031:0> system("time /t")
12:11
=> true                  # system also outputs a boolean on whether it was succesful. 

# now let's fail on purpose:           
irb(main):032:0> system("timetttt /t")
=> nil

# Now let's capture the output:
irb(main):004:0> result = system("time /t")
14:06
=> true
irb(main):005:0> puts result        
true                   # notice that the result variable only holds the boolean value 
                       # and not the output itself.  
=> nil

</pre>

The "time /t" comand is actually a <a href="http://ss64.com/nt/">cmd command</a>. By default anything passed into backticks/system is assumed to be a windows cmd command. This means a command like "ls" won't work, instead you have to use the cmd equivalent, whcih is "dir". The same is true if you try to run any powershell commands, e.g.:

<pre>
irb(main):005:0> `get-childitem`
Errno::ENOENT: No such file or directory - get-childitem
        from (irb):5:in ``'
        from (irb):5
        from C:/Program Files (x86)/Ruby/Ruby193/bin/irb:12:in `<main>'
irb(main):006:0> system("get-childitem")
=> nil
irb(main):007:0>
</pre>

However you can run powershell comands from inside ruby, by prepending your command with "powershell", like this:

<pre>
irb(main):014:0> `powershell get-childitem`
=> "\n\n    Directory: C:\\\n\n\nMode                LastWriteTime     Length Name
                                \n----                -------------     ------ ----
                                 \nd----        24/10/2014     11:51            DeployTasks
                                  \nd----        23/04/2014     11:29            GeoWorkspaces
                                   \nd----        15/08/2014     14:50            HashiCorp
                                    \nd----        08/03/2014     11:21            Intel
                                     \nd----        27/06/2014     12:10            IWTemp
                                      \nd----        30/05/2014     12:14            jar
                                       \nd----        16/03/2014     14:52            oracle
                                        \nd----        14/07/2009     04:20            PerfLogs
                                         \nd----        30/05/2014     09:23            Portable-winscp550
                                          \nd----        16/03/2014     12:54            Powershell
                                           \nd-r--        18/07/2014     12:29            Program Files
                                            \nd-r--        12/11/2014     10:32            Program Files (x86)
                                             \nd----        30/05/2014     12:14            PuTTY

irb(main):015:0> system("powershell get-childitem")


    Directory: C:\


Mode                LastWriteTime     Length Name
----                -------------     ------ ----
d----        24/10/2014     11:51            DeployTasks
d----        23/04/2014     11:29            GeoWorkspaces
d----        15/08/2014     14:50            HashiCorp
d----        08/03/2014     11:21            Intel
d----        27/06/2014     12:10            IWTemp
d----        30/05/2014     12:14            jar
d----        16/03/2014     14:52            oracle
d----        14/07/2009     04:20            PerfLogs
d----        30/05/2014     09:23            Portable-winscp550
d----        16/03/2014     12:54            Powershell
d-r--        18/07/2014     12:29            Program Files
d-r--        12/11/2014     10:32            Program Files (x86)
d----        30/05/2014     12:14            PuTTY



=> true

irb(main):016:0> system("powershell ls")

   Directory: C:\


Mode                LastWriteTime     Length Name
----                -------------     ------ ----
d----        24/10/2014     11:51            DeployTasks
d----        23/04/2014     11:29            GeoWorkspaces
d----        15/08/2014     14:50            HashiCorp
d----        08/03/2014     11:21            Intel
d----        27/06/2014     12:10            IWTemp
d----        30/05/2014     12:14            jar
d----        16/03/2014     14:52            oracle
d----        14/07/2009     04:20            PerfLogs
d----        30/05/2014     09:23            Portable-winscp550
d----        16/03/2014     12:54            Powershell
d-r--        18/07/2014     12:29            Program Files
d-r--        12/11/2014     10:32            Program Files (x86)
d----        30/05/2014     12:14            PuTTY


=> true
irb(main):017:0>

</pre>

You can use this approach to also run powershell scripts, i.e.:

<pre>
PS C:\Temp> cat .\powershell_script.ps1
echo "Hello CodingBee!"
PS C:\Temp> irb
irb(main):001:0> system('powershell -File c:\temp\powershell_script.ps1')  # Note, is single quotes.
Hello CodingBee!
=> true
irb(main):002:0>
</pre>



<h2>Linux</h2> 

In Linux, you can use the same approach as above. Essentially if a command runs on the bash command line, then it should work from inside ruby. 

Here is an example of <a href="http://blog.bigbinary.com/2012/10/18/backtick-system-exec-in-ruby.html">executing bash commands from inside ruby</a> 























By default, the backtick and system approach both outputs the following:




]]></Content>
		<Date><![CDATA[2014-11-12]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|PowerShell|ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Creating classes and objects]]></Title>
		<Content><![CDATA[To create a new class, you use the class keyword:

<pre>
class Employee
   # code goes here. 
end
</pre>

Previously we mentioned that variable names always start with a lower case. However in the case of classes, the convention is that class names always starts with an upper case, and it follows the CamelCase style. The convention also states that accronyms that are in uppercase too. Here's an example:

<pre>
class XMLReaderAndWriter
  # code goes here. 
end 
</pre>


In ruby you have a special keyword called "new" which is used to create an object from a class (i.e. instantiate a class), e.g.:


<pre>
NewObject = ClassName.new

</pre>

For, example:

<pre>
NewObject = Employee.new

</pre>


The "new" keyword will automatically trigger the class's "initiliaze" method. The initialize method is the equivalent to c#'s constructor method. 



  ]]></Content>
		<Date><![CDATA[2014-11-12]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Methods and Instance Variables]]></Title>
		<Content><![CDATA[In ruby you define a method with the <strong>def </strong>keyword. In the following example I've defined a method called "salary" which takes an input parameter called "job_title" : 

<pre>
class Employee
  def salary(job_title)
  end
end
</pre>


Next we want to store the input parameter as data that is particular to a given a instance of this class. To do this we create an "instance variable". Instance variable are variables that stores data for a particular instance of a class. You can create an instance variable by prefixing the variable name with an "@", e.g.:

 
<pre>
class Employee
  def salary(job_title)
    @job_title = job_title
  end
end

# Now let's try out this class:
john = Employee.new
john.salary("Level 2 Manager")
puts john.inspect                  # a shorthand for this line is to simply type "p ship"

</pre>

In this case I have named this ruby script file as employeeclass.rb, so from the powershell command line you do:

<pre>
> ruby c:\path\to\file.rb
</pre>

Hence the above script outputs:

<pre>
PS C:\Temp\irb> ruby .\employeeclass.rb
#<Employee:0x1f2f000 @job_title="Level 2 Manager">    # this is the output from the inspect method.
PS C:\Temp\irb>
</pre>

Also the following will not work:


<pre>
class Employee
  def salary(job_title)
    @job_title = job_title
  end
end

john = Employee.new
john.salary("Level 2 Manager")

puts john.job_title      # This line will fill due to OOP encapsulation rules. 

</pre>

The same OOP principes of c#, also applies here. 

The OOP law of encapsulation applies here, this means that you can't (and not supposed) to get/set instance variables directly. i.e. instance variables are supposed to be private, and internal to the class. The only elements that are public in a class, are the methods. 

This means that the only way to set/get instance variables, is indirectly through methods, e.g.:


<pre>
class Employee
  def salary(job_title)
    @job_title = job_title
  end

  def job_title      # here we created a new method just to get the instance variable. 
    @job_title
  end
end

john = Employee.new
john.salary("Level 2 Manager")

puts john.job_title      # This line will fill due to OOP encapsulation rules. 

</pre>

This outputs:

<pre>
PS C:\Temp\irb> ruby .\employeeclass.rb
Level 2 Manager
</pre>


 



]]></Content>
		<Date><![CDATA[2014-11-12]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Duck Typing]]></Title>
		<Content><![CDATA[Duck Typing is a bit like an alternative concept to the inheritance concept. It basically let's you create a standalone method, that resides outside a class. Another way to look at it is that duck typing is a bit like creating your very own "static method".

Before we start explaining what "duck typing" is, let's first take a look at an example of inheritance in action:

<pre>
class Animal 
  def quack
    puts "unknown animal that makes a quack noise"
  end

  def feathers
    puts "this animal has something to do with feathers"
  end
  
  # Here we have taken the slightly unusual step of passing the object into the method. 
  # a better approach would be to not pass anything in, and just use the "self" keyword instead. 
  def in_the_forest(an_object)
    an_object.quack
    an_object.feathers
  end 
  
end


class Duck   < Animal
  def quack
    puts "Quaaaaaack!"
  end

  def feathers
    puts "The duck has white and gray feathers."
  end  

end
 

class Person   < Animal
  def quack
    puts "The person imitates a duck."
  end
  
  def feathers
    puts "The person takes a feather from the ground and shows it."
  end
end
 

 

donald = Duck.new
john = Person.new
mystery_animal = Animal.new

# If we used the "self" keyword earlier on then we wouldn't have needed to pass in the object itself in again. 
donald.in_the_forest(donald)
john.in_the_forest(john)
mystery_animal.in_the_forest(mystery_animal)
</pre>

Here we have taken a slightly odd approach in the way we have defined the in_the_forest method. This will make better sense later on. Also notice that both the quack and feathers method has been over-ridden, and only in_the_forest method hasn't been overridden in the child classes. 



This outputs:

<pre>
PS C:\Temp\irb> ruby .\inhertance.rb
Quaaaaaack!
The duck has white and gray feathers.
The person imitates a duck.
The person takes a feather from the ground and shows it.
unknown animal that makes a quack noise
this animal has something to do with feathers
PS C:\Temp\irb>
</pre>

Now let's say that we actually want to instantiate the Animal class. Also the quack and feathers classes will always be redefined in all of Animals child class, then we can in theory refactor the code to:


<pre>
class Animal 
  
  def in_the_forest(an_object)
    an_object.quack
    an_object.feathers
  end 
  
end


class Duck   < Animal
  def quack
    puts "Quaaaaaack!"
  end

  def feathers
    puts "The duck has white and gray feathers."
  end  

end
 

class Person   < Animal
  def quack
    puts "The person imitates a duck."
  end
  
  def feathers
    puts "The person takes a feather from the ground and shows it."
  end
end
 


donald = Duck.new
john = Person.new

donald.in_the_forest(donald)
john.in_the_forest(john)
</pre>

This outputs:

<pre>
PS C:\Temp\irb> ruby .\inhertancet.rb
Quaaaaaack!
The duck has white and gray feathers.
The person imitates a duck.
The person takes a feather from the ground and shows it.
PS C:\Temp\irb>
</pre>


At this poinnt, the (parent) Animal class only contains one method. Having a parent that only contains one method, can be thaught of as a bit overkill. It could be argued that it would be better to get rid of the Animal class altogether and have the in_the_forest method as a standalone method. This is possible like this:

<pre>
def in_the_forest(an_object)
  an_object.quack
  an_object.feathers
end 


class Duck   
  def quack
    puts "Quaaaaaack!"
  end

  def feathers
    puts "The duck has white and gray feathers."
  end  

end
 

class Person 
  def quack
    puts "The person imitates a duck."
  end
  
  def feathers
    puts "The person takes a feather from the ground and shows it."
  end
end
 

donald = Duck.new
john = Person.new

in_the_forest(donald)
in_the_forest(john)

</pre>

This outputs:

<pre>
PS C:\Temp\irb> ruby .\inhertancet.rb
Quaaaaaack!
The duck has white and gray feathers.
The person imitates a duck.
The person takes a feather from the ground and shows it.
PS C:\Temp\irb>
</pre>

Here, you can use the in_the_forest method for any objects whose corresponding class contains the quack and feathers method.

If we head originally wrote the in_the_forest method using the "self" syntax then this wouldn't have been possible and the in_the_forest method would have needed to be rewritten to the way it is written, i.e. requiring an object as in input parameter. 

As mentioned there are few cases where you might need to use ducktyping.   


The key benefit of duck typing is that it lets you avoid writing unnecessary classes just to hold generic methods. ]]></Content>
		<Date><![CDATA[2014-11-14]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Inheritance]]></Title>
		<Content><![CDATA[Here's an example of inheritance in action:


<pre>
# Here we define a parent class
class Animal 
  def quack
    puts "unknown animal that makes a quack noise"
  end

  def legs
    puts "this animal has a pair of legs"
  end
  
end

# Here we define a child class of the animal class. In this case we 
# have over-ridden the "quack" method. But it implicitly inherits the "legs" method as-is. 
class Duck   < Animal
  def quack
    puts "Quaaaaaack!"
  end
 

end
 
 
# Here we define another child class of the animal class. In this case we 
# have over-ridden the "quack" method. But it implicitly inherits the "legs" method as-is.  
class Person   < Animal
  def quack
    puts "The person imitates a duck."
  end
end
 

donald = Duck.new
john = Person.new

# the child class's method should be run in both these instances. 
donald.quack
john.quack
 
# The parent class's method should end up being run in these two instances. 
donald.legs
john.legs


mystery_animal = Animal.new
mystery_animal.quack
mystery_animal.legs
</pre>


The above outputs:

<pre>
PS C:\Temp\irb> ruby .\inhertancet.rb
Quaaaaaack!
The person imitates a duck.
this animal has a pair of legs
this animal has a pair of legs
unknown animal that makes a quack noise
this animal has a pair of legs
</pre>


]]></Content>
		<Date><![CDATA[2014-11-14]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Use "super" to extend inherited methods]]></Title>
		<Content><![CDATA[If you want a child class, which has inherited methods, but you dont want to fully replace the method, instead you want to either prepend/append some code to the method's code, then you can do so using the "super" keyword, like this:

<pre>
# here's the parent class. 
class Animal 
  def quack
    puts "Here is a statement about the quack noise:"
  end

  def feathers
    puts "The above line is a statement relating to feathers:"
  end  
end

# Here's the child class. 
class Duck   < Animal
  def quack
    super
	puts "Quaaaaaack!"
  end

  def feathers
    puts "The duck has white and gray feathers."
	super
  end  

end

donald = Duck.new

donald.quack
donald.feathers

</pre>


this outputs:

<pre>
PS C:\Temp\irb> ruby .\inhertancet.rb
Here is a statement about the quack noise:
Quaaaaaack!
The duck has white and gray feathers.
The above line is a statement relating to feathers:
PS C:\Temp\irb>
</pre>




If the method that is being "supered" requires input variables, then they need to be passed in as part of the corresponding super keyword:



<pre>
# here's the parent class. 
class Animal 
  def quack(message1,message2)
    puts "the message is #{message1} and #{message2}"
  end

  def feathers
    puts "The above line is a statement relating to feathers:"
  end
  
  
end

# Here's the child class. 
class Duck   < Animal
  def quack
    super("duckling1","duckling2")
	puts "Quaaaaaack!"
  end

  def feathers
    puts "The duck has white and gray feathers."
	super
  end  

end
 

donald = Duck.new


donald.quack
donald.feathers
</pre>

This outputs:

<pre>
PS C:\Temp\irb> ruby .\inhertancet.rb
the message is duckling1 and duckling2
Quaaaaaack!
The duck has white and gray feathers.
The above line is a statement relating to feathers:
PS C:\Temp\irb>
</pre>
 ]]></Content>
		<Date><![CDATA[2014-11-14]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Class Methods]]></Title>
		<Content><![CDATA[A class method is essentially the c# equivalent of a "static method". A good example would be a "calculator" class, where you have a "class method" called "add" which adds 2 numbers together. 

In this situation, it makes no sense to create a "calculator" object and then use the "add" method. Instead we would want to use the "add" method directly.  

To define a class-method, we once again make use of the "self" keyword:

<pre>
class Calculator 
  def self.add(first_number,second_number)
    first_number + second_number             # remember the output is automatically returned. 
                                             # You don't need to explicitly state a return statement
											 
  end
  
end

result = Calculator.add(2,5)     # here we are just drilling down to the class 
                                 # that has the "add" method, similar to c#.

puts "The sum of 2 and 5 is: #{result}"
</pre>

]]></Content>
		<Date><![CDATA[2014-11-14]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Class Variables]]></Title>
		<Content><![CDATA[Class Variable are essentially variables that are stored by the class itself. By default class varables are not visible outside the class, i.e. they are private. So you can read/write class variables indirectly via methods (either class or instance methods will do)

<pre>
class Calculator 
  @@pi = 3.141592
  def self.pi        # note, we created a class method, so to avoid instantiating a calculator object. 
    @@pi
  end 

  def self.add(first_number,second_number)
    first_number + second_number            						 
  end
  
end

result = Calculator.add(2,5)     

puts "The sum of 2 and 5 is: #{result}"

pi_value = Calculator.pi
puts "the value of pi is #{pi_value}"

</pre>

A class variable is shared across all objects of that class. So if you design the class variable to be read-only, then everything is fine. But if you have a method that can edit the value of the class variable, then this could have an knock on effect to other objects which requires the class variable to be of the previous value. 

Because of this, class variables are not used that often. 

To make things worse, class variables are shared out across all it's child classes too!!! Here's an example of where the (parent) Calculator class get's it's "pi" class variable overwritten by the NormalCalculator class, and then overwritten again by the ScientificCalculator class:

<pre>
class Calculator 
  @@pi = 3
  def self.pi  
    @@pi
  end   
end

class NormalCalculator  < Calculator
  @@pi = 3.14
end

class ScientificCalculator  < Calculator 
  @@pi = 3.1412952
end

puts NormalCalculator.pi

</pre>

This should output "3.14", but instead it outputs:

<pre>
PS C:\Temp\irb> ruby .\class-variable.rb
3.1412952
</pre>

That's because ruby overwrites the shared class variable at the time of reading in the ruby script! It is also because class variable are shared out across all it's child classes too!!!

For this reason, class variables are not that popular. To get the NormalCalculator.pi outputting "3.14", then you need to use class instance variable instead. 
]]></Content>
		<Date><![CDATA[2014-11-14]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Class Instance Variables]]></Title>
		<Content><![CDATA[In the last lesson we saw how we only have one copy of a class variable that is shared across a class and all it's child class. If you want to have a class variable that is only shared across it's and not it's child classes, then you need to use something called "Class instance Variables" instead. A Class Instance Variable is essentially an instance variable that is set at the class level rather than the method level. 


Previously we explained that instance variables are used to store information of a particular instance of a class. Also due to encapsulation, you can't get/set instance variable's directly, instead you have to set/get instance variables indirectly through the use of methods, here's an example:

<pre>
class Employee
  def set_job_title(job_title)
    @job_title = job_title
  end
  
  def get_job_title
    @job_title
  end  
end

john = Employee.new
john.set_job_title("Level 2 Manager")

puts john.get_job_title
</pre>

This outputs:

<pre>
PS C:\Temp\irb> ruby .\instance-variable.rb
Level 2 Manager
</pre>


You can rewrite the above in shorthand form with the help of the accessor notation:

<pre>
class Employee
  attr_accessor :job_title
end

john = Employee.new
john.job_title = "Level 2 Manager"

puts john.job_title
</pre>

Again you use the the get/set methods, but they are all hidden away in the backend. 

This again outputs:

<pre>
PS C:\Temp\irb> ruby .\instance-variable.rb
Level 2 Manager
</pre>

So up to now, we have defined/edited instance variables from inside the method blocks. 

However you can also define instance variables directly under the class level. These class level, instance variables are refered to as "Class Instance Variables".

Here's the calculator example, from the previous lesson, but this time using class instance variables rather than class variables:


<pre>
class Calculator 
  @pi = 3
  def self.pi  
    @pi
  end   
end

class NormalCalculator  < Calculator
  @pi = 3.14
end

class ScientificCalculator  < Calculator 
  @pi = 3.1412952
end

puts NormalCalculator.pi
</pre>
All that has been done here is tha the every class variable reference has been replaced by a class instance reference, by simply removing the extra "@" symbol. The output is now:

<pre>
PS C:\Temp\irb> ruby .\class-instance-variable.rb
3.14
PS C:\Temp\irb>
</pre>


]]></Content>
		<Date><![CDATA[2014-11-14]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Public and Private Methods]]></Title>
		<Content><![CDATA[In Ruby, all methods are public by default. 

However you can set methods to private so that they can't be accessible from outside the class. However private methods however are still inherited by child classes. 



Let's take a look at an of a class where we may want to make some methods private:

<pre>
class Employee
  
  def firstname(name)
    @firstname = name
  end
  
  def level(level)
    @level = level	
  end
  
  def salary
    @salary = 10000 * @level
	puts "salary has been calculated"
  end
  
  def net_salary
    salary                   # here we are calling the salary method. 
    puts @salary
    puts @salary - 500
  end
  
end


employee_1 = Employee.new

employee_1.firstname("John")

employee_1.level(2)

employee_1.salary     

employee_1.net_salary
</pre>




This outputs:

<pre>
PS C:\Temp\irb> ruby .\privatemethods.rb
salary has been calculated
salary has been calculated    
20000
19500
</pre>

As you can see all the methods are public by default. 

Now let's say we want to make the "salary" method private, to do this, we use the following syntax:

<pre>
private :{method-name1} 
</pre>

Here's an exmample of this syntax:


<pre>
class Employee
  
  def firstname(name)
    @firstname = name
  end
  
  def level(level)
    @level = level	
  end
  
  def salary
    @salary = 10000 * @level
	puts "salary has been calculated"
  end
  private :salary
  
  def net_salary
    salary                   
    puts @salary
    puts @salary - 500
  end
  
end


employee_1 = Employee.new

employee_1.firstname("John")

employee_1.level(2)

employee_1.salary     # this should now fail becuase salary is now a private method and can't be called from outside

employee_1.net_salary
</pre>


Now the output is:

<pre>
PS C:\Temp\irb> ruby .\privatemethods.rb
./privatemethods.rb:31:in `<main>': private method `salary' called for #<Employee:0x22cd508 @firstname="John", @level=2>
 (NoMethodError)
PS C:\Temp\irb>
</pre>


If we now comment out the "employee_1.salary" call, then the output becomes:

<pre>
PS C:\Temp\irb> ruby .\privatemethods.rb
salary has been calculated
20000
19500
PS C:\Temp\irb>
</pre>

If you also want to make the level method private too, then you can use the comma seperated syntax, then you can do:


<pre>
class Employee
  
  def firstname(name)
    @firstname = name
  end
  
  def level(level)
    @level = level	
  end
  
  def salary
    @salary = 10000 * @level
    puts "salary has been calculated"
  end
  private :salary, :level                   # use the comma seperated syntax
  
  def net_salary
    level(2)                             # I added this line so that it continues to work. 
    salary                   
    puts @salary
    puts @salary - 500
  end
  
end


employee_1 = Employee.new

employee_1.firstname("John")

# employee_1.level(2)   # is now private method so commented out

# employee_1.salary     # is now private method so commented out

employee_1.net_salary
</pre>

This will now output:

<pre>
PS C:\Temp\irb> ruby .\privatemethods.rb
salary has been calculated
20000
19500
PS C:\Temp\irb>
</pre>

So to summarise the, syntax for making multiple class's private is:

<pre>
private :{method-name1}, :{method-name1}....
</pre>

Note, if you want to make class-methods private (as opposed to instance methods), then you use the following slightly different syntax instead:

<pre>
private_class_method :{method-name1}, :{method-name1}....
</pre>



There's an alternative to using the following "private :{method-name1}, :{method-name1}...." syntax. And that is to encase the private/public parts of your code using the private/public keywords. Heres an example of this, where we have made the level and salary methods private:


<pre>
class Employee
  
  def firstname(name)
    @firstname = name
  end
  

  private                         # encasing begins here.

  def level(level)
    @level = level	
  end
  
  
  def salary
    @salary = 10000 * @level
    puts "salary has been calculated"
  end

  public                          # encasing ends here. 
  
  def net_salary
    level(2)
    salary                   
    puts @salary
    puts @salary - 500
  end
  
end


employee_1 = Employee.new

employee_1.firstname("John")

employee_1.net_salary
</pre>

Note, you can omit public if you want all the remaining methods in the class to be private. This again outputs:

<pre>
PS C:\Temp\irb> ruby .\privatemethods.rb
salary has been calculated
20000
19500
</pre>


There is also a special override that you can do to force access a method from outside the class. This is rarely used, but can be useful if you want to unit test a private method in isolation using a unit testing tool like RSpec. You use a special high level method called the "send" method to do this, here is it's syntax:


object.send(:{method_name}, intputparam1, inputparam2,....)

Here's an example:

<pre>
class Employee
  
  def firstname(name)
    @firstname = name
  end
  
  private
  def level(level)
    @level = level	
  end
  
  
  def salary
    @salary = 10000 * @level
    puts "salary has been calculated"
  end
  public
  
  def net_salary
    # level(2)
    salary                   
    puts @salary
    puts @salary - 500
  end
  
end


employee_1 = Employee.new

employee_1.firstname("John")

#employee_1.level(2)
employee_1.send(:level, 18)    # here we are using the private "level" method by force. 



# employee_1.salary   


employee_1.net_salary
</pre>

this outputs:

<pre>
PS C:\Temp\irb> ruby .\privatemethods.rb
salary has been calculated
180000
179500
PS C:\Temp\irb>
</pre>

One final thing to mention, is that child classes also automatically inherits private class just like it inherits public classes. Here's an example:


<pre>
class Employee
  
  def firstname(name)
    @firstname = name
  end
  
  private
  def level(level)
    @level = level	
  end
  
  
  def salary
    @salary = 10000 * @level
	puts "salary has been calculated"
  end
  public
  
  def net_salary
    # level(2)
	salary                   
    puts @salary
    puts @salary - 500
  end
  
end

class JuniorEmployee  < Employee
  def initialize (name,joblevel)
    firstname(name)
	level(joblevel)        # notice that private methods are still being called 
	salary                 # from inside child class, since the child class has already
                               # implicitly inherited all methods, public and private.  
	net_salary             
  end
end

employee_1 = JuniorEmployee.new("David",5)
</pre>

This outputs:

<pre>
PS C:\Temp\irb> ruby .\privatemethods.rb
salary has been calculated
salary has been calculated
50000
49500
PS C:\Temp\irb>
</pre>
]]></Content>
		<Date><![CDATA[2014-11-14]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Protected Methods]]></Title>
		<Content><![CDATA[Protected methods is something that is very rarely used, so I have skipped this for now. 


Lets say you have two obects in instantiated from the same class:

- objectA
- objectB

Let's also say you have a protected method called "prot_method". 

In order to apply this method on objectB, then you can do it indirectly via applying a method to objectA, and passing objectB as a parameter into that object. 

See "excerpt section of this post"
]]></Content>
		<Date><![CDATA[2014-11-14]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Executable Class Bodies]]></Title>
		<Content><![CDATA[It is possible for a class to be used in the style of a method. 

Here's an example:

<pre>
total_sum = class SimpleSum
  sum_of_two_numbers = 7 + 3
  sum_of_two_numbers
end

puts total_sum
puts SimpleSum.superclass
</pre>

This outputs:

<pre>
PS C:\Temp\irb> ruby .\simplesum.rb
10
Object
PS C:\Temp\irb>
</pre>

It is very unlikely that you will need to classes in this way in real life. But it is possible if you need to. 

]]></Content>
		<Date><![CDATA[2014-11-17]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Open Classes and Monkey Patching]]></Title>
		<Content><![CDATA[Let's say you have the following:

<pre>
class Human
  def greeting 
    puts "hello everybody"
  end
end

john = Human.new

john.greeting
</pre>

Next let's say you want to add a new method, called "hungry", then you can do this:


<pre>
class Human
  def greeting 
    puts "hello everybody"
  end

  def hungry 
    puts "I am hungry"
  end
end

john = Human.new

john.greeting
</pre>

However another approach is as follows:

<pre>
class Human
  def greeting 
    puts "hello everybody"
  end
end

john = Human.new

john.greeting

# the following line will fail, because you called before defining the "hungry" method. 
# john.hungry

class Human
  def hungry 
    puts "I am hungry"
  end
end

john.hungry
</pre>

Here we have extended the existing class with a new method. This was done by creating a follow up Human class block. This is possible because Ruby supports a concept known as "Open classes", which lets you do exactly this, i.e. extend an existing class, without altering the original class-block. 

"Monkey Patching" is something that's made possible thanks to the concept of open classes. "Monkey Patching" is the term used to describing extending a class's functionality (as shown above), or overwrite existing methods. 

Monkey Patching is especially useful for modifying code from third party vendors. 

Note if you wan to overwrite a private method, then the new method you have written needs to also be set top private. 

<pre>
class Human
  def greeting 
    puts "hello everybody"
  end
  
  def hungry 
    puts "I am hungry"
  end
end

john = Human.new

john.greeting

john.hungry

class Human
  def hungry 
    puts "I could eat a horse"
  end
end

john.hungry
</pre>

This will output:

<pre>
PS C:\Temp\irb> ruby .\openclasses.rb
hello everybody
I am hungry
I could eat a horse
PS C:\Temp\irb>
</pre>

You can even use this approach to modify the behaviour of the classes that are in the standard library, such as the string class.  For example:

<pre>
puts "hello".size

class String 
  def size
    puts "goodbye"
  end
end

puts "hello".size
</pre>

This outputs:

<pre>
PS C:\Temp\irb> ruby .\overwritingstandardlibrary.rb
5
goodbye
</pre>

Making changes in this way should be avoided because changing core functionality will confuse other people, and could stop othe ruby applications from working. 



]]></Content>
		<Date><![CDATA[2014-11-17]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Checking whether 2 objects are equal]]></Title>
		<Content><![CDATA[By definition 2 objects cannot be identical (even if they are instantiated from the same class). If that were not the case, then that would defeat the whole point of OOP. 

<pre>
class Human
  attr_reader :name
 
  def initialize(firstname) 
    @name = firstname
  end 

  def greeting 
    puts "hello everybody"
  end
  
  def hungry 
    puts "I am hungry"
  end
end

john = Human.new("Jonathon")
puts john.name
puts john.object_id

john = Human.new("Jonathon")
puts john.name
puts john.object_id
</pre>

This outputs something like:

<pre>
Jonathon
18214200
Jonathon
21469788
</pre>

As you can see, these are 2 different objects. In fact since we have used the same name for the object, the second instantiation replaced the first instantiation. 

However if we did give each object a different label, e.g.:

<pre>
class Human
  attr_reader :name
 
  def initialize(firstname) 
    @name = firstname
  end 

  def greeting 
    puts "hello everybody"
  end
  
  def hungry 
    puts "I am hungry"
  end
end

john1 = Human.new("Jonathon")
puts john1.name
puts john1.object_id

john2 = Human.new("Jonathon")
puts john2.name
puts john2.object_id
</pre>

Then the output still shows something like:

<pre>
PS C:\temp\irb> ruby .\comparing2objects.rb
Jonathon
18083124
Jonathon
21338700
PS C:\temp\irb>
</pre>

So while we may recognise these 2 objects as actually referring to the same real-life object. In Ruby they are viewed as two separate objects. Just to confirm this is the case, you can also do:

<pre>
puts john1.equal?(john2)  # this will output false. 
</pre>

So the best way to determine whether two object are actually referring to the same real life object, you first need to determine what attribute must match. In the above case, there is just one attribute, called "name", so to determine, if they are referring to the same real life object, then you can do:

<pre>
john1.name == john2.name   # which will output true. 
</pre>]]></Content>
		<Date><![CDATA[2014-11-21]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - If-else statements]]></Title>
		<Content><![CDATA[Here's an example of a simple if-statement:

<pre>
a_number = 5

if a_number < 10
  puts "#{a_number} is less than 10"
end  
</pre>

This outputs:

<pre>
PS C:\temp\irb> .\if-else.rb
PS C:\temp\irb> ruby .\if-else.rb
5 is less than 10
PS C:\temp\irb>
</pre>

Now here's how an if-else statement looks like:

<pre>
a_number = 13

if a_number < 10
  puts "#{a_number} is less than 10"
else  
  puts "#{a_number} is greater than 10"
end  
</pre>

This outputs:

<pre>
PS C:\temp\irb> ruby .\if-else.rb
13 is greater than 10
PS C:\temp\irb>
</pre>

Now here's an example of an elsif statment (note, it is spelt "elsif" and not "elseif"):

<pre>
a_number = 13

if a_number < 10
  puts "#{a_number} is less than 10"
elsif  a_number == 13
  puts "#{a_number} is an unlucky number"
else
  puts "#{a_number} is greater than 10"
end  
</pre>

This outputs:

<pre>
PS C:\temp\irb> ruby .\if-else.rb
13 is an unlucky number
PS C:\temp\irb>
</pre>

In ruby, only the keywords "nil" and "false" evaluates to be false. Everything else (including empty strings/arrays and "0") evaluates to be true. Here are some examples:

<pre>
class Human
  attr_reader :name
 
  def initialize(surname) 
    @name = surname
  end 

  def greeting 
    puts "hello everybody"
	false
  end
  
  def hungry 
    puts "I am hungry"
	true
  end
end

john = Human.new(nil)


if john.name 
  puts "john has a surname which is '#{john.name}'"
else 
  puts "john has no surname"  
end  


if john.greeting 
  puts "this is a geniune greeting"
else 
  puts "this is a sarcastic greeting"
end  
 
if john.hungry
  puts "John is genuinely hungry"
else 
  puts "John is lying and isn't that hungry"
end  

</pre>

This outputs:

<pre>
PS C:\temp\irb> ruby .\if-else.rb
john has no surname
hello everybody
this is a sarcastic greeting
I am hungry
John is genuinely hungry
PS C:\temp\irb>
</pre>


One of the things commonly done, is to use an if-else statment to determine a value for a variable. Here is an example where we are trying to determine the value of the variable called "message":


<pre>
a_number = 13

if a_number < 10
  puts "#{a_number} is less than 10"
  message = "Hence this is a small number."
else  
  puts "#{a_number} is greater than 10"
  message = "Hence this is a big number."
end  

puts message
</pre>

This outputs:

<pre>
PS C:\temp\irb> ruby .\if-else.rb
13 is greater than 10
Hence this is a big number.
PS C:\temp\irb>
</pre>


You can rewrite this in a way to show to show that you are capturing the output of the if statement, like this:

<pre>
a_number = 13

message = if a_number < 10
  puts "#{a_number} is less than 10"
  "Hence this is a small number."
else  
  puts "#{a_number} is greater than 10"
  "Hence this is a big number."
end  

puts message
</pre>

If you have a really simple if-else statement, e.g.:

<pre>
age = 15

price = if age <= 16   
  "half price"
else  
  "full price"
end  

puts price            # outputs: "half price"
</pre>

Then it might be worth writing this if-else statement on a single line without sacrificing readability:

<pre>
age = 15

price = if age <= 16 then "half price" else "full price" end  

puts price            # outputs: "half price"
</pre>

Notice the inclusion of the "then" keyword here. You only use the "then" keyword when you squeeze an if-statement into one line, as we have done above. 

If you just have a very simply if statement, and no else statement, then you can have even shorter single line if statements, here's an example:


<pre>
def success_message
  puts "this is a success"
end

def result 
  true
end

success_message if result      # outputs "this is a success"
</pre>



 






 









 





]]></Content>
		<Date><![CDATA[2014-11-21]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - "unless" statements ]]></Title>
		<Content><![CDATA[Sometimes you might want to create an "if not" statement. You can use the if-statement to do this, but another option is using the "unless" statement instead. That's because the unless-statement can be a little easier to read.

Here's an example:

<pre>
weight = 90

unless weight < 85
  puts "you are overweight, so you need to go on a diet and have plenty of exercise"
end
</pre>

You should read this statement, as "do whatever is inside the block, unless weight is less than 85".

This will output:

<pre>
PS C:\temp\irb> ruby .\unless.rb
you are overweight, so you need to go on a diet and have plenty of exercise
PS C:\temp\irb>
</pre>

You can also add an "else" clause into the unless statement, but it is best to avoid this since it can make the code a little difficult to follow. 

With if-statements we saw how you could condense an if statement into a single line. We can do the same with the unless statement too, here's an example:


<pre>
def fail_message
  puts "this is a failure"
end

def result 
  false
end

fail_message unless result  # this outputs: "this is a failure"
</pre>





]]></Content>
		<Date><![CDATA[2014-11-21]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[terms and conditions]]></Title>
		<Content><![CDATA[https://termsfeed.com]]></Content>
		<Date><![CDATA[2014-11-23]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[termsfeed]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Ternary Operator (single line If-Else statement)]]></Title>
		<Content><![CDATA[Previously we saw how you could fit an if-else statement into a single line with the help of the "then" keyword. However there is another commonly used syntax that you could use instead, which is known as the <a href="http://en.wikipedia.org/wiki/Ternary_operation">ternary operator (:?)</a>. Ternary Operators are also used in other languages such as java and c#. The ternary operator syntax is essentially as follows:

<pre>
{condition} ? {if-code-block} : {else-code-block}
</pre>

Here's an example:

<pre>
def success_message
  puts "this is a success"
end

def fail_message
  puts "this is a failure"
end

def result 
  true
end

result ? success_message : fail_message    # this outputs "this is a success"

def result 
  false
end

result ? success_message : fail_message    # this outputs "this is a failure"
</pre>









 ]]></Content>
		<Date><![CDATA[2014-11-25]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Conditional Initialization]]></Title>
		<Content><![CDATA[What if you want to create a new variable/object (aka initialise a varaible), but only if it doesn't already exist. This is possible using the following assingment operator:

||=    # this is called the "or equal assignment operator"

There is also a similar operator, "&&=" but this isn't used that often. 

Here's an example:

<pre>
john ||= Employee.new
</pre>

Here an object called "john" is created, but only if it doesn't already exist. 

This also works for simple variables, e.g. 

<pre>
irb(main):017:0* x = 5
=> 5
irb(main):018:0> puts x
5
=> nil
irb(main):019:0> x ||= 10
=> 5
irb(main):020:0> puts x
5
=> nil
irb(main):021:0> x = nil
=> nil
irb(main):024:0> puts x

=> nil
irb(main):025:0> x ||= 10
=> 10
irb(main):026:0> puts x
10
=> nil
irb(main):027:0>
</pre>

Note, conditional inizialation also wroks with other data types, such as strings and arrays. The only exception is that it doesn't work for setting/unsetting booleans. 

There are alternatives to using "||=", for example, by making use of the "unless" keyword:

<pre>
john = Employee.new unless john
</pre>



 ]]></Content>
		<Date><![CDATA[2014-11-25]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - The "and" and "or" operators]]></Title>
		<Content><![CDATA[The "and" operatory can be used to string together several methods, here's an example:
<pre>def hello
  puts "Hello"
  "returing: not-nil or false"
end

def aloha
  puts "aloha"
  # "returing: not-nil or false"
end

def bonjour
  puts "bonjour"
  "returing: not-nil or false"
end


hello and aloha and bonjour
</pre>
This outputs:
<pre>PS C:\Temp\irb&gt; ruby .\and.rb
Hello
aloha
bonjour
PS C:\Temp\irb&gt;
</pre>


If one of the method returns false or nil, then any methods following it will no longer run, here's an example:
<pre>def hello
  puts "Hello"
  "returning: not-nil or false"
end

def aloha
  puts "aloha"
  # "returning: not-nil or false"      here we are now doing a nil output. 
end

def bonjour
  puts "bonjour"
  "returning: not-nil or false"
end


hello and aloha and bonjour
</pre>
Now the output is:
<pre>PS C:\Temp\irb&gt; ruby .\and.rb
Hello
aloha
PS C:\Temp\irb&gt;

</pre>

Notice this time that the bonjour method did not run this time.

Now here's the "or" assignment in action:

<pre>
def hello
  puts "Hello"
  "returning: not-nil or false"
end

def aloha
  return false      # Here we are terminating the method early by explictly using the "return" keyword. 
  puts "aloha"
end

def bonjour
  puts "bonjour"
  "returning: not-nil or false"
end

hello and aloha or bonjour
</pre>


Here's a few things to note:
<ul>
	<li>&amp;&amp; and || have much higher priority over "and" and "or"</li>
	<li>&amp;&amp; has higher priority over ||</li>
	<li>"and" and "or" have equal priority over each other.</li>
</ul>
]]></Content>
		<Date><![CDATA[2014-11-25]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - The case statement]]></Title>
		<Content><![CDATA[Here is how a case statement looks like in ruby:

<pre>
def salutations (string_var)
  case string_var
  when "goodbye"
    puts "bye for now"
  when "handshake"
    puts "let shake hands"
  when "Hello"
    puts "Hello world"
  else
    puts "The message is: #{string_var}"
  end  
end

salutations "handshake"
salutations "hi"
</pre>

This outputs:

<pre>
PS C:\Temp\irb> ruby .\CaseStatement.rb
let shake hands
The message is: hi
PS C:\Temp\irb>
</pre>

Notice that the "else" key word is used here in the context of a catchall system. The else keyword is optional and can be left out, in which case if no matches are made then the case statement won't output anything. 

You can also capture the output of a case statement into a variable, here's an example:


<pre>

def salutations (string_var)
  case string_var
  when "goodbye"
    puts "bye for now"
	return "Goodbye to you too"
  when "handshake"
    puts "let shake hands"
	return "I will shake hands with you to"
  when "Hello"
    puts "Hello world"
	return "Hello to you too"
  else
    puts "The message is: #{string_var}"
	return "I don't know how to respond to that"
  end  
end

response =  salutations "handshake"
puts response

response = salutations "hi"
puts response
</pre>

Here's another way to do it:

<pre>
weight = 45


response =  case
  when weight == 65
	 "You are average weight"
  when weight > 65
	 "You are overweight"
  when weight < 65
     "you are under weight"
  else
	"not sure whether you are over or under weight"
  end  

puts response
</pre>
]]></Content>
		<Date><![CDATA[2014-11-25]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - The while loop]]></Title>
		<Content><![CDATA[In most programming languages, while/until/for loops are used quite heavily. These types of loops are also available in Ruby as well, but are surprisingly used only sparingly. That's because two of the most common uses of loops are: 

- to cycle through a collection (e.g. an array) 
- cycle through the same code block of code a set number of times. 

Both of these are better handled using Ruby iterator feature (cover later). 

Here's a simple example of while loop:


<pre>
x = 0
while x <= 10
  puts x
  x+=1
end
</pre>

You can also squeeze this into a single line:

<pre>
x = 0
while x <= 10 do puts x ; x+=1 end
</pre>
Notice, that I used the "do" keyword. You should only use the "do" keyword when writing a while loop into a single line I have also added a ";" to delimit the code block. 

Either of the above two, outputs:

<pre>
PS C:\Temp\irb> ruby .\while.rb
0
1
2
3
4
5
6
7
8
9
10
PS C:\Temp\irb>
</pre>




Now what if you want to do a do-while loop? The difference between a while-loop and a do-while loop is that, that a while-loop won't run the block if the condition is initially met, whereas the do-while loop will run the code block once first before, the condition is checked to see whether it is met or not. This means that with a do-while loop, the code block is run at least once, whereas with a while loop it is possible for the code-block to have never have been run. 


For example, let's say you have:

<pre>
x = 11
while x <= 10
  puts x
  x+=1
end
</pre>

This won't output anything since the while loops prevented the code block to even run. If you want to just run it at least once, then you do this using the do-while code block, as shown here:

<pre>
 x = 11
 
 begin 
   puts x
   x+=1
 end while x <= 10
</pre>

This outputs:

<pre>
PS C:\Temp\irb> ruby .\while.rb
11
PS C:\Temp\irb>
</pre>
 

]]></Content>
		<Date><![CDATA[2014-11-25]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - The "until" loop]]></Title>
		<Content><![CDATA[The until-loop is basically the reverse of the until-loop. The syntax of the until loop, here's an example:

<pre>
x = 0
until x == 10
  puts x
  x+=1
end
</pre>
 
This outputs:

<pre>
PS C:\Temp\irb> ruby .\until.rb
0
1
2
3
4
5
6
7
8
9
PS C:\Temp\irb>
</pre>

As before, you can squeeze this until-loop into a single line, like this:

<pre>
x = 0
until x == 10 do puts x ; x+=1 end
</pre>


Finally we have the do-until loop, which is equivalent to the do-while loop that we saw earlier:

<pre>
x = 11
begin
  puts x
  x+=1
end until x >= 10
</pre>

This outputs:

<pre>
PS C:\Temp\irb> ruby .\until.rb
11
PS C:\Temp\irb>
</pre>
]]></Content>
		<Date><![CDATA[2014-11-25]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - The "for" loop]]></Title>
		<Content><![CDATA[Here's an example of the for loop:

<pre>
for i in [1,2,3]
  puts i
end
</pre>

This outputs:

<pre>
PS C:\Temp\irb> ruby .\for.rb
1
2
3
PS C:\Temp\irb>
</pre>

You can also iterate through a range, e.g.:


<pre>
for i in (1..15)    # Notice that we used round bracket here. 
  puts i
end

</pre>

The ".." indicates that we are interested in a range. 

This outputs:

<pre>
PS C:\Temp\irb> ruby .\for.rb
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
PS C:\Temp\irb>
</pre>

Note, the for loop actually makes use of the "each" method behind the scenes. More about this in the next lesson.  
]]></Content>
		<Date><![CDATA[2014-11-25]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Iterators and  Blocks ]]></Title>
		<Content><![CDATA[In ruby, you'll encounter an object that is actually a container that contains other objects. An array is the most common example of this. 

In Ruby, you create an array like this:

<pre>
weekdays = ["Monday", "Tuesday", "Wednesday", "Thursday", "Friday"]
puts weekdays      # outputs: array
puts weekdays[1]   # outputs: Tuesday
puts weekdays[4]   # outputs: Friday
</pre>

Note: The "class" method is defined is all classes and is used to output the name of the class used to instantiate the given object. 

If you take a look at what methods are in the "array" class that are available to the array, then you will find:

<pre>
irb(main):021:0> weekdays.public_methods.sort
=> [:!, :!=, :!~, :&, :*, :+, :-, :<<, :<=>, :==, :===, :=~, :[], :[]=, :__id__, :__send__, :all?, :any?, :assoc, :at, :chunk, :class, :clear, :clone, :collect, :collect!, :collect_concat, :combination, :compact, :compact!, :concat, :count, :cycle, :define_singleton_method, :delete, :delete_at, :delete_if, :detect, :display, :drop, :drop_while, :dup, <strong>:each</strong>, :each_cons, :each_entry, :each_index, :each_slice, :each_with_index, :each_with_object, :empty?, :entries, :enum_for, :eql?, :equal?, :extend, :fetch, :fill, :find, :find_all, :find_index, :first, :flat_map, :flatten, :flatten!, :freeze, :frozen?, :grep, :group_by, :hash, :include?, :index, :initialize_clone, :initialize_dup, :inject, :insert, :inspect, :ins
tance_eval, :instance_exec, :instance_of?, :instance_variable_defined?, :instance_variable_get, :instance_variable_set,:instance_variables, :is_a?, :join, :keep_if, :kind_of?, :last, :length, :map, :map!, :max, :max_by, :member?, :method, :methods, :min, :min_by, :minmax, :minmax_by, :nil?, :none?, :object_id, :one?, :pack, :partition, :permutation, :pop, :private_methods, :product, :protected_methods, :public_method, :public_methods, :public_send, :push, :rassoc, :reduce, :reject, :reject!, :repeated_combination, :repeated_permutation, :replace, :respond_to?, :respond_to_missing?, :reverse,:reverse!, :reverse_each, :rindex, :rotate, :rotate!, :sample, :select, :select!, :send, :shift, :shuffle, :shuffle!, :singleton_class, :singleton_methods, :size, :slice, :slice!, :slice_before, :sort, :sort!, :sort_by, :sort_by!, :taint, :tainted?, :take, :take_while, :tap, :to_a, :to_ary, :to_enum, :to_s, :transpose, :trust, :uniq, :uniq!, :unshift, :untaint, :untrust, :untrusted?, :values_at, :zip, :|]
irb(main):022:0>

</pre>

 
Here, the one we are interested in is the "each" method.

The "each" method is used a lot in Ruby, and it works a bit like a foreach loop, but much more powerful. 

Here is very simple example of the each-method in action:

<pre>
weekdays = ["Monday", "Tuesday", "Wednesday", "Thursday", "Friday"]
weekdays.each do
  puts "This is a weekday"
end
</pre>

Notice that we used the "do...end" block. 

Which outputs:

<pre>
PS C:\Temp\irb> ruby .\block.rb
This is a weekday
This is a weekday
This is a weekday
This is a weekday
This is a weekday
PS C:\Temp\irb>
</pre>

As you can see, the each-method works pretty much like a for-each loop. Now there's some terminology that is used to different parts of this looping code. 

iterator: this is used to refer to the "{variable}.each" statement. 
block : this is used to refer to the do...end block and it's content. You can also replace the "do" and "end" keywords with curly brackets, and it works exactly the same way. However the convention is to only use curly braces for single line blocks, e.g.:

<pre>
weekdays = ["Monday", "Tuesday", "Wednesday", "Thursday", "Friday"]
weekdays.each {  puts "This is a weekday" }
</pre>

This again outputs:

<pre>
PS C:\Temp\irb> ruby .\block.rb
This is a weekday
This is a weekday
This is a weekday
This is a weekday
This is a weekday
PS C:\Temp\irb>
</pre>

You can also think of this as follows:

The each method requires an arguement. And this argument is in the form of a block, either do...end or {...}. The each method executes the contents of the block for each item in the iterator. 

However this construct has another vital feature, and that is that the each method can pass parameters into the block. The most obvious thing you'll want to pass in is the array item that is currently active in the iterator. You do this using the pipe notation "|{parameter}|", here's an example:


<pre>
weekdays = ["Monday", "Tuesday", "Wednesday", "Thursday", "Friday"]
weekdays.each do |day|
  puts "This is #{day}"
end
</pre>

or alternatively:

<pre>
weekdays = ["Monday", "Tuesday", "Wednesday", "Thursday", "Friday"]
weekdays.each { |day|  puts "This is #{day}" }
</pre>

both approach outputs:

<pre>
PS C:\Temp\irb> ruby .\block.rb
This is Monday
This is Tuesday
This is Wednesday
This is Thursday
This is Friday
PS C:\Temp\irb>
</pre>


Notice that the pipe syntax is the very first thing to be specified at the start of the block. 


  
 ]]></Content>
		<Date><![CDATA[2014-11-26]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Creating an infinite loop]]></Title>
		<Content><![CDATA[You can also create an infinite loop using the "loop" key word and the "do...end" block:

<pre>
loop do
  puts Time.now
  sleep(5)        # 5 second sleep 
end
</pre>

This will output something like:

<pre>
irb(main):001:0> exit
PS C:\Temp\irb> ruby .\loop.rb
2014-11-26 10:14:08 +0000
2014-11-26 10:14:13 +0000
2014-11-26 10:14:18 +0000
2014-11-26 10:14:23 +0000
.
.
.
....etc
</pre>

You can also achieve the same outcome by setting up a while-true loop.


]]></Content>
		<Date><![CDATA[2014-11-26]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Looping through numbers tips and tricks]]></Title>
		<Content><![CDATA[Here we have a number of handy integer (fixnum) class methods:

 
<pre>
irb(main):012:0> 5.upto(10) {|number| puts number}
5
6
7
8
9
10
=> 5
irb(main):013:0> 5.downto(1) {|number| puts number}
5
4
3
2
1
=> 5
irb(main):014:0> 4.times { puts "hello"}
hello
hello
hello
hello
=> 4
irb(main):015:0> 2.step(15,3) {|i| puts i}
2
5
8
11
14
=> 2
irb(main):016:0>
</pre>]]></Content>
		<Date><![CDATA[2014-11-26]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Controlling loops using next/break/redo]]></Title>
		<Content><![CDATA[So far we have looked at various types of loops, e.g. while loops, for loops,....etc. 

What ever type of loop you are using, there will be times when you want to one of the following:

<ul>
	<li>skip to the next iteration while part way through the current interation - this is done using the "next" keyword</li>
	<li>exit the loop early - this is done using the "break" keyword.</li>
	<li>redo the current iteration - this is done using the "redo" keyword.</li>
</ul>

We will demonstrate the next/break/redo keywords using a simple for-loop (although they can work in any type of loop)

<h2>Skipping a current loop iteration - using "next" </h2>
<pre>
for i in (1..5)
  next if i == 3
  puts i
end
</pre>

This outputs:

<pre>
PS C:\Temp\irb> ruby .\nextloop.rb
1
2
4
5
PS C:\Temp\irb>
</pre>




<h2>Exiting the whole loop early - using "break" </h2>

Here's an example:

<pre>
for i in (1..5)
  break if i == 3
  puts i
end
</pre>

This outputs:

<pre>
PS C:\Temp\irb> ruby .\breakloop.rb
1
2
PS C:\Temp\irb>
</pre>

The break keyword has an extra feature for catching an output from a loop when the break keyword gets triggered, here's an example of how this works:

<pre>
answer = for i in (1..5)
  break "loop exited early" if i == 3
  puts i
end         # outputs: 1 2

puts answer    # this outputs "loop exited early"

</pre>

This technique could be useful for various purposes, including capturing possible error messages and storing it into logs. 


<h2>Redoing the current iteration of a loop - using "redo" </h2>

Here's an example of the redo key-word in action:

<pre>
for i in (1..5)
  i += 1            # I had to add this line in to avoid an infinite loop. 
  redo if i == 3
  puts i
end
</pre>

This outputs:
<pre>
PS C:\Temp\irb> ruby .\redoloop.rb
2
4
4
5
6
PS C:\Temp\irb>
</pre>
]]></Content>
		<Date><![CDATA[2014-11-26]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Exceptions Handling]]></Title>
		<Content><![CDATA["Exceptions" is just another word for "Error"

In programming, error can occur, some of these errors can be handled, which means that the program can carry on working without by fixing these errors by itself. 

For example, lets say you have a online web form that needs to be filled in by the user. This form has several mandatory fields/check-boxes/dropdown-lists.

In this situation a user might fail to complete the some of teh mandatory fields and try to submit. To overcome this you can add some error handling in the form of some javasctript code, so that if user does this then a pop window prompts the user to complete the remaining mandatory fields. 


However the worst case scenario is that, an error can't be handled, in which we need to "raise the exception", which is covered later. 




Now let's see how we can apply error handling in ruby, you can use if-statement for simple error handling. Let say you want to install a gem, only if gem is not already installed, here's a simply way to do this:

<pre>
if puppetversion = ENV['PUPPET_GEM_VERSION'] 
  puts "puppet gem is already installed" 
else
  gem 'puppet'
end
</pre>

Here, the if-condition attempts to create a non-nil variable using an environment variable. This has a 0-or-1 exit status. The if-else clause runs depending on what the exit code is. This is a very simple error-handling scenario, the above type of logic is typically added to a gem file.







Now let's say we have:


<pre>
def generic_method
  puts "method has started"
  begin
    puts "hello"
    randomNonsense
    puts "world"
  end
  puts "method has completed"
end

generic_method

</pre>

This will obviously fail, and it gives the following output:

<pre>
PS C:\Temp\irb> ruby .\exceptionhandling.rb
method has started
hello
./exceptionhandling.rb:5:in `generic_method': undefined local variable or method `randomNonsense' for main:Object (NameError)
        from ./exceptionhandling.rb:11:in `<main>'
PS C:\Temp\irb>
</pre>

Now to add some error handling, we need to insert the "rescue" clause, like this:

<pre>
def generic_method
  puts "method has started"
  begin
    puts "hello"
    randomNonsense
    puts "world"
  rescue
    puts "Ruby failed to understand what randomNonsense is. Some code is placed here to resolve this"
  end
  puts "method has completed"
end

generic_method
</pre>

This now outputs:

<pre>
PS C:\Temp\irb> ruby .\exceptionhandling.rb
method has started
hello
Ruby failed to understand what randomNonsense is. Some code is placed here to resolve this
method has completed
PS C:\Temp\irb>
</pre>

This time the failure was encountered, and it got handled, so that the method was allowed to continue running. 

So far we only applied the rescue clause, to only part of the method, which is encased in the begin-end block. You can also actually apply the rescue clause to the method as a whole:

<pre>
def generic_method
  puts "method has started"
  puts "hello"
  randomNonsense
  puts "world"
  puts "method has completed"
  true     # this could be useful for the caller. hence entered here as good practice. 
rescue
  puts "Ruby failed but not sure why. Some code is placed here to resolve this"
  false    # this could be useful for the caller. hence entered here as good practice. 
end

generic_method
</pre>

This outputs:

<pre>
PS C:\Temp\irb> ruby .\exceptionhandling.rb
method has started
hello
Ruby failed but not sure why. Some code is placed here to resolve this
PS C:\Temp\irb>
</pre>

Apply a rescue clause to a whole method has a drawback, which is that it makes it harder to determine which part of the method failed. 

However the good news is that you can use the "rescue" clause to capture the error information that occured during runtime, aka RuntimeError. This error information  actually exists in the form of an object. There are a number of different types of error class's that an error object can be instantiated from, the class that is used depends on the type of error encountered that occured during runtime. All the error classes are themselve objects too, and they are instantiated from a class called "StandardError". StandardError in turn belongs to a more general class called "Exception".


Now Here's how tio capture the error-information using the rescue clause:

<pre>
def generic_method
  puts "method has started"
  puts "hello"
  randomNonsense
  puts "world"
  puts "method has completed"
rescue StandardError => some_error
  puts "Ruby failed but not sure why. Some code is placed here to resolve this"
  puts some_error.class               # outputs: NameError, which is one of a number of runtime based error-classes. 
  puts some_error.class.superclass    # outputs: StandardError
  puts some_error.class.superclass.superclass   # outputs: Exception
  puts some_error.class.superclass.superclass.superclass   # outputs: Object
end

generic_method
</pre>

The "StandardError => some_error" basically means that if an error is generated from the  "StandardError" class, then capture it into a variable called "some_error"

Now let's see what public_methods are available for the NameError class:

 
<pre>

def generic_method
  puts "method has started"
  puts "hello"
  randomNonsense
  puts "world"
  puts "method has completed"
rescue StandardError => some_error           # here we are capturing the error into a variable. 
  puts "Ruby failed but not sure why. Some code is placed here to resolve this"
  puts some_error.public_methods.sort
  
end

generic_method

</pre>

This outputs:

<pre>
PS C:\Temp\irb> ruby .\exceptionhandling.rb
method has started
hello
Ruby failed but not sure why. Some code is placed here to resolve this
!
!=
!~
<=>
==
===
=~
__id__
__send__
<strong>backtrace</strong>
class
clone
define_singleton_method
display
dup
enum_for
eql?
equal?
exception
extend
freeze
frozen?
hash
initialize_clone
initialize_dup
inspect
instance_eval
instance_exec
instance_of?
instance_variable_defined?
instance_variable_get
instance_variable_set
instance_variables
is_a?
kind_of?
<strong>message</strong>
method
methods
name
nil?
object_id
private_methods
protected_methods
public_method
public_methods
public_send
respond_to?
respond_to_missing?
send
set_backtrace
singleton_class
singleton_methods
taint
tainted?
tap
to_enum
to_s
trust
untaint
untrust
untrusted?
PS C:\Temp\irb>
</pre>

Above I have highlighted 2 of the more useful methods, message (which outputs the content of the actual error message) and "backtrace", which outputs the line number where the failure occurred. 

Hence we can now do this:


<pre>
def generic_method
  puts "method has started"
  puts "hello"
  randomNonsense
  puts "world"
  puts "method has completed"
rescue StandardError => some_error           # here we are capturing the error into a variable. 
  puts "Ruby failed but not sure why. Some code is placed here to resolve this"
  puts "The 'message' method outputs:"
  puts some_error.message
  puts "The 'backtrace' method outputs:"
  puts some_error.backtrace
end

generic_method
</pre>

This outputs:

<pre>
PS C:\Temp\irb> ruby .\exceptionhandling.rb
method has started
hello
Ruby failed but not sure why. Some code is placed here to resolve this
The 'message' method outputs:
undefined local variable or method `randomNonsense' for main:Object
The 'backtrace' method outputs:
./exceptionhandling.rb:4:in `generic_method'
./exceptionhandling.rb:15:in `<main>'
PS C:\Temp\irb>
</pre>

So far we have looked at capturing errors generated by the StandardError class. However you can also create your own custom error classes, e.g. you can create a class called CodingBeeError, which is a child of StandardError.  

With this approach, can set up multiple rescue clauses, each handling a different kind of error message. 

Here is an example of how this would look like:


<pre>
def generic_method
  puts "method has started"
  puts "hello"
  randomNonsense
  puts "world"
  puts "method has completed"
rescue CodingBeeError
  puts "My personal custom error message"
rescue StandardError
  puts "Ruby failed but not sure why. Some code is placed here to resolve this"
end

generic_method
</pre>

I can't run the above yet, becuase we haven't defined the CodingBeeError class yet. 

Note, you should insert the child classes before their respective parent classes, otherwise the lower child rescue clauses gets ignored.  


Like the while and if statements, you also have modifiers for the rescue clause, here's an example:


<pre>
def rescuing_method
  puts "Ruby failed to understand what randomNonsense is. Some code is placed here to resolve this"
end 


def generic_method
  puts "method has started"
  begin
    puts "hello"
    randomNonsense
    puts "world"                # this won't run now, as method exits as soon as error encountered. 
  end
  puts "method has completed"    # this won't run now for same reason. 
end

generic_method rescue rescuing_method
</pre>


This outputs:

<pre>
PS C:\Temp\irb> ruby .\rescue-modifier.rb
method has started
hello
Ruby failed to understand what randomNonsense is. Some code is placed here to resolve this
PS C:\Temp\irb>
</pre>

]]></Content>
		<Date><![CDATA[2014-11-26]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Raising Exceptions]]></Title>
		<Content><![CDATA[Note: this post needs to be investigate further. 

So far we have looked at how to capture an error and handle/resolve it if possible. 

However you can write code that raisez an error within your own code. This opens up the possibility of then writing a corresponding error handling for it later down the road once you have figured out how to handle it. Or alternatively refactor the code so that error doesn't need to be raised in the first place (if possible).

 

<pre>
class CustomException < StandardError

  # custom_message = "heeeeeeeeeeeeeeeelp!!!!!!!!!"
  def custom_message
    puts "something went very very wrong."
  end

end



class Duck
  def generic_method
    puts "method has started"
    begin
      puts "hello"
      randomNonsense
      puts "world"
    rescue 
      puts "Ruby failed to understand what randomNonsense is. Some code is placed here to resolve this"
	  raise CustomException, "something went wrong"
    end
    puts "method has completed"
  end
end 

donald = Duck.new

donald.generic_method


</pre>

This outputs:


<pre>
PS C:\Temp\irb> ruby .\raise-exceptions.rb
method has started
hello
Ruby failed to understand what randomNonsense is. Some code is placed here to resolve this
./raise-exceptions.rb:21:in `rescue in generic_method': something went wrong (CustomException)
        from ./raise-exceptions.rb:15:in `generic_method'
        from ./raise-exceptions.rb:29:in `<main>'

</pre>


Note, you actually attach an if-statement at the end of the raise command like this:

<pre>
raise CustomException, "something went wrong" if a_method_that_returns_true_or_false?
</pre>]]></Content>
		<Date><![CDATA[2014-11-26]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Cleanup (the "ensure" clause)]]></Title>
		<Content><![CDATA[Now let's say in a given method, you open a file. 

At the end of the method, this file has to then be closed. Within a method, there are three places where you want to include the "file.close" code:

<ul>
	<li>Just before the end of the method, providing method runs successfully. </li>
	<li>Via the "raise" keyword if something unexpected happens</li>
	<li>inside the rescue clause. </li>
<ul>

i.e.

<pre>
def method_name
  a_file = File.open("testfile.txt")   # this has to be closed no matter how the method ends. 
  # ..
  # ..
  raise CostomError "custom message" if custom_method?   # here we can close the file. 
  # ..
  # ..
  # we can close the file here, just before the method ends successfully.   
  true
rescue
  # ..
  # we can also close the file here. 
  false
end



</pre>


This will result in code duplication and can get messy. That's why there is a better approach, and that is to use the "ensure" clause. The code in the ensure clause get run reqardless of whether the method ran successfully or it encountered an error.




<pre>
def method_name
  a_file = File.open("testfile.txt")   # this has to be closed no matter how the method ends. 
  # ..
  # ..
  raise CostomError "custom message" if custom_method?   
  # ..
  # ..
  true
rescue
  # ..
  # ..
  false
ensure
  a_file.close if a_file
end

</pre>

Now we only have one place that will close the file. We also appended an if-statement just to cover the possible scenarion that File.open(...) itself fails. 

Also, don't put any return statement in the ensure clause, as that could end up over-riding any initial returns of "false". 

Another thing you can do is place an "else" clause between the "rescue" and "ensure" clause. The code in this block only gets run, if no errors have been encountered, here's an exmaple:


<pre>
def method_name
  a_file = File.open("testfile.txt")   # this has to be closed no matter how the method ends. 
  # ..
  # ..
  raise CostomError "custom message" if custom_method?   
  # ..
  # ..
  true
rescue
  # ..
  # ..
  false
else
  puts "woohoo no error messages encountered."
ensure
  a_file.close if a_file
end

note, the ensure clause get's executed in all cases, regardless of whether there is an else clause or not. 

]]></Content>
		<Date><![CDATA[2014-12-02]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Retrying if error encountered the first time]]></Title>
		<Content><![CDATA[In some cases you may want to re-attempt a block of code, if it encounters a failure for the first time. 

A typical example would be if you were trying to do a SOAP/REST request to a web service that is known to be occasionally intermittent. 


One way to handle this is with the use of loops:


<pre>
def soap_request
  1.upto (3) do |attempt|
    begin 
      retrieve_soap_object = API.request(www.WorldWeather.com)
      break   # this exit the re-attempting loop as soon as there is a success. 
    rescue  RunetimeError => error_object 
      puts error_object.message
      if attempt == 3
        puts "request failed."
        raise
      end  # end of if-statment check. 
    end    # end of begin statement
  end      # end of 1..3 loop 
end   # end of method. 

</pre>

However there is a special syntax you can use just for this purpose, which is using the "retry" statement:


<pre>
def soap_request
  retrieve_soap_object = API.request(www.WorldWeather.com)
  # ..
  # ..
  # ..  
rescue  RunetimeError => error_object 
  attempts ||= 0       # this creates variable if it doesn't exist yet. 
  attempts += 1
  if attempts < 3
    puts error_object.message + ". Reattempting SOAP/REST request again."
    retry                      # this instructs ruby to return to the first line of the method again.
                               # i.e. it will try to recreate retrieve_soap_object again.   
  else
    puts "Request failed."
    raise
  end      # end of if-statement
end   # end of method. 

</pre>

As you can see, this approach has a lot less nesting and is easer to read. 



 

]]></Content>
		<Date><![CDATA[2014-12-02]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Throw and Catch]]></Title>
		<Content><![CDATA[In Ruby, there is an alternative to exceptions handling, and that is using the "throw/catch" system.

Throw/catch are specifically designed for getting out of nested loops. 

For example, say of you are 4 loops deep, and you only want exit out to the second loop, then you can use throw/catch to do this. 

 

Here's an example:

<pre>

catch :abort do 
  [1,2,3,4,5,6,7].each do |number|
    while number == 5
	  puts number
	  puts "need to abort infinite loop"
	  throw :abort 
    end
	  puts number
  end
end

</pre>

The symbol ":abort" is used here in the sense of a label. It is used here to pair up the throw statement with the corresponding abort statement. 


throw-catch even works across methods. 

Throw/catch is just an alternative option you can use. But it's unlikely that you will use it that heavily. 


]]></Content>
		<Date><![CDATA[2014-12-02]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Variable Scope]]></Title>
		<Content><![CDATA[previously we saw that any variables set in a method will be confined to that method's scope. Also a method can't access any variables outside it's scope. e.g.:


<pre>
website_name = "codingbee"
puts website_name

def normal_welcome
  # puts website_name           # this will output an error message, hence commented out.  
  website_name = "google"
  puts website_name
end

normal_welcome

puts website_name
</pre>

This outputs:

<pre>
PS C:Tempirb> ruby .scope.rb
codingbee
google
codingbee
PS C:Tempirb>
</pre>

However this rule doesn't apply to lower level blocks such as:

<ul>
	<li>if-else blocks</li>
	<li>loop blocks</li>
	<li>begin-end blocks</li>
</ul>

Here's some examples:

<pre>
number = 5

if number < 10
  a = 20
  puts a
end

puts a

begin 
   b = 25
   puts b
end

puts b

c = 1
while c < 5
  c +=1
  puts c
end

puts c
</pre>

This outputs:

<pre>
PS C:\Temp\irb> ruby .\scope.rb
20
20
25
25
2
3
4
5
5
PS C:\Temp\irb>
</pre>

However blocks are an exception to this and they do introduce scope:

<pre>
1.upto(10) { |i|
  a = i
  puts i
  puts a
}
  
# puts i    # error: undefined variable. 
# puts a    # error: undefined variable.    
</pre>

this outputs:

<pre>
PS C:\Temp\irb> ruby .\blockscope.rb
1
1
2
2
3
3
4
4
5
5
6
6
7
7
8
8
9
9
10
10
PS C:\Temp\irb>
</pre>
]]></Content>
		<Date><![CDATA[2014-12-02]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Booleans]]></Title>
		<Content><![CDATA[In Ruby, there is actually no class called "Boolean" that instantiates true/false values. Instead, we have the following 2 classes:

<ul>
	<li>TrueClass - which instantiates "true" values</li>
	<li>FalseClass - which instantiates "false" values</li>
</ul>

Both of these classes are dervived from the "Object" class. This means that these true/false objects have access to all the methods that are defined in Object, e.g.:


<pre>
puts true.class
puts false.class
puts nil.class


puts true.class.superclass
puts false.class.superclass
puts nil.class.superclass


puts true.to_s      # outputs the string "true"
puts false.nil?     # outputs false, since false itself is an 
                    # object that has instantiated from "FalseClass" class. 
</pre>

This outputs:

<pre>
PS C:\Temp\irb> ruby .\boolean.rb
TrueClass
FalseClass
NilClass
Object
Object
Object
true
false
PS C:\Temp\irb>

</pre>



 

]]></Content>
		<Date><![CDATA[2014-12-02]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Numbers]]></Title>
		<Content><![CDATA[Ruby has 2 classes from which integers can be instantiated from:

<ul>
	<li>Fixnum</li>
	<li>Bignum</li>
</ul>

Both of these classes are derived from the class called "Integer". I.e. they are child class of Integer.

The range of number that Fixnum supports depends on your machine's setting and version of Ruby installed. whereas Bignum is limited by the amount of ram you have on your machine.  Here is the boundary on my machine:

<pre>
puts 1070000000.class
puts 1080000000.class
</pre>  

Outputs:

<pre>
PS C:\Temp\irb> ruby .\numbers.rb
Fixnum
Bignum
PS C:\Temp\irb>
</pre>

Ruby handles the conversions of these two types automatically. 

Floating numbers, on the otherhand are instantiated from the "Float" class, e.g.:

<pre>
puts 10.5321345
puts 10.5e3

puts 10.5321345.class
puts 10.5e3.class
</pre>

This outputs:

<pre>
PS C:\Temp\irb> ruby .\numbers.rb
10.5321345
10500.0
Float
Float
PS C:\Temp\irb>
</pre>

Note: you need to have one digit before a the decimal point, and one after. 

Note, objects that hold numbers, are set to read-only. Becuase of this, increment/decrement notations like i++ and i-- don't exist in ruby. Instead the closest thing to this that you have is:

<pre>
a = 1
puts a
a = a + 1
puts a
a += 1    # shorthand way of writing a = a +1
puts a
# a++     # this won't work and will output error message. 

</pre>

This outputs:

<pre>
PS C:\Temp\irb> ruby .\numbers.rb
1
2
3
PS C:\Temp\irb>

</pre>

]]></Content>
		<Date><![CDATA[2014-12-02]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Strings]]></Title>
		<Content><![CDATA[



In Ruby, you can find the encoding used like this:

<pre>
irb(main):005:0* puts "abc".encoding
UTF-8
=> nil
irb(main):006:0>
</pre>

You can change this by writing the follow special comment:

<pre>
irb(main):005:0* puts "abc".encoding
UTF-8
=> nil
irb(main):006:0>
irb(main):002:0> # encoding: us-ascii
irb(main):003:0* puts "abc".encoding
US-ASCII
=> nil
</pre>

In a ruby script, this encoding command has to appear on the first line of the file, or the second line, if the first line is a she-bang line, e.g.:

<pre>
#!/usr/bin/ruby
# encoding: UTF-8
.
.
.
</pre>



]]></Content>
		<Date><![CDATA[2014-12-02]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Escaping character in strings]]></Title>
		<Content><![CDATA[You can escape characters that are in quotes. In most languages you can only escape things that are in double quotes. However in Ruby you can escape in single-quotes. 

Here's an example:

<pre>

irb(main):023:0> string = 'a dog\'s bone'
=> "a dog's bone"
irb(main):024:0> puts string
a dog's bone
=> nil
irb(main):025:0>
irb(main):025:0> string = "a dog\'s bone"
=> "a dog's bone"
irb(main):026:0> puts string
a dog's bone
=> nil
</pre>


Instead of using single/double quotes, you can specify your own "quote-encaser". This is done using the "%q" syntax, where the "q" stands for quote. But this encaser has to be a non-alphanumeric character, e.g. !,?,-....etc. Also encaser's that comes in pair's such as [] or {}, can also be used in the form of a pair. e.g.:

<pre>
irb(main):050:0* string = %q-a dog's bone-
=> "a dog's bone"
irb(main):051:0> puts string
a dog's bone
=> nil
irb(main):052:0> string = %q[a dog's bone]
=> "a dog's bone"
irb(main):053:0> puts string
a dog's bone
=> nil
irb(main):054:0> string = %q(a dog's bone)
=> "a dog's bone"
irb(main):055:0> puts string
a dog's bone
=> nil
irb(main):056:0> string = %q+a dog's bone+
=> "a dog's bone"
irb(main):057:0> puts string
a dog's bone
=> nil
</pre>
 
The advantage of using the %q is that it means that you don't need to do as much escaping. 



<h2>Double quotes</h2>
Double quotes works like single quotes, but it lets you ordinary characters special meaning by using back-slash to escape them. It also gives does the reverse for special characters. However one of the most important thing you can do is embed values of variables into the double-quotes, using the #{variable-name} notation.  

e.g.:

<pre>

irb(main):077:0* string = "a dog's bone"
=> "a dog's bone"
irb(main):078:0> puts string
a dog's bone
=> nil
irb(main):079:0> string = "a dog\"s bone"
=> "a dog\"s bone"
irb(main):080:0> puts string
a dog"s bone
=> nil
irb(main):081:0> string = "a dog's \n bone"
=> "a dog's \n bone"
irb(main):082:0> puts string
a dog's
 bone
=> nil
irb(main):083:0> name = "Jack"
=> "Jack"
irb(main):084:0> string = "The dog is called: \t #{name}"   # here we embeded a variable and used tab. 
=> "The dog is called: \t Jack"
irb(main):085:0> puts string
The dog is called:       Jack
=> nil
irb(main):086:0>
</pre> 

You can also do calculations inside the #{...} notation, e.g.:

<pre>
irb(main):086:0> num1 = 8
=> 8
irb(main):087:0> num2 = 5
=> 5
irb(main):088:0> puts "the sum is: #{num1 + num2}"
the sum is: 13
=> nil
irb(main):089:0>

</pre>
]]></Content>
		<Date><![CDATA[2014-12-02]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Multiline String Variable (using heredoc)]]></Title>
		<Content><![CDATA[Ruby supports the "heredoc" syntax for strings.  Here's an example:

<pre>
string = <<EOS
  This is the first line.
  This is the second line. 
  This is the third and final line. 
EOS                                  

puts string
</pre>

This outputs:

<pre>
PS C:\Temp\irb> ruby .\heredoc.rb
  This is the first line.
  This is the second line.
  This is the third and final line.
PS C:\Temp\irb>
</pre>

Note, you can't put any spacing before the closing "EOS" terminator, otherwise it will fail. EOS has to start at the very beginning of the line. However you can override this by prefixing "-" at the opening delimiter:

<pre>
string = <<-EOS
  This is the first line.
  This is the second line. 
  This is the third and final line. 
                  EOS

puts string
</pre>

This time I was allowed to indent it. But the output remains the same. ]]></Content>
		<Date><![CDATA[2014-12-02]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - String Operators]]></Title>
		<Content><![CDATA[<pre>
PS C:\Temp\irb> irb
irb(main):001:0> "Hello world"[1]   # Returns a single character from specified position
=> "e"
irb(main):002:0> "Hello world"[2,3]   # Returns the next 3 characters from position 2
=> "llo"
irb(main):003:0> "Hello world"["ell"]   # Searches for a match
=> "ell"
irb(main):004:0> "Hello world"["elx"]   # Searches for a match
=> nil
irb(main):005:0> message = "Hello CodingBee"
=> "Hello CodingBee"
irb(main):006:0> puts message
Hello CodingBee
=> nil
irb(main):007:0> message["Bee"] = "Duck"  # This does a find and replace, and makes it a permenant change to string. But only changes the firstInstance of a match. 
=> "Duck"
irb(main):008:0> puts message
Hello CodingDuck                
=> nil
irb(main):001:0> "hello! " *3
=> "hello! hello! hello! "
irb(main):002:0> "abc" + "def"
=> "abcdef"
irb(main):003:0>


</pre>



]]></Content>
		<Date><![CDATA[2014-12-02]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - String Methods]]></Title>
		<Content><![CDATA[<pre>
irb(main):003:0> message = "hello"
=> "hello"
irb(main):004:0> message.upcase
=> "HELLO"
irb(main):005:0> puts message
hello
=> nil
irb(main):006:0> message.upcase!
=> "HELLO"
irb(main):007:0> puts message
HELLO
=> nil
irb(main):008:0> message.downcase!
=> "hello"
irb(main):013:0> greetings = "                    hello world!           "
=> "                    hello world!           "
irb(main):014:0> puts greetings
                    hello world!
=> nil
irb(main):015:0> greetings.strip!.capitalize!        # here we are chaining methods. 
=> "Hello world!"
irb(main):016:0> puts greetings
Hello world!
=> nil
irb(main):017:0>



irb(main):061:0* htmlgreeting = "[html][body][strong]hello world[/strong][/body][/html]"
=> "[html][body][strong]hello world[/strong][/body][/html]"
irb(main):062:0> puts htmlgreeting
[html][body][strong]hello world[/strong][/body][/html]
=> nil
irb(main):063:0> puts htmlgreeting.gsub("[","<").gsub("]",">")
<html><body><strong>hello world</strong></body></html>
=> nil
irb(main):064:0>



irb(main):064:0> weekdays = "monday->tuesday->wednesday->thursday->friday"
=> "monday->tuesday->wednesday->thursday->friday"
irb(main):065:0> array_weekdays = weekdays.split("->")
=> ["monday", "tuesday", "wednesday", "thursday", "friday"]
irb(main):066:0> puts array_weekdays[2]
wednesday
=> nil
irb(main):067:0>


</pre>]]></Content>
		<Date><![CDATA[2014-12-02]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Vagrant - starting/stopping Vm]]></Title>
		<Content><![CDATA[to simulate a shutdow, do:

<pre>vagrant halt machinename</pre>

to Switch vm back on again, do:

<pre>vagrant reloead machinename</pre>]]></Content>
		<Date><![CDATA[2014-12-03]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[vagrant|VirtualBox]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Vagrant]]></Categories>
	</post>
	<post>
		<Title><![CDATA[bundle exec rake lint/validate/spec/beaker]]></Title>
		<Content><![CDATA[befor you run a spec test, you first have to run:
<pre>bundle install</pre>
This will download all the dependencies (i.e ruby gems).

Then you do:

&nbsp;
<pre>bundle exec rake {task}</pre>
&nbsp;

This in parts means:

bundle exec - "execute the following script using the gems specified in the gemfile"

This in turn executes:
<pre>rake {task}</pre>
Here since we are calling rake it means that rake calls the rakefile, as a script, and it passes the "{task}" as an input parameter. This input parameter can take any of the following values:
<ul>
	<li>lint           - check for good formatting, e.g. code is properly indented</li>
	<li>validate  - checks for syntax error</li>
	<li>spec     - runs the rspec tests (unit tests)</li>
	<li>beaker  - runs the beaker tests (acceptance tests, also written in rspec).</li>
</ul>
&nbsp;

If you want to see all the available input parameters for rake, do (run this while in the directory containing the gemfile):

<pre>
[root@puppetmaster dummyModule]# bundle exec rake 
rake beaker            # Run beaker acceptance tests
rake beaker_nodes      # List available beaker nodesets
rake build             # Build puppet module package
rake clean             # Clean a built module package
rake coverage          # Generate code coverage information
rake help              # Display the list of available rake tasks
rake lint              # Check puppet manifests with puppet-lint
rake spec              # Run spec tests in a clean fixtures directory
rake spec_clean        # Clean up the fixtures directory
rake spec_prep         # Create the fixtures directory
rake spec_standalone   # Run spec tests on an existing fixtures directory
rake syntax            # Syntax check Puppet manifests and templates
rake syntax:hiera      # Syntax check Hiera config files
rake syntax:manifests  # Syntax check Puppet manifests
rake syntax:templates  # Syntax check Puppet templates
rake validate          # Check syntax of Ruby files and call :syntax
[root@puppetmaster dummyModule]# 
</pre>



&nbsp;

&nbsp;

&nbsp;

&nbsp;

What this does is that

&nbsp;

&nbsp;

Each puppet module has to contain the following files:

&nbsp;
<ul>
	<li><strong>rakefile</strong> - this basically runs a sequence of tasks</li>
	<li><strong>gemfile</strong> - this contains a set of gemfiles required to run the rspec tests.</li>
	<li><strong>fixtures.yml</strong> - this contains the puppet module dependencies, i.e. one module may require another module in order for it to run. Note, this often a hidden file, located at teh top level of the module folder</li>
	<li><strong>spec_helper.rb</strong> - this contains the actual rspec code.</li>
	<li><strong>spec_acceptance.rb</strong> - this is to do with beaker</li>
</ul>
&nbsp;

&nbsp;

&nbsp;

On each puppet machine, the following exists:
<ul>
	<li>/etc/puppet/puppet.conf</li>
	<li>/etc/hiera.yaml</li>
	<li>/etc/r10k.yaml</li>
</ul>
&nbsp;

You should only have one of these files per puppet master/agent.

&nbsp;


If you want to test against a particular version of puppet, then you can do something like:

<pre>PUPPET_GEM_VERSION="~> 4.0" bundle exec rake spec</pre>

In this example it will test against the latest 4+ puppet version, e.g. 4.3.2. 

This will work if you gem file has a builtin if statement, e.g.:


<pre>$ cat ../Gemfile
source 'https://rubygems.org'

group :development, :test do
  gem 'puppetlabs_spec_helper', '1.0.1', :require => false
  gem 'rspec-puppet',           '2.3.2',  :require => false
  gem 'rspec-core',             '3.1.7',  :require => false
  gem 'puppet-lint-strict_indent-check',  :require => false
  gem 'metadata-json-lint',               :require => false
  gem 'rspec-puppet-facts',               :require => false
end

if puppetversion = ENV['PUPPET_GEM_VERSION']
  gem 'puppet', puppetversion, :require => false
else
  gem 'puppet', '~> 3.4', :require => false
end

# vim:ft=ruby
</pre>


Resources

<a href="https://www.google.co.uk/search?q=bundle+exec+rake&amp;ie=utf-8&amp;oe=utf-8&amp;aq=t&amp;rls=org.mozilla:en-GB:official&amp;client=firefox-a&amp;channel=sb&amp;gfe_rd=cr&amp;ei=9iV_VN29EMHH8ge6x4H4DA">bundle exec rake</a>]]></Content>
		<Date><![CDATA[2014-12-03]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|RSpec]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Regular Expressions]]></Title>
		<Content><![CDATA[In ruby, regular expressions are instantiated from the "<a href="http://ruby-doc.org/core-2.1.1/Regexp.html">Regexp</a>" class.  

The Rubymine ide has a very useful way to check any regexs that you have written. There's also an online <a href="http://rubular.com/">ruby regex checker called rubular</a>. 


You can write regular expressions in the following 2 ways:

<pre>/regular-expression/        # i.e. encased in /.../
%r(regular-exression)       # i.e. contained inside %r(...)</pre>

In Ruby, regex is commonly used to find matches in a string, and to test for a regex match, you need to use the "=~" operator:

<pre>
PS C:\Temp\irb> irb
irb(main):001:0> "hello world" =~ /el/
=> 1
irb(main):002:0> "hello world" =~ /elx/
=> nil
irb(main):003:0> /el/ =~ "hello world"               # it also works the other way round. 
=> 1
</pre>

As you can see, the return value of "1"  means a match has been found, otherwise it is nil. you can also check for zero matches using "!~"


<pre>
irb(main):004:0> "hello world" !~ /elx/
=> true
irb(main):005:0> "hello world" !~ /el/
=> false
irb(main):006:0>
</pre>

You can also capture match into an object. This is possible because the rexexp class has a "match" method for this purpose:

<pre>

irb(main):015:0* matches = /(\d+):(\d+)/.match("The current time is 15:45am")
=> #<MatchData "15:45" 1:"15" 2:"45">

</pre>

In this example, we are actually doing 3 regex matches with a single regex statement. This is possible by the use of round-brackets in our regex statement. The results of each match has been captured into the "matches" variable.

The regex we used is /(\d+):(\d+)/, lets look at this in parts:

- (\d+) - this means find a match of 1 or more digit. This will only return the first match. It captures the first match, and then stores it into the matches variable. 
- (\d+) - This does same as before, but looks for the 2nd match. 
- \d+:\d+ - the final match is done by removing all the brackets to find a whole match. 

Hence in the above example we go a single whole match, followed by 2 sub matches. 

Now we can output various parts of the matches (including non-matches) like this:


<pre>
irb(main):015:0* matches = /(\d+):(\d+)/.match("The current time is 15:45am")
=> #<MatchData "15:45" 1:"15" 2:"45">
irb(main):016:0> matches.class
=> MatchData
irb(main):017:0> matches.pre_match      # sub-string leading up to the first match.
=> "The current time is "                
irb(main):018:0> matches.post_match    # sub-string after the last match.
=> "am"
irb(main):019:0> matches[0]          # whole match
=> "15:45"
irb(main):020:0> matches[1]          # first sub match
=> "15"
irb(main):021:0> matches[2]          # second sub match
=> "45"
irb(main):022:0> matches[3]          # nothing after that. 
=> nil
irb(main):023:0>
</pre>

Note, the pre_match and post_match will return the substrings before and after the whole matches[0].
 
There are also a bunch of <strong>global variables</strong> that gets set/updated every time you use the "=~" or ".match" method, they are:

<ul>
	<li>$` - this is the equivalent to the pre_match method</li>
	<li>$' - this is equivalent to the pust_match method</li>
	<li>$1, $2....etc   - these are the sub-matches resulting from the (...)</li>


</ul>

note, there is no $0 variable. 

Here they are in action (after starting up a new irb session):


<pre>



PS C:\Temp\irb> irb
irb(main):001:0> $`       # to begin with the don't hold values. 
=> nil                
irb(main):002:0> $'       
=> nil
irb(main):003:0> $1
=> nil
irb(main):004:0> "hello world" =~ /elx/ 
=> nil
irb(main):005:0> $`           # Still doesn't hold values since no match made
=> nil
irb(main):006:0> $'
=> nil
irb(main):007:0> $1
=> nil
irb(main):008:0> "hello world" =~ /el/
=> 1
irb(main):009:0> $`
=> "h"
irb(main):010:0> $'
=> "lo world"
irb(main):011:0> $1          # Still doesn't hold value, since ther were no sub-expressions, .i.e. (...)
=> nil
irb(main):012:0> matches = /(\d+):(\d+)/.match("The current time is 15:45am")
=> #<MatchData "15:45" 1:"15" 2:"45">
irb(main):013:0> $`
=> "The current time is "
irb(main):014:0> $'
=> "am"
irb(main):015:0> $1
=> "15"
irb(main):016:0> $2
=> "45"
irb(main):017:0>

</pre>

If you want to match for all patterns and capture them, rather than just capturing just the first match, then you can use the "scan" method instead of the match method, i.e. :

<pre>
irb(main):040:0* all_matches = "The current time is 15:45am".scan(/\d+/)
=> ["15", "45"]
irb(main):041:0> all_matches[0]
=> "15"
irb(main):042:0> all_matches[1]
=> "45"
</pre>

As you can see, the way the scan method is written is slightly different to the match method. 

Also there is no pre_match or post_match, e.g. like pre_scan and post_scan don't exist. 

However, the global variables still work:

<pre>
irb(main):047:0> all_matches = "The current time is 15:45am".scan(/\d+/)
=> ["15", "45"]
irb(main):048:0> all_matches[0]
=> "15"
irb(main):049:0> all_matches[1]
=> "45"
irb(main):050:0> $`
=> "The current time is 15:"  # Here the 15: means the we captured this during the last "scan-cycle"
irb(main):051:0> $'
=> "am"
</pre>



Next we have the "gsub" string, this is a bit of a combination between regex and linux's awk system, here's an example:

<pre>irb(main):068:0> message = "hello world"
=> "hello world"
irb(main):069:0> reverse_message = message.gsub(/([A-Za-z]+) ([A-Za-z]+)/, "\\2 \\1")
=> "world hello"
irb(main):070:0> puts reverse_message
world hello
=> nil
irb(main):071:0></pre>

Note the "\\2 \\1" could have been rewritten with single quotes '\2 \1', e.g. :

<pre>
irb(main):072:0> reverse_message = message.gsub(/([A-Za-z]+) ([A-Za-z]+)/, '\2 wide \1')
=> "world wide hello"
</pre>

gsub also lets you do some more complex stuff by allowing you to pass the matches into a code block:

<pre>
irb(main):073:0> uncrypted = "Password: LiverPool"
=> "Password: LiverP0ol"
irb(main):074:0> encrypted = uncrypted.gsub(/(Password:)\s+(\w+)/) { |matches| "#{$1} " + "?"*$2.length }
=> "Password: ?????????"
irb(main):075:0>
</pre>

Here's a quick reference to the escaped characters:

\d - a digit
\D - anything except a digit
\h - an character  (0-9a-zA-Z)
\H - anything except a character
\R - A generic linebreak sequence, i.e. \r\n
\s - Any whitespace character, e.i. space or tab
\S - Anaything except a whitespace character
\w - A word, i.e. shorthand for [a-zA-Z1-9]+
\W - anything except a word



You can match the start and end of a line like this:

<pre>


irb(main):105:0> matches = /^el/.match("hello world")
=> nil
irb(main):106:0> matches = /^hel/.match("hello world")
=> #<MatchData "hel">
irb(main):107:0> matches[0]
=> "hel"
irb(main):111:0> matches = /worl$/.match("hello world")
=> nil
irb(main):112:0> matches = /world$/.match("hello world")
=> #<MatchData "world">
irb(main):113:0> matches[0]
=> "world"
irb(main):114:0>

</pre>


You can ignore upper/lower case using the "i" option, by placing it straight after the regex expression:



<pre>irb(main):121:0> matches = /hel/i.match("hEllo world")
=> #<MatchData "hEl">
</pre>]]></Content>
		<Date><![CDATA[2014-12-04]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Symbols]]></Title>
		<Content><![CDATA[Sybmols are essentially a special kind of object. They are denoted by a colon prefixing the object's name, e.g.:

:{object-name}

You can think of them as a mix between a unique constant and a string, i.e. a constant/string hybrid.  

You can think of a symbol, where the name of the variable itself is what we're interested in, and not what value it hold's, hence by default a symbol's value defaults to holding the string of it's own name: 

<pre>
irb(main):124:0> :var1
=> :var1
irb(main):125:0> puts :var1
var1
=> nil
irb(main):126:0> :var1 = "test"   # in fact you can't do this, it will give an error message.
</pre>


It also means that you don't need to set a value of a symbol, you just start using it when you need it.  


We've already come across symbols, as a way to define attributes for a class.  


<pre>
# Here we are defining a class called "Employee"
class Employee
  attr_accessor :age     # Notice that we used the special "attr_accessor"  
end

john = Employee.new
john.age = "John"      # here we are setting the instance variable
puts employee1.age           # here we are getting the instance variable
</pre>

What happens here is that attr_accessor has to generate a "get" and "set" method, along with an instance variable. In order to do this, attr_accessor needs to first figure out what to name these methods, and that's where the ":age" symbol comes in. attr_accessor essentially uses ":age" as a template to generate the method names and instance name, hence behind the scenes attr_accessor, get's expanded into something like this:

<pre>
class Employee
  def set_age (years)
    @age = years
  end

  def get_age
    @age
  end

end

john = Employee.new
john.set_age(25)      # Notice that the syntax changed a bit, i.e. we are no longer using "="
puts john.get_age     # also notice that we now have to use "set_age" and "get_age" methods.     
</pre>
     
Notice that we also had to change the syntax a bit, using the new methods set_age and get_age. we also have to use the "john.set_age(25)" rather than using the "=". However you can still go back to using the "=" sign by using the following syntactic approach:


<pre>
class Employee
  def set_age=(years)      # notice the "=" syntax. 
    @age = years
  end

  def get_age
    @age
  end

end

john = Employee.new
john.set_age = 25      # here we can use the "=" again
puts john.get_age         
</pre>

This is getting really complicated, that's why as you can see, with symbols (and in this case with conjunction with attr_accessor), the code looks and reads a lot simpler. Symbols make life easier elsewhere too. 


Each symbol is an instance of the <a href="http://www.ruby-doc.org/core-2.1.5/Symbol.html">Symbol</a> class. 

symbol literal:

:"abcd"
:"5"


name = ""


http://www.randomhacks.net/2007/01/20/13-ways-of-looking-at-a-ruby-symbol

http://programmers.stackexchange.com/questions/24460/what-is-a-symbol-in-ruby

]]></Content>
		<Date><![CDATA[2014-12-04]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - What is Facter]]></Title>
		<Content><![CDATA[Puppet is two things:
<ul>
 	<li>It is a command line tool for querying node specific information</li>
 	<li>It is a place where a node specific key-value hash is stored.</li>
</ul>
Facter is a standalone tool that house's environment-level variables. In that sense, It is a bit like the bash equivalent of the "env" command, in fact there is an overlap between info stored in facts and info stored in the "env" command.

Each key-value in facter is referred to as "facts". You can view all the facts and their values using the facter command on it's own:
<pre>facter
</pre>
This will list all the various environment variables, and their respective values. These collection of facts comes with facter straight out of the box and are referred to as core facts. However you can add your own "custom facts" to this collection, which we will cover later.

If you just to view one variable's value, then you can just do:
<pre>facter  {variable-name}
</pre>
For example:
<pre>[root@puppetmaster ~]# facter virtual
virtualbox
</pre>
However the key thing that makes facter important to puppet, is that facters facts are available throughout your puppet code as "global variables". Here is an example:
<pre>[root@puppetmaster modules]# tree user_account
user_account
└── manifests
    └── init.pp

1 directory, 1 file
[root@puppetmaster modules]# cat user_account/manifests/init.pp 
class user_account {

  user { 'homer':
    ensure =&gt; 'present',
    uid =&gt; '121',
    shell =&gt; '/bin/bash',
    home =&gt; '/home/homer',
  }


  file {'/tmp/testfile.txt':
    ensure =&gt; file,
    content =&gt; "the value for the 'osfamily' fact is: $osfamily \n",
  }

}
[root@puppetmaster modules]# puppet agent --test
Notice: /Stage[main]/Activemq::Service/Service[activemq]/ensure: ensure changed 'stopped' to 'running'
Info: /Stage[main]/Activemq::Service/Service[activemq]: Unscheduling refresh on Service[activemq]
Notice: Finished catalog run in 4.09 seconds
[root@puppetmaster modules]# cat /tmp/testfile.txt 
the value for the 'osfamily' fact is: RedHat 
[root@puppetmaster modules]# facter osfamily
RedHat
[root@puppetmaster modules]# 

</pre>
Notice here, that we didn't define $osfamily, we just used it like it was a normal variable. However there are <a href="https://docs.puppetlabs.com/puppet/latest/reference/lang_facts_and_builtin_vars.html#accessing-facts-from-puppet-code">other ways to refer to a fact in your puppet code</a>:
<ul>
 	<li><code>$fact_name</code></li>
 	<li><code>$::fact_name</code> - this is better practice</li>
 	<li><code>$facts['fact_name'] - this is best practice, put only works from puppet 3.7 onwards</code></li>
</ul>
If

In puppet, each of these environment variables are referred to as "facts". There are 3 types of facts:

- core facts
- custom facts
- external facts

All core facts are defined in the top level scope and are accessible anywhere in your code.

You can create your own facter on the <a href="http://puppetlabs.com/blog/facter-part-1-facter-101">bash command line</a> like this:
<pre>export FACTER_{facter_name}="string"    # e.g.:

[root@puppetmaster /]# export FACTER_alphabet="abcdefg" 
[root@puppetmaster /]# facter alphabet
abcdefg
[root@puppetmaster /]# 


</pre>
There is another place that you can access environment, these are <a title="Puppet – The Puppet main config file (puppet.conf)" href="http://codingbee.net/tutorials/puppet/puppet-puppet-main-config-file-puppet-conf/">puppet specific config settings</a>, which are accessble through:
<pre>puppet config print

http://serverfault.com/questions/725595/extract-nested-hash-facts-from-puppets-facter-command-line-tool-how?newreg=03145631d44641e688bb6ef527e91600
</pre>
http://terrarum.net/blog/puppet-infrastructure.html

https://docs.puppetlabs.com/facter/latest/fact_overview.html#

https://docs.puppetlabs.com/puppet/latest/reference/lang_facts_and_builtin_vars.html

https://docs.puppetlabs.com/facter/latest/core_facts.html

https://docs.puppetlabs.com/facter/latest/custom_facts.html]]></Content>
		<Date><![CDATA[2014-12-09]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Rails - Installing Rails on a Windows Machine]]></Title>
		<Content><![CDATA[Rails (aka ruby on rails), is a collection of ruby related products. 

On windows, it is possible to install Ruby, and install rails separately, however it is much more trouble-free if you install them together using the <a href="http://railsinstaller.org/en">railsinstaller</a>.    


First install:

http://railsinstaller.org/en

You might also need to add the following variables to the path variable:

<pre>
C:\RailsInstaller\Ruby2.1.0
C:\RailsInstaller\Ruby2.1.0\bin    </pre>

Note, the ruby version numbwers could be slightly different, so first locate your bin directory and ensures that it exists before adding it tot the path variable. 

This bin, directory above has a number of essential <a href="http://codingbee.net/tutorials/ruby/ruby-command-line-utilities/" title="Ruby – Command Line Utilities">ruby/rails command line utilities</a>. 

Now open up a powershell terminal and check what version of the tools we have installed:


<pre>
PS C:\Temp\rails> ruby -v
ruby 2.1.5p273 (2014-11-13 revision 48405) [i386-mingw32]
PS C:\Temp\rails> rails -v
DL is deprecated, please use Fiddle
Rails 4.1.8
PS C:\Temp\rails> bundler -v
DL is deprecated, please use Fiddle
Bundler version 1.7.7
PS C:\Temp\rails> gem -v
2.2.2
PS C:\Temp\rails> irb -v
DL is deprecated, please use Fiddle
irb 0.9.6(09/06/30)
PS C:\Temp\rails> rake --version
rake, version 10.4.2
</pre>

This also is a way to check that all the tools have been installed and are responding. 



<h2>Troubleshooting</h2>



Also, follow this guide if you have matching error message:

https://gist.github.com/fnichol/867550

if you are following above troubleshooting instructions about ssl certificate, then to make it permenant, you simply create a new  sytem environment variable, i.e:

<a href="http://codingbee.net/wp-content/uploads/2014/12/system-environment-variable.png"><img src="http://codingbee.net/wp-content/uploads/2014/12/system-environment-variable.png" alt="" width="1233" height="712" class="alignnone size-full wp-image-2211" /></a>









]]></Content>
		<Date><![CDATA[2014-12-13]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[rails|ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Rails]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Command Line Utilities]]></Title>
		<Content><![CDATA[There are a bunch of command line utilities that comes with ruby and rails,

You can find them via your ruby/rail installation's bin folder, e.g. on a windows machine, your bin folders, could be something similar to:

C:\RailsInstaller\Ruby2.1.0\bin


Here are the main utilities you'll find in this folder:


<ul>
	<li>bundle</li>
	<li>bundler</li>
	<li>gem</li>
	<li>irb</li>
	<li>rails</li>
	<li>rake</li>
</ul>


]]></Content>
		<Date><![CDATA[2014-12-13]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Rails - Create a new rails project]]></Title>
		<Content><![CDATA[To create a new rails project on a windows machine, you need to take the following steps: 


1. start powershell terminal

2. create a folder that will house all your rails project

<pre>
PS C:\> mkdir RailsProjects


    Directory: C:\


Mode                LastWriteTime     Length Name
----                -------------     ------ ----
d----        13/12/2014     15:57            RailsProjects

PS C:\>
</pre>

3. cd into that directory

<pre>
PS C:\> cd .\RailsProjects
PS C:\RailsProjects>
</pre>

4. Next we have to set up this folder as our main folder that will house all our rails:


<pre>

PS C:\RailsProjects> gem install rails
Successfully installed rails-4.1.8
Parsing documentation for rails-4.1.8
Done installing documentation for rails after 2 seconds
1 gem installed
PS C:\RailsProjects> gem install rails

</pre>


5. Now run the following:
Now run:


<pre>
rails new {rails-project-name}
</pre>

For, example, here I am creating a rails project called "My-First-Rails-Project":

<pre>
PS C:\RailsProjects> rails new My-First-Rails-Project
DL is deprecated, please use Fiddle
      create
      create  README.rdoc
      create  Rakefile
      create  config.ru
      create  .gitignore
      create  Gemfile
      create  app
      create  app/assets/javascripts/application.js
      create  app/assets/stylesheets/application.css
      create  app/controllers/application_controller.rb
      create  app/helpers/application_helper.rb
      create  app/views/layouts/application.html.erb
      create  app/assets/images/.keep
      create  app/mailers/.keep
      create  app/models/.keep
      create  app/controllers/concerns/.keep
      create  app/models/concerns/.keep
      create  bin
      create  bin/bundle
      create  bin/rails
      create  bin/rake
      create  config
      create  config/routes.rb
      create  config/application.rb
      create  config/environment.rb
      create  config/secrets.yml
      create  config/environments
      create  config/environments/development.rb
      create  config/environments/production.rb
      create  config/environments/test.rb
      create  config/initializers
      create  config/initializers/assets.rb
      create  config/initializers/backtrace_silencers.rb
      create  config/initializers/cookies_serializer.rb
      create  config/initializers/filter_parameter_logging.rb
      create  config/initializers/inflections.rb
      create  config/initializers/mime_types.rb
      create  config/initializers/session_store.rb
      create  config/initializers/wrap_parameters.rb
      create  config/locales
      create  config/locales/en.yml
      create  config/boot.rb
      create  config/database.yml
      create  db
      create  db/seeds.rb
      create  lib
      create  lib/tasks
      create  lib/tasks/.keep
      create  lib/assets
      create  lib/assets/.keep
      create  log
      create  log/.keep
      create  public
      create  public/404.html
      create  public/422.html
      create  public/500.html
      create  public/favicon.ico
      create  public/robots.txt
      create  test/fixtures
      create  test/fixtures/.keep
      create  test/controllers
      create  test/controllers/.keep
      create  test/mailers
      create  test/mailers/.keep
      create  test/models
      create  test/models/.keep
      create  test/helpers
      create  test/helpers/.keep
      create  test/integration
      create  test/integration/.keep
      create  test/test_helper.rb
      create  tmp/cache
      create  tmp/cache/assets
      create  vendor/assets/javascripts
      create  vendor/assets/javascripts/.keep
      create  vendor/assets/stylesheets
      create  vendor/assets/stylesheets/.keep
         run  bundle install
DL is deprecated, please use Fiddle
Fetching gem metadata from https://rubygems.org/..........
Resolving dependencies...
Using rake 10.4.2
Using i18n 0.6.11
Using json 1.8.1
Using minitest 5.5.0
Using thread_safe 0.3.4
Using tzinfo 1.2.2
Using activesupport 4.1.8
Using builder 3.2.2
Using erubis 2.7.0
Using actionview 4.1.8
Using rack 1.5.2
Using rack-test 0.6.2
Using actionpack 4.1.8
Using mime-types 2.4.3
Using mail 2.6.3
Using actionmailer 4.1.8
Using activemodel 4.1.8
Using arel 5.0.1.20140414130214
Using activerecord 4.1.8
Using bundler 1.7.7
Using coffee-script-source 1.8.0
Using execjs 2.2.2
Using coffee-script 2.3.0
Using thor 0.19.1
Using railties 4.1.8
Using coffee-rails 4.0.1
Using hike 1.2.3
Using multi_json 1.10.1
Using jbuilder 2.2.5
Using jquery-rails 3.1.2
Using tilt 1.4.1
Using sprockets 2.12.3
Using sprockets-rails 2.2.2
Using rails 4.1.8
Using rdoc 4.2.0
Using sass 3.2.19
Using sass-rails 4.0.5
Using sdoc 0.4.1
Using sqlite3 1.3.10
Using turbolinks 2.5.3
Using tzinfo-data 1.2014.10
Using uglifier 2.6.0
Your bundle is complete!
Use `bundle show [gemname]` to see where a bundled gem is installed.
PS C:\RailsProjects>   
</pre>    


This creates the folder for the rails project, of the same name:

<pre>
PS C:\RailsProjects> ls


    Directory: C:\RailsProjects


Mode                LastWriteTime     Length Name
----                -------------     ------ ----
d----        13/12/2014     17:18            My-First-Rails-Project


PS C:\RailsProjects>
</pre>


At this point the rails project has now been created and is housed inside this folder. 
 

6. Now let's check the contents of the new rails project folder. First cd into it, then view it's contents:

<pre>
PS C:\RailsProjects>
PS C:\RailsProjects> cd .\My-First-Rails-Project
PS C:\RailsProjects\My-First-Rails-Project> ls


    Directory: C:\RailsProjects\My-First-Rails-Project


Mode                LastWriteTime     Length Name
----                -------------     ------ ----
d----        13/12/2014     17:17            app
d----        13/12/2014     17:17            bin
d----        13/12/2014     17:17            config
d----        13/12/2014     17:17            db
d----        13/12/2014     17:17            lib
d----        13/12/2014     17:17            log
d----        13/12/2014     17:17            public
d----        13/12/2014     17:17            test
d----        13/12/2014     17:17            tmp
d----        13/12/2014     17:17            vendor
-a---        13/12/2014     17:17        466 .gitignore
-a---        13/12/2014     17:17        154 config.ru
-a---        13/12/2014     17:17       1288 Gemfile
-a---        13/12/2014     17:18       2836 Gemfile.lock
-a---        13/12/2014     17:17        249 Rakefile
-a---        13/12/2014     17:17        478 README.rdoc


PS C:\RailsProjects\My-First-Rails-Project>
</pre>

You should then see contents similar to this. 

7. While in this folder, we can now check whether the rails project is set up correctly by seeing if we can start up the rails web server, which is done like this:  

<pre>
PS C:\RailsProjects\My-First-Rails-Project> rails server
=> Booting WEBrick
=> Rails 4.1.8 application starting in development on http://0.0.0.0:3000
=> Run `rails server -h` for more startup options
=> Notice: server is listening on all interfaces (0.0.0.0). Consider using 127.0.0.1 (--binding option)
=> Ctrl-C to shutdown server
[2014-12-13 17:30:35] INFO  WEBrick 1.3.1
[2014-12-13 17:30:35] INFO  ruby 2.1.5 (2014-11-13) [i386-mingw32]
[2014-12-13 17:30:35] INFO  WEBrick::HTTPServer#start: pid=8216 port=3000


Started GET "/" for 127.0.0.1 at 2014-12-13 17:30:36 +0000
Processing by Rails::WelcomeController#index as HTML
  Rendered C:/RailsInstaller/Ruby2.1.0/lib/ruby/gems/2.1.0/gems/railties-4.1.8/lib/rails/templates/rails/welcome/index.html.erb (13.0ms)
Completed 200 OK in 122ms (Views: 70.0ms | ActiveRecord: 0.0ms)


Started GET "/" for 127.0.0.1 at 2014-12-13 17:31:12 +0000
Processing by Rails::WelcomeController#index as HTML
  Rendered C:/RailsInstaller/Ruby2.1.0/lib/ruby/gems/2.1.0/gems/railties-4.1.8/lib/rails/templates/rails/welcome/index.html.erb (0.0ms)
Completed 200 OK in 10ms (Views: 9.0ms | ActiveRecord: 0.0ms)
</pre>
Note, at this point, you can't enter any new commands into the terminal while the rails web-server is running.

9. Now open up a web browser, and go to:
<pre>
http://localhost:3000
</pre>

<a href="http://codingbee.net/wp-content/uploads/2014/12/rails-web-server.png"><img src="http://codingbee.net/wp-content/uploads/2014/12/rails-web-server.png" alt="" width="923" height="560" class="alignnone size-full wp-image-2233" /></a>


10. If you see this page, then it means that you have successfully set up a blank rails project, which you can now start using to build your project. 

11. You can now go ahead and stop the rail server by typing ctrl+c in the terminal. This will return you back to the command prompt.  

]]></Content>
		<Date><![CDATA[2014-12-13]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[rails|ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Rails]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Rails - Introduction]]></Title>
		<Content><![CDATA[]]></Content>
		<Date><![CDATA[2014-12-13]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[rails|ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Rails]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Arrays]]></Title>
		<Content><![CDATA[<pre>
irb(main):001:0> array1 = ["a","b","c"]
=> ["a", "b", "c"]
irb(main):002:0> array1[0]
=> "a"
irb(main):003:0> array1[1]
=> "b"
irb(main):004:0> array1[2]
=> "c"
irb(main):015:0> array1.class
=> Array
irb(main):016:0> array1.first
=> "a"
irb(main):017:0> array1.last
=> "c"
irb(main):018:0> empty_array=[]
=> []
irb(main):019:0> empty_array.class
=> Array
irb(main):020:0> arr = Array.new(4) # creates an array with 4 place holders.
=> [nil, nil, nil, nil]
irb(main):022:0> arr = Array.new(3, "hello")
=> ["hello", "hello", "hello"]     # creates an array with default values. 
# Note the above three elements are all referencing the same object. This means 
# that if you apply a method to one of thes elements, it will impact all the elements:

irb(main):020:0> arr[0].upcase!
=> "HELLO"
irb(main):021:0> arr
=> ["HELLO", "HELLO", "HELLO"]

# However you can make one element point to a different object:

irb(main):022:0> arr[0] = "goodbye"
=> "goodbye"
irb(main):023:0> arr
=> ["goodbye", "HELLO", "HELLO"]
 
# If you don't want all array elements referencing the same object, then create the array using the block-form style, like this:

irb(main):026:0> arr1 = Array.new (3)  {"abc"}  
=> ["abc", "abc", "abc"]    
# Here the {...} get's iterated every time the Array generates an element for the array. 
# The block's return-value is stored as an object and the newly generated element points to this.  
# That therefore means:
irb(main):027:0> arr1[0].upcase!
=> "ABC"
irb(main):028:0> arr1
=> ["ABC", "abc", "abc"]

# This approach also lets you created multi-dimensinaol arrays, e.g.:


irb(main):044:0* arr1 = Array.new (3)  {Array.new (3)}
=> [[nil, nil, nil], [nil, nil, nil], [nil, nil, nil]]
irb(main):045:0> arr1[1]
=> [nil, nil, nil]
irb(main):046:0> arr1[1][2]
=> nil

# You can also convert a string into an array, using the "%w" notation:

irb(main):068:0> arr1 = %w(hello amazing world of Ruby)   # note, we don't use quotes here. 
=> ["hello", "amazing", "world", "of", "Ruby"]
irb(main):069:0> arr1[0]
=> "hello"                  # by default, the white space is used as the delimiter. 

# you can use a black space to prevent "%w" to use a certain space as a delimter. 

irb(main):077:0* arr1 = %w(hello\ amazing\ world of Ruby)
=> ["hello amazing world", "of", "Ruby"]
irb(main):078:0> arr1[0]
=> "hello amazing world"

# Now if you do:

irb(main):085:0* arr1 = %w(2+2 is #{2+2})
=> ["2+2", "is", "\#{2+2}"]

# This happens because by default, %w placed the string into single quotes before 
#processing into an array. If you want to use double-quotes instead, in order to evaluate 
# the string before splitting it into an array, then use "%W":

 
irb(main):086:0> arr1 = %W(2+2 is #{2+2})
=> ["2+2", "is", "4"]
irb(main):087:0>

# You can also use the %i notation to generate an array containing symbols (data type):

irb(main):098:0> arr1 = %i(up down left right)     # note this only work in ruby 2.0+

# You can also access arrays in reverse-order. 
irb(main):117:0> arr = ["a","b","c","d"]
=> ["a", "b", "c", "d"]
irb(main):118:0> arr[-1] 

# You can also obtain a subset of an array:

irb(main):135:0* arr = ["a","b","c","d"]
=> ["a", "b", "c", "d"]
irb(main):136:0> arr[0..2]      # here we use the array's positional numbers. 
=> ["a", "b", "c"]

# You can also replace parts of an array:

irb(main):186:0* arr = ["a","b","c","d","e"]
=> ["a", "b", "c", "d", "e"]
irb(main):187:0> arr[1]
=> "b"
irb(main):188:0> arr[2]
=> "c"
irb(main):189:0> arr[3]
=> "d"
irb(main):190:0> arr[1..3] = ["hello", "amazing","world"]
=> ["hello", "amazing", "world"]
irb(main):191:0> arr
=> ["a", "hello", "amazing", "world", "e"]

# You can also append an array, like this:

irb(main):192:0> arr << "f"
=> ["a", "hello", "amazing", "world", "e", "f"]

# You can also add 2 arrays together using the plus operator:

irb(main):194:0> arr = ["the","quick"] + ["brown","fox"]
=> ["the", "quick", "brown", "fox"]
irb(main):195:0> arr[3]
=> "fox"
irb(main):196:0>

# you can also duplicate an array:

irb(main):196:0> arr = ["the","quick"] * 3
=> ["the", "quick", "the", "quick", "the", "quick"]

# The "*" can also be used to convert a an array into a string:

irb(main):202:0> ["the","quick", "brown","fox"] * "_"
=> "the_quick_brown_fox"
irb(main):203:0> ["the","quick", "brown","fox"] * " "
=> "the quick brown fox"

# You can also remove (i.e.) particular elements  of an array like this:

irb(main):204:0> [1,35,2,89,3,520,4] - [35,89,520]
=> [1, 2, 3, 4]



</pre>

Now let's take a look at some handy array methods:


<pre>

irb(main):104:0* arr = ["a","b","c","d"]
=> ["a", "b", "c", "d"]
irb(main):112:0> arr.first    # accesses the array's first element. Alternatively use arr[0]
=> "a"
irb(main):113:0> arr.last     # accesses the array's last element. Alternatively use arr[-1]
=> "d"
</pre>


Earlier we saw the use of the {...} block form style in conjunction with an array. This can be used to pass in the array's current iteration's positional number like this:

<pre>
irb(main):205:0> arr1 = Array.new (3)  {|number| number = number*2}
=> [0, 2, 4] 
</pre>

This is really powerfull becuase you can put a multiline code-block within the {...} block. What we are seeing here, is a taster of the use of a new interface, which is known as "enumerables". We'll cover more about enumerables in the next lesson.  



Here's an exmaple of how to generate an array from a method:

<pre>
class Testclass

  def arraygen (value)
    #puts value
	arr1 = []
    for i in [1,2,3]  
	  result = i + value
      arr1 << result
	  
    end
    arr1
  end
end

sampleclass = Testclass.new

arr = sampleclass.arraygen(7)

puts arr[1]
</pre>




See also:
Also checkout the ruby <a href="http://www.ruby-doc.org/core-2.1.5/Array.html">array official documentation</a>.

http://www.tutorialspoint.com/ruby/ruby_arrays.htm






]]></Content>
		<Date><![CDATA[2014-12-14]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Enumerables]]></Title>
		<Content><![CDATA[In ruby "Enumerables" is actually name of module. A module is essentially a collection of constants and methods. 

The Enumerables module contains a collection of methods that are specifically designed to iterate through a collection of objects (e.g. elements in an array), and run some ruby code against each object.    

for this reason, the enumerables methods comes built into the array class. 

One commonly used enumerable is the "map", this method lets you generate a new a new array from an existing class, using the block-form syntax:

<pre>
irb(main):241:0* arr = [11,12,13,14]
=> [11, 12, 13, 14]
irb(main):242:0> arr.map {|value| value - 2 }
=> [9, 10, 11, 12]
</pre>


There's also a "<a href="http://ruby-doc.org/core-2.1.5/Enumerable.html#method-i-reduce">reduce</a>" that you can use to sum up the numbers in an array:


<pre>
irb(main):251:0> [1,2,3].reduce(0) {|sum, v| sum + v}
=> 6
</pre>

The zero parameter for the "reduce" method is the initial value that the "sum" variable takes, e.g.:

<pre>
irb(main):252:0> [1,2,3].reduce(4) {|sum, v| sum + v}
=> 10
</pre>

The enumerable method also has a "select" and "sort" methods:

<pre>
irb(main):276:0> [100,12,43,5,58,150].select {|n| n.even?}
=> [100, 12, 58, 150]
irb(main):277:0> [100,12,43,5,58,150].sort.select {|n| n.even?}
=> [12, 58, 100, 150]
irb(main):278:0>
</pre> 

As you can see the "select" method lets you intelligently select a subset of an array, based on elements meeting a certain criteria. E.g. if "n.even?" is true, then element get's selected. 


See also:
http://ruby-doc.org/core-2.1.5/Enumerable.html]]></Content>
		<Date><![CDATA[2014-12-14]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Hashes]]></Title>
		<Content><![CDATA[Hashes are instantiated from the Hash class. 

Haskes are essentially an array where each element's value is given it's own name, rather than an array's default 1,2,3 positional numbers (i.e. number index). 

Hashes are essentially a list of key/value pairs. 

<pre>
my_hash = {}    # Using {} brackets creates an empty hash. 

# Now here's an example:

contacts = {
  "ceo" => "Mr Smith",        # notice we used double-quotes for the keys. 
  "cio" => "Mrs Jones",
  "cfo" => "Mr Anderson"      # note 
}

puts contacts["ceo"]        # Outputs: "Mr Smith"     # also notice we used double quotes here too. 


# You can rewrite this using this alternative syntax:

contacts = {
  ceo:  "Mr Smith",            # In this syntax, the keys are automatically assumed to be 
  cio: "Mrs Jones",            # symbols  
  cfo: "Mr Anderson"
}

puts contacts[:ceo]         # notice that we now call via a symbol as the key.

# It makes more sense to use "symbols" in conjunction with hashes. 

# you can also append a new key/value pair to an existing hash, like this:


contacts = {
  ceo:  "Mr Smith",
  cio: "Mrs Jones",
  cfo: "Mr Anderson"
}

puts contacts         # outputs:  "{:ceo=>"Mr Smith", :cio=>"Mrs Jones", :cfo=>"Mr Anderson"}"

contacts[:chairman] = "Mr Donald"

puts contacts[:chairman]   # outputs:   "Mr Donald"

puts contacts         # "{:ceo=>"Mr Smith", :cio=>"Mrs Jones", :cfo=>"Mr Anderson", :chairman=>"Mr Donald"}"

# Now if you try to pull out a value using a non existant key, then by default you will get "nil":


PS C:\Temp\irb> irb
irb(main):001:0> contacts = {
irb(main):002:1*   ceo:  "Mr Smith",
irb(main):003:1*   cio: "Mrs Jones",
irb(main):004:1*   cfo: "Mr Anderson"
irb(main):005:1> }
=> {:ceo=>"Mr Smith", :cio=>"Mrs Jones", :cfo=>"Mr Anderson"}
irb(main):006:0> contacts[:ceo]
=> "Mr Smith"
irb(main):007:0> contacts[:manager]
=> nil
irb(main):008:0>


# You can change the default response by using the new method to instantiate the hash
# and feed in a default value:

irb(main):014:0* contacts = Hash.new("Unassigned")
=> {}
irb(main):015:0> contacts[:ceo] = "Mr Smith"
=> "Mr Smith"
irb(main):016:0> contacts[:cio] = "Mrs Jones"
=> "Mrs Jones"
irb(main):017:0> contacts[:cfo] = "Mr Anderson"
=> "Mr Anderson"
irb(main):018:0> contacts
=> {:ceo=>"Mr Smith", :cio=>"Mrs Jones", :cfo=>"Mr Anderson"}
irb(main):019:0> contacts[:cio]
=> "Mrs Jones"
irb(main):020:0> contacts[:manager]
=> "Unassigned"
irb(main):021:0>
</pre>




# You can also pass each key/value pair into a {..} block in turn using the each method:

<pre>
contacts = {
  ceo:  "Mr Smith",
  cio: "Mrs Jones",
  cfo: "Mr Anderson"
}

contacts.each {|mykey, myvalue| puts "The key is #{mykey} and the value is #{myvalue}" }

# this will output:
The key is ceo and the value is Mr Smith
The key is cio and the value is Mrs Jones
The key is cfo and the value is Mr Anderson
=> {:ceo=>"Mr Smith", :cio=>"Mrs Jones", :cfo=>"Mr Anderson"}
irb(main):045:0> 

</pre>

Notice that we defined 2 block arguments, "mykey" and "myvalue", that's because we know that each in each iteratio, 2 bits of info will be passed into the block, hence we are able to capture them intelligently using this approach. 

Using multiple block arguments are commonly used. 









]]></Content>
		<Date><![CDATA[2014-12-14]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Ranges]]></Title>
		<Content><![CDATA[This Range class basically generates a sequence of numbers:

<pre>
irb(main):045:0> (1..10)
=> 1..10
irb(main):046:0> (1..10).begin
=> 1
irb(main):047:0> (1..10).end
=> 10
irb(main):048:0> (1...10).begin
=> 1
irb(main):049:0> (1..10)
=> 1..10
irb(main):050:0> (1..10).class
=> Range
irb(main):051:0> (1..10).begin
=> 1
irb(main):052:0> (1..10).end
=> 10
irb(main):054:0> (1..10).include?(7)
=> true
irb(main):055:0> (1..10).include?(12)
=> false

# If you want to exclude the last value in a range, then use the ... notation while using the block-iterater:

irb(main):099:0> (1..5).each {|v| puts v}
1
2
3
4
5
=> 1..5
irb(main):100:0> (1...5).each {|v| puts v}
1
2
3
4
=> 1...5
irb(main):101:0>

 
</pre>


A range also includes the enumerable module, this means you can do thing like as follows:

<pre>
# Use a range to create an array:
irb(main):056:0> (1..10).map {"hello"}
=> ["hello", "hello", "hello", "hello", "hello", "hello", "hello", "hello", "hello", "hello"]

# or:

irb(main):058:0> (1..10).map {|number| number * 2}
=> [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]

# You can also iterate through strings:

irb(main):080:0* ("aa".."ad").each {|v| puts v}
aa
ab
ac
ad
=> "aa".."ad"

</pre>

ranges also has a good integration with case-statements:

<pre>
# If we have the following script:
def bignumberchecker (value)
  case value
  when 1..10 
    puts "small number: #{value}"
  when 11..100
    puts "big number: #{value}"
  else
    puts "unknown number size: #{value}"
  end  
end

bignumberchecker 7
bignumberchecker 70
bignumberchecker 700

# The the output is:
PS C:\Temp\irb> ruby hashes.rb
small number: 7
big number: 70
unknown number size: 700

</pre>
 
]]></Content>
		<Date><![CDATA[2014-12-14]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Parrallel Assignment]]></Title>
		<Content><![CDATA[This is to do with defining multiple variables on a single line:

<pre>
irb(main):001:0> a,b = 7,16
=> [7, 16]
irb(main):002:0> a
=> 7
irb(main):003:0> b
=> 16
irb(main):004:0>
</pre>

A similar approach can also be used to create an array:

<pre>
irb(main):004:0> arr = 1,5,10,15,20
=> [1, 5, 10, 15, 20]
irb(main):005:0> arr[2]
=> 10
irb(main):006:0>
</pre>


You can also create a bunch of variables by taking values from an array:

<pre>
irb(main):001:0> a,b,c,d = ["alpha","beta","charlie","delta"]
=> ["alpha", "beta", "charlie", "delta"]
irb(main):002:0> a
=> "alpha"
irb(main):003:0> b
=> "beta"
irb(main):004:0> c
=> "charlie"
irb(main):005:0> d
=> "delta"
</pre>

If you had fewer varable names than elements in an array, then the remaining array values are ignored. 


Alternatively you can use the splat operator to capture the remaining elements into an array:

<pre>
irb(main):030:0* a, *b = ["alpha","beta","charlie","delta"]
=> ["alpha", "beta", "charlie", "delta"]
irb(main):031:0> a
=> "alpha"
irb(main):032:0> b
=> ["beta", "charlie", "delta"]
irb(main):033:0>
</pre>

Splat acts in a greedy way, taking as much elements is it logically can:

<pre>
irb(main):034:0> a, *b, d = ["alpha","beta","charlie","delta"]
=> ["alpha", "beta", "charlie", "delta"]
irb(main):035:0> a
=> "alpha"
irb(main):036:0> b
=> ["beta", "charlie"]
irb(main):037:0> d
=> "delta"
irb(main):038:0>
</pre>

This could be used to quickly capture the first and last values into an array. 

you can use splatting on the right side as well:

<pre>
irb(main):043:0> a,b,c = 1..3
=> 1..3
irb(main):044:0> a
=> 1..3
irb(main):045:0> b    # notice that this didn't work, we have nil. 
=> nil
irb(main):046:0> c
=> nil
irb(main):047:0> a,b,c = *1..3
=> [1, 2, 3]
irb(main):048:0> a
=> 1
irb(main):049:0> b        # success
=> 2
irb(main):050:0> c        # more success. 
=> 3
irb(main):051:0>
</pre>
]]></Content>
		<Date><![CDATA[2014-12-14]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Unit Testing with RSpec]]></Title>
		<Content><![CDATA[http://puppetlabs.com/blog/the-next-generation-of-puppet-module-testing

http://puppetlabs.com/presentations/puppet-loves-rspec-why-you-should-too

http://puppetlabs.com/blog/verifying-puppet-checking-syntax-and-writing-automated-tests

http://puppetlabs.com/presentations/beaker-automated-cloud-based-acceptance-testing-alice-nodelman-puppet-labs

http://rspec-puppet.com/tutorial/

Beaker:

http://puppetlabs.com/search/node/beaker

http://puppetlabs.com/presentations/beaker-automated-cloud-based-acceptance-testing-alice-nodelman-puppet-labs

http://puppetlabs.com/podcasts/podcast-beaker-cloud-enabled-acceptance-testing-tool]]></Content>
		<Date><![CDATA[2014-12-15]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>rspec-puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Certificates troubleshooting]]></Title>
		<Content><![CDATA[useful link:  
https://docs.puppetlabs.com/puppet/3.7/reference/ssl_regenerate_certificates.html 
http://gerrit.googlecode.com/svn/documentation/2.2.1/pgm-init.html



If you get the following message:

<pre>
Error: Could not request certificate: SSL_connect returned=1 errno=0 state=SSLv3 read server certificate B: certificate verify failed:
</pre>


When you get this error message, try the following:

1. locate the puppetmaster's ssl file:

<pre>
puppet config print ssldir
</pre>


2. Delete the ssl directory:

<pre>
rm -rf ssl/
</pre>

3. Restart the puppetmaster service:

<pre>
service puppetmaster restart
</pre>


4. On the puppet agent, then run:

<pre>
puppet agent -t --trace -debug
</pre>


This should work now. 

The reason this problem occured is because the agent tried to contact a master with a particular ssl. It is bit like the puppetmaster signing the certificate with the wrong signature to the one the master is expecting from the agent requests. 


See also:

https://docs.puppetlabs.com/background/ssl/
]]></Content>
		<Date><![CDATA[2014-12-19]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Vagrant - Installing Vagrant on Windows]]></Title>
		<Content><![CDATA[<h2>Setting up Vagrant on Windows</h2>
You can install virtualbox and vagrant from the powershell command-line with the help of Powershell's community repository, <a href="https://chocolatey.org/">Chocolatey</a>:

1. Open up a powershell terminal (in admin mode)
2. Run the following to connect to the chocolatey library (you can skip this step if the "choco" command already works):
<pre>iex ((new-object net.webclient).DownloadString('https://chocolatey.org/install.ps1'))</pre>
3. Now install vagrant, virtualbox, and cyg-get using the following command:
<pre>cinst virtualbox vagrant cyg-get
</pre>
note: cinst is a an alias for "choco install"

Note: cyg-get is something we are installing as part of an intermediary step for installing openssh. We'll cover more about this later.

4. Also install <a href="https://msysgit.github.io/">git-bash</a>.

5. Check that similar folders to these have now been created:
<ul>
	<li>- VirtualBox : C:\Program Files\Oracle\VirtualBox</li>
	<li>- Vagrant : C:\HashiCorp\Vagrant\bin</li>
	<li>- cyg-get : C:\tools\cygwin and also: C:\tools\cygwin\bin</li>
	<li>- chocolatey : C:\ProgramData\chocolatey\bin</li>
</ul>
Notice that vagrant doesn't get installed in your "program files" folder like other software do. The same is true for cyg-get.

10. Once you have located all these directories, you need to append all of them to the window's path environment variable.

11. Close your powershell terminal and reopen it again in admin mode. This is so that your powershell loads in the newly edited path variable.

12. Verify that vagrant is installed correctly by running the following:
<pre>PS C:\Windows\system32&gt; vagrant -v
Vagrant 1.6.5
</pre>
12. Verify that virtualbox is installed properly by running the following:
<pre>PS C:\Windows\system32&gt; vboxmanage -v
4.3.12r93733
</pre>
12c. Install the following components, but this time using the cyg-get command:
<pre>cyg-get openssh 
cyg-get rsync 
cyg-get ncurses
</pre>



You are now ready to provision your first VM using vagrant. 




&nbsp;]]></Content>
		<Date><![CDATA[2014-12-20]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[vagrant|VirtualBox]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Vagrant]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Vagrant - Configuring your VM]]></Title>
		<Content><![CDATA[So far we succesfully started up our machine, but we encountered a few problems. 


The vagrantfile holds all the configurations of our vm. It holds this data in the form of an object that's been instantiated from the <a href="https://docs.vagrantup.com/v2/vagrantfile/machine_settings.html">config.vm</a> class. 

This section has number attributes that you can set, this attributes are effectively the vm's settings. 


<h2>Resolve the timeout warning
</h2>

According to the documentation, we can fix this by increasing the config.vm.boot_timeout setting: In my case I inserted the following in the block:
<pre>
config.vm.boot_timeout = 500</pre>






]]></Content>
		<Date><![CDATA[2014-12-21]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[vagrant|VirtualBox]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Vagrant]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Vagrant - Provisioning your first VM]]></Title>
		<Content><![CDATA[1. Open up git-bash terminal and create a folder that will contain all your personal vagrant files, in my case I have created a top level folder called "vagrant":
<pre>mkdir c:\vagrant\centos</pre>
Note: I also create the centos directory in order to keep all my vagrant vms organised.

2. cd into this directory.

3. By default vagrant is designed to automatically download OS images from <a href="https://vagrantcloud.com/discover/featured">vagrantcloud</a>. This website is a repository that host hundreds of OS images. The cool thing about this is that it is free to use! On <a href="https://vagrantcloud.com/discover/featured">VagrantCloud</a>, search for an image that you are interested in (make sure you pick one that is compatible with your hypervisor, which in my case is virtualbox). In this example, I have searched for centos and will go with the following:

<a href="http://codingbee.net/wp-content/uploads/2014/10/IADphNO.png"><img class="alignnone size-full wp-image-1648" src="http://codingbee.net/wp-content/uploads/2014/10/IADphNO.png" alt="" width="1026" height="117" /></a>

Note, vagrant cloud isn't the only place to download vagrant boxes, you can also use:

- http://www.vagrantbox.es/
- http://puppet-vagrant-boxes.puppetlabs.com/ (official puppetlab boxes)

4. Make a note of the name, which in this case is "puppetlabs/centos-6.5-64-puppet"

5. Now tell vagrant you want to use this image by running the following command:
<pre>PS C:\vagrant\centos&gt; vagrant init puppetlabs/centos-6.5-64-puppet
</pre>

6. You should get the following output:
<pre>PS C:\vagrant\centos&gt; vagrant init puppetlabs/centos-6.5-64-puppet
A `Vagrantfile` has been placed in this directory. You are now
ready to `vagrant up` your first virtual environment! Please read
the comments in the Vagrantfile as well as documentation on
`vagrantup.com` for more information on using Vagrant.
PS C:\vagrant\centos&gt;
</pre>
Confirm the existence of the above file and take a look at it's content. We will cover a lot more about this file later.

7. While in the directory that contains vagrantfile, run the following (and then wait 10-20 mins):
<pre>vagrant up
</pre>
This should end up downloading an in OS image to:
<pre>C:\Users\{username}\VirtualBox VMs</pre>
You should get an output like this:

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

<a href="http://codingbee.net/wp-content/uploads/2014/12/vagrant-up-output.png"><img class="alignnone size-full wp-image-2374" src="http://codingbee.net/wp-content/uploads/2014/12/vagrant-up-output.png" alt="" width="677" height="616" /></a>

9. Now we can log into the vm from our git bash terminal using:

<pre>
vagrant ssh           
</pre>
This is helpful because we don't know the "vagrant" user's password. 

If you did know what the password is, then you can use an ssh client (e.g. putty) to connect to the vm. 

10. We can then switch to root like this:

<pre>
sudo su -
</pre>





11. Once you have finished with your vm, you can gracefully shutdown your vm using:
<pre>vagrant halt</pre>

12. Also you can destroy your vm like this if you no longer need it:

<pre>
vagrant destroy
</pre>

The particular image we are using above doesn't come pre-installed with the gnome gui interface. This means that you can only use this vm in command line mode. However you can reconfigure it run in gui mode which we'll cover in the next lesson.  by first isntalling gnome:

<pre> 
yum -y groupinstall "Desktop" "Desktop Platform" "X Window System" "Fonts"
</pre>

Then you can switch to the gui mode using the telinit command.

You can also change the default <a href="http://codingbee.net/tutorials/redhat3/linux-runlevels-part-1/" title="Linux – Runlevels (part 1)">runlevel</a> by updating the following file:

<pre>
vi /etc/inittab
</pre>

 
]]></Content>
		<Date><![CDATA[2014-12-21]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[vagrant|VirtualBox]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Vagrant]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Vagrant - Enabling a CentOS VM's gui mode]]></Title>
		<Content><![CDATA[Most vagrant boxes comes without the gui desktop interface. This tutorial will show you how install and access to that gui interface.

Let's use the standard CentOS 7 vm:

<pre>
$ vagrant init centos/7
</pre>

The above command will create a Vagrantfile. 

Next open up the vagrant file ensure the following <a href="https://docs.vagrantup.com/v2/virtualbox/configuration.html">virtualbox setting</a> section exists:

<pre>
config.vm.provider "virtualbox" do |v|
  v.gui = true
end
</pre>


Log into your vm:

<pre>$ vagrant ssh</pre>

Then switch to root:

<pre>$ sudo -i </pre>

Then install the gui desktop collection of packages:


<pre>$ yum group install 'gnome desktop'</pre>

Next switch to the gui target:

<pre>$ systemctl isolate graphical.target</pre>

You should now see the gui desktop in your virtualbox window. 

Finally enable the graphical target, so that the graphical mode starts up by default on the next reboot. 

<pre>$  systemctl set-default graphical.target</pre>

 
]]></Content>
		<Date><![CDATA[2014-12-21]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[vagrant|VirtualBox]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Vagrant]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Vagrant - The Vagrant Command-Line]]></Title>
		<Content><![CDATA[]]></Content>
		<Date><![CDATA[2014-12-21]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[vagrant|VirtualBox]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Vagrant]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Vagrant - Naming your VMs]]></Title>
		<Content><![CDATA[You can assign a name to your VM, both in what's displayed by the vagrant output, and also on the Oracle Virtualbox gui. 


<pre>
config.vm.define :puppetmaster-abc do |puppetmaster_config|        # this names it for vagrant
  puppetmaster_config.vm.hostname = "puppetmaster.codingbee.dyndns.org"
  puppetmaster_config.vm.provider "virtualbox" do |v, override|
    v.name = "puppetmaster-def"                                     # this names it for oracle gui 
  end
end

</pre> ]]></Content>
		<Date><![CDATA[2014-12-22]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[vagrant|VirtualBox]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Vagrant]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Vagrant - Plugins]]></Title>
		<Content><![CDATA[vagrant plugin install <a href="https://github.com/adrienthebo/vagrant-hosts">vagrant-hosts</a>
vagrant plugin install <a href="https://github.com/tmatilai/vagrant-proxyconf">vagrant-proxyconf</a>


Behind the scenes these plugins get installed in:
C:\Users\{username}\.vagrant.d

Or:

C:\.vagrant.d   (that I think is if you are running git bash as admin user)


http://priyaaank.tumblr.com/post/50707609769/snapshotting-vagrant]]></Content>
		<Date><![CDATA[2014-12-22]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[vagrant|VirtualBox]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Vagrant]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - The Puppet main config file (puppet.conf)]]></Title>
		<Content><![CDATA[Here's a list of all the settings that you can place into the /etc/puppet/puppet.conf file:

https://docs.puppetlabs.com/references/stable/configuration.html

https://docs.puppetlabs.com/learning/modules1.html#aside-configprint

https://docs.puppetlabs.com/references/latest/configuration.html

https://docs.puppetlabs.com/puppet/latest/reference/config_file_main.html

https://docs.puppetlabs.com/puppet/latest/reference/config_important_settings.html

You can view these changes like this:

puppet config print


For more details:
https://docs.puppetlabs.com/puppet/latest/reference/config_print.html
https://docs.puppetlabs.com/puppet/latest/reference/config_about_settings.html
https://docs.puppetlabs.com/puppet/latest/reference/config_file_main.html

https://docs.puppetlabs.com/puppet/latest/reference/environments_classic.html]]></Content>
		<Date><![CDATA[2014-12-22]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - r10k]]></Title>
		<Content><![CDATA[Here's the help info:

<pre>

[vagrant@puppetmaster /]$ r10k --help --verbose
NAME
    r10k - Killer robot powered Puppet environment deployment

USAGE
    r10k <subcommand> [options]

DESCRIPTION
    r10k is a suite of commands to help deploy and manage puppet code for
    complex environments.

COMMANDS
    deploy          Puppet dynamic environment deployment
    environment     DEPRECATED: Operate on a specific environment
    help            show help
    module          DEPRECATED: Operate on a specific puppet module
    puppetfile      Perform operations on a Puppetfile
    synchronize     DEPRECATED: Fully synchronize all environments
    version         Print the version of r10k

OPTIONS
    -c --config     Specify a global configuration file (deprecated, use `r10k deploy -c`)
    -h --help       Show help for this command
    -t --trace      Display stack traces on application crash
    -v --verbose    Set log verbosity. Valid values: fatal, error, warn, notice, info, debug, debug1, debug2
[vagrant@puppetmaster /]$ 

</pre>


There is more help info for each of the above commands. 

For example, for the "puppetfile" subcommand, we have:

<pre>
[vagrant@puppetmaster /]$ r10k puppetfile --help
NAME
    puppetfile - Perform operations on a Puppetfile

USAGE
    r10k puppetfile <subcommand>

DESCRIPTION
    `r10k puppetfile` provides an implementation of the librarian-puppet
    style Puppetfile (http://bombasticmonkey.com/librarian-puppet/).

SUBCOMMANDS
    check       Try and load the Puppetfile to verify the syntax is correct.
    install     Install all modules from a Puppetfile
    purge       Purge unmanaged modules from a Puppetfile managed directory

OPTIONS FOR R10K
    -c --config     Specify a global configuration file (deprecated, use `r10k deploy -c`)
    -h --help       Show help for this command
    -t --trace      Display stack traces on application crash
    -v --verbose    Set log verbosity. Valid values: fatal, error, warn, notice, info, debug, debug1, debug2
[vagrant@puppetmaster /]$ 


</pre>


We can also go even deeper:


<pre>
[vagrant@puppetmaster /]$ r10k puppetfile check --help --verbose
NAME
    check - Try and load the Puppetfile to verify the syntax is correct.

USAGE
    r10k puppetfile check

OPTIONS FOR PUPPETFILE
    -c --config     Specify a global configuration file (deprecated, use `r10k deploy -c`)
    -h --help       Show help for this command
    -t --trace      Display stack traces on application crash
    -v --verbose    Set log verbosity. Valid values: fatal, error, warn, notice, info, debug, debug1, debug2
[vagrant@puppetmaster /]$ 


</pre>

Now lets try this command:


<pre>
$ r10k puppetfile check 
Syntax OK
</pre> 

This will whether the syntax is ok, but won't check if there are any missing dependencies, for this you need to do:

<pre>
$ puppet module --tree --modulepath /etc/puppetlabs/code/environments/{env_name}/modules
</pre>

If there are missing dependencies, then a quick way to fix these is by using:

https://github.com/rnelson0/puppet-generate-puppetfile
 
Going back to the help info, let's look up the help info for the "deploy" subcommand:


<pre>
$ r10k deploy
NAME
    deploy - Puppet dynamic environment deployment

USAGE
    r10k deploy <subcommand>

DESCRIPTION
    `r10k deploy` implements the Git branch to Puppet environment workflow
    (https://puppetlabs.com/blog/git-workflow-and-puppet-environments/).

SUBCOMMANDS
    display         Display environments and modules in the deployment
    environment     Deploy environments and their dependent modules
    module          Deploy modules in all environments

OPTIONS 
    -c --config     Specify a configuration file

OPTIONS FOR R10K
    -c --config     Specify a global configuration file (deprecated, use `r10k deploy -c`)
    -h --help       Show help for this command
    -t --trace      Display stack traces on application crash
    -v --verbose    Set log verbosity. Valid values: fatal, error, warn, notice, info, debug, debug1, debug2
[vagrant@puppetmaster /]$ 

</pre>

Another useful help info is:


<pre>
$ r10k deploy environment --help 
NAME
    environment - Deploy environments and their dependent modules

USAGE
    r10k deploy environment <options>
    <environment> <...>

DESCRIPTION
    `r10k deploy environment` creates and updates Puppet environments based
    on Git branches.

    Environments can provide a Puppetfile at the root of the directory to
    deploy independent Puppet modules. To recursively deploy an environment,
    pass the `--puppetfile` flag to the command.

    **NOTE**: If an environment has a Puppetfile when it is instantiated a
    recursive update will be forced. It is assumed that environments are
    dependent on modules specified in the Puppetfile and an update will be
    automatically scheduled. On subsequent deployments, Puppetfile deployment
    will default to off.

OPTIONS
    -p --puppetfile    Deploy modules from a puppetfile

OPTIONS FOR DEPLOY
    -c --config        Specify a configuration file
    -c --config        Specify a global configuration file (deprecated, use `r10k deploy -c`)
    -h --help          Show help for this command
    -t --trace         Display stack traces on application crash
    -v --verbose       Set log verbosity. Valid values: fatal, error, warn, notice, info, debug, debug1, debug2
[vagrant@puppetmaster dev_virtualbox_v2]$

</pre>


If your current working directory contains the Puppetfile, manifests folder, modules folder, and environment.conf:


<pre>
$ pwd
/etc/puppet/environments/production
$ ls -l
total 16
-rw-rw-rw-.  1 root root  374 Oct 14 16:46 environment.conf
drwxrwxrwx.  2 root root   20 Oct 14 16:46 manifests
drwxrwxrwx. 69 root root 4096 Oct 15 12:34 modules
-rw-rw-rw-.  1 root root 4906 Oct 15 12:30 Puppetfile
$ ls -l modules/               # notice it is currently empty. 
</pre>

Then you can populate your modules folder by either running the following (if currently in the same directory as the Puppetfile):
<pre>
$ r10k puppetfile install
</pre>

Or by running:

<pre>
$ r10k deploy environment -v -p {prefix_branchname}
</pre>

Note, The prefix is whatever you specified in your hiera.yaml file. 

e.g. let's say we have 

<pre>
$ cat /etc/r10k.yaml
:cachedir: '/var/cache/r10k'

:sources:

 puppet_System_X:
    remote: https://github.com/control-repo-System_X.git
    basedir: '/etc/puppet/environments'
    prefix: 'System_X'
 hiera_System_X:
    remote: https://github.com/hiera-repo-System_X.git
    basedir: '/etc/puppet/hiera'
    prefix: 'System_X'
 puppet_System_Y:
    remote: https://github.com/control-repo-System_Y.git
    basedir: '/etc/puppet/environments'
    prefix: 'System_Y'
 hiera_System_Y:
    remote: https://github.com/hiera-repo-System_Y.git
    basedir: '/etc/puppet/hiera'
    prefix: 'System_Y'
 puppet_System_Z:
    remote: https://github.com/control-repo-System_Z.git
    basedir: '/etc/puppet/environments'
    prefix: 'System_Z'
 hiera_CSP:
    remote: https://github.com/hiera-repo-System_Z.git
    basedir: '/etc/puppet/hiera'
    prefix: 'System_Z'

:purgedirs:
  - /etc/puppet/environments
  - /etc/puppet/hiera
  
</pre>

Now let's say we are interested in "System_X" of which we are only insterested in this repo's branch called "Production", then we do:

<pre>
$ r10k deploy environment -v -p System_X_Production
</pre>

Note: if the branch name contains periods, forward slashes, or hyphens, then they get replaced with underscores. 

Now let's check this has worked:

<pre>
$ ls -l modules/ 
drwxrwxrwx.  4 root    root    4096 Oct 14 16:47 alternatives
drwxrwxrwx.  8 root    root    4096 Oct 14 16:47 apache
drwxrwxrwx.  5 root    root    4096 Oct 14 16:46 augeas
drwxrwxrwx.  5 root    root    4096 Oct 14 16:46 augeasproviders_base
drwxrwxrwx.  6 root    root    4096 Oct 14 16:46 augeasproviders_core
drwxrwxrwx.  5 root    root    4096 Oct 14 16:46 augeasproviders_pam
drwxrwxrwx.  7 root    root    4096 Oct 14 16:47 autofs
.
.
...etc

</pre>  







https://www.google.co.uk/search?q=r10k&ie=utf-8&oe=utf-8&aq=t&rls=org.mozilla:en-GB:official&client=firefox-a&channel=sb&gfe_rd=cr&ei=jJCZVJHdKMyq8weP3IGwBQ


http://puppetlabs.com/search/node/r10k

http://rnelson0.com/2014/05/19/puppet-and-git-201-r10k-setup-installation/

http://rnelson0.com/2014/05/26/puppet-and-git-202-r10k-setup-conversion-deployment/]]></Content>
		<Date><![CDATA[2014-12-23]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Directory Environments]]></Title>
		<Content><![CDATA[let's say you have two groups of puppet agents, these 2 sets have the following names:

<ul>
	<li>prod</li>
	<li>test</li>
</ul>

Let's now say that the same puppetmaster manages these two groups. 
In which case te site.pp file will have node definitions for both prod and test agents. 

This approach isn't a good idea, that's because, while testing, if you introduce an error into your site.pp, then that could also break prod as well.  

One way around this is to have two site.pp files. This is by setting up something called "Directory Environments". 

With "Directory Environments", you can have multiple site.pp files on the puppetmaster. These site.pp will also have exclusive access to it's own modules folder, amongst other things.


To start using "Directory Environments", you need to take the following steps:

<ol>
	<li>Configure Puppet Master to use directory environments</li>
	<li>Create your environments</li>
	<li>Assign nodes to their environments.</li>
</ol>

<h2>Configure Puppet Master to use directory environments</h2>

This involves:

<ol>
	<li>Enable Directory Environment on puppet.conf file</li>
	<li>Create a Directory Environment</li>
</ol>

<h3>Enable Directory Environment on puppet.conf file</h3>

Here we set the following parameters:

<ul>
	<li><code>environmentpath</code> - Before you set this, you first need to ensure you have a folder for this parameter to point to, e.g. I created a folder called /etc/custom-environments and set this parameter to this. I think best practice is to set this to <code>$confdir/environments</code>. Also see the <a href="https://docs.puppetlabs.com/references/latest/configuration.html#environmentpath">environmentpath</a> reference. This value must be set in the <a href="https://docs.puppetlabs.com/puppet/3.7/reference/environments_configuring.html#environmentpath">[main]</a> section.</li>
	<li><code>basemodulepath</code> - This is optional, it's a place to for storing modules that are to be used by all environments. It acts as a fallback if the environment's own module doesn't contain the module. Also see the "<a href="https://docs.puppetlabs.com/references/latest/configuration.html#basemodulepath">basemodulepath</a>" reference.</li>
	<li><code>default_manifest</code> - This is optional, also see the <a href="https://docs.puppetlabs.com/references/latest/configuration.html#defaultmanifest">default_manifest</a> referece. </li>
</ul>




<h3>Create a Directory Environment</h3>

Under the $environmentpath directory, you need to create a folder for each of your environments, hence the folder's should be named after your environment. I.e. The directory name is the same as the environment's name.

Within each of these folders, create the following folders/files:

<ul>
	<li>modules</li>
	<li>manifests</li>
	<li>environment.conf - this is optional. <a href="https://docs.puppetlabs.com/puppet/latest/reference/config_file_environment.html">environment.conf</a> overrides defaults.</li>
</ul>

For example if we want to create an environment called "test1", and our $environmentpath is equal to /etc/custom-environments, then we will end up with the following structure:

<pre>


</pre>

See also Puppetlabs page for more info about how to <a href="https://docs.puppetlabs.com/puppet/latest/reference/environments_creating.html">create directory environments</a>.

Puppetlabs also has a page covering <a href="https://docs.puppetlabs.com/puppet/latest/reference/environments_creating.html">environment.conf</a>. 

<h3>Restart PuppetMaster</h3>
Whenever you make changes to the puppet.conf, you need to to then restart the puppetmaster to load the changes:

<pre>
service puppetmaster restart
</pre>

<h3>Assign agents to environments</h3>
When ever an agents requests a catalog from the puppetmaster (i.e. doing a puppet run), it will need to tell the master what environment it belongs to. 

You can do this through <a href="https://docs.puppetlabs.com/puppet/latest/reference/environments.html#assigning-nodes-to-environments">ENC or via the agent's puppet.conf</a> file. 

We'll look at the <a href="https://docs.puppetlabs.com/puppet/latest/reference/environments_assigning.html#assigning-environments-via-an-enc">ENC approach</a> later. For now we'll just look at <a href="https://docs.puppetlabs.com/puppet/latest/reference/environments_assigning.html#assigning-environments-via-the-agents-config-file">assining agent to an environment using teh config file</a>. 








See also:
https://docs.puppetlabs.com/puppet/latest/reference/environments.html

http://puppetlabs.com/blog/git-workflow-and-puppet-environments

http://puppetlabs.com/blog/git-workflow-and-puppet-environments]]></Content>
		<Date><![CDATA[2014-12-23]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Class Parameters]]></Title>
		<Content><![CDATA[In the previous lesson we had the following example:

<pre>
class homer_simpson {

  user { 'homer':
    ensure => present,
    uid    => '101',
    shell  => '/bin/bash',
    home   => '/home/homer',
  }

}

node 'PuppetAgent1' {
  class {homer_simpson:}
}

node 'PuppetAgent2' {
  class {homer_simpson:}
}
</pre>

Now what if we want the first agent to have the user, "homer", but the second agent to have the user, "bart". In that case we can't use the homer_simpson class anymore in it's current form. Instead we need to make this class more versatile by allowing it to accept parameters, aka class parameters. In this case we need to pass a parameter of "username", and also change the class's name to something more appropriate e.g. "user_account". This is how the code now looks:  

<pre>
[root@puppetmaster ~]# cat /etc/puppet/manifests/site.pp
class user_account ($username){

  user { $username:
    ensure => present,
    uid    => '101',
    shell  => '/bin/bash',
    home   => "/home/$username",
  }

}

node 'PuppetAgent1' {
  class {user_account:
    username => "homer",
  }
}

node 'PuppetAgent2' {
  class {user_account:
    username => "bart",
  }
}
</pre>

On agent1 this results in: 

<pre>
[root@puppetagent1 ~]# puppet agent --test
Info: Retrieving pluginfacts
Info: Retrieving plugin
Info: Caching catalog for puppetagent1.codingbee.dyndns.org
Info: Applying configuration version '1419452655'
Notice: /Stage[main]/User_account/User[homer]/ensure: created
Notice: Finished catalog run in 0.15 seconds
[root@puppetagent1 ~]# cat /etc/passwd | grep "homer"
homer:x:101:501::/home/homer:/bin/bash
[root@puppetagent1 ~]#
</pre>

And on agent2 we have:
<pre>
[root@puppetagent2 ~]# puppet agent --test
Info: Retrieving pluginfacts
Info: Retrieving plugin
Info: Caching catalog for puppetagent2.codingbee.dyndns.org
Info: Applying configuration version '1419452725'
Notice: /Stage[main]/User_account/User[bart]/ensure: created
Notice: Finished catalog run in 0.19 seconds
[root@puppetagent2 ~]# cat /etc/passwd | grep "bart"
bart:x:101:501::/home/bart:/bin/bash
[root@puppetagent2 ~]#

</pre>

We can also set a default value for a class parameter like this:


<pre>
[root@puppetmaster ~]# cat /etc/puppet/manifests/site.pp
class user_account ($username = 'homer'){

  user { $username:
    ensure => present,
    uid    => '101',
    shell  => '/bin/bash',
    home   => "/home/$username",
  }

}

node 'PuppetAgent1' {
  class {user_account:}
}

node 'PuppetAgent2' {
  class {user_account:
    username => "bart",
  }
}
[root@puppetmaster ~]#

</pre>

<h2>Variable Scope</h2>

In the above examples we have also introduced varables which are denoted by a "$" prefix. 

Any variables defined within a class are only accessible from within that class, if you use it's short-hand name. However you can still access it from outside the class by using it's full name:

<pre>${classname}::${variable_name}</pre>

We'll cover more about variables later. 

Here we used the double colon syntax to drill down to the correct location.  
]]></Content>
		<Date><![CDATA[2014-12-24]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Organising your puppet code into separate files]]></Title>
		<Content><![CDATA[So far we have looked at how to organise your code by compartmentalizing your resources into classes.

However, your site.pp can still get really big as your write more and more classes. That's why the next logical step is to store your classes in separate manifests, and then "dot source" them. In Puppet there are three ways to do this:

<ul>
	<li>Using the <a href="http://codingbee.net/tutorials/puppet/puppet-using-import-keyword/" title="Puppet – Using the “import” keyword">import</a> keyword</li>
	<li>The "<a href="http://codingbee.net/tutorials/puppet/puppet-puppet-apply-approach/" title="Puppet – The “puppet apply” approach">apply puppet</a>" method</li>
	<li><a href="http://codingbee.net/tutorials/puppet/puppet-modules/" title="Puppet – Modules">Modules</a> - this is the best approach</li>
</ul>

We'll now look at each approach in turn.  

]]></Content>
		<Date><![CDATA[2014-12-24]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - The "puppet apply" approach]]></Title>
		<Content><![CDATA[Please ignore this post. and go to the next post. 

In the world of Puppet, classes are essentially functions. Once you have written your classes, they can then be loaded into memory (aka "define" the classes) and then triggered (aka "declared") from inside other manifests. These classes are then organized into <strong>modules</strong>, which we'll cover later.  

Going back to the previous example, lets now keep everything the same, except comment out the import key word:

<pre>
[root@puppetmaster manifests]# ls -l
total 12
-rw-r--r--. 1 root root 174 Dec 24 20:57 myclass.pp
-rw-r--r--. 1 root root 145 Dec 24 20:59 site.pp
[root@puppetmaster manifests]# mv myclass.pp myclasses.pp
[root@puppetmaster manifests]# ls -l
total 12
-rw-r--r--. 1 root root 174 Dec 24 20:57 myclasses.pp
-rw-r--r--. 1 root root 299 Dec 24 20:56 site24-12-14-v1.pp
-rw-r--r--. 1 root root 145 Dec 24 20:59 site.pp
[root@puppetmaster manifests]# cat myclasses.pp
class user_account ($username = 'homer'){

  user { $username:
    ensure => present,
    uid    => '101',
    shell  => '/bin/bash',
    home   => "/home/$username",
  }

}
[root@puppetmaster manifests]# cat site.pp
import 'myclass.pp'                          # note the use of the "import" keyword here

node 'PuppetAgent1' {
  class {user_account:}
}

node 'PuppetAgent2' {
  class {user_account:
    username => "bart",
  }
}
[root@puppetmaster manifests]#
</pre>

This time the puppetmaster will fail to generate the catalog when requested by an agent, because the puppetmaster has no idea where the user_account class is. However we can manually load it into Puppet's memory (aka define the class) by running the following command on the command line:

<pre>
[root@puppetmaster manifests]# puppet apply /etc/puppet/manifests/myclasses.pp
Notice: Compiled catalog for puppetmaster.codingbee.dyndns.org in environment production in 0.03 seconds
Notice: Finished catalog run in 0.03 seconds
[root@puppetmaster manifests]#
</pre>
]]></Content>
		<Date><![CDATA[2014-12-24]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Using the "import" keyword]]></Title>
		<Content><![CDATA[The "import" key word is actually going to be depecrated in future releases of Puppet, and that's because using modules are a much more powerful alternative. 

However it is still worth learning about "import" so to understand the origins of modules.

Previously we had:

<pre>
[root@puppetmaster ~]# cat /etc/puppet/manifests/site.pp
class user_account ($username = 'homer'){

  user { $username:
    ensure => present,
    uid    => '101',
    shell  => '/bin/bash',
    home   => "/home/$username",
  }

}

node 'PuppetAgent1' {
  class {user_account:}
}

node 'PuppetAgent2' {
  class {user_account:
    username => "bart",
  }
}
[root@puppetmaster ~]#

</pre>

The next logical step is to separate out the class definitions and place them into a separate manifest. In this case the manifest will reside in the same folder as the site.pp, e.g.:

<pre>
[root@puppetmaster manifests]# ls -l
total 12
-rw-r--r--. 1 root root 174 Dec 24 20:57 myclass.pp
-rw-r--r--. 1 root root 145 Dec 24 20:59 site.pp
[root@puppetmaster manifests]# mv myclass.pp myclasses.pp
[root@puppetmaster manifests]# ls -l
total 12
-rw-r--r--. 1 root root 174 Dec 24 20:57 myclasses.pp
-rw-r--r--. 1 root root 299 Dec 24 20:56 site24-12-14-v1.pp
-rw-r--r--. 1 root root 145 Dec 24 20:59 site.pp
[root@puppetmaster manifests]# cat myclasses.pp
class user_account ($username = 'homer'){

  user { $username:
    ensure => present,
    uid    => '101',
    shell  => '/bin/bash',
    home   => "/home/$username",
  }

}
[root@puppetmaster manifests]# cat site.pp
import 'myclass.pp'                          # note the use of the "import" keyword here

node 'PuppetAgent1' {
  class {user_account:}
}

node 'PuppetAgent2' {
  class {user_account:
    username => "bart",
  }
}
[root@puppetmaster manifests]#
</pre>

We have pretty much dot-sourced the myclasses.pp in the site.pp file. 

The above gives the same results as having all the code in the site.pp file. 


<strong>
See also</strong>
https://docs.puppetlabs.com/puppet/latest/reference/lang_import.html]]></Content>
		<Date><![CDATA[2014-12-24]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Modules, a quick taster]]></Title>
		<Content><![CDATA[We previously saw how to "dot-source" a manifest using the "import" keyword. Now this time, let's comment out the import keyword:

<pre>
[root@puppetmaster manifests]# pwd
/etc/puppet/manifests
[root@puppetmaster manifests]# ls -l
total 8
-rw-r--r--. 1 root root 173 Dec 24 23:07 myclasses.pp
-rw-r--r--. 1 root root 236 Dec 24 23:20 site.pp
[root@puppetmaster manifests]# cat myclasses.pp
class user_account ($username = 'homer'){

  user { $username:
    ensure => present,
    uid    => '101',
    shell  => '/bin/bash',
    home   => "/home/$username",
  }

}
[root@puppetmaster manifests]# cat site.pp
# import 'myclasses.pp'                          # note the use of the "import" keyword here

node 'PuppetAgent1' {
  class {user_account:}
}

node 'PuppetAgent2' {
  class {user_account:
    username => "bart",
  }
}
</pre>

Now if we comment out the import keyword, and then do a manual puppet run, it fails:

<pre>
[root@puppetagent1 ~]# puppet agent --test
Info: Retrieving pluginfacts
Info: Retrieving plugin
Notice: /File[/var/lib/puppet/lib/puppet]/ensure: removed
Notice: /File[/var/lib/puppet/lib/facter]/ensure: removed
Error: Could not retrieve catalog from remote server: Error 400 on SERVER: Puppet::Parser::AST::Resource failed with error ArgumentError: Could not find declared class user_account at /etc/puppet/manifests/site.pp:10 on node puppetagent1.codingbee.dyndns.org
Warning: Not using cache on failed catalog
Error: Could not retrieve catalog; skipping run
[root@puppetagent1 ~]#
</pre> 

This has failed because the puppetmaster finds a reference to the user_account class is the site.pp, but has no idea where the pp file is where this class has been defined (since we have commented out the import statement). 

Now we'll perform a few tasks, just follow along for now and we'll explain what we have done later. Let's run the following command:

<pre>
[root@puppetmaster manifests]# puppet config print modulepath
/etc/puppet/modules:/usr/share/puppet/modules
[root@puppetmaster manifests]#
</pre>

Now do the following:
<ol>
	<li>cd into /etc/puppet/modules</li>
	<li>then create a folder called "user_account" (i.e. the folder name matches the above class's name)</li>
	<li>cd Into this new folder, then create a folder called "manifests"</li>
	<li>move the myclasses.pp file into this folder and also rename it to init.pp</li>
</ol>

Therefore:

<pre>
[root@puppetmaster manifests]# cd /etc/puppet/modules/
[root@puppetmaster modules]# ls
[root@puppetmaster modules]# mkdir user_account
[root@puppetmaster modules]# cd user_account/
[root@puppetmaster user_account]# mkdir manifests
[root@puppetmaster user_account]# cd manifests/
[root@puppetmaster manifests]# mv /etc/puppet/manifests/myclasses.pp init.pp
[root@puppetmaster manifests]# pwd
/etc/puppet/modules/user_account/manifests
[root@puppetmaster manifests]# ls
init.pp
[root@puppetmaster manifests]# cat init.pp
class user_account ($username = 'homer'){

  user { $username:
    ensure => absent,
    uid    => '101',
    shell  => '/bin/bash',
    home   => "/home/$username",
  }

}

</pre> 

Now if we do a manual run on an agent, this time it worked:

<pre>
[root@puppetagent1 ~]# puppet agent --test
Info: Retrieving pluginfacts
Info: Retrieving plugin
Info: Caching catalog for puppetagent1.codingbee.dyndns.org
Info: Applying configuration version '1419480383'
Notice: /Stage[main]/User_account/User[homer]/ensure: created
Notice: Finished catalog run in 0.16 seconds
[root@puppetagent1 ~]# cat /etc/passwd | grep "homer"
homer:x:101:501::/home/homer:/bin/bash
[root@puppetagent1 ~]#
</pre> 

This time it has worked. That's because, when an agent submits a catalog request to the puppetmaster (which we did manually using "puppet apply --test"), the puppetmaster starts to determine what the catalog's content should be by first reading the content of the site.pp file. Here it encounters a reference to the user_account class. Since this class isn't defined in the site.pp file, the puppetmaster starts to look for it elsewhere, and by default it takes the following approach when looking for the class:

<ol>
	<li>First it finds what directory paths are listed in puppet's modulepath setting.</li>
	<li>It then looks inside this directory for a folder that has the same name as the class's name</li>
	<li>If it finds that, it then traverses into this directory and then looks for a folder called "manifests"</li>
	<li>If it finds that, it then traverses into this directory and then looks for a manifest called "init.pp"</li>
	<li>If it finds this manifests, and then looks into this file for the class it is looking for. </li>
	<li>If it finds the class, it then loads that class into memory and then repeats this process for any other class names in the site.pp.</li>
	<li>Once it has located all the classes that are referenced in the site.pp, it then compiles the catalog and sends it to the agent. 
</li>The agent reads the catalog and make any changes necessary to bring itself to the desired state. 	<li>

</ol>

As you can see, when the puppetmaster encounters a class name that it hasn't seen before, the puppetmasters default behaviour is to automatically look for the class's definition in specific manifest (called "init.pp"). But not just any init.pp file, this init.pp file must be in the following location:

<pre>
{modulepath}/{folder that's named after the class}/manifests/init.pp
</pre>

In puppet, the folder that's named after the class, and all it's content is referred to as a "module".

This behaviour of puppet makes it possible to organise all your resources into classes. And you can organise all your classes into modules (note, the rule is that init.pp has to contain one class definition, which is the name of the class that the puppet master is looking for). This will lead to the site.pp file to only contain node definitions which lists various classes.  
 
So far we have made a barebones module. We'll explore much more about modules as we progress this course. That's because modules are a core part of what makes puppet so powerful.    ]]></Content>
		<Date><![CDATA[2014-12-24]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Modules (the "files" folder)]]></Title>
		<Content><![CDATA[One thing you may want to do (for a given class), is distribute a particular static file to your agent.

This can easily be done using modules. Let's take the previous "user_account" example, and say we want to distribute a file called "user.txt" to the /tmp folder of the agents. Let's say that the content of this file is "hello world".

To do this, we first create user.txt and place it in the user_account module's files folder:

<pre>
[root@puppetmaster files]# pwd
/etc/puppet/modules/user_account/files
[root@puppetmaster files]# echo "Hello world" > user-master-copy.txt
[root@puppetmaster files]# ls
user-master-copy.txt
</pre>

Notice, that we have named the file "user-master-copy.txt" rather than user.txt. This is to give you a better view of what's happening. This file will be copied across and then renamed to user.txt by the resource declaration, which we'll do next. 


Now to distribute this file, we have to define a <a href="https://docs.puppetlabs.com/references/latest/type.html#file">file resource</a> to do this:

<pre>
file { '/tmp/user.txt':
  ensure  => file,
  source  => "puppet:///modules/user_account/user-master-copy.txt",
}
</pre>

The "puppet:///modules/{module-name}/.../{filename}" syntax is the way we tell puppet that the source file is located in the "files" folder in the user_account module, and it is called "user-master-copy.txt". Note, the "..." means that under the files folder, you can create your own custom folder structure in order to organise all your source files. In which you need to provide puppet with the relative path. 

Now let's way we want to apply this as part of the user_account class, in that case we can insert this resource declaration into this class:

<pre>
[root@puppetmaster manifests]# pwd
/etc/puppet/modules/user_account/manifests
[root@puppetmaster manifests]# cat init.pp
class user_account ($username = 'homer'){

  user { $username:
    ensure => present,
    uid    => '101',
    shell  => '/bin/bash',
    home   => "/home/$username",
  }

  file { '/tmp/user.txt':
    ensure  => file,
    source  => "puppet:///modules/user_account/user-master-copy.txt",
  }


}
[root@puppetmaster manifests]#

</pre>
Note, we don't need to make any changes to the site.pp file since it already calls the user_account, and the changes we have made are internal to this class. 

Now if you we do a puppet run on the agent, we get:

<pre>

[root@puppetagent1 tmp]# pwd
/tmp
[root@puppetagent1 tmp]# ls -l user.txt
ls: cannot access user.txt: No such file or directory
[root@puppetagent1 tmp]# puppet agent --test
Info: Retrieving pluginfacts
Info: Retrieving plugin
Info: Caching catalog for puppetagent1.codingbee.dyndns.org
Info: Applying configuration version '1419488509'
Notice: /Stage[main]/User_account/File[/tmp/user.txt]/ensure: defined content as '{md5}f0ef7081e1539ac00ef5b761b4fb01b3'
Notice: Finished catalog run in 0.24 seconds
[root@puppetagent1 tmp]# ls -l user.txt
-rw-r--r--. 1 root root 12 Dec 25 06:32 user.txt
[root@puppetagent1 tmp]#

</pre>

Now what if we want to create a lower level tree structure inside the files folder, to further organise our source files, e.g. lets say we create a new samples folder, and place our file into it:

<pre>
[root@puppetmaster files]# pwd
/etc/puppet/modules/user_account/files
[root@puppetmaster files]# ls
user-master-copy.txt
[root@puppetmaster files]# mkdir samples
[root@puppetmaster files]# ls -l
total 8
drwxr-xr-x. 2 root root 4096 Dec 25 06:44 samples
-rw-r--r--. 1 root root   12 Dec 25 06:05 user-master-copy.txt
[root@puppetmaster files]# mv user-master-copy.txt samples/
[root@puppetmaster files]# cd samples/
[root@puppetmaster samples]# ls
user-master-copy.txt
</pre>

Now if we do a puppet run:

<pre>

[root@puppetagent1 tmp]# puppet agent --test
Info: Retrieving pluginfacts
Info: Retrieving plugin
Info: Caching catalog for puppetagent1.codingbee.dyndns.org
Info: Applying configuration version '1419488509'
Error: /Stage[main]/User_account/File[/tmp/user.txt]: Could not evaluate: Could not retrieve information from environment production source(s) puppet:///modules/user_account/user-master-copy.txt


</pre>

This failed because we need to update our file-resource's source attribute to specify the source file's new (relative) location:

<pre>
[root@puppetmaster manifests]# cat init.pp
class user_account ($username = 'homer'){

  user { $username:
    ensure => present,
    uid    => '101',
    shell  => '/bin/bash',
    home   => "/home/$username",
  }

  file { '/tmp/user.txt':
    ensure  => file,
    source  => "puppet:///modules/user_account/samples/user-master-copy.txt",
  }


}

</pre>

Now when we do a puppet run it works:


<pre>

[root@puppetagent1 tmp]# puppet agent --test
Info: Retrieving pluginfacts
Info: Retrieving plugin
Info: Caching catalog for puppetagent1.codingbee.dyndns.org
Info: Applying configuration version '1419490479'
Notice: /Stage[main]/User_account/File[/tmp/user.txt]/ensure: defined content as '{md5}f0ef7081e1539ac00ef5b761b4fb01b3'
Notice: Finished catalog run in 0.19 seconds
[root@puppetagent1 tmp]# cat user.txt
Hello world
[root@puppetagent1 tmp]#


</pre>








]]></Content>
		<Date><![CDATA[2014-12-25]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Adding other manifests/classes to a module]]></Title>
		<Content><![CDATA[In the previous tutorial we created a new file resource declaration and inserted it into the main class. But what if we instead want to have this file-resource in it's own standalone class, but still inside the same module. This is can be done by doing the following:

<ol>
	<li>Create a separate manifest and place it inside the manifests folder</li>
	<li>In this manifest write a single class, and insert the resource declaration into it.</li>
	<li>the manifest and the class must share the same name.</li>
	<li>Update the site.pp file to reference the new class, and use "::" to help puppet locate the new class</li>
</ol>

Using our user_account example, let's call our new class "user_tmp_file". In that case, here are the changes we make to the module:

<pre>
[root@puppetmaster modules]# pwd
/etc/puppet/modules
[root@puppetmaster modules]# tree .
.
└── user_account
    ├── files
    │   └── samples
    │       └── user-master-copy.txt
    ├── lib
    ├── manifests
    │   ├── init.pp
    │   └── user_tmp_file.pp
    ├── spec
    └── templates

7 directories, 3 files
[root@puppetmaster modules]# cat user_account/manifests/init.pp
class user_account ($username = 'homer'){

  user { $username:
    ensure => present,
    uid    => '101',
    shell  => '/bin/bash',
    home   => "/home/$username",
  }

}
[root@puppetmaster modules]# cat user_account/manifests/user_tmp_file.pp
class user_account::user_tmp_file {

  file { '/tmp/user.txt':
    ensure  => file,
    source  => "puppet:///modules/user_account/samples/user-master-copy.txt",
  }

}
[root@puppetmaster modules]# cat user_account/files/samples/user-master-copy.txt
Hello world
[root@puppetmaster modules]#

</pre>

Notice: In the above we used "::" for namespacing, i.e. to help with drilling down to a particular class. It is a bit like an FQDN for a class in a module. 



So far so good. Now we reference this new class (along with which module it belongs to) in the site.pp:


<pre>
[root@puppetmaster modules]# cat /etc/puppet/manifests/site.pp

node 'PuppetAgent1' {
  class {user_account:}
  class {user_account::user_tmp_file:}           # notice the "::" namespace syntax
}

node 'PuppetAgent2' {
  class {user_account:
   username => "bart",
  }
}
[root@puppetmaster modules]#
</pre>


Now if we do a puppet run, we get:

<pre>

[root@puppetagent1 tmp]# puppet agent --test
Info: Retrieving pluginfacts
Info: Retrieving plugin
Info: Caching catalog for puppetagent1.codingbee.dyndns.org
Info: Applying configuration version '1419492288'
Notice: /Stage[main]/User_account::User_tmp_file/File[/tmp/user.txt]/ensure: defined content as '{md5}f0ef7081e1539ac00ef5b761b4fb01b3'
Notice: Finished catalog run in 0.20 seconds
[root@puppetagent1 tmp]# ls -l user.txt
-rw-r--r--. 1 root root 12 Dec 25 07:29 user.txt
[root@puppetagent1 tmp]#
</pre>

In the manifests folder in our module, we can create our custom folder structure to further organize our manifests. In which case we need to update our namespacing to reflect the new structure. For example let's say our new manifest needs to be placed into a new folder called static-files-manifests. In that case we would have:


<pre>
[root@puppetmaster manifests]# pwd
/etc/puppet/modules/user_account/manifests
[root@puppetmaster manifests]# ls
init.pp  user_tmp_file.pp
[root@puppetmaster manifests]# mkdir static-files-manifests
[root@puppetmaster manifests]# mv user_tmp_file.pp static-files-manifests/
[root@puppetmaster manifests]# cd ..
[root@puppetmaster user_account]# cd ..
[root@puppetmaster modules]# tree .
.
└── user_account
    ├── files
    │   └── samples
    │       └── user-master-copy.txt
    ├── lib
    ├── manifests
    │   ├── init.pp
    │   └── static-files-manifests            # this is the new folder
    │       └── user_tmp_file.pp
    ├── spec
    └── templates

8 directories, 3 files

</pre>

Now we update the manifest and site.pp to reflect the new folder structure, using namespaces:

<pre>
[root@puppetmaster static-files-manifests]# cat user_tmp_file.pp
class user_account::static-files-manifests::user_tmp_file {

  file { '/tmp/user.txt':
    ensure  => file,
    source  => "puppet:///modules/user_account/samples/user-master-copy.txt",
  }

}
[root@puppetmaster static-files-manifests]# cat /etc/puppet/manifests/site.pp
# import '/etc/puppet/modules/create_user/manifests/init.pp'


node 'PuppetAgent1' {
  class {user_account:}
  class {user_account::static-files-manifests::user_tmp_file:}
}

node 'PuppetAgent2' {
  class {user_account:
   username => "bart",
  }
}
[root@puppetmaster static-files-manifests]#

</pre>

Notice that we updated the namespacing for both the manifests and the site.pp. We always have to do this to help puppet reference everything without any ambiguity. 

Now if we do a puppet run:

<pre>
[root@puppetagent1 tmp]# ls -l user.txt
ls: cannot access user.txt: No such file or directory
[root@puppetagent1 tmp]# puppet agent --test
Info: Retrieving pluginfacts
Info: Retrieving plugin
Info: Caching catalog for puppetagent1.codingbee.dyndns.org
Info: Applying configuration version '1419493159'
Notice: /Stage[main]/User_account::Static-files-manifests::User_tmp_file/File[/tmp/user.txt]/ensure: defined content as '{md5}f0ef7081e1539ac00ef5b761b4fb01b3'
Notice: Finished catalog run in 0.20 seconds
[root@puppetagent1 tmp]# ls -l user.txt
-rw-r--r--. 1 root root 12 Dec 25 07:39 user.txt
[root@puppetagent1 tmp]#

</pre>




<strong>See also:</strong>
https://docs.puppetlabs.com/puppet/latest/reference/lang_namespaces.html]]></Content>
		<Date><![CDATA[2014-12-25]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Using third-party modules from Puppetforge]]></Title>
		<Content><![CDATA[Puppet comes with the "modules" subcommand. This command let's you do a number of things including installing modules from <a href="https://forge.puppetlabs.com/modules">puppetforge</a>, which is the official library where the wider puppet community share their modules with others. 

The modules subcommand can list all installed modules:

<pre>
[root@puppetmaster puppet]# puppet module list
/et/puppet/modules
├── adrien-alternatives (v0.3.0)
├── adrien-boolean (v1.0.1)
├── garethr-docker (v3.3.1)
├── garethr-erlang (v0.3.0)
├── iaac-eclipse_platform (v0.1.0)
....etc.
</pre>

If any modules are installed but they are dependent on other modules that haven't been installed yet, then this command will output a warning message to indicate this. 

Now let's say we want to install the <a href="https://forge.puppetlabs.com/puppetlabs/apache">apache</a> module, then we run:

<pre>
[root@puppetmaster /]# puppet module install puppetlabs-apache
Notice: Preparing to install into /git_source/puppet/environments/dev_virtualbox_v2/modules ...
Notice: Downloading from https://forge.puppetlabs.com ...
Notice: Installing -- do not interrupt ...
/etc/puppet/modules
└─┬ puppetlabs-apache (v1.2.0)
  └── puppetlabs-concat (v1.1.2)
[root@puppetmaster /]#
</pre>

Notice that the concat module has also been installed. That's because concat is a <a href="https://forge.puppetlabs.com/puppetlabs/apache/dependencies">dependency of the apache module</a>. It hasn't downlaoded the puppetlabs/stdlib module because that is already installed on our puppetmaster. 

Hence in that respect, the puppet module install command manages dependancies just like yum does with rpm packages.  

In the background, what really ends up happening is that puppet creates a folder for each module in the modules folder. This folder is just the name of the module, i.e. without the author prefix:

<pre>
[root@puppetmaster modules]# ls -l | grep -E 'apache|concat' 
drwxr-xr-x 8 root    root    4096 Nov 12 01:38 apache
drwxr-xr-x 7 root    root    4096 Oct 28 19:13 concat
</pre>

It then downlaods the module files in <a href="https://forgeapi.puppetlabs.com/v3/files/puppetlabs-apache-1.2.0.tar.gz">tar ball form</a> (the tarball link is also available on the module puppetforge page but in very small font) and extracts it into the folder.

When we then do a "puppet module list", puppet scans each modules folder for a  "<a href="https://docs.puppetlabs.com/puppet/latest/reference/modules_publishing.html#write-a-metadatajson-file">metadata.json</a>" files, and extract the module's fqdn (authorName-moduleName) along with the version. This file also lists the dependendant modules, which is how "puppet module install" new which dependancies to automatically install.   









 







https://docs.puppetlabs.com/learning/modules1.html#the-puppet-module-subcommand

https://docs.puppetlabs.com/puppet/latest/reference/modules_installing.html#using-the-module-tool




]]></Content>
		<Date><![CDATA[2014-12-25]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Modules (the "templates" folder)]]></Title>
		<Content><![CDATA[Earlier we saw how to make static files (that are stored in the files folder) present in agents.

However in most case you want to have config files present in an agent that has been modified before being placed on the target machine.

This is made possible thanks to <a title="Ruby – The ERB templating system" href="http://codingbee.net/tutorials/ruby/ruby-the-erb-templating-system/">to Ruby's ERB templating system</a>. An ERB file is basically a text file that contains static content which is mixed in witch some ruby code that generates dynamic conent. The ruby code is encased in a bunch of &lt;%...%&gt; and &lt;%=...%&gt; tags which are embeded amongst the static texts. These tags acts as placeholders and contains ruby code that are executed during rendering.

If these tags are supposed to do some processing, but not output any text, then we use &lt;%...%&gt;. However if they are supposed to do some processing, and then output some texts, then we use &lt;%=...%&gt;, this text then replaces the entire ruby code and tags.

Puppet's templating system makes use of the file resource type's "content" attribute. So take a look at the content attribute first. Let's say we have the following in the site.pp file:
<pre>[root@puppetmaster manifests]# pwd
/etc/puppet/manifests
[root@puppetmaster manifests]# ls
site.pp
[root@puppetmaster manifests]# cat site.pp
node 'PuppetAgent1' {
  class {user_account:}
}

node 'PuppetAgent2' {
  class {user_account:
   username =&gt; "bart",
  }
}
[root@puppetmaster manifests]#


</pre>
Now the user_account class resides in the init.pp file of the user_account module:
<pre>[root@puppetmaster modules]# pwd
/etc/puppet/modules
[root@puppetmaster modules]# tree
.
└── user_account
    ├── files
    ├── lib
    ├── manifests
    │   └── init.pp        # this houses the "user_account" class. 
    ├── spec
    └── templates
        └── user-master-copy.erb

6 directories, 2 files
[root@puppetmaster modules]#

</pre>
If we look at the contents of the init.pp, we find:
<pre>class user_account ($username = 'homer'){

  user { $username:
    ensure =&gt; present,
    uid    =&gt; '101',
    shell  =&gt; '/bin/bash',
    home   =&gt; "/home/$username",
  }

  $message = "hello world"
  file { '/tmp/user.txt':
    ensure  =&gt; file,
    content  =&gt; "The message is: $message.\n The 'osfamily' fact's value is: $osfamily. \n",
  }
}
</pre>
As you can see, this class defines the state of 2 resources, first it ensures a particular user account exists (which defaults to username "homer", if site.pp file doesn't specify otherwise by passing through a class parameter). Second it ensures the existence of a file called "/tmp/user.txt" on the agents.

Notice a couple of things here, in the previous post, when we wanted to generate a file resource, using a static file, we used the file resource type's "<a href="https://docs.puppetlabs.com/references/latest/type.html#file-attribute-source">source</a>" attribute, which is designed for copying across static files to the agents. However this time we are using the "<a href="https://docs.puppetlabs.com/references/latest/type.html#file-attribute-content">content</a>" attribute. The "content" attribute is used to populate the file's content with a string, which in the above case, the "string content" is derived from the $message variable as well as a facter variable's value. Whereas, the "source" attribute is limited to simple copy+paste of a static file.
<pre>[root@puppetagent1 tmp]# pwd
/tmp
[root@puppetagent1 tmp]# cat user.txt
cat: user.txt: No such file or directory
[root@puppetagent1 tmp]# puppet agent --test
Info: Retrieving pluginfacts
Info: Retrieving plugin
Info: Caching catalog for puppetagent1.codingbee.dyndns.org
Info: Applying configuration version '1419796069'
Notice: /Stage[main]/User_account/File[/tmp/user.txt]/ensure: defined content as '{md5}a80267da88c5ccc0b7e956cf4e2615cc'
Notice: Finished catalog run in 0.07 seconds
[root@puppetagent1 tmp]# cat user.txt
The message is: hello world.
 The 'osfamily' fact's value is: RedHat.
[root@puppetagent1 tmp]#

</pre>
As you can see, with the help of the content attribute, we are already started creating files with some content generated inside it. This approach is suitable if the file is intended to be a really short file.

However if the file is supposed to be bigger, then it's better to use templates.

From above, we have shown you that we have a erb file in the templates folder called "user-master-copy.erb". Let's say this file's content is just "Hello world":
<pre>
/etc/puppet/modules/user_account/templates
[root@puppetmaster templates]# cat user-master-copy.erb
Hello world.

</pre>
Now let's say we want our user_account class file to use this erb file when ensuring the /tmp/user.txt resource, in the case the init.pp file needs to be modified to:
<pre>class user_account ($username = 'homer'){

  user { $username:
    ensure =&gt; present,
    uid    =&gt; '101',
    shell  =&gt; '/bin/bash',
    home   =&gt; "/home/$username",
  }

  file { '/tmp/user.txt':
    ensure  =&gt; file,
    content  =&gt; template("user_account/user-master-copy.erb"),
  }
}

</pre>
Here we now have to make use of the <a href="https://docs.puppetlabs.com/references/latest/function.html#template">template function</a>. All this function does is processes the erb file in order to generate a string. This string is then assigned to the content attribute. Since our erb file didn't contain any ruby code, it meant that the template simply outputs the static content as is, therefore when we do a puppet run, we simply get the user.txt file with "Hello world":
<pre>[root@puppetagent1 tmp]# puppet agent --test
Info: Retrieving pluginfacts
Info: Retrieving plugin
Info: Caching catalog for puppetagent1.codingbee.dyndns.org
Info: Applying configuration version '1419797221'
Notice: /Stage[main]/User_account/File[/tmp/user.txt]/ensure: defined content as '{md5}fa093de5fc603823f08524f9801f0546'
Notice: Finished catalog run in 0.07 seconds
[root@puppetagent1 tmp]# cat user.txt
Hello world.
[root@puppetagent1 tmp]# 
</pre>
Now let's place some ruby code in the erb file, and then see what happens. For example we'll use Ruby's "<a href="http://ruby-doc.org/core-2.2.0/Time.html">Time</a>" class, along with it's "now" method to output the time inside the erb file:
<pre>[root@puppetmaster templates]# pwd
/etc/puppet/modules/user_account/templates
[root@puppetmaster templates]# cat user-master-copy.erb
Hello world. The time is now &lt;%= Time.now %&gt;. Have a great day.
[root@puppetmaster templates]#
</pre>
Now if we do a puppet run, we get:
<pre>[root@puppetagent1 tmp]# puppet agent --test
Info: Retrieving pluginfacts
Info: Retrieving plugin
Info: Caching catalog for puppetagent1.codingbee.dyndns.org
Info: Applying configuration version '1419798013'
Notice: /Stage[main]/User_account/File[/tmp/user.txt]/ensure: defined content as '{md5}0f04308d1651949f08e8379e4ceb4933'
Notice: Finished catalog run in 0.09 seconds
[root@puppetagent1 tmp]# cat user.txt
Hello world. The time is now Sun Dec 28 20:20:13 +0000 2014. Have a great day.
[root@puppetagent1 tmp]#

</pre>
One of the crucial things you'll want in puppet is the ability to make call/use the following types of data from within your template:
<ul>
	<li>class parameters</li>
	<li>facts</li>
	<li>variables from current scope, or parent scopes.</li>
	<li>variables from another scope:
<ul>
	<li>variable from another class within same module</li>
	<li>varaible from another module</li>
</ul>
</li>
</ul>
&nbsp;
<h3>parameters, facts, and current/parent scope variables</h3>
Let's first look at how to use class parameters, facts, and current/parent scope variables, in erb files.

Let's say we have the following module:

&nbsp;
<pre>
[root@puppetmaster modules]# tree /etc/puppet/modules/
/etc/puppet/modules/
└── user_account
    ├── files
    ├── lib
    ├── manifests
    │   └── init.pp
    ├── spec
    └── templates
        └── user-master-copy.erb

6 directories, 2 files
[root@puppetmaster modules]#

</pre>
&nbsp;

&nbsp;
The site.pp file contains:
&nbsp;
<pre>
[root@puppetmaster modules]# cat /etc/puppet/manifests/site.pp
$siteppvar = "testing 123"

node 'PuppetAgent1' {
  class {user_account:}
}

node 'PuppetAgent2' {
  class {user_account:
    username => "bart",
  }
}
[root@puppetmaster modules]#

</pre>

Note here that we defined a variable called "$siteppvar". We'll call this variable later, within the erb file. 
&nbsp;
And the init.pp file contains:


<pre>
[root@puppetmaster modules]# cat user_account/manifests/init.pp
class user_account ($username = 'homer'){

  user { $username:
    ensure => present,
    uid    => '101',
    shell  => '/bin/bash',
    home   => "/home/$username",
  }

  $greetings = "have a great day!!!"

  file { '/tmp/user.txt':
    ensure  => file,
    content  => template("user_account/user-master-copy.erb"),
  }
}
[root@puppetmaster modules]#

</pre>
&nbsp;
Notice here that we have defined a variable called "$greetings". We are going to call this variable from within the erb file, later. 

Also notice that the file's "content" attribute calls the template function which in turn has the erb file passed into it as an argument. 

&nbsp;
Now here's the content of the erb file:
&nbsp;

<pre>
[root@puppetmaster templates]# cat user-master-copy.erb
Hello <%= @username %>.                        <%# here is a class parameter %>
<%- if @username == "homer" -%>
  Your name is: <%= @username %>
<%- else -%>
  Sorry you are not homer.
<%- end -%>
<%-# Here is some simple ruby code -%>
The time is now <%= Time.now %>.

<%-# Here is a simply ruby for-loop -%>
<%- for fruit in ['apple','banana','mango'] -%>
  A <%= fruit %> is a fruit.
<%- end -%>

<%-# here is a facter -%>
The OS of this machine = <%= @osfamily %>

<%-# here is a variable that is one scope up -%>
Here is a init.pp variable: <%= @greetings %>

<%-# here is a variable that is 2 scope up -%>
Here is a site.pp variable: <%= @siteppvar %>


</pre>

The cool thing about Puppet, is that all class parameters, facter facts, and parent variables, are pre-loaded into the erb file automatically. These data can then be called from within the erb file by prefixing the variable name with an "@", instead of "$". In the world of ruby, the "@" denotes instance variables. 

Therefore, when we do a puppet run on an agent, we get:

<pre>
[root@puppetagent1 ~]# puppet agent --test
Info: Retrieving pluginfacts
Info: Retrieving plugin
Info: Caching catalog for puppetagent1.codingbee.dyndns.org
Info: Applying configuration version '1419895997'
Notice: /Stage[main]/User_account/File[/tmp/user.txt]/ensure: defined content as '{md5}b05ac2c2803531f329649b0038b1f4d7'
Notice: Finished catalog run in 0.08 seconds
[root@puppetagent1 ~]# cat /tmp/user.txt
Hello homer.
  Your name is: homer
The time is now Mon Dec 29 23:33:43 +0000 2014.

  A apple is a fruit.
  A banana is a fruit.
  A mango is a fruit.

The OS of this machine = RedHat

Here is a init.pp variable: have a great day!!!

Here is a site.pp variable: testing 123

[root@puppetagent1 ~]#
</pre>

Also just to ensure that the if-statement is working, if we do the puppet run on the 2nd agent, then you get:

<pre>
[root@puppetagent2 ~]# puppet agent --test
Info: Retrieving pluginfacts
Info: Retrieving plugin
Info: Caching catalog for puppetagent2.codingbee.dyndns.org
Info: Applying configuration version '1419895997'
Notice: /Stage[main]/User_account/File[/tmp/user.txt]/ensure: defined content as '{md5}0cdb669f843cfe3a0e4c4116c7ba5aa7'
Notice: Finished catalog run in 0.08 seconds
[root@puppetagent2 ~]# cat /tmp/user.txt
Hello bart.
  Sorry you are not homer.
The time is now Mon Dec 29 23:35:00 +0000 2014.

  A apple is a fruit.
  A banana is a fruit.
  A mango is a fruit.

The OS of this machine = RedHat

Here is a init.pp variable: have a great day!!!

Here is a site.pp variable: testing 123

[root@puppetagent2 ~]#
</pre>

<h3>Out of scope variables</h3>

Out of scope variables are variables that don't reside in the current scope or any of the parent scopes. These are therefore usually variables that are defined in the current module but in a different class (i.e. outside the init.pp file) or in a different module altogether. out-of-scope variables are not automatically loaded into the erb files like we saw earlier. 

However puppet does pass into the erb file an object called "scope". And this object contains all out-of-scope variables. You can access this info by using one of the scope object's method, which is called "lookupvar". This method requires one arguement, which is the fqdn of the variable that you're interested in. 

To see this in action, we created a new class (called "another_class") within the existing module and also separate module (called "another_module") too:

<pre>

[root@puppetmaster modules]# tree /etc/puppet/modules/
/etc/puppet/modules/
├── another_module
│   └── manifests
│       └── init.pp
└── user_account
    ├── files
    ├── lib
    ├── manifests
    │   ├── another_class.pp
    │   └── init.pp
    ├── spec
    └── templates
        └── user-master-copy.erb

8 directories, 4 files
[root@puppetmaster modules]#


</pre>

Now the site.pp file shows:

<pre>
[root@puppetmaster manifests]# cat site.pp
node 'PuppetAgent1' {
  class {another_module:}
  class {user_account::another_class:}
  class {user_account:}
}

node 'PuppetAgent2' {
  class {user_account:
    username => "bart",
  }
}
[root@puppetmaster manifests]#

</pre>

In this example, the user_account module contains the call to the erb template. Notice how this class is called after the other class and module. This is important because we need to have this classes and module's data loaded into the "scope" object before we try to retrieve them.

The main class's init file shows:


<pre>
[root@puppetmaster puppet]# cat modules/user_account/manifests/init.pp
class user_account ($username = 'homer'){

  user { $username:
    ensure => present,
    uid    => '101',
    shell  => '/bin/bash',
    home   => "/home/$username",
  }


  file { '/tmp/user.txt':
    ensure  => file,
    content  => template("user_account/user-master-copy.erb"),
  }
}
[root@puppetmaster puppet]#
</pre>
 

The another_class.pp file contains:

<code>

[root@puppetmaster puppet]# cat modules/user_account/manifests/another_class.pp
class user_account::another_class {

  $sidescope = "this variable is from a different class."
}
[root@puppetmaster puppet]#

</code>


Next we have the other module's init.pp file:



<pre>



[root@puppetmaster manifests]# cat init.pp
class another_module {

  $sidescope = "this variable is from a different module."
}
[root@puppetmaster manifests]#

</pre>


Finally our erb file shows:


<pre>

[root@puppetmaster templates]# cat user-master-copy.erb
Here is a variable from another class: <%= scope.lookupvar('user_account::another_class::sidescope') %>

Here is variable from another module: <%= scope.lookupvar('another_module::sidescope') %>
[root@puppetmaster templates]#
</pre>

Notice how we use the lookupvar method to retrieve the variable's value from the scope objecct. We used the variable's fqdn otherwise the lookupvar method will fail due to ambiguities. The fqdn is usually in the following form:

<pre>
{module-name}::{variable-name}   # if the variable is defined in the init file.   

{module-name}::{class-name}::{variable-name}   # if the variable is defined in a seperate file.
                                               # note, you need also include folder names if the
                                               # manifest file is placed in other folders 
                                               # inside the manifest folder.   
</pre>

Now if we do a puppet run, we get:

<pre>
[root@puppetagent1 tmp]# puppet agent --test
Info: Retrieving pluginfacts
Info: Retrieving plugin
Info: Caching catalog for puppetagent1.codingbee.dyndns.org
Info: Applying configuration version '1419959486'
Notice: /Stage[main]/User_account/File[/tmp/user.txt]/ensure: defined content as '{md5}24b67407291deaacfe9e6f32dc3f9138'
Notice: Finished catalog run in 0.12 seconds
[root@puppetagent1 tmp]# cat user.txt
Here is a variable from another class: this variable is from a different class.

Here is variable from another module: this variable is from a different module.
[root@puppetagent1 tmp]#
</pre>

Success!




]]></Content>
		<Date><![CDATA[2014-12-25]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - The ERB templating system]]></Title>
		<Content><![CDATA[The ERB templating system is a really versatile way for dynamically generating customised output from a template. 


It's similar to how php works when it generates web pages, but it is a lot more easier to embed ruby code in ERB files. It has some really powerful intelligence that lets you weave in and out of the template data where necessary.

ruby's ERB template files, are files with the ".erb". These files are basically text files with a bunch of ruby tags embeded in them.  There are a few different types of these ruby tags:

[table id=erb-template-tags /] 



The content that falls outside these tags can be pretty much any data you want, html, xml, plain text....etc. 

You can write .erb files and then test them using the .erb command line utility. 

For example, lets say we have the following erb file, called time.erb:

<pre>
<%# here we display the current time. %>
Hello world. The time is now <%= Time.now %>. Have nice day.
</pre>

Then from the command line, you do:

<pre>
PS C:\Temp\ruby\erb> erb .\time.erb

Hello world. The time is now 2014-12-28 16:48:18 +0000. Have nice day.
</pre>

Note: the leading blank line is caused by the fact that first line starts on the second line. 

Now let's see how to embed a loop into an erb file. Here's a file called simpleloop.erb

<a href="http://codingbee.net/wp-content/uploads/2014/12/sample-html-erb.png"><img src="http://codingbee.net/wp-content/uploads/2014/12/sample-html-erb.png" alt="" width="537" height="257" class="alignnone size-full wp-image-2539" /></a>

Now lets try it:

<a href="http://codingbee.net/wp-content/uploads/2014/12/hnednTb.png"><img src="http://codingbee.net/wp-content/uploads/2014/12/hnednTb.png" alt="" width="385" height="182" class="alignnone size-full wp-image-2540" /></a>

This looks good. Erb noticed that there is a "for" tag and was smart enough to expect an "end" tag later on. When it did encounter the end tag, it managed to recongise that it is the end of the for-loop and every code tags inbetween should be treated as part of that loop. 

We can use <a href="http://codingbee.net/tutorials/powershell/powershell-combine-commands-together-using-pipes/" title="PowerShell – Combine commands together using pipes">Powershell's out-file command</a> to capture the output into an html file:

<pre>
PS C:\Temp\ruby\erb> erb .\simpleloop.erb | Out-File C:\Temp\simpleloop.html
PS C:\Temp\ruby\erb>
</pre>

This will create a new html file, if we open this file we should see:

<a href="http://codingbee.net/wp-content/uploads/2014/12/erb-output1.png"><img src="http://codingbee.net/wp-content/uploads/2014/12/erb-output1.png" alt="" width="277" height="89" class="alignnone size-full wp-image-2536" /></a>


Since the "for" tag and and the "end" tag are single lines, then we could use the "%" syntax instead:


We can also re-write the above erb code in the form of a block, rather than an for-loop:

<a href="http://codingbee.net/wp-content/uploads/2014/12/1wXPBoO.png"><img src="http://codingbee.net/wp-content/uploads/2014/12/1wXPBoO.png" alt="" width="558" height="298" class="alignnone size-full wp-image-2543" /></a>

Notice this time, the start block iterator occupies a few lines, this is possible because the <%...%> tags allows for multi-line ruby code. Once again if you output it:

<a href="http://codingbee.net/wp-content/uploads/2014/12/hnednTb.png"><img src="http://codingbee.net/wp-content/uploads/2014/12/hnednTb.png" alt="" width="385" height="182" class="alignnone size-full wp-image-2540" /></a>


Then capture the output into a html file:

<pre>
PS C:\Temp\ruby\erb> erb .\simpleblock.erb | Out-File C:\Temp\simpleblock.erb
PS C:\Temp\ruby\erb>
</pre>

You'll find that the output is the same again:

<a href="http://codingbee.net/wp-content/uploads/2014/12/erb-output1.png"><img src="http://codingbee.net/wp-content/uploads/2014/12/erb-output1.png" alt="erb-output1" width="277" height="89" class="alignnone size-full wp-image-2536" /></a>







]]></Content>
		<Date><![CDATA[2014-12-26]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Facts]]></Title>
		<Content><![CDATA[Just before an agent requests for a catalog from the master, the agent first compiles a list of information about itself (in the form of key-value pairs). This information is gathered by a tool called "Facter", and each key-value item is referred to as a "fact". You can run the following command on the agent to view the full of facts that the agent sends to the master, as part of it's catalog request. 

<pre>
[root@puppetagent1 ~]# facter
architecture => x86_64
augeasversion => 1.0.0
bios_release_date => 12/01/2006
bios_vendor => innotek GmbH
bios_version => VirtualBox
blockdevice_sda_model => VBOX HARDDISK
blockdevice_sda_size => 22020587520
blockdevice_sda_vendor => ATA
blockdevice_sr0_model => CD-ROM
blockdevice_sr0_size => 1073741312
blockdevice_sr0_vendor => VBOX
blockdevices => sda,sr0
boardmanufacturer => Oracle Corporation
boardproductname => VirtualBox
boardserialnumber => 0
domain => codingbee.dyndns.org
facterversion => 2.1.0
filesystems => ext4,iso9660
fqdn => puppetagent1.codingbee.dyndns.org
hardwareisa => x86_64
hardwaremodel => x86_64
hostname => puppetagent1
id => root
interfaces => eth0,lo
ipaddress => 192.168.1.175
ipaddress_eth0 => 192.168.1.175
ipaddress_lo => 127.0.0.1
is_virtual => true
kernel => Linux
kernelmajversion => 2.6
kernelrelease => 2.6.32-431.23.3.el6.x86_64
kernelversion => 2.6.32
lsbdistcodename => Final
lsbdistdescription => CentOS release 6.5 (Final)
lsbdistid => CentOS
lsbdistrelease => 6.5
lsbmajdistrelease => 6
lsbrelease => :base-4.0-amd64:base-4.0-noarch:core-4.0-amd64:core-4.0-noarch:graphics-4.0-amd64:graphics-4.0-noarch:printing-4.0-amd64:printing-4.0-noarch
macaddress => 08:00:27:95:E3:60
macaddress_eth0 => 08:00:27:95:E3:60
manufacturer => innotek GmbH
memoryfree => 805.86 MB
memoryfree_mb => 805.86
memorysize => 996.14 MB
memorysize_mb => 996.14
mtu_eth0 => 1500
mtu_lo => 16436
netmask => 255.255.255.0
netmask_eth0 => 255.255.255.0
netmask_lo => 255.0.0.0
network_eth0 => 192.168.1.0
network_lo => 127.0.0.0
operatingsystem => CentOS
operatingsystemmajrelease => 6
operatingsystemrelease => 6.5
osfamily => RedHat
partitions => {"sda1"=>{"uuid"=>"d74a4fa8-0883-4873-8db0-b09d91e2ee8d", "size"=>"1024000", "mount"=>"/boot", "filesystem"=>"ext4"}, "sda2"=>{"size"=>"41981952", "filesystem"=>"LVM2_member"}}
path => /usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin
physicalprocessorcount => 1
processor0 => Intel(R) Core(TM) i7 CPU         920  @ 2.67GHz
processor1 => Intel(R) Core(TM) i7 CPU         920  @ 2.67GHz
processor2 => Intel(R) Core(TM) i7 CPU         920  @ 2.67GHz
processorcount => 3
productname => VirtualBox
ps => ps -ef
puppetversion => 3.6.2
rubysitedir => /usr/lib/ruby/site_ruby/1.8
rubyversion => 1.8.7
selinux => true
selinux_config_mode => enforcing
selinux_config_policy => targeted
selinux_current_mode => enforcing
selinux_enforced => true
selinux_policyversion => 24
serialnumber => 0
sshdsakey => AAAAB3NzaC1kc3MAAACBAK5fYwRM3UtOs8zBCtRTjuHLw56p94X/E0UZBZwFR3q7WH0x5+MNsjfmdCxKvpY/WlIIUcFJzvlfjXm4qDaTYalbzSZJMT266njNbw5WwLJcJ74KdW92ds76pjgmCsjAh+R9YnyKCEE35GsYjGH7whw0gl/rZVrjvWYKQDOmJA2dAAAAFQCoYABgjpv3EkTWgjLIMnxA0GfudQAAAIBM4U6/nerfn6Qvt43FC2iybvwVo8ufixJl5YSEhs92uzsW6jiw68aaZ32q095/gEqYzeF7a2knrOpASgO9xXqStYKg8ExWQVaVGFTR1NwqhZvz0oRSbrN3h3tHgknoKETRAg/imZQ2P6tppAoQZ8wpuLrXUCyhgJGZ04Phv8hinAAAAIBN4xaycuK0mdH/YdcgcLiSn8cjgtiETVzDYa+jF5n8y7oN031I2ZVMsfyu9Qn6uPzrZNbVE4xk8C+N9Yy6zjBqD09PKbptqGhwLBFh4OrC5/4QA8V/8/PmtBvq/sGh2IK8XKZRJUncFdumd+NrbAIUDc/pshaF2DfkEFuwTihdkQ==
sshfp_dsa => SSHFP 2 1 e3fe40f161a0e09a5713e9d642c3e20b82c3b932
SSHFP 2 2 d75eb88e5e7444008737949725f2dd7ba62f47dd4a23fcdf53dc5a14849a96ab
sshfp_rsa => SSHFP 1 1 c4cedb9dff46020f599a7346d9a923336afd8613
SSHFP 1 2 5d2a76840cbfa407a8bb2cad3e5d06cc9c99fbc5b5917c4c366e242495afc4b9
sshrsakey => AAAAB3NzaC1yc2EAAAABIwAAAQEA3dX6XCeJFjb8uX4fdKlry7XRVWXRqA+nsCzKI3+BC89h2aVfWnyO1BHceJAOFJIyFnQxy2PWWX18Qu1KZMQ/VB4uzP3G1ykVIC1miPGriiAzN6mpfJWWb5jEKno9iTEbCqJEahliJuLGJdPca8peReImudWbYod4SyhCeITJ6sb4gKS1L4VA3TIQvQLstAINdjxN8od1nr7g3V4WTGVqelLXkGfP9WzfGmxoTZgSbi8xWpiIrLk3H07PQ5IHLE4Y02y6k9AFH35wRaLwkC9SFdJCYIOSA7D6IKHP8+kiVZdYUb3iuRjIxeZU2Z1EppWa+Fj3oS2/tXGzqmdwgxlseQ==
swapfree => 1.97 GB
swapfree_mb => 2015.99
swapsize => 1.97 GB
swapsize_mb => 2015.99
timezone => GMT
type => Other
uniqueid => a8c0af01
uptime => 6:04 hours
uptime_days => 0
uptime_hours => 6
uptime_seconds => 21865
uuid => BD8B9D85-1BFD-4015-A633-BF71D9A6A741
virtual => virtualbox
[root@puppetagent1 ~]#

</pre>

Note, some of the these facts overlap with the data already available from the "env". However Puppet does not make direct use of env data. Instead it makes use of all facter data, and to puppet, facter data is treated as global variables.    

The agent always provides this list of facts as part of it's catalog request during the start of a puppet run. 

You can find out more about these <a href="https://docs.puppetlabs.com/facter/latest/core_facts.html">facts on the puppetlabs website</a>. 

If you just want to view the value of just one fact, e.g. osfamily, then you simply do:

<pre>
[root@puppetagent1 ~]# facter osfamily
RedHat
</pre>

These facts are then available to the puppet master as normal top level variables, aka a global variable, and the master can call on the given node's facts in the form of a global variable in any of it manifests in order to compile the catalog for the requesting agent.

Facters are referenced in a manifest, in the same way as an ordinary variable, i.e. with a $ prefix. For example let's say we have the following manifest stored on our agent:

<pre>
[root@puppetagent1 /]# cat /tmp/ostype.pp
if ($osfamily == "RedHat"){
  $message = "This machine OS is of the type $osfamily \n"
}
else {
  $message = "This machine is unknown \n"
}

file { "/tmp/machinetype.txt":

  ensure => file,
  content => "$message"

}
[root@puppetagent1 /]#

</pre>

This manifest ensures the state of a single text file called machinetype.txt, where the content of this file is dictated by the fact, osfamily. As a result, if we apply this manifest, we get:

<pre>
[root@puppetagent1 /]# facter osfamily
RedHat
[root@puppetagent1 /]# puppet apply /tmp/ostype.pp
Notice: Compiled catalog for puppetagent1.codingbee.dyndns.org in environment production in 0.07 seconds
Notice: /Stage[main]/Main/File[/tmp/machinetype.txt]/ensure: defined content as '{md5}f59dc5797d5402b1122c28c6da54d073'
Notice: Finished catalog run in 0.04 seconds
[root@puppetagent1 /]# cat /tmp/machinetype.txt
This machine OS is of the type RedHat
[root@puppetagent1 /]#

</pre> 

<h1>
Custom facts</h1>
The facts we have seen so far are the default ones that facter provides out of the box, hence these are known as "core facts".

However you can also add your own custom facts for a given node. There are a few ways to do this. 

<ul>
	<li>Using the "export FACTER_..." syntax.
	<li>via the $LOAD_PATH setting</li>
	<li>FACTERLIB</li>
	<li>Pluginsync (distribute facts through modules)</li>
</ul>



You can manually add your own facts from the command line, using the "export FACTER_{fact's name}" syntax:

<pre>
[root@puppetagent1 facter]# export FACTER_tallest_mountain="Everest"
[root@puppetagent1 facter]# facter tallest_mountain
Everest
[root@puppetagent1 facter]# env | grep "tallest_mountain"
FACTER_tallest_mountain=Everest
[root@puppetagent1 facter]#


</pre>

In this case, you have to use the export command, otherwise this won't work. As a result the fact also become available as a OS environment variable. However environment variable are recognised as facts by default:

<pre>
[root@puppetagent1 facter]# export hello="world"
[root@puppetagent1 facter]# env | grep "hello"
hello=world
[root@puppetagent1 facter]# facter hello

[root@puppetagent1 facter]#

</pre>  

Custom facts created using this approach only persist in the current session. As soon as you restart your bash session, it disappears. As a result this approach to creating custom facts has very little real world applications. 


<h3>
The $LOAD_PATH approach</h3>

In ruby, $LOAD_PATH is the ruby equivalent of Bash's special parameters, although it is an analogy of bash's environment variable, $PATH. However the $LOAD_PATH is not actually an environment variable, instead it is a <a href="http://en.wikibooks.org/wiki/Ruby_Programming/Syntax/Variables_and_Constants#Pre-defined_Variables">pre-defined variable</a>.

The $LOAD_PATH has a synonym which is "$:". This variable is an array of places to search for and load files.  

To view this variable simply do:

<pre>
[root@puppetagent1 ~]# ruby -e 'puts $LOAD_PATH'           # note you have to use single quotes. 
/usr/lib/ruby/site_ruby/1.8
/usr/lib64/ruby/site_ruby/1.8
/usr/lib64/ruby/site_ruby/1.8/x86_64-linux
/usr/lib/ruby/site_ruby
/usr/lib64/ruby/site_ruby
/usr/lib64/site_ruby/1.8
/usr/lib64/site_ruby/1.8/x86_64-linux
/usr/lib64/site_ruby
/usr/lib/ruby/1.8
/usr/lib64/ruby/1.8
/usr/lib64/ruby/1.8/x86_64-linux
.
</pre>

or, from the irb command line do:

<pre>
[root@puppetagent1 ~]# irb
irb(main):001:0> puts $LOAD_PATH
/usr/lib/ruby/site_ruby/1.8
/usr/lib64/ruby/site_ruby/1.8
/usr/lib64/ruby/site_ruby/1.8/x86_64-linux
/usr/lib/ruby/site_ruby
/usr/lib64/ruby/site_ruby
/usr/lib64/site_ruby/1.8
/usr/lib64/site_ruby/1.8/x86_64-linux
/usr/lib64/site_ruby
/usr/lib/ruby/1.8
/usr/lib64/ruby/1.8
/usr/lib64/ruby/1.8/x86_64-linux
.
=> nil
irb(main):002:0>

</pre>

Let's cd into one of these directories, then create a folder called "facter" and create a blank rb file in it:


<pre>

[root@puppetagent1 ~]# cd /usr/lib/ruby/site_ruby/
[root@puppetagent1 site_ruby]# mkdir facter
[root@puppetagent1 site_ruby]# cd facter/
[root@puppetagent1 facter]# ls
[root@puppetagent1 facter]# touch custom_facts.rb


</pre>

In this blank rb file, lets add the following content:

<pre>
[root@puppetagent1 facter]# cat custom_facts.rb
Facter.add('tallest_mountain') do
  setcode "echo Everest"
end
[root@puppetagent1 facter]#
</pre>

As you can see, we don't define a fact by using something straight forward such as <code>{fact's name} = {fact's value}</code>. Instead you have to define the facter in the above style which is made up of 2 parts the <code>Facter.add('fact_name')</code> element and the <code>setcode</code> element.

This syntax may appear overly declarative and complicated. That's because it is common for a facter's value, to take  the output from a shell command/script.  

The way that facter works is that every time it runs, it scans all the folders listed in $LOAD_PATH, and looks for a directory called "facter", which can be located anywhere within the folder's tree structures. If if finds this folder, it then looks for any ruby files in the facter folder, and loads any defined facts into memory. Note, it is best practice to:

<ul>
	<li>define a single fact per ruby file.</li>
	<li>the ruby file is named after the fact's name. So in the above example, I should rename the "custom_facts.rb" to "tallest_mountain.rb", in order to comply with best practice</li>
</ul>

To summarise, the $LOAD_PATH ruby special variable is used by facter to "autoload" facts.

<h3>The FACTERLIB approach</h3>
FACTERLIB works in a similar way to how $LOAD_PATH works, but it is an OS level environment parameter rather than a ruby special variable.     

By default, this environment variable may not be set:

<pre>
[root@puppetagent1 facter]# env | grep "FACTERLIB"
[root@puppetagent1 facter]#

</pre>


To show how this works, let's create a folder called "some_facts" and set up the following structure. 

<pre>
[root@puppetagent1 tmp]# tree /tmp/some_facts/
/tmp/some_facts/
├── river
│   └── longest_river.rb
└── wall
    └── longest_wall.rb

2 directories, 2 files
[root@puppetagent1 tmp]#
</pre>


The longest_river.rb content is:

<pre>
[root@puppetagent1 river]# cat longest_river.rb
Facter.add('longest_river') do
  setcode "echo Nile"
end
</pre>



And the longest_wall.rb file's content is:

<pre>
[root@puppetagent1 wall]# cat longest_wall.rb
Facter.add('longest_wall') do
  setcode "echo 'China Wall'"
end
[root@puppetagent1 wall]#
</pre>

Now if we see if these facts exists:

<pre>
[root@puppetagent1 wall]# facter longest_river

[root@puppetagent1 wall]# facter longest_wall

[root@puppetagent1 wall]#
</pre>

No luck. That's expected since we haven't defined facterlib yet, which we will do now:

<pre>
[root@puppetagent1 /]# export FACTERLIB="/tmp/some_facts"
[root@puppetagent1 /]# env | grep "FACTERLIB"
FACTERLIB=/tmp/some_facts
</pre>

Let's now try again:

<pre>
[root@puppetagent1 /]# facter longest_wall

[root@puppetagent1 /]# facter longest_river

[root@puppetagent1 /]#

</pre>

Still no luck. That's because, facter_lib approach is not as intelligent as $LOAD_PATH, and doesn't recursively look for rb files from a top level directory. Therefore we now try:

<pre>
[root@puppetagent1 /]# export FACTERLIB="/tmp/some_facts/river:/tmp/some_facts/wall"
[root@puppetagent1 /]# env | grep "FACTERLIB"
FACTERLIB=/tmp/some_facts/river:/tmp/some_facts/wall
</pre>


And then see if facter loads the new facts:


<pre>
[root@puppetagent1 /]# facter longest_river
Nile
[root@puppetagent1 /]# facter longest_wall
China Wall
</pre>

Success!

<h2>Distribute facts through modules</h2>

Note, you may need to update the <a href="https://docs.puppetlabs.com/guides/plugins_in_modules.html#enabling-pluginsync">pluginsync</a> setting in puppet.conf on both master and agent. 


One of the main reasons why we may need custom facts, is becuase we have written a module that requires a custom fact in the first place. In this scenario, a good approach would be that if a module requires some custom facts, then bundle those custom facts into the module itself.

I think what would happen here, is that when an agents submit core facter info to the master during a puppet run, then an intermediary step will happen where the master will send the agent a bunch of custom facts rb files for the agent to process and return a bunch of custom facts back to the master. The master will then make use of the core facts along with the custom facts to generate the catalog.

Now let's see how we can package a custom fact into a module. 

Let's say that we want our module (called user_account) to create a file called /tmp/licence.txt. And this file has to contain:

Licence = {licence-info}  

The licence info itself is the output from the following command:

<pre>
[root@puppetmaster manifests]# uname -o
GNU/Linux
</pre>

In which case the content should be:


<pre>y

Licence = GNU/Linux
</pre>

To achieve is, our site.pp file shows:

<pre>
[root@puppetmaster manifests]# cat site.pp
node 'PuppetAgent1' {
  class {user_account:}
}

node 'PuppetAgent2' {
  class {user_account:
    username => "bart",
  }
}
[root@puppetmaster manifests]#
</pre>

And our user_account module looks like:

<pre>
[root@puppetmaster user_account]# tree
.
├── files
├── lib
│   └── facter
│       └── licence.rb
├── manifests
│   └── init.pp
├── spec
└── templates

6 directories, 2 files
[root@puppetmaster user_account]#

</pre>

Notice here that we created a new folder called facter under the lib folder. And inside our facter folder we created our custom-fact rb file, licence.rb. The content of the rb file is:

<pre>
[root@puppetmaster user_account]# cat lib/facter/licence.rb
Facter.add('licence') do
  setcode "uname -o"
end
[root@puppetmaster user_account]#
</pre>

Here we see that we have defined a fact called "licence" and it is equal to the output from the "uname -o" shell command. 

If we now look at the init file, we see:

<pre>
[root@puppetmaster user_account]# cat manifests/init.pp
class user_account ($username = 'homer'){

  user { $username:
    ensure => present,
    uid    => '101',
    shell  => '/bin/bash',
    home   => "/home/$username",
  }


  file { '/tmp/licence.txt':
    ensure  => file,
    content  => "Licence: $licence \n",
  }
}
[root@puppetmaster user_account]#

</pre>

Notice here that "$licence" is used like a an ordinary variable. That's as expected because the custom fact is designed to make the licence fact available like a global variable. Now when we do a puppet run, we get:

<pre>
[root@puppetagent1 tmp]# puppet agent --test
Info: Retrieving pluginfacts
Info: Retrieving plugin
Info: Loading facts in /var/lib/puppet/lib/facter/licence.rb
Info: Caching catalog for puppetagent1.codingbee.dyndns.org
Info: Applying configuration version '1420033998'
Notice: /Stage[main]/User_account/File[/tmp/licence.txt]/ensure: defined content as '{md5}a1eba342eec9d0cbf768208b3b9b18ea'
Notice: Finished catalog run in 0.04 seconds
[root@puppetagent1 tmp]# cat /tmp/licence.txt
Licence: GNU/Linux
[root@puppetagent1 tmp]#
</pre>

Success!

<strong>See also:</strong>

https://docs.puppetlabs.com/facter/latest/custom_facts.html
https://docs.puppetlabs.com/facter/latest/fact_overview.html



https://docs.puppetlabs.com/puppet/3.7/reference/lang_facts_and_builtin_vars.html

http://www.tutorialspoint.com/ruby/ruby_predefined_variables.htm

  ]]></Content>
		<Date><![CDATA[2014-12-30]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Writing Custom Facts]]></Title>
		<Content><![CDATA[The custom facts rb files that we have seen so far ran shell commands on the agents. This obviously won't work for certain linux distros, or a windows agent. That's why custom facts need more code logic to be added so that it is compatible with other distros and windows machine. 

This can be done using an over-ride feature, in the form of the confine keyword, i think.

Also so far we have seen one line shell command's output being what we required. However sometime we need to write a multiline ruby/shell code block. In which case we can use the do...end block. 

Finally, you can create facts that actually contains an array or hash. These types of facts are referred to as <a href="https://docs.puppetlabs.com/facter/latest/fact_overview.html#writing-structured-facts">structured facts</a>.  




https://docs.puppetlabs.com/facter/latest/custom_facts.html
https://docs.puppetlabs.com/facter/latest/fact_overview.html


Once you have been added to an agent, you can then query them withe the -p option:

<pre>
$ facter -p | wc -l
132
$ facter | wc -l
99
</pre>

Here you can see that there are about 33 (132-99) custom facts available on top of the standard facts. 




]]></Content>
		<Date><![CDATA[2014-12-31]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Install Hiera]]></Title>
		<Content><![CDATA[Before you can start using hiera, you first need to install the hiera rpm package as well as the hiera gem. You only isntall hiera on the master. However if you are managing a standalone agent, then you need to install it on these agents too. 


To check if hiera rpm is already installed do:

<pre>
[root@puppetmaster ~]# rpm -qa | grep "hiera"
hiera-1.3.4-1.el6.noarch
[root@puppetmaster ~]#
</pre>

If nothing is outputted, then you can do:

<pre>
yum install hiera
</pre>

or get puppet to install it for you:

<pre>

[root@puppetmaster ~]# puppet resource package hiera ensure=installed

</pre>



Next you need the hiera gem installed. You can check if that is so by running:

<pre>
[root@puppetmaster ~]# gem list

*** LOCAL GEMS ***

json (1.5.5)
[root@puppetmaster ~]#
</pre>

If it's not installed then do:

<pre>
[root@puppetmaster /]# gem install hiera
Successfully installed json_pure-1.8.1
Successfully installed hiera-1.3.4
2 gems installed
Installing ri documentation for json_pure-1.8.1...
Installing ri documentation for hiera-1.3.4...
Installing RDoc documentation for json_pure-1.8.1...
Installing RDoc documentation for hiera-1.3.4...
[root@puppetmaster /]#
</pre>

This also installs all dependencies. After that you should find the hiera gem is now installed:


<pre>
[root@puppetmaster /]# gem list

*** LOCAL GEMS ***

hiera (1.3.4)
json (1.5.5)
json_pure (1.8.1)
[root@puppetmaster /]#
</pre>












]]></Content>
		<Date><![CDATA[2015-01-01]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Best practice to writing modules]]></Title>
		<Content><![CDATA[the ntp module is an example of a really well written module. 


https://docs.puppetlabs.com/guides/module_guides/bgtm.html



any yum related activicivities should reside  in it's own manifest called: <code>install.pp</code>
any config file related activities should reside in it's own manifest called: <code>config.pp</code>
any service start/stop related activities should reside in it's own manifest called: <code>service.pp</code>


These are best practice, although I don't think they are mandatory. 


All class parameters should have default values, so that people can use the module just be calling "include class_name". All the default parameter values should be defined in it's own manifest, which we should call <code>params.pp</code>.]]></Content>
		<Date><![CDATA[2015-01-02]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Special Variables]]></Title>
		<Content><![CDATA[https://docs.puppetlabs.com/puppet/latest/reference/lang_facts_and_builtin_vars.html#special-variables-added-by-puppet]]></Content>
		<Date><![CDATA[2015-01-02]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Class Inheritance]]></Title>
		<Content><![CDATA[Let's say want to create a module that let's you set up any of the following scenarios on each of your nodes:

<ul>
	<li>Creates a user (called 'homer') only</li>
	<li>Creates a user (called 'homer') and textfile. </li>
        <li>Creates a user (called 'homer') and ensures a service is running</li>
</ul>

Note here that the user called "homer" must always exist. 





There are 4 main ways to achieve this in your module's design:

<ol>
	<li>create 3 classes, one for each resource. In this scenario, you would have 3 manifests, e.g., init.pp (which contains the user_account class), ensure_file.pp, and ensure_service.pp.	</li>
	<li>have all resources in a single class in the init.pp file, and control weather file or service resource using class parameters.</li>
	<li>Write a class for each resource, assume the "user" resource is the main class. Also the file and service class also includes a user clss. </li>
	<li>use inheritance</li>
</ol>







Option 1 - this is not good, because it relies on the module's to remember to always call the user_account class before the ensure_file/ensure_service classes. This means that it makes it possible for user to only ensure textfile (by only including the user_account::esnure_file class), which is not how the module is supposed to be used, becuase it isn't one of the scenarios defined above. 

option 2 - Also not good because you have introduce extra class parameters along with if/else logic, which can be avoided. 

option 3 - Also not good, because it will result in code duplication, since the user resource has to be defined 3 times, once for each manifest.  

Option 4 - this is the best option. But only use <a href="https://docs.puppetlabs.com/puppet/latest/reference/lang_classes.fin">inheritance sparingly</a>. 

To achieve inheritance, we first have to start with implementing option 1. This means that we end up with:

<pre>
[root@puppetmaster modules]# tree user_account/
user_account/
└── manifests
    ├── ensure_file.pp
    ├── ensure_service.pp
    └── init.pp

[root@puppetmaster modules]# cd user_account/manifests/
[root@puppetmaster manifests]# cat init.pp 
class user_account {

  user { 'homer':
    ensure => 'present',
    uid => '121',
    shell => '/bin/bash',
    home => '/home/homer',
  }

}
[root@puppetmaster manifests]# cat ensure_file.pp 
class user_account::ensure_file {

  file {'/tmp/testfile.txt':
    ensure => file,
    content => "some important data. \n",
  }

}
[root@puppetmaster manifests]# cat ensure_service.pp 
class user_account {

  service {'sssd':
    ensure => running,
  }

}
[root@puppetmaster manifests]# 

</pre>

Note: here we used the "sssd" service just as an example. 

now our site.pp shows:

<pre>
[root@puppetmaster manifests]# cat site.pp 

node 'puppetmaster' {
  include user_account::ensure_file       # note, we don't reference the main class, only side one. 
}

[root@puppetmaster manifests]# 
</pre>

Now if we do a puppet run:

<pre>
[root@puppetagent1 tmp]# puppet agent --test
Info: Retrieving pluginfacts
Info: Retrieving plugin
Info: Loading facts in /var/lib/puppet/lib/facter/licence.rb
Info: Caching catalog for puppetagent1.codingbee.dyndns.org
Info: Applying configuration version '1420274668'
Notice: /Stage[main]/User_account::Ensure_file/File[/tmp/testfile.txt]/ensure: defined content as '{md5}171ec60a7385ef54e21ed94eeab21850'
Notice: Finished catalog run in 0.04 seconds
[root@puppetagent1 tmp]# cat /etc/passwd | grep "homer"
[root@puppetagent1 tmp]# cat /tmp/testfile.txt
some important data.
[root@puppetagent1 tmp]#

</pre>

As you can see the user has not been created, but the file has been created. 

Now let's add the inheritance syntax to ensure_file.pp:

<pre>
[root@puppetmaster manifests]# cat ensure_file.pp
class user_account::ensure_file inherits user_account {

  file {'/tmp/testfile.txt':
    ensure => file,
    content => "some important data. \n",
  }

}

[root@puppetmaster manifests]#

</pre>

Notice the "inherits <fqdn of class name>" syntax. Now if we do the puppet run again:

<pre>
[root@puppetagent1 tmp]# cat /etc/passwd | grep "homer"
[root@puppetagent1 tmp]# cat /tmp/testfile.txt
cat: /tmp/testfile.txt: No such file or directory
[root@puppetagent1 tmp]# puppet agent --test
Info: Retrieving pluginfacts
Info: Retrieving plugin
Info: Loading facts in /var/lib/puppet/lib/facter/licence.rb
Info: Caching catalog for puppetagent1.codingbee.dyndns.org
Info: Applying configuration version '1420275361'
Notice: /Stage[main]/User_account/User[homer]/ensure: created
Notice: /Stage[main]/User_account::Ensure_file/File[/tmp/testfile.txt]/ensure: defined content as '{md5}171ec60a7385ef54e21ed94eeab21850'
Notice: Finished catalog run in 0.11 seconds
[root@puppetagent1 tmp]# cat /etc/passwd | grep "homer"
homer:x:101:501::/home/homer:/bin/bash
[root@puppetagent1 tmp]# cat /tmp/testfile.txt
some important data.
[root@puppetagent1 tmp]#
</pre>

On this occasion, the module's sub class, ensure_file inherits the main class. You can think of inheritance, as a way to insert the entire "remote class" to the very beginning of the class in question. 

In real world situations, inheritance is used rarely. However one common way it is used is for all classes in a module inheriting a class that set's all the module parameter info. The <a href="https://github.com/puppetlabs/puppetlabs-ntp/tree/master/manifests">ntp puppet module</a> is a good example of this in action. 

In fact it is recommended to <a href="http://garylarizza.com/blog/2014/02/17/puppet-workflow-part-1/">only use class inheritance for inheriting params.pp</a>.  



<strong>See also:</strong>

https://docs.puppetlabs.com/puppet/latest/reference/lang_classes.html#inheritance

</pre>]]></Content>
		<Date><![CDATA[2015-01-02]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - What is Hiera]]></Title>
		<Content><![CDATA[Hiera is a powerful way to store (class parameter) data outside of your pp files. Hiera also stores this data in a efficient hierarchial structure so to minimize code duplication. This is especially useful when you want to declare a class that requires a lot of class parameters.  

For example, Let's say our user_account's module's class requires 5 class parameters. Then if we declare this class across all our nodes, then it will look something like this:



<pre>
[root@puppetmaster ~]# cat /etc/puppet/manifests/site.pp

node 'PuppetAgent1' {
  class {user_account:
    username => "bart",
    password => "Liverpool",
    password_max_age => "90",
    password_min_age => "10",
    shell => /bin/bash,
  }
}

node 'PuppetAgent2' {
  class {user_account:
    username => "bart",
    password => "Liverpool",
    password_max_age => "90",
    password_min_age => "10",
    shell => /bin/bash,
  }
}

node 'PuppetAgent3' {
  class {user_account:
    username => "bart",
    password => "Liverpool",
    password_max_age => "90",
    password_min_age => "10",
    shell => /bin/bash,
  }
}

</pre>

As you can see we are passing the same parameter values for these nodes. There's 2 problems with this. First, if for each node, we need to change the passord parameter from "Liverpool" to another value, e.g. "Manchester", then that's going to become tedious. Secondly declaring class's using the class-syntax takes up a lot of lines of code. 

However thanks to Hiera, we can seperate out the class parameter data so that our manifest now looks like this:

<pre>  
node 'PuppetAgent1' {
  include user_account
}

node 'PuppetAgent2' {
  include user_account
}

node 'PuppetAgent3' {
  include user_account
}

node 'PuppetAgent4' {
  include user_account
}

</pre>

This is possible, thanks to hiera. With hiera, all the data is stored in a bunch of yaml files, and then Puppet requests for the data through hiera, which in turn queries the yaml files. Hiera queries the yaml files in "fallback style". For example you may have heard of the saying:



<blockquote>
You always need a plan B, just in case plan A doesn't work. 
</blockquote>

Hiera (along with Puppet) takes this concept to the next level, and has plan c, plan d, plan e....and etc. In the context of puppet+hiera, these plans are to do with locating the data, when classes are declared using the "include" statement.  

<h2>The Hiera lookup order</h2>
So when puppet encounters an include statement, but the corresponding class requires class parameters, then the following happens:
<ol>
	<li>puppetmaster asks hiera to find the info. </li>
	<li>Hiera then asks puppetmaster for the agent's facter data and puppet builtin variables. These are available within the manifests node-defintion.</li>
	<li>hiera then identifies a list of yaml files that it needs to look through to find the data. This is done through hiera hierarchy. We will cover more about this later.</li>
	<li>Hiera then checks if the first file exists (plan A), if it doesn't exist, or file exist, but file exists but doesn't have the info, then hiera moves on to the next file, aka plan B.</li>
	<li>It repeats the previous process with second file (plan B). And if it fails again, then moves on to the next file, aka plan C. This cycle continues until the data is found, or hiera has run out of yaml files.</li>
	<li>if hiera fails to find the data, then it will notify puppetmaster of this by returning "nil"</li>
	<li>puppetmaster will then, as a last resort, see if the class has any default parameters defined. If it does, then it will use them instead.</li>
	<li>If no default values are defined in the class definitions, then the current puppet run will terminate and an error will be displayed, since the puppetmaster was unable to compile the catalog.</li>
 

</ol>

If we reach the last step then it means we need to investigate and fix the issue. 

 




See also:
https://docs.puppetlabs.com/hiera/1/index.html]]></Content>
		<Date><![CDATA[2015-01-02]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Adding a class to a node (aka declaring a class)]]></Title>
		<Content><![CDATA[Let's say we have:

<pre>
[root@puppetmaster modules]# tree user_account/
user_account/
├── files
├── lib
├── manifests
│   └── init.pp
├── spec
└── templates

5 directories, 1 file
[root@puppetmaster modules]# cat user_account/manifests/init.pp
class user_account ($username = 'homer'){

  user { $username:
    ensure => present,
    uid    => '101',
    shell  => '/bin/bash',
    home   => "/home/$username",
  }

}
[root@puppetmaster modules]#
</pre>

So far, we have seen 2 ways for declaring a class (aka adding a class to a node). 


The first way is to use the include-like syntax:

<pre>
node 'PuppetAgent1' {
  include user_account
}

</pre>


However there is a limitation with this, which is that the <code>include</code> syntax doesn't let you specify any class parameters. This means that you have to stick with the class parameter's default value. 



The second way is to use the class-like syntax:
<pre>
node 'PuppetAgent1' {
  class {user_account:}                  # this will create a user called "homer"
}

</pre>

The class-like syntax appears to be a better alternative to the include statement, because it let's you set class parameters:


<pre>
node 'PuppetAgent1' {
  class {user_account:
    username => "bart",
  }
}

</pre>

However the class-like syntax however has it's own limitations, which is that you can't declared the class more than once, even with different class-parameters, e.g. if you have the following in your site.pp file:

<pre>
[root@puppetmaster modules]# cat /etc/puppet/manifests/site.pp
node 'PuppetAgent1' {

  class {user_account:}

  class {user_account:
    username => "bart",
  }

  class {user_account:
    username => "lisa",
  }

}

node 'PuppetAgent2' {
  class {user_account:
    username => "bart",
  }
}
[root@puppetmaster modules]#

</pre>

Here the intention is to create 3 users, homer, bart, and lisa.


However when we do a puppet run, we get:


<pre>
[root@puppetagent1 tmp]# puppet agent --test
Info: Retrieving pluginfacts
Info: Retrieving plugin
Error: Could not retrieve catalog from remote server: Error 400 on SERVER: Duplicate declaration: Class[User_account] is already declared in file /etc/puppet/manifests/site.pp:3; cannot redeclare at /etc/puppet/manifests/site.pp:7 on node puppetagent1.codingbee.dyndns.org
Warning: Not using cache on failed catalog
Error: Could not retrieve catalog; skipping run
[root@puppetagent1 tmp]#
</pre>

Note: you also can't use an include hack like this to get around this:

<pre>
[root@puppetmaster modules]# cat /etc/puppet/manifests/site.pp
node 'PuppetAgent1' {

  include user_account
  $username = "bart"
  include user_account
  #$username = "lisa"
  #include user_account
}

node 'PuppetAgent2' {
  class {user_account:
    username => "bart",
  }
}
[root@puppetmaster modules]#

</pre>

That's becuase the class will use the class parameter's default value, rather than looking up higher scopes. Also you you can't <a href="https://docs.puppetlabs.com/puppet/latest/reference/lang_variables.html#no-reassignment">reassign variable within the same scope</a>. Hence will fail when trying reassign $username to the value "Lisa"


There are a couple of solutions to this, first, you can rewrite your class into very own customised resource type, which in puppet is referred to as a "defined type". We'll cover how to write defined types in the next article. 

There is another option, which is achieved using <a href="https://docs.puppetlabs.com/learning/modules2.html#why-include-cant-directly-take-class-parameters">Hiera</a>. We'll cover Hiera later in the course. 




<strong>See also</strong>
https://docs.puppetlabs.com/puppet/latest/reference/lang_classes.html#declaring-classes]]></Content>
		<Date><![CDATA[2015-01-03]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - The hiera.yaml config file]]></Title>
		<Content><![CDATA[If you followed the previous tutorial, then hiera should now be installed. Therefore you should now find a new config file present on your system, which is called hiera.yaml. This file should be located at: 

<pre>
[root@puppetmaster /]# rpm -qc hiera
/etc/hiera.yaml
[root@puppetmaster /]# ls -l /etc/hiera.yaml
-rw-r--r--. 1 root root 314 Jun  6  2014 /etc/hiera.yaml
[root@puppetmaster /]#
</pre>

Hiera is a standalone command line utility. The hiera.yaml config file is used to control how the hiera command operates when we use hiera on it's own. However, in practice we only  troubleshooting only. The main purpose of Hiera is that it is used by Puppet. When puppet uses hiera, it instructs hiera to look for the hiera.yaml file in "$confdir/hiera.yaml" instead. You can get the $confdir value like this:    

<pre>
[root@puppetmaster /]# puppet config print confdir
/etc/puppet
[root@puppetmaster /]# ls -l /etc/puppet/hiera.yaml
ls: cannot access /etc/puppet/hiera.yaml: No such file or directory
</pre>

As you can see, in my case the hiera.yaml is not where puppet expects to find it. We can therefore move the /etc/hiera.yaml to /etc/puppet/hiera.yaml. However in doing so it will stop the hiera standalone from working. Therefore the best option is to create a symbolic link instead:

<pre>
[root@puppetmaster /]# ls -l /etc/puppet/hiera.yaml
lrwxrwxrwx. 1 root root 15 Jan  5 20:01 /etc/puppet/hiera.yaml -> /etc/hiera.yaml
</pre>


By default, the content of this file should look something like:

An alternative to creating symbolic links is to set the <a href="https://docs.puppetlabs.com/references/latest/configuration.html#hieraconfig">hiera_config</a> in the puppet.conf file.

<pre>
[root@puppetmaster /]# cat /etc/puppet/hiera.yaml
---
:backends:
  - yaml
  - json
:hierarchy:
  - defaults
  - "%{clientcert}"
  - "%{environment}"
  - global
:logger: console
:yaml:
# datadir is empty here, so hiera uses its defaults:
# - /var/lib/hiera on *nix
# - %CommonAppData%\PuppetLabs\hiera\var on Windows
# When specifying a datadir, make sure the directory exists.
  :datadir:
[root@puppetmaster /]#
</pre>




This config file's content is in the form of the <a href="http://www.yaml.org/YAML_for_ruby.html">yaml structure</a>. 

As you can see, we have a few settings that are in the form of ruby symbols:

<ul>
	<li>:backends</li>
	<li>:hierarchy</li>
	<li>:logger</li>
	<li>:yaml</li>
</ul>


Let's assume, that on array called "hiera" exists which stores all this information in this config file. And each setting is an array item in this object. With this in mind, lets take a look at each setting in turn:

<h3>:backends setting</h3>

This is actually <a href="http://www.yaml.org/YAML_for_ruby.html#sequence_in_a_mapping">hash table where the value is in the form of an array</a>. Also this hash table only contains 1 key-value pair, although the array (i.e. the value) contains two items:	

For example:

<pre>
PS C:\Windows\system32> irb
DL is deprecated, please use Fiddle
irb(main):020:0* hiera = { 'backend' => ['yaml','json'] }
=> {"backend"=>["yaml", "json"]}
irb(main):021:0> hiera['backend']
=> ["yaml", "json"]
irb(main):022:0> hiera['backend'][0]
=> "yaml"
irb(main):023:0> hiera['backend'][1]
=> "json"
irb(main):024:0>
</pre>

This setting let's you choose what kind of config files it can work with. If you plan to write all your data config files in yaml form, then this should suffice. In this example hiera will first lookup the data by first scanning all available yaml files, if it can find what it is after then it will fallback to searching all available json files (if any are available).

Note, if this setting isn't specified, then the default value is "yaml" only.



<h3>:logger setting</h3>

This is a hash table with a single key-value pair, and where the value is a string.  

<pre>
irb(main):028:0* hiera = { 'logger' => 'console' }
=> {"logger"=>"console"}
irb(main):029:0> hiera['logger']
=> "console"
irb(main):030:0>
</pre>

Hiera comes with 3 built-in loggers:

<ul>
	<li>console - here messages go directly to logger.</li>
	<li>puppet - here message go directly to puppet's logging system</li>
	<li>noop - here messages are silenced</li>
</ul>


Note, if this setting isn't specified, then the default value is "console". However puppet overrides this and set's it to puppet. 

<h3>:hierarchy setting</h3>

This is actually <a href="http://www.yaml.org/YAML_for_ruby.html#sequence_in_a_mapping">hash table where the value is in the form of an array</a>. Also this hash table only contains 1 key-value pair, although the array (i.e. the value) contains 4 items.	

<pre>
will add code snippet later.
</pre>

The <a href="https://docs.puppetlabs.com/hiera/1/variables.html#interpolation-tokens">%{...} syntax</a> is used to refer to any <a href="https://docs.puppetlabs.com/hiera/1/variables.html#from-puppet">facter or puppet built-in variables</a> that are available. If non are available then they are treated as null. 

The hierarchy is actually a list of generic filenames, generic in the sense that the names doesn't include the extension, so in our case, for the "defaults" entry it can refer to a defaults.yaml or defaults.json file.   

Since we can dynamically generate these files names. We can create node specific .yaml files, by include the %{hostname} as a possible generic filename. 

We'll cover more about hierarchy in the next post. 




<h3>:yaml, :json, and other backend settings</h3>
In our :backend settings, we enabled the :yaml and :json backend settings.

For each enable backend settings, you need to specify further settings specific to the ones that are enabled. for each backend setting, we need to specify the ":datadir" sub-setting. This setting specifies the folder that contain the specified data files. For example, for :yaml's :datadir setting, it is the path to the folder that contains all the .yaml files. This is also a good place to use the <a href="https://docs.puppetlabs.com/hiera/1/configuring.html#yaml-and-json">environment variable</a>. 



See also
https://github.com/puppetlabs/hiera/blob/master/docs/tutorials/getting_started.md ]]></Content>
		<Date><![CDATA[2015-01-03]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Create a custom resource type (aka Defined Types)]]></Title>
		<Content><![CDATA[Let's say you have serveral wordpress servers, and each server needs to a different set of plugins installed on them.

In this case, you may want to create a class called "required_plugin" which requires a single parameter called "$plugin" and then declare this class multiple times, each time with a different parameter value (aka plugin name). This actually isn't possible with puppet, since you can only declare a class once within each node definition.

However you can achieve this with something called a "defined type" instead. You can declare a defined type multiple times within the same node definition.

Each defined type may ensure a collection of resource, however one thing you need to ensure is that the names of each resource (aka namevar) is unique. Otherwise we will have resources with the same names which will cause errors (even though resource attributes are different). One way to achieve this is to ensure that the resource's names are somehow derived from the input parameters, in order to make them unique.

The syntax for a defined type is nearly the same as a class's syntax.

https://docs.puppetlabs.com/puppet/latest/reference/lang_defined_types.html

https://docs.puppetlabs.com/learning/definedtypes.html

&nbsp;

http://grokbase.com/t/gg/puppet-users/13b6xec1bp/require-a-defined-type-from-another-module

&nbsp;

&nbsp;]]></Content>
		<Date><![CDATA[2015-01-03]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - How Puppet Modules makes use of Hiera]]></Title>
		<Content><![CDATA[<h3>Modules making use of simple string data that has been retrieved from Hiera</h3>
There are 2 syntax styles that you can use to declare a class, <a href="https://docs.puppetlabs.com/puppet/latest/reference/lang_classes.html#include-like-vs-resource-like">include-like, and resource-like</a>. Hence we have:

<pre>
include user_account

# or 

class {'user_account':}
</pre>


During a puppet run, whenever a puppet master encounters a resource declaration for a class, then the puppetmaster will do the following:

<ol>
	<li>checks to see if the class has any class parameters.</li>
	<li>If there are class parameters, If resource-like declaration is used, then check if any class parameters have been defined. </li>
	<li>if no luck, then it requests hiera to find the information</li>
	<li>If hiera returns nil, then it will resort to using the class parameter's default values, if any has been defined in the class definition</li>
	<li>If no defaults have been specified then the puppet master fails and returns an error</li>
</ol>




Let's assume for now that we haven't setup/configured hiera yet. Also let's say that in our site.pp we have:
<pre>
node 'puppetmaster' {
  include user_account
}

</pre>
It's impossible to tell from this whether we need to specify any class parameters for the user_account class, and if so, whether there are any default values pre-defined as part of the class definition.

Hence we need to take a look at the class definition, which is in the module's init.pp file:

<pre>
[root@puppetmaster /]# tree /etc/puppet/modules/user_account/
/etc/puppet/modules/user_account/
├── files
├── lib
├── manifests
│   └── init.pp
├── spec
└── templates

5 directories, 1 file

[root@puppetmaster /]# cat /etc/puppet/modules/user_account/manifests/init.pp
class user_account ($username) {

  user { $username:
    ensure => present,
    shell  => '/bin/bash',
    home   => "/home/$username",
  }

  file {"/tmp/$username.txt":
    ensure => file,
    content => "This machine is called $::hostname \n",     #notice, that we used a facter value here.
  }

}
</pre>
Here we can see that, we do have a class parameter, "$username", which needs to be defined.  However the class doesn't specify a default value for this class parameter. That means that if we do a puppet run as is, then the puppet master won't be able to resolve the class variable and will then fail, like this:
<pre>[root@puppetmaster /]# puppet agent -t
Info: Retrieving pluginfacts
Info: Retrieving plugin
<span style="color: #ff0000;">Error: Could not retrieve catalog from remote server: </span>
<span style="color: #ff0000;">Error 400 on SERVER: Must pass username to Class[User_account] at /etc/puppet/manifests/site.pp:2 on node puppetmaster.codingbee.dyndns.org
Warning: Not using cache on failed catalog
Error: Could not retrieve catalog; skipping run
</span>[root@puppetmaster /]#
</pre>

To fix this, we can define the class parameter in the site.pp file, by using the resource-like class declaration syntax. However that will end up adding a lot more lines of code in our site.pp file. Hence we want to continue to use the "include" syntax instead of the resource-like class syntax. 


Otherwise the puppetmaster by default will automatically ask Hiera for this information, aka <a href="https://docs.puppetlabs.com/hiera/1/puppet.html#automatic-parameter-lookup">automatic parameter lookup</a>. If this also fails, then the puppetmaster resorts to using the default parameter values, if any have been defined in the class definition. If not, then the Puppetmaster throws an error. 

When puppetmaster submits a request to hiera, it provides the parameter's key in the form a of an fqdn. This key hence takes the form of:

<pre>
{class-name}::{parameter-name}
</pre>

Note, if the class parameter is in a sub-class, then we do:

<pre>
{module-name}::{class-name}::{parameter-name}
</pre>
 


The puppetmaster also always tell's hiera the path to the hiera.yaml file it should use, in the form of <a href="https://docs.puppetlabs.com/references/latest/configuration.html#hieraconfig">hiera_config</a> puppet.conf variable, which by default is set to /etc/puppet/hiera.yaml. It does this via hiera's "--config" option:


<pre>
Usage: hiera [options] key [default value] [variable='text'...]

The default value will be used if no value is found for the key. Scope variables
will be interpolated into %{variable} placeholders in the hierarchy and in
returned values.

    -V, --version                    Version information
    -d, --debug                      Show debugging information
    -a, --array                      Return all values as an array
    -h, --hash                       Return all values as a hash
    <strong>-c, --config CONFIG              Configuration file</strong>
    -j, --json SCOPE                 JSON format file to load scope from
    -y, --yaml SCOPE                 YAML format file to load scope from
    -m, --mcollective IDENTITY       Use facts from a node (via mcollective) as scope
    -i, --inventory_service IDENTITY Use facts from a node (via Puppet's inventory service) as scope

</pre> 
  
Therefore, when the puppetmaster automatically submit's a lookup request to hiera (as in the case above), then behind the scenes puppet runs the following hiera command:

<pre>

hiera user_account::username --config /etc/puppet/hiera.yaml

</pre>

This means our yaml file needs to contain a key called "user_account::username". For Hiera, the double colons are treated like any other literal characters, and hence doesn't have any special meaning to hiera. So let's say, our hiera.yaml file points to just one yaml file, which is /etc/hieradata/yaml/global.yaml, and this file contains:

<pre>
---
user_account::username: homer 

</pre>
  
In that case we would end up with:

<pre>
[root@puppetmaster /]# hiera user_account::username --config /etc/puppet/hiera.yaml
homer

</pre>

So if our site.pp file contains:
<pre>
[root@puppetmaster puppet]# cat manifests/site.pp
node 'puppetmaster' {
 class {user_account:}
}
</pre>

Note here that we didn't specify any parameter values, so this will prompt puppetmaster to try a hiera lookup. The same would have happened if we used the include-syntax. 

Then we would get the following during an agent run:

<pre>
[root@puppetmaster puppet]# cat /etc/passwd | grep "homer"
[root@puppetmaster puppet]# puppet agent -t
Info: Retrieving pluginfacts
Info: Retrieving plugin
Info: Caching catalog for puppetmaster.codingbee.dyndns.org
Info: Applying configuration version '1421705732'
Notice: /Stage[main]/User_account/User[homer]/ensure: created
Notice: Finished catalog run in 0.34 seconds
[root@puppetmaster puppet]# cat /etc/passwd | grep "homer"
homer:x:501:504::/home/homer:/bin/bash
[root@puppetmaster puppet]#
</pre>

Success!


<h3>Modules making use of arrays that has been retrieved from Hiera</h3>

Earlier we saw that yaml files can be used to store mapped sequences (aka arrays). In most programming languages, arrays are iterated through loops. However in puppet there is no such thing as "loops" in manifest. However you can pass an array straight into a resource's title.  

For example, lets say our class shows:

<pre>
[root@puppetmaster modules]# tree user_account/
user_account/
├── files
├── lib
├── manifests
│   └── init.pp
├── spec
└── templates
    └── list-of-fruits.erb

5 directories, 2 files
[root@puppetmaster modules]# cat user_account/manifests/init.pp
class user_account ($username) {

  user { $username:
    ensure => present,
    shell  => '/bin/bash',
  }

}

[root@puppetmaster modules]# cat /etc/puppet/manifests/site.pp
node 'puppetmaster' {
 class {user_account:}
}

</pre>

So far nothing special. However the hiera.yaml file points to the following (and only) yaml file:

<pre>
[root@puppetmaster modules]# cat /etc/hieradata/yaml/global.yaml
---

user_account::username:
  - homer
  - marge
  - bart
  - lisa
  - maggie

fruits:
  - apple
  - banana
  - carrot
[root@puppetmaster modules]#

</pre>

As you can see, the user_account::username key is actually storing a array. Now if we run this, we get:

<pre>
[root@puppetmaster user_account]# cat /etc/passwd | grep -E 'homer|marge|bart|lisa|maggie'
[root@puppetmaster user_account]# puppet agent -t
Info: Retrieving pluginfacts
Info: Retrieving plugin
Info: Caching catalog for puppetmaster.codingbee.dyndns.org
Info: Applying configuration version '1421710886'
Notice: /Stage[main]/User_account/User[marge]/ensure: created
Notice: /Stage[main]/User_account/User[lisa]/ensure: created
Notice: /Stage[main]/User_account/User[homer]/ensure: created
Notice: /Stage[main]/User_account/User[maggie]/ensure: created
Notice: /Stage[main]/User_account/User[bart]/ensure: created
Notice: Finished catalog run in 0.43 seconds
[root@puppetmaster user_account]# cat /etc/passwd | grep -E 'homer|marge|bart|lisa|maggie'
marge:x:503:506::/home/marge:/bin/bash
lisa:x:504:504::/home/lisa:/bin/bash
homer:x:505:507::/home/homer:/bin/bash
maggie:x:506:508::/home/maggie:/bin/bash
bart:x:507:509::/home/bart:/bin/bash
[root@puppetmaster user_account]#
</pre> 


That is how arrays are used in manifests. In the above example we created multiple user account, but you can also do the same think with any resource types, e.g. package, service, file....etc. 
















See also:

http://www.craigdunn.org/2011/10/puppet-configuration-variables-and-hiera/

This makes reference to:

class myapplication ( $webname = hiera("webname") ) {
...
}

Which is a handy way to force a class to look up the data from hiera. In my case, I should use:

$username = hiera("user_account::username")]]></Content>
		<Date><![CDATA[2015-01-05]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Using Hiera on it's own]]></Title>
		<Content><![CDATA[To get a good handle on hiera, let's forget about Puppet for the time being and just look at hiera on it's own. 

Hiera is a tool that lets you query data files and returns the data that matches your query. Hiera can only query two types of data files, yaml files and json files. It can potentially search through a bunch of data files to find a match. It will search through these files in a particular order, and will only stop searching once it has found a match, or has run out data files to look in.    

Hiera has it's own config file, called hiera.yaml. This file has 3 main settings, they are ":backend", ":hierarchy", and ":datadir". We'll cover more about this file later. 

Before you can start using hiera, you first need to tell hiera (via the hiera.yaml file):
  
<ol>
	<li>What types of files to search through (i.e. yaml files, or json files, or both). We specify this under the ":backend" setting.</li>
	<li>What are the names of these data files.  We specify this under the :hierarchy setting.</li>
	<li>In what order to look look through these files.  We specify this under the ":hierarchy" setting, (and also in the ":backend" setting which dictates which file type should be scanned first)</li>
	<li>In which directory all the data files are located.  We specify this under the ":datadir" setting.</li>
</ol>

All this information is provided to Hiera via the hiera.yaml file:

<pre>
[root@puppetmaster ~]# rpm -qc hiera
/etc/hiera.yaml
[root@puppetmaster ~]#
</pre>

Let's say the hiera.yaml file contains:

<pre>
[root@puppetmaster yaml]# cat /etc/hiera.yaml
---
:backends:
  - yaml
  - json

:hierarchy:
  - fileA
  - fileB
  - global

:yaml:
  :datadir: /etc/hieradata/yaml

:json:
  :datadir: /etc/hieradata/json
</pre>

In our case, this config file instructs hiera that whenever it receives a lookup request:

<ul>

	<li>It must first search through all yaml files, before looking through all the json. That's because the :backends section lists yaml first and then json.</li>
	<li>It must only scan for yaml files stored in the <code>/etc/hieradata/yaml</code>. That's as specified by the :yaml setting's :datadir sub-setting. </li>
	<li>It must only scan for json files stored in the <code>/etc/hieradata/json</code>. That's as specified by the :json setting's :datadir sub-setting. </li>
	<li>It therefore needs to search through the following files, in the following order until it hit's a match:
	<ol>
	      <li><code>/etc/hieradata/yaml/fileA.yaml</code></li>
              <li><code>/etc/hieradata/yaml/fileB.yaml</code></li>
              <li><code>/etc/hieradata/yaml/global.yaml</code></li>
	      <li><code>/etc/hieradata/json/fileA.json</code></li>
              <li><code>/etc/hieradata/json/fileB.json</code></li>
              <li><code>/etc/hieradata/json/global.json</code></li>              
        </ol>
        The filenames are as defined by the <code>:hierarchy</code> setting. As you can see, this ordering is dictated by the :backend setting, followed by the :hierarchy setting. 
        </li>
</ul>


Now let's create the following yaml file:

 
<pre>
[root@puppetmaster /]# cat /tmp/testfile.yaml
---
dad: homer
[root@puppetmaster /]#

</pre>

Now we want to see if hiera can retrieve the value for the "dad" item, which in this case is "homer". Hence we send a lookup request to hiera like this:

<pre> 
[root@puppetmaster hiera]# hiera dad
nil
[root@puppetmaster hiera]#
</pre>

The word "nil" means that hiera searched the yaml and json files and failed to find a match. That's to be expected since we have placed our yaml file in the wrong directory, and hence hiera ignored it. In fact, we haven't even created the correct directory. Let's now create the directory and move the yaml file into this directory. Then do the lookup request again:

<pre>
[root@puppetmaster /]# mkdir -p /etc/hieradata/yaml
[root@puppetmaster /]# mv /tmp/testfile.yaml /etc/hieradata/yaml/
[root@puppetmaster /]# hiera dad
nil
[root@puppetmaster /]#
</pre>
 
Still no luck, that's because hiera is looking for data files with specific names. Our yaml file's name didn't match any of these, that's why hiera once again ignored our file. To fix this, let's rename our testfile.yaml to a matching name, e.g. global.yaml:


<pre>
[root@puppetmaster /]# mv /etc/hieradata/yaml/testfile.yaml /etc/hieradata/yaml/global.yaml
[root@puppetmaster /]# hiera dad
homer
[root@puppetmaster /]#
</pre>

Success!

Notice that fileA.yaml and fileB.yaml didn't exist. In these instances Hiera would simply ignore these filenames and skip to the next available file. In other words we are seeing the fallback approach in action.  Let's now create a file called FileA.yaml which has the following content:

<pre>
[root@puppetmaster yaml]# pwd
/etc/hieradata/yaml
[root@puppetmaster yaml]# ls -l
total 8
-rw-r--r--. 1 root root 25 Jan  9 21:41 fileA.yaml
-rw-r--r--. 1 root root 15 Jan  7 16:00 global.yaml
[root@puppetmaster yaml]# cat fileA.yaml
---
dad: Homer J Simpson
[root@puppetmaster yaml]# cat global.yaml
---
dad: homer
[root@puppetmaster yaml]#
</pre>

Now if we repeat the hiera lookup for "dad", we get:

<pre>
[root@puppetmaster /]# hiera dad
Homer J Simpson
</pre>

This time, it returned the value from fileA.yaml rather than global.yaml. That's because fileA.yaml is listed higher up the lookup order (i.e. first in the list) than global.yaml (which is third on the list). You can think of this as a "fall back" style to locating data. 


]]></Content>
		<Date><![CDATA[2015-01-06]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Linux - How to build an RPM package]]></Title>
		<Content><![CDATA[To do this, you need to use the <strong>rpmbuild</strong> command.

First you have to install this utility by installing the following package:
<pre>
$ yum install rpmdevtools
</pre>

And maybe also the following too:

<pre>
$ yum install gcc make rpm-build patch
# the following I think are optional:
$ yum pcre-devel openssl-devel rrdtool-devel
</pre>

You might need to do some patching:


<pre>
$ patch -p1 --dry-run < /path/to/patch.patch
$ patch -p1 < /path/to/patch.patch</pre>

For more info about checkout out:

http://www.thegeekstuff.com/2014/12/patch-command-examples/

<pre>
$ yum info patch
Loaded plugins: rhnplugin, security, ulninfo
*Note* Spacewalk repositories are not listed below. You must run this command as root to access Spacewalk repositories.
Installed Packages
Name        : patch
Arch        : x86_64
Version     : 2.6
Release     : 6.el6
Size        : 172 k
Repo        : installed
From repo   : anaconda-OracleLinuxServer-201507280245.x86_64
Summary     : Utility for modifying/upgrading files
URL         : http://www.gnu.org/software/patch/patch.html
License     : GPLv2+
Description : The patch program applies diff files to originals.  The diff command
            : is used to compare an original to a changed file.  Diff lists the
            : changes made to the file.  A person who has the original file can then
            : use the patch command with the diff file to add the changes to their
            : original file (patching the file).
            :
            : Patch should be installed because it is a common way of upgrading
            : applications.

</pre>

Next run rpmdev-setuptree, this will result in the rpmbuild folder being created:
<pre>[root@puppetmaster ~]# ls
anaconda-ks.cfg  install.log  install.log.syslog
[root@puppetmaster ~]# rpmdev-setuptree 
[root@puppetmaster ~]# ls
anaconda-ks.cfg  install.log  install.log.syslog  rpmbuild
[root@puppetmaster ~]# tree rpmbuild/
rpmbuild/
├── BUILD
├── RPMS
├── SOURCES
├── SPECS
└── SRPMS

5 directories, 0 files
[root@puppetmaster ~]# 
</pre>

http://tecadmin.net/create-rpm-of-your-own-script-in-centosredhat/#
http://www.thegeekstuff.com/2015/02/rpm-build-package-example/

https://www.google.co.uk/search?q=rpm+setuptree&ie=utf-8&oe=utf-8&gws_rd=cr&ei=iqy9Vs6LMcve6QTXiLGQBw

http://www.cyberciti.biz/tips/building-a-source-rpm-using-rpmbuild-command.html

https://fedoraproject.org/wiki/How_to_create_an_RPM_package

https://www.google.co.uk/search?q=fpm&amp;ie=utf-8&amp;oe=utf-8&amp;aq=t&amp;rls=org.mozilla:en-GB:official&amp;client=firefox-a&amp;channel=sb&amp;gfe_rd=cr&amp;ei=TksAVYqBJsve-gbMzoCgDg#safe=off&amp;rls=org.mozilla:en-GB:official&amp;channel=sb&amp;q=fpm+rpm

https://www.google.co.uk/search?q=fpm&amp;ie=utf-8&amp;oe=utf-8&amp;aq=t&amp;rls=org.mozilla:en-GB:official&amp;client=firefox-a&amp;channel=sb&amp;gfe_rd=cr&amp;ei=TksAVYqBJsve-gbMzoCgDg#safe=off&amp;rls=org.mozilla:en-GB:official&amp;channel=sb&amp;q=maven+rpm]]></Content>
		<Date><![CDATA[2015-01-06]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Retrieving different data types using Hiera]]></Title>
		<Content><![CDATA[Hiera is a "data lookup" command line tool. This means that you tell Hiera what piece of data you are interested in, and it will retrieve that data for you by looking through one or more yaml files. It can also look up json files, but we'll stick with yaml files, as our mark-up language of choice.

Earlier we saw how to lookup data from a really basic yaml file. However the yaml standard allows for more sophisticated yaml files, yet still remain quite human readable.

Yaml files can hold data in various types, the types we are interested in are:
<ul>
 	<li><a href="http://www.yaml.org/YAML_for_ruby.html#simple_mapping">Simple mapping</a> - In the world of yaml, the word "mapping" means linking something to a key. This "something" can be anything, e.g. a string, or a array. However a "simple mapping" specifically refers to mapping a string to a key. Hence this is how represent a simple string variable in yaml form.</li>
 	<li><a href="www.yaml.org/YAML_for_ruby.html#simple_mapping">Mapped Sequence</a> - In the world of yaml, a "sequence" is a another word for array. So in this case we have a variable that's storing an array. You can also write this in a single line using the <a href="http://www.yaml.org/YAML_for_ruby.html#simple_inline_array">simple inline array</a> syntax</li>
 	<li><a href="http://www.yaml.org/YAML_for_ruby.html#nested_mappings">Nested Mappings</a> - The word "nested" here refers to key that holding one or more other simple mappings. I.e. This is how you create a hash table. You can also write this in a single line by using the <a href="http://www.yaml.org/YAML_for_ruby.html#simple_inline_hash">simple inline hash</a> syntax</li>
 	<li><a href="http://www.yaml.org/YAML_for_ruby.html#boolean">Boolean</a> - these are essentially simple mappings that are storing special reserved strings, which are any of the following: true/false, on/off, yes/no.</li>
 	<li><a href="http://www.yaml.org/YAML_for_ruby.html#integers">Integers</a> and <a href="http://www.yaml.org/YAML_for_ruby.html#floats">floats</a> - These are stored using the same syntax as creating a string variable, aka simple mapping</li>
</ul>
Now, in yaml, you can <a href="http://www.yaml.org/YAML_for_ruby.html#simple_sequence">declare</a> <a href="http://www.yaml.org/YAML_for_ruby.html#nested_sequences">data</a> <a href="http://www.yaml.org/YAML_for_ruby.html#mixed_sequences">without</a> a key, however in the context of puppet, these are meaningless and hence are not used.

Looking up data using hiera on the command line is actually quite limited, because you can only retrieve whole data items, and not a particular part, e.g you can't pick out a particular item in an array.

Hence querying data using the hiera command is mainly used for <a title="Puppet – Hiera troubleshooting" href="http://codingbee.net/tutorials/puppet/puppet-hiera-troubleshooting/">hiera troubleshooting</a>.

Let's say that our hiera.yaml file points to the following yaml file, called global.yaml:
<pre>[root@puppetmaster yaml]# pwd
/etc/hieradata/yaml
[root@puppetmaster yaml]# cat global.yaml
---
# The first line of a yaml file always starts with 3 hyphens, as shown above.

# Here's a simple mappingi, aka a simple string variable. This is analogous to
# creating a variable called "dad" which houses a value of "homer"
dad: homer


# Here, the "::" doesn't have a special meaning. It is simply treated as part of the
# variable's name, i.e. in this case the variable's name is "family_member::son" and
# it holds the value "bart"
family_member::son: bart


# Here's a mapped sequence, aka an array. This is analogous to creating a
# an array called "fruits" which contain 3 array items.
fruits:
 - apple
 - banana
 - carrot

# Here's a slightly differently way to write a mapped sequence.
vegetables: - potatoe
            - carrot
            - spinage

# Here's an example of a nested mapping, aka a hashtable.
stuff:
 fruit: apple
 name: steve
 sport: baseball

[root@puppetmaster yaml]#
</pre>
Now here's how you lookup and retrieve data from the yaml file using hiera on it's own:
<pre>[root@puppetmaster yaml]# hiera dad
homer
[root@puppetmaster yaml]# hiera family_member::son
bart
[root@puppetmaster yaml]# hiera fruits
["apple", "banana", "carrot"]
[root@puppetmaster yaml]# hiera stuff
{"fruit"=&gt;"apple", "sport"=&gt;"baseball", "name"=&gt;"steve"}
[root@puppetmaster yaml]#
</pre>
Note: the output for array and hashtable is actually in the ruby syntax. This means you can make use of these data in ruby code, e.g. in templates. Which we'll cover later.

See also:
https://docs.puppetlabs.com/hiera/1/data_sources.html]]></Content>
		<Date><![CDATA[2015-01-09]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Wordpress - Set up a navigation structure like w3schools and tutorialspoint]]></Title>
		<Content><![CDATA[If you have a collection of posts that you want your viewers to read in a certain sequence, then you need the following navigational features:

<ol>
	<li>You need a navigation menu in the side column, which lists all the posts in the correct order</li>
	<li>The navigation menu should only be visible when a viewer is on a post that is part of the series</li>
	<li>There needs to be a "Previous" and "Next" links at the bottom of each post that allows the viewer to visit adjacent posts in the series.</li>
        <li>The current post that a viewer is reading should appear as bold in the custom menu so that the viewer can where they are in the series</li>
</ol>

A lot of tutorials websites such as w3schools and tizag (as well as this website, codingBee) have all the above features, so that a viewer can follow a series of posts (i.e. tutorials) in sequence. Here's the good news, in Wordpress you can implement the same navigation structure in your website too! There are 2 parts you need set up, the functionality and the cs  styling:

<h2>Setting up the functionality</h2>
<ol>
	<li><strong>Ensure you have side bar enabled</strong> - This is where your post-series's navigation menu will appear.</li>
	<li><strong>Write your Wordpress posts</strong> - if you haven't done already done so.</li>
	<li><strong>Add your posts to a category</strong> - All the posts that are going to be in the same series needs to be assigned to the same category. You can also tag your posts in the usual way.</li>
	<li><strong>Create a new Custom menu</strong> - This menu's name must be the same name as your category's name. </li>
	<li><strong>Add your posts to the custom menu</strong> - Make sure you only add posts that belongs to the said category. Tip: you might need to enable posts visibility under screen options, otherwise you won't be able to add any posts to the custom menu.</li>
	<li><strong>Arrange the order of your posts</strong> - this is just a case of dragging and dropping the menu items within the custom menu. This is part where we are ordering the posts.</li>
	<li><strong>Install the <a href="https://wordpress.org/plugins/display-widgets/">Display Widgets plugin</a></strong> - We'll be using this plugin to ensure that the navigation menu is only visible when the viewer is viewing a post that is listed under the said menu</li>
	<li><strong>Add the custom menu to the side bar</strong> - you do this via Appearance->Widgets. Then drag the custom menu widget into the side bar. Then give the menu any title you like, and then select the menu from the dropdown list</li>
	<li><strong>Select "Show on Checked Pages" from the dropdown list</strong> - This is still part of configuring the widget.</li>
	<li><strong>Place a tick beside the said category's name</strong> - This will now ensure that the menu is only visible when the viewer is browsing the series of posts. Then save your changes.</li>
	<li><strong>Install our plugin which is called <a href="https://wordpress.org/plugins/custom-menu-driven-prevnext-links/">Custom Menu Driven Prev/Next Links plugin</strong></a></li>
	<li><strong>Insert function tag into your theme</strong> - Now comes a tricky part, go to Appearance -> Editor. Here you need to insert: 
[php light="true"]&lt;?php CMD_prev_next('Tutorial'); ?&gt;[/php] 

...into one of the php files (which are listed on the far right). Exactly which file you need to edit depends on your particular WordPress theme. A good starting point could be going to a file called "single.php". Also, you can change the word "Tutorial" to something more appropriate for your website. For example if your website is about cooking, then you might want to change this to something like "Recipe". Then your previous/next links will display as "Previous Recipe" and "Next Recipe" respectively. After you have done this your posts in the series should now start displaying previous/next links that correspond to your new navigation menu. However the styling of the previous next links may look all wrong. We will fix this in the next step.</li>
</ol>


<h2>Setting up the CSS styling</h2>

At this point you should have the previous next links showing up at the bottom of your posts. But at this point, they may not look like how you want them to. You can fix this by adding some css styling. We recommend the following way to add the css styling: 

<ol>

	<li><strong>Install the <a href="https://wordpress.org/plugins/simple-custom-css/">Simple Custom CSS plugin</a></strong> - note if you are using wordpress 4.7+, then I don't think you need to use this plugin, because: <a href="https://wptavern.com/a-preview-of-the-custom-css-editor-added-to-the-customizer-in-wordpress-4-7">https://wptavern.com/a-preview-of-the-custom-css-editor-added-to-the-customizer-in-wordpress-4-7</a></li>

	<li><strong>Apply custom CSS</strong> - Go to <code>Appearance -> Custom CSS</code>. Then write the css code to customise how your Previous/Next links look. The css classes and ids that you need add styles to all have the "cmd" prefix in their name. To get you started, you can try to copy and paste  in the following css code:

<pre>
/* Previous-Next links */

.cmd_post_nav li { 
  list-style: none;
}

.cmd_post_nav .cmd_previous { 
  float:left;
  width: 48%;
}

.cmd_post_nav .cmd_next { 
  float:right;
  width: 48%;
  text-align: right;
}

.cmd_post_nav a {
  font-size: 16px;
  font-weight: 300;
}

.cmd_previous .meta-nav {
  float: left;
  font-size: 44px;
  font-weight: 500;
  line-height: 30px;
  padding-right: 10px;
}

.cmd_next .meta-nav {
  float: right;
  font-size: 44px;
  font-weight: 500;
  line-height: 30px;
  padding-left: 10px;
}

.cmd_right {
    float: right;
    width: 85%;
    word-wrap: break-word;
}

.cmd_left {
    float: left;
    width: 85%;
    word-wrap: break-word;
}

</pre>

</li>

	<li><strong>Apply styling to currently active menu item</strong> - You may also want to add some more css styling to the currently selected item in the custom menu, so to make it standout from the rest. This is helpful for the viewer to see how far along they are in the series. To be do this all you need to do is add the following:

<pre>
/* Current custom menu item styling */
.current-menu-item > a {
   color:red;
   font-weight: bold;
}
</pre>

</li>


</ol>

Hopefully that helps. If you have any problems with implementing this solution, then please <a href="http://codingbee.net/contact/">contact us</a> and for a nominal fee of $15, we will insert the php tag into your theme, and apply a basic css styling to get you started. 
]]></Content>
		<Date><![CDATA[2015-01-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[wordpress|wordpress plugins]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>WordPress]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Working with a dynamic hiera.yaml file]]></Title>
		<Content><![CDATA[One of the really really powerful thing with the hiera is that you can embed variables inside your hiera.yaml file to make it more dynamic. 




<blockquote>
<strong>Note:</strong> If you make any changes to the hiera.yaml file, then always restart the puppetmaster service for the changes to take affect.</blockquote>



For example, let's say you have created a new data-source file called NewData1.yaml and you want to see if hiera can successfully query it. In that case you need to update hiera.yaml to have this file included:


<pre>
 ---
:backends:
  - yaml
:yaml:
  :datadir: /etc/puppet/hieradata/yaml
:hierarchy:
  - NewData1
  - filaA
  - fileB
  - common
</pre>

After that you can run your hiera query like normal. However what if in the coming days there are several more files you want to test, e.g. NewData2.yaml, NewData3.yaml, NewData4.yaml,... and etc. In that case you will be constantly inserting yaml filenames into your hiera.yaml file, and consequently your hiera.yaml file will get longer and longer.   

However an alternative approach is to embed a parameter, that's called something like "CustomFileName" into the hiera.yaml: 


<pre>
 ---
:backends:
  - yaml
:yaml:
  :datadir: /etc/puppet/hieradata
:hierarchy:
  - "%{CustomFileName}"
  - filaA
  - fileB
  - common
</pre>

Now you can resolve this via the hiera commandline, using the <em>variable='test'</em> syntax as suggested in the hiera help info:

<pre>
[root@puppetmaster etc]# hiera --help
Usage: hiera [options] key [default value] [variable='text'...]

The default value will be used if no value is found for the key. Scope variables
will be interpolated into %{variable} placeholders in the hierarchy and in
returned values.

    -V, --version                    Version information
    -d, --debug                      Show debugging information
    -a, --array                      Return all values as an array
    -h, --hash                       Return all values as a hash
    -c, --config CONFIG              Configuration file
    -j, --json SCOPE                 JSON format file to load scope from
    -y, --yaml SCOPE                 YAML format file to load scope from
    -m, --mcollective IDENTITY       Use facts from a node (via mcollective) as scope
    -i, --inventory_service IDENTITY Use facts from a node (via Puppet's inventory service) as scope
</pre>


Let's say I have the following setup:

<pre>
[root@puppetmaster yaml]# cat /etc/puppet/hieradata/yaml/common.yaml
---

username: Jerry
[root@puppetmaster yaml]# cat /etc/puppet/hieradata/yaml/NewData3.yaml
---

username: Tom
[root@puppetmaster yaml]# cat /etc/puppet/hiera.yaml
 ---
:backends:
  - yaml
:yaml:
  :datadir: /etc/puppet/hieradata/yaml
:hierarchy:
  - %{CustomFileName}
  - filaA
  - fileB
  - common

[root@puppetmaster yaml]#

</pre>

Now if we don't pass in the value for CustomFileName, then we get:


<pre>
[root@puppetmaster yaml]# hiera username
Jerry
</pre>

Here, we ended up retrieving the value from common.yaml. 


However, if we now set a value for the filename:

<pre>
[root@puppetmaster yaml]# hiera username CustomFileName="NewData3"
Tom
</pre>

Then this time we retrieve the value from NewData3.yaml, since it is placed higher up the hierarchy from common.yaml, and it contains a match for "username". 


Now the cool thing with puppet is that by default puppet also utilizes this feature to load in facter and puppet builtin data into hiera.   i.e. it passes in:



<pre>
[root@puppetagent1 ~]# facter
architecture => x86_64
augeasversion => 1.0.0
bios_release_date => 12/01/2006
bios_vendor => innotek GmbH
bios_version => VirtualBox
blockdevice_sda_model => VBOX HARDDISK
.
.
...etc
</pre>

as well as:

<pre>
[root@puppetagent1 tmp]# puppet config print all | sort
agent_catalog_run_lockfile = /var/lib/puppet/state/agent_catalog_run.lock
agent_disabled_lockfile = /var/lib/puppet/state/agent_disabled.lock
allow_duplicate_certs = false
allow_variables_with_dashes = false
archive_file_server = puppetmaster.codingbee.dyndns.org
archive_files = false
async_storeconfigs = false
autoflush = true
autosign = /etc/puppet/autosign.conf
basemodulepath = /etc/puppet/modules:/usr/share/puppet/modules
.
.
.
</pre>

This means when puppetmaster runs hiera behind the scenes, it is actually running a really long hiera command, which hundreds of space seperated <em>variable='text'</em> value pairs. One thing to note is that each of these variables that the puppetmaster passes into hiera are not prefixed with "::" but they are prefixed in the hiera.yaml itself, this is to indicate that they are <a href="https://docs.puppetlabs.com/hiera/1/puppet.html#best-practices">top level data</a>. 

By embedding an agent's facter data (and builtin variables) into the hiera.yaml file, it means you can create a yaml data file that is specific to a particular puppet agent. A typical way of doing this is by making use of the agent's "hostname" fact, here's an example of this in action:

First we insert a facter based parameter into our hiera.yaml
<pre>
[root@puppetmaster puppet]# cat /etc/puppet/hiera.yaml
---
:backends:
  - yaml
:hierarchy:
  <strong>- %{::hostname}</strong>
  - global

:yaml:
  :datadir: /etc/hieradata/yaml
[root@puppetmaster puppet]#
</pre>


 
Here we have "%{::hostname}" which is referred to as an <a href="https://docs.puppetlabs.com/hiera/1/variables.html#interpolation-tokens">interpolation token</a>.

Next our site.pp shows:

<pre>
[root@puppetmaster puppet]# cat /etc/puppet/manifests/site.pp
node 'puppetagent1' {
  include user_account
}
</pre>

The user_account class in turn contains:

<pre>
[root@puppetmaster puppet]# cat /etc/puppet/modules/user_account/manifests/init.pp
class user_account ($username = "tempuser"){

  user { "$username":
    ensure => present,
    shell  => '/bin/bash',
  }


  file {"/tmp/$username.txt":
    ensure => file,
    content => $var,
  }

}
[root@puppetmaster puppet]#

</pre>

We don't want the username to default to "tempuser", instead we want to set this to "Tom", which we do by creating the following yaml file

<pre>
[root@puppetmaster puppet]# cat /etc/puppet/hieradata/yaml/puppetagent1.yaml
---

user_account::username: Tom
</pre>

This file is called "puppetagent1" because that's our agent's hostname:


<pre>
[root@puppetagent1 tmp]# facter hostname
puppetagent1

</pre>

Now let's confirm that we can retrieve this value from the command line:


<pre>
[root@puppetmaster puppet]# hiera user_account::username hostname="puppetagent1"
Tom
</pre>

However to eliminate ambiguity on which hiera.yaml file is used you should specify the path to your hiera.yaml file on the command line as well, e.g.:


<pre>
<strong>[root@puppetmaster puppet]# hiera user_account::username --config /etc/puppetlabs/code/hiera.yaml hostname="puppetagent1"
</strong>Tom
</pre>

Also check out the <a href="https://rubygems.org/gems/hiera_explain">hiera_explain</a> gem.

Also check out the new puppet lookup command: 

<a href="https://docs.puppet.com/puppet/latest/lookup_quick.html#if-you-already-use-hiera-in-environments">https://docs.puppet.com/puppet/latest/lookup_quick.html#if-you-already-use-hiera-in-environments</a>

Note: the puppet lookup command is quite new and experimental. 

Now when we do a puppet run, we get:

<pre>
[root@puppetagent1 /]# puppet agent -t
Info: Retrieving pluginfacts
Info: Retrieving plugin
Info: Caching catalog for puppetagent1.codingbee.dyndns.org
Info: Applying configuration version '1424548177'
Notice: /Stage[main]/User_account/User[Tom]/ensure: created
Notice: /Stage[main]/User_account/File[/tmp/Tom.txt]/ensure: created
Notice: Finished catalog run in 0.10 seconds
[root@puppetagent1 /]# cat /etc/passwd | grep "Tom"
Tom:x:505:506::/home/Tom:/bin/bash
[root@puppetagent1 /]# ls -l |
</pre>

Success!!

As you can see here, this class requires a parameter to be fed into it (if the puppetmaster is unable to provide a value, then the class will default to "tempuser"). Since we are declaring the class using the include statement, it means that we are not declaring the class parameter's value in the site.pp file. The Puppetmaster will next use hiera behind the scenes to see if a matching value is stored in any of the yaml files. When puppetmaster submits the hiera requests it also passes in all the facter and puppet builtin data in the form of variable=text pairs, one of which is crucially the facter, hostname. This get's resolved by hiera, and consequently hiera tries looking for a yaml file named after the puppet agent's hostname, which in this case is  "puppetagent1". It successfully finds the yaml file because I have created it and then reads the content of it. Finally it finds a match for "username" and returns this back to the puppetmaster.   


<h2>Embed variables in your datasource</h2>
One final thing to point out is that you can take embed parameters in your actual yaml datasource files, to make them dynamic as well. It is done in exactly the same style as with the hiera.yaml file. 




<h2>Organising your datasource files</h2>

So far we have:


<pre>
[root@puppetmaster puppet]# cat /etc/puppet/hiera.yaml
---
:backends:
  - yaml
:hierarchy:
  <strong>- %{::hostname}</strong>
  - global

:yaml:
  :datadir: /etc/hieradata/yaml
[root@puppetmaster puppet]#
</pre>

In this situation, if you have thousands of puppet agents, then your /etc/hieradata/yaml folder will end up containing potentially thousands of yaml files. 


Luckily another cool trick you can do with your hiera.yaml file is organise your yaml datasource files into sub folders. This is done by specifying relative folder structures in your hiera.yaml file's hierarchy section. For example you can do:

<pre>
[root@puppetmaster puppet]# cat /etc/puppet/hiera.yaml
---
:backends:
  - yaml
:hierarchy:
  - %{::environment}/%{::hostname}
  - %{::environment}/global

:yaml:
  :datadir: /etc/hieradata/yaml
[root@puppetmaster puppet]#
</pre>

Here we used puppet's builtin variable called "environment" which is used for setting up <a href="https://docs.puppetlabs.com/puppet/latest/reference/environments.html#assigning-nodes-to-environments">directory environments</a>.

For more info, see:

<a href="https://docs.puppet.com/hiera/latest/configuring.html" rel="nofollow">https://docs.puppet.com/hiera/latest/configuring.html</a>


<a href="https://docs.puppet.com/puppet/latest/lookup_quick.html">https://docs.puppet.com/puppet/latest/lookup_quick.html</a>
]]></Content>
		<Date><![CDATA[2015-01-13]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Hiera troubleshooting]]></Title>
		<Content><![CDATA[There will be times when Hiera just isn't returning the information that you are expecting from it. In these situations you need to do a bit of digging. The first place to start is looking at hiera's help info:


<pre>

[root@puppetmaster yaml]# hiera --help
Usage: hiera [options] key [default value] [variable='text'...]

The default value will be used if no value is found for the key. Scope variables
will be interpolated into %{variable} placeholders in the hierarchy and in
returned values.

    -V, --version                    Version information
<strong>    -d, --debug                      Show debugging information</strong>
    -a, --array                      Return all values as an array
    -h, --hash                       Return all values as a hash
    -c, --config CONFIG              Configuration file
    -j, --json SCOPE                 JSON format file to load scope from
    -y, --yaml SCOPE                 YAML format file to load scope from
    -m, --mcollective IDENTITY       Use facts from a node (via mcollective) as scope
    -i, --inventory_service IDENTITY Use facts from a node (via Puppet's inventory service) as scope
[root@puppetmaster yaml]#
</pre>

Note: using the "debug" is really helpful. 

Also check out the <a href="https://rubygems.org/gems/hiera_explain">hiera_explain</a> gem:



Also here's a way to do hiera lookup straight from the command line:


<pre>
PUPPET_GEM_VERSION=3.7.5 bundle exec puppet apply --parser=future -e "notice(hiera('foo', undef))"
</pre>
However this doesn't output the value of anything. but it confirms that it found something 


https://tickets.puppetlabs.com/browse/PUP-3863


See also:

puppetlabs.com/blog/debugging-hiera

http://puppetlabs.com/blog-tags/hiera

http://www.craigdunn.org/2011/10/puppet-configuration-variables-and-hiera/ (which shows how to force puppet to only use hiera data)]]></Content>
		<Date><![CDATA[2015-01-14]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - What is bundler]]></Title>
		<Content><![CDATA[http://bundler.io/

http://bundler.io/v1.7/man/bundle.1.html]]></Content>
		<Date><![CDATA[2015-01-16]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Installing RVM]]></Title>
		<Content><![CDATA[First off you should avoid installing RVM as the root user. Instead, you should install it as a normal user. However at some points while using rvm, you may need to elevate your privilege using sudo (or better yet, rvmsudo). Because of this it is best that you first login as root user, and run "visudo" then add the following line just below the "root" line:

<pre>
$ usermod -a -G wheel {usernam}
</pre>

Note: the "wheel" group is a special group that comes in most Linux distros. All members of the "wheel" group automatically gets root privileges. You might need to restart your bash terminal after this for changes to take affect. 

After that return to your user and continue with the rest of the steps. 

<a href="https://rvm.io/rvm/basics">RVM Basics</a>

Here is the main <a href="https://rvm.io/rvm">RVM</a> documentation. 

http://stackoverflow.com/questions/5758276/how-do-i-install-ruby-gems-when-using-rvm

Info about <a href="http://unix.stackexchange.com/questions/47434/what-is-the-difference-between-curl-and-wget">curl</a>.

http://stackoverflow.com/questions/4604064/rubygems-bundler-and-rvm-confusion

http://curl.haxx.se/docs/manpage.html

http://stackoverflow.com/questions/9394338/how-do-rvm-and-rbenv-actually-work/9422296#9422296

Note it is recommended to <a href="https://rvm.io/rvm/install#installation-explained">install rvm as non-root user</a>. 

<pre>
gpg --keyserver hkp://keys.gnupg.net --recv-keys D39DC0E3
</pre>

The output from this is:


<pre>
[sher@puppetmaster ~]$ gpg --keyserver hkp://keys.gnupg.net --recv-keys D39DC0E3
gpg: requesting key D39DC0E3 from hkp server keys.gnupg.net
gpg: key D39DC0E3: public key "Michal Papis (RVM signing) <mpapis@gmail.com>" imported
gpg: no ultimately trusted keys found
gpg: Total number processed: 1
gpg:               imported: 1  (RSA: 1)
[sher@puppetmaster ~]$
</pre>


Then run:

<pre>
curl -sSL https://get.rvm.io | bash -s stable
</pre>

Note: you might need to install curl via yum before you can run the above command. 

Let's breakdown what this command is doing by first looking at the curl part. In this case, the curl command basically downloads <a href="https://get.rvm.io">this shell script</a>. Which is also available on github as is called the <a href="https://github.com/wayneeseguin/rvm/blob/master/binscripts/rvm-installer">rvm-installer</a>.  

Here's what the curl options means:

L: this stands for (L)ocation 
s: This means run in (s)ilent mode. This is optional and it's best to omit this so that you can see what's going on. 
S: This means (S)how errors. This is optional, but it is best to leave this in so that you can see any errors/warnings. 

If you review the script, you'll find that it requires a positional parameter, one of the values that this parameter can take is "stable", which according to the shell script will install the latest version of rvm. 

Since curl just retrieves the script, you can easily just pipe it to a file. 

However in the above case we have piped it to bash to run it. We also instructed bash (via <a href="http://superuser.com/questions/519882/seeking-to-upgrade-my-bash-magic-help-decipher-this-command-bash-s-stable">the bash -s option</a>) to pass the first parameter as "stable".  


Therefore when you run this, you get:

<pre>
[sher@puppetmaster ~]$ curl -SL https://get.rvm.io | bash -s stable
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 22817  100 22817    0     0  18550      0  0:00:01  0:00:01 --:--:--  293k
Downloading https://github.com/wayneeseguin/rvm/archive/master.tar.gz

Installing RVM to /home/sher/.rvm/
    Adding rvm PATH line to /home/sher/.profile /home/sher/.mkshrc /home/sher/.bashrc /home/sher/.zshrc.
    Adding rvm loading line to /home/sher/.profile /home/sher/.bash_profile /home/sher/.zlogin.
Installation of RVM in /home/sher/.rvm/ is almost complete:

  * To start using RVM you need to run `source /home/sher/.rvm/scripts/rvm`
    in all your open shell windows, in rare cases you need to reopen all shell windows.

# sher,
#
#   Thank you for using RVM!
#   We sincerely hope that RVM helps to make your life easier and more enjoyable!!!
#
# ~Wayne, Michal & team.

In case of problems: http://rvm.io/help and https://twitter.com/rvm_io

  * WARNING: You have '~/.profile' file, you might want to load it,
    to do that add the following line to '/home/sher/.bash_profile':

      source ~/.profile

[sher@puppetmaster ~]$

</pre>

Then follow the advise:

<pre>

[sher@puppetmaster ~]$ source ~/.profile

</pre>

Notice that running the above command resulted in the rvm getting installed under the user's home directory, in a folder called ".rvm":

<pre>
[sher@puppetmaster ~]$ pwd
/home/sher
[sher@puppetmaster ~]$ which rvm
~/.rvm/bin/rvm
[sher@puppetmaster ~]$
</pre>



Now to confirm that rvm is now installed and working, you can now use the rvm command:

<pre>
[sher@puppetmaster ~]$ rvm --version
rvm 1.26.9 (master) by Wayne E. Seguin <wayneeseguin@gmail.com>, Michal Papis <mpapis@gmail.com> [https://rvm.io/]
[sher@puppetmaster ~]$
</pre>

However at this point nothing else has changed i.e. no new gems have been installed (gem list), the gem environment is unchanged (gem environment), and ruby version is unchanged (ruby --version)



You can also access the help info:


<pre>
rvm help
</pre>

Now, rvm may require other packages to be installed (using yum along with rvmsudo). To check this, run <code>rvm requirements</code>:


<pre>
[sher@puppetmaster ~]$ rvm requirements
Checking requirements for centos.
Installing requirements for centos.
Installing required packages: libyaml-devel, autoconf, gcc-c++, readline-devel, zlib-devel, libffi-devel, openssl-devel, automake, libtool, bison, sqlite-devel..sher password required for 'yum install -y libyaml-devel autoconf gcc-c++ readline-devel zlib-devel libffi-devel openssl-devel automake libtool bison sqlite-devel':

</pre>

Here you'll get a password prompt. In which case exit this and run the above yum command (using rvmsudo). after that rerun <code>rvm requirements</code> to confirm everything is in place. 

However at this point, still nothing else has changed i.e. no new gems have been installed (gem list), the gem environment is unchanged (gem environment), and ruby version is unchanged (ruby --version)

<pre>

</pre>

https://rvm.io/rubies/rubygems (do not use sudo)

See also:

http://www.pluralsight.com/courses/building-linux-server-for-ruby-on-rails

https://www.digitalocean.com/community/tutorials/how-to-install-ruby-on-rails-on-centos-6-with-rvm

https://www.digitalocean.com/community/tutorials/how-to-install-ruby-2-1-0-on-centos-6-5-using-rvm]]></Content>
		<Date><![CDATA[2015-01-16]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Gems]]></Title>
		<Content><![CDATA[<pre>
[vagrant@puppetmaster /]$ gem --help
RubyGems is a sophisticated package manager for Ruby.  This is a
basic help message containing pointers to more information.

  Usage:
    gem -h/--help
    gem -v/--version
    gem command [arguments...] [options...]

  Examples:
    gem install rake
    gem list --local
    gem build package.gemspec
    gem help install

  Further help:
    gem help commands            list all 'gem' commands
    gem help examples            show some examples of usage
    gem help gem_dependencies    gem dependencies file guide
    gem help platforms           gem platforms guide
    gem help <COMMAND>           show help on COMMAND
                                   (e.g. 'gem help install')
    gem server                   present a web page at
                                 http://localhost:8808/
                                 with info about installed gems
  Further information:
    http://guides.rubygems.org
[vagrant@puppetmaster /]$ 

</pre>

Also see:

<pre>
[vagrant@puppetmaster pod_env]$ gem environment
RubyGems Environment:
  - RUBYGEMS VERSION: 2.4.4
  - RUBY VERSION: 1.9.3 (2014-11-13 patchlevel 551) [x86_64-linux]
  - INSTALLATION DIRECTORY: /usr/local/rvm/gems/ruby-1.9.3-p551
  - RUBY EXECUTABLE: /usr/local/rvm/rubies/ruby-1.9.3-p551/bin/ruby
  - EXECUTABLE DIRECTORY: /usr/local/rvm/gems/ruby-1.9.3-p551/bin
  - SPEC CACHE DIRECTORY: /home/vagrant/.gem/specs
  - SYSTEM CONFIGURATION DIRECTORY: /usr/local/rvm/rubies/ruby-1.9.3-p551/etc
  - RUBYGEMS PLATFORMS:
    - ruby
    - x86_64-linux
  - GEM PATHS:
     - /usr/local/rvm/gems/ruby-1.9.3-p551
     - /usr/local/rvm/gems/ruby-1.9.3-p551@global
  - GEM CONFIGURATION:
     - :update_sources => true
     - :verbose => true
     - :backtrace => false
     - :bulk_threshold => 1000
  - REMOTE SOURCES:
     - https://rubygems.org/
  - SHELL PATH:
     - /usr/local/rvm/gems/ruby-1.9.3-p551/bin
     - /usr/local/rvm/gems/ruby-1.9.3-p551@global/bin
     - /usr/local/rvm/rubies/ruby-1.9.3-p551/bin
     - /usr/lib64/qt-3.3/bin
     - /usr/local/bin
     - /usr/bin
     - /bin
     - /usr/local/sbin
     - /usr/sbin
     - /sbin
     - /usr/local/rvm/bin
     - /home/vagrant/bin
[vagrant@puppetmaster pod_env]$ 

</pre>

If you want to see where all the gems are, cd into the "INSTALLATION DIRECTORY" or "GEM PATHS" (as shown above). Then you can view all gems there. 


http://www.pluralsight.com/courses/building-linux-server-for-ruby-on-rails

http://guides.rubygems.org/what-is-a-gem/

http://guides.rubygems.org/

http://guides.rubygems.org/rubygems-basics/

Gems is basically the ruby equivalent of the linux yum command. 
]]></Content>
		<Date><![CDATA[2015-01-16]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Hiera lookup functions]]></Title>
		<Content><![CDATA[By default, puppet only submits a hiera query when it is trying to find a value for a class parameter. OUtside of that, hiera doesn't get used by puppet. As for your manifests, you will mainly be using data available from facter and puppet built-in variables. 


However if there is certain data that you want to use in your manifests, but are stored in yaml files. Then you can instruct puppet to perform a hiera lookup using a number of functions:

<ul>
	<li><a href="https://docs.puppetlabs.com/references/latest/function.html#hiera">hiera</a></li>
	<li><a href="https://docs.puppetlabs.com/references/latest/function.html#hieraarray">hiera_array</a></li>
	<li><a href="https://docs.puppetlabs.com/references/latest/function.html#hierahash">hiera_hash</a></li>
	
</ul>

 
There is a fourth a hiera related function, which is called "<a href="https://docs.puppetlabs.com/references/latest/function.html#hierainclude">hiera_include</a>". <a href="https://docs.puppetlabs.com/hiera/1/puppet.html#assigning-classes-to-nodes-with-hiera-hierainclude">Hiera include can assign classes to nodes</a>.  Will cover more about that later. 

https://docs.puppetlabs.com/hiera/1/puppet.html#hiera-lookup-functions]]></Content>
		<Date><![CDATA[2015-01-19]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Assigning a class to a node using Hiera]]></Title>
		<Content><![CDATA[Once you have moved all your class parameter definitions out of site.pp and into hiera, all that will left will be your node definitions, and these node definitions will just contain a list of include statements for various modules.


All the the other data will mostly be stored in your various yaml files which are most likely named after your agent's hostname, e.g. hostname.yaml. 

However for each hostname.yaml file you can add in an array which lists all your classes, e.g.:

<pre>
[root@puppetmaster puppet]# cat /etc/puppet/hieradata/yaml/puppetagent1.yaml
---

classes:
  - user_account

user_account::username: Tom
[root@puppetmaster puppet]#
</pre>

 
After that in your site.pp file, you can use the hiera_include function which will redirect the puppetmaster to seek the list of classes (to be included) from hiera. Therefore in our site.pp file we will now end up with just the following:

<pre>
[root@puppetmaster puppet]# cat /etc/puppet/manifests/site.pp
node default {
  hiera_include('classes')
}
[root@puppetmaster puppet]#
</pre>

In this example we chose the lookup key to be "classes", which is best practice, but not mandatory and can be changed to something else. 


Now if we do a puppet run: 
 
<pre>
[root@puppetagent1 /]# puppet agent -t
Info: Retrieving pluginfacts
Info: Retrieving plugin
Info: Caching catalog for puppetagent1.codingbee.dyndns.org
Info: Applying configuration version '1424555958'
Notice: /Stage[main]/User_account/User[Tom]/ensure: created
Notice: /Stage[main]/User_account/File[/tmp/Tom.txt]/ensure: created
Notice: Finished catalog run in 0.27 seconds
[root@puppetagent1 /]# cat /etc/passwd | grep "Tom"
Tom:x:505:506::/home/Tom:/bin/bash
[root@puppetagent1 /]# ls -l /tmp/Tom.txt
-rw-r--r--. 1 root root 0 Feb 21 21:59 /tmp/Tom.txt
[root@puppetagent1 /]# puppet agent -t
</pre>


The benefit of this approach is that we have turn into hiera into a form of an External Node Classifier (ENC) via the use of the hiera_include function. However there are a lot of alternative ENCs which we'll cover next. 




See also:

 
https://docs.puppetlabs.com/hiera/1/puppet.html#best-practices

https://docs.puppetlabs.com/hiera/1/complete_example.html#assigning-a-class-to-a-node-with-hiera]]></Content>
		<Date><![CDATA[2015-01-19]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Generate files from templates using Hiera data]]></Title>
		<Content><![CDATA[Note: The approach I am about to cover is not best practice, for more info see: <a href="https://docs.puppet.com/hiera/latest/puppet.html#dont-use-the-lookup-functions-from-templates" target="_blank" rel="nofollow">https://docs.puppet.com/hiera/latest/puppet.html#dont-use-the-lookup-functions-from-templates</a> 



You can use hiera data to populate the placeholders in your template files. For 

For example if your class looks like this:

<pre>
# your class contains:
[root@puppetmaster user_account]# cat manifests/init.pp
class user_account ($username) {

  user { $username:
    ensure => present,
    shell  => '/bin/bash',
  }

  file {"/tmp/$username.txt":
    ensure => file,
    content => template("user_account/list-of-fruits.erb"),
  }

}

# your template contains:
[root@puppetmaster user_account]# cat templates/list-of-fruits.erb
<%-# Here is a simple ruby for-loop -%>
<%- for fruit in scope.function_hiera(["fruits"]) -%>
  A <%= fruit %> is a fruit.
<%- end -%>


# your global yaml file contains:
[root@puppetmaster user_account]# cat /etc/hieradata/yaml/global.yaml
---
user_account::username: homer

fruits:
  - pineapple
  - banana
  - carrot


</pre>

First notice that the fruits key holds an array. Also notice the hiera lookup syntax we used inside the erb file. When we want to call a function from inside an erb file, we have to always have to prefix the function name with "scope.function_" and the key has to be encased in double/single quotes followed by square brackets, and finally round brackets. 

When we then run this, we get:

<pre>
[root@puppetmaster user_account]# cat /tmp/homer.txt
cat: /tmp/homer.txt: No such file or directory
[root@puppetmaster user_account]# puppet agent -t
Info: Retrieving pluginfacts
Info: Retrieving plugin
Info: Caching catalog for puppetmaster.codingbee.dyndns.org
Info: Applying configuration version '1421712095'
Notice: /Stage[main]/User_account/File[/tmp/homer.txt]/ensure: defined content as '{md5}0e2aaf2b34725bde9da190a23655960c'
Notice: Finished catalog run in 0.04 seconds
[root@puppetmaster user_account]# cat /tmp/homer.txt
  A pineapple is a fruit.
  A banana is a fruit.
  A carrot is a fruit.
[root@puppetmaster user_account]#
</pre>

However calling hiera lookups within erb files is not considered good practice. 

See also:
https://docs.puppetlabs.com/hiera/1/puppet.html#using-the-lookup-functions-from-templates]]></Content>
		<Date><![CDATA[2015-01-19]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Environment Variable]]></Title>
		<Content><![CDATA[You can read all the environment variables via the irb using the "ENV" command:


<pre>
irb(main):231:0* puts ENV.sort
COLUMNS
188
HISTCONTROL
ignoredups
HISTSIZE
1000
HOME
/root
HOSTNAME
puppetmaster.local
LANG
en_GB.UTF-8
.
.
...etc
</pre>


Here the keys are in capital and the values are in string. 

Alternatively we could use "pretty print" (pp) which comes with the Ruby standard library:


<pre>
$ irb
irb(main):001:0> pp ENV.sort
NoMethodError: undefined method `pp' for main:Object
        from (irb):1
        from /usr/bin/irb:12:in `<main>'
</pre>


This didn't work because pp needs be required first:

<pre>
irb(main):006:0* require 'pp'
=> true
irb(main):007:0> pp ENV.sort
[["COLUMNS", "188"],
 ["HISTCONTROL", "ignoredups"],
 ["HISTSIZE", "1000"],
 ["HOME", "/root"],
 ["HOSTNAME", "puppetmaster.local"],
 ["LANG", "en_GB.UTF-8"],
 ["LESSOPEN", "||/usr/bin/lesspipe.sh %s"],
 ["LINES", "58"],
 ["LOGNAME", "root"],
.
.
...etc.
</pre>

Another option is to exit out of IRB and then install a gem called "awesome_print" and then use that tool instead:


<pre>
$ gem install awesome_print
Fetching: awesome_print-1.6.1.gem (100%)
Successfully installed awesome_print-1.6.1
Parsing documentation for awesome_print-1.6.1
Installing ri documentation for awesome_print-1.6.1
1 gem installed
$
$
$
irb(main):080:0* awesome_print ENV
{
               "XDG_SESSION_ID" => "2637",
                     "HOSTNAME" => "puppetmaster.local",
       "SELINUX_ROLE_REQUESTED" => "",
                         "TERM" => "xterm",
                        "SHELL" => "/bin/bash",
                     "HISTSIZE" => "1000",
                   "SSH_CLIENT" => "192.168.50.1 57615 22",
    "SELINUX_USE_CURRENT_RANGE" => "",
                      "SSH_TTY" => "/dev/pts/3",
                         "USER" => "root",
.
.
.
}
=> nil
</pre>

You can also create environment variables using the following syntax:

<pre>
ENV['key'] = 'value'
</pre>

For example:

<pre>
[root@puppetmaster pod_env]# irb
1.9.3-p551 :002 > require 'awesome_print'
 => true 
1.9.3-p551 :003 > awesome_print ENV
{
          "rvm_bin_path" => "/usr/local/rvm/bin",
              "HOSTNAME" => "puppetmaster.ordsvy.gov.uk",
              "GEM_HOME" => "/usr/local/rvm/gems/ruby-1.9.3-p551",
                 "SHELL" => "/bin/bash",
                  "TERM" => "xterm",
              "HISTSIZE" => "1000",
                 "IRBRC" => "/usr/local/rvm/rubies/ruby-1.9.3-p551/.irbrc",
                 "QTDIR" => "/usr/lib64/qt-3.3",
                "OLDPWD" => "/git_source/puppet/environments/dev_virtualbox_v2/modules",
          "MY_RUBY_HOME" => "/usr/local/rvm/rubies/ruby-1.9.3-p551",
                 "QTINC" => "/usr/lib64/qt-3.3/include",
                  "USER" => "root",
             "LS_COLORS" => "rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=01;05;37;41:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arj=01;31:*.taz=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lz=01;31:*.xz=01;31:*.bz2=01;31:*.tbz=01;31:*.tbz2=01;31:*.bz=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.rar=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.jpg=01;35:*.jpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.axv=01;35:*.anx=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=01;36:*.au=01;36:*.flac=01;36:*.mid=01;36:*.midi=01;36:*.mka=01;36:*.mp3=01;36:*.mpc=01;36:*.ogg=01;36:*.ra=01;36:*.wav=01;36:*.axa=01;36:*.oga=01;36:*.spx=01;36:*.xspf=01;36:",
          "_system_type" => "Linux",
              "rvm_path" => "/usr/local/rvm",
            "rvm_prefix" => "/usr/local",
                  "MAIL" => "/var/spool/mail/root",
                  "PATH" => "/usr/local/rvm/gems/ruby-1.9.3-p551/bin:/usr/local/rvm/gems/ruby-1.9.3-p551@global/bin:/usr/local/rvm/rubies/ruby-1.9.3-p551/bin:/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/usr/local/rvm/bin:/root/bin",
                   "PWD" => "/git_source/puppet/environments/dev_virtualbox_v2/modules/pod_env",
                  "LANG" => "en_US.UTF-8",
          "_system_arch" => "x86_64",
       "_system_version" => "6",
           "SSH_ASKPASS" => "/usr/libexec/openssh/gnome-ssh-askpass",
           "HISTCONTROL" => "ignoredups",
           "rvm_version" => "1.26.3 (latest)",
                 "SHLVL" => "1",
                  "HOME" => "/root",
               "LOGNAME" => "root",
                 "QTLIB" => "/usr/lib64/qt-3.3/lib",
               "CVS_RSH" => "ssh",
              "GEM_PATH" => "/usr/local/rvm/gems/ruby-1.9.3-p551:/usr/local/rvm/gems/ruby-1.9.3-p551@global",
              "LESSOPEN" => "|/usr/bin/lesspipe.sh %s",
               "DISPLAY" => ":0.0",
          "RUBY_VERSION" => "ruby-1.9.3-p551",
          "_system_name" => "RedHat",
    "G_BROKEN_FILENAMES" => "1",
            "XAUTHORITY" => "/root/.xauthZ90kSk",
                     "_" => "/usr/local/rvm/rubies/ruby-1.9.3-p551/bin/irb",
                 "LINES" => "60",
               "COLUMNS" => "237"
}
 => nil 
1.9.3-p551 :004 > puts ENV['EDITOR'] 

 => nil 
1.9.3-p551 :005 > ENV['EDITOR'] = 'gedit' 
 => "gedit" 
1.9.3-p551 :006 > puts ENV['EDITOR'] 
gedit
 => nil 
1.9.3-p551 :007 > 
1.9.3-p551 :007 > awesome_print ENV
{
          "rvm_bin_path" => "/usr/local/rvm/bin",
              "HOSTNAME" => "puppetmaster.ordsvy.gov.uk",
              "GEM_HOME" => "/usr/local/rvm/gems/ruby-1.9.3-p551",
                 "SHELL" => "/bin/bash",
                  "TERM" => "xterm",
              "HISTSIZE" => "1000",
                 "IRBRC" => "/usr/local/rvm/rubies/ruby-1.9.3-p551/.irbrc",
                 "QTDIR" => "/usr/lib64/qt-3.3",
                "OLDPWD" => "/git_source/puppet/environments/dev_virtualbox_v2/modules",
          "MY_RUBY_HOME" => "/usr/local/rvm/rubies/ruby-1.9.3-p551",
                 "QTINC" => "/usr/lib64/qt-3.3/include",
                  "USER" => "root",
             "LS_COLORS" => "rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=01;05;37;41:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arj=01;31:*.taz=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lz=01;31:*.xz=01;31:*.bz2=01;31:*.tbz=01;31:*.tbz2=01;31:*.bz=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.rar=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.jpg=01;35:*.jpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.axv=01;35:*.anx=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=01;36:*.au=01;36:*.flac=01;36:*.mid=01;36:*.midi=01;36:*.mka=01;36:*.mp3=01;36:*.mpc=01;36:*.ogg=01;36:*.ra=01;36:*.wav=01;36:*.axa=01;36:*.oga=01;36:*.spx=01;36:*.xspf=01;36:",
          "_system_type" => "Linux",
              "rvm_path" => "/usr/local/rvm",
            "rvm_prefix" => "/usr/local",
                  "MAIL" => "/var/spool/mail/root",
                  "PATH" => "/usr/local/rvm/gems/ruby-1.9.3-p551/bin:/usr/local/rvm/gems/ruby-1.9.3-p551@global/bin:/usr/local/rvm/rubies/ruby-1.9.3-p551/bin:/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/usr/local/rvm/bin:/root/bin",
                   "PWD" => "/git_source/puppet/environments/dev_virtualbox_v2/modules/pod_env",
                  "LANG" => "en_US.UTF-8",
          "_system_arch" => "x86_64",
       "_system_version" => "6",
           "SSH_ASKPASS" => "/usr/libexec/openssh/gnome-ssh-askpass",
           "HISTCONTROL" => "ignoredups",
           "rvm_version" => "1.26.3 (latest)",
                 "SHLVL" => "1",
                  "HOME" => "/root",
               "LOGNAME" => "root",
                 "QTLIB" => "/usr/lib64/qt-3.3/lib",
               "CVS_RSH" => "ssh",
              "GEM_PATH" => "/usr/local/rvm/gems/ruby-1.9.3-p551:/usr/local/rvm/gems/ruby-1.9.3-p551@global",
              "LESSOPEN" => "|/usr/bin/lesspipe.sh %s",
               "DISPLAY" => ":0.0",
          "RUBY_VERSION" => "ruby-1.9.3-p551",
          "_system_name" => "RedHat",
    "G_BROKEN_FILENAMES" => "1",
            "XAUTHORITY" => "/root/.xauthZ90kSk",
                     "_" => "/usr/local/rvm/rubies/ruby-1.9.3-p551/bin/irb",
                 "LINES" => "60",
               "COLUMNS" => "237",
                "EDITOR" => "gedit"
}
 => nil 
1.9.3-p551 :008 > 
</pre>

Note, here that this environment variable only lasts for as long as the irb session. As soon as you restart irb, it disappears again. ]]></Content>
		<Date><![CDATA[2015-01-20]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - GemFile]]></Title>
		<Content><![CDATA[Ruby gemfile

the ":require => false" basically means that isn't auto required. 


this means in your rb files, you have to have lots require statements to load in the gem's utility before you can use it. 


You can also include if-else-conditions in your gem file. This gives you two ways to pass variables to be resolved in your gemfile, the first way is by setting the environment variable:




<pre>
[root@puppetmaster demo_module]# bundle exec gem list | grep "^puppet "
puppet (3.4.3)
[root@puppetmaster demo_module]# cat Gemfile
source ENV['GEM_SOURCE'] || "https://rubygems.org"
group :development, :test do
  gem 'rake',                    :require => false
  gem 'rspec-puppet',            :require => false
  gem 'puppetlabs_spec_helper',  :require => false
end
if puppetversion = ENV['PUPPET_GEM_VERSION']           # here is the if statement. 
  gem 'puppet', puppetversion, :require => false
else
  gem 'puppet', '~> 3.4.2', :require => false
end
[root@puppetmaster demo_module]# export PUPPET_GEM_VERSION="3.7.2"
[root@puppetmaster demo_module]# env | grep "PUPPET_GEM_VERSION"
PUPPET_GEM_VERSION=3.7.2
[root@puppetmaster demo_module]# bundle install
Don't run Bundler as root. Bundler can ask for sudo if it is needed, and installing your bundle as root will break this application for all non-root users on
this machine.
Fetching gem metadata from https://rubygems.org/.........
Resolving dependencies...
Using rake 10.4.2
Using CFPropertyList 2.3.0
Using addressable 2.3.6
Using archive-tar-minitar 0.5.2
.
.
.
Installing puppet 3.7.2 (was 3.4.3)
.
.

Using bundler 1.7.7
Your bundle is complete!
Use `bundle show [gemname]` to see where a bundled gem is installed.
[root@puppetmaster demo_module]# bundle exec gem list | grep "puppet "
puppet (3.7.2)
rspec-puppet (1.0.1)
[root@puppetmaster demo_module]# 

</pre>

This effectively gives up a dynamic gemfile. 


Now if we unset this variable and try again, we get:

<pre>
[root@puppetmaster demo_module]# unset PUPPET_GEM_VERSION
[root@puppetmaster demo_module]# bundle exec gem list | grep "puppet "
puppet (3.4.3)
rspec-puppet (1.0.1)
[root@puppetmaster demo_module]# 
</pre>

In the above example, we created a linux system environment variable, which got resolved inside the Gemfile, however it can also be Jenkins environment variable (i.e. build step parameter) and it will still work the same way. 

The second approach which essentially temporarily sets up the environment variable while the command is running. This means your "env" command doesn't get cluttered up with lots of settings. Here's how this is done:

<pre>

[vagrant@puppetmaster demo_module]$ PUPPET_GEM_VERSION="3.7.3" bundle install

</pre>
Notice that we didn't need to do any piping. 


This should also work with jenkins as well, I think. 

Here's an example of a <a href="https://github.com/puppetlabs/puppetlabs-ntp/blob/master/Gemfile">dynamic gemfile</a>. 


Here's another handy technique:

<pre>
source ENV['GEM_SOURCE'] || "https://rubygems.org"

group :development, :test do
  gem 'puppetlabs_spec_helper', '1.0.1',  :require => false
  gem 'rspec-puppet',           '2.3.0',  :require => false
  gem 'rspec-core',             '3.1.7',  :require => false <mark>if RUBY_VERSION =~ /^1.8/</mark>
  gem 'puppet-lint-strict_indent-check',  :require => false
  gem 'metadata-json-lint',               :require => false
  gem 'rspec-puppet-facts',               :require => false
end

if puppetversion = ENV['PUPPET_GEM_VERSION']
  gem 'puppet', puppetversion, :require => false
else
  gem 'puppet', '~> 3.4', :require => false
end

# vim:ft=ruby

</pre>

installing rspec-puppet, will automatically install the latest version of rspec-core. However if you want to use a particular version of rspec-core when running ruby 1.8.x, then you include the above line along with the if condition. 


]]></Content>
		<Date><![CDATA[2015-01-20]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Hiera lookup Behaviours]]></Title>
		<Content><![CDATA[In this tutorial we'll carry on looking at hiera as a standalone tool, and still ignore Puppet.

The default behaviour of hiera when it performs a lookup is to stop searching as soon as at is finds a match and returns that given match. 

However you can also instruct Hiera to search all the yaml files and collate all matches into an array (using option -a) or a hash (using option -h). 

<pre>
[root@puppetmaster ~]# hiera --help
Usage: hiera [options] key [default value] [variable='text'...]

The default value will be used if no value is found for the key. Scope variables
will be interpolated into %{variable} placeholders in the hierarchy and in
returned values.

    -V, --version                    Version information
    -d, --debug                      Show debugging information
    -a, --array                      Return all values as an array
    -h, --hash                       Return all values as a hash
    -c, --config CONFIG              Configuration file
    -j, --json SCOPE                 JSON format file to load scope from
    -y, --yaml SCOPE                 YAML format file to load scope from
    -m, --mcollective IDENTITY       Use facts from a node (via mcollective) as scope
    -i, --inventory_service IDENTITY Use facts from a node (via Puppet's inventory service) as scope
[root@puppetmaster ~]#

</pre>

Let's now look at these 2 behaviours in action.

First off, lets lay out the ground work. Here's  what's in our hiera.yaml file and the corresponding yaml data files:

<pre>
[root@puppetmaster /]# cat /etc/hiera.yaml
---
:backends:
  - yaml

:hierarchy:
  - first
  - second
  - third
  - global

:yaml:
  :datadir: /etc/hieradata/yaml
[root@puppetmaster /]# cd /etc/hieradata/yaml/
[root@puppetmaster yaml]# ###############################
[root@puppetmaster yaml]# cat first.yaml
---

# Here's a simple mapping, aka string variable
username: homer

# Heres a nested mapping, aka hashtable
stuff:
  fruit:     pineapple
  furniture: chair
  sport:     tennis
[root@puppetmaster yaml]# ##############################
[root@puppetmaster yaml]# cat second.yaml
---

# Here's a simple mapping, aka string variable
username: marge
[root@puppetmaster yaml]# ##############################
[root@puppetmaster yaml]# cat third.yaml
---

# Here's a simple mapping, aka string variable
username: bart

# Heres a nested mapping, aka hashtable
stuff:
  vegetable: pineapple
  element:   hydrogen
  music:     tennis
[root@puppetmaster yaml]#

</pre>
   



<h3>Hiera array lookup (aka hiera_array)</h3>

Now let's try option "a":
<pre>
[root@puppetmaster yaml]# hiera -a username
["homer", "marge", "bart"]
</pre>

This behaviour only works when you are collating a collection of simple mappings, aka string variables. Hence it would give an error message if any of the username keys contained a nested mapping (aka hash table). 


<h3>Hiera array lookup (aka hiera_array)</h3>
Now let's try option "h":

<pre>
[root@puppetmaster yaml]# hiera -h stuff
{"vegetable"=>"pineapple",
 "sport"=>"tennis",
 "furniture"=>"chair",
 "fruit"=>"pineapple",
 "element"=>"hydrogen",
 "music"=>"tennis"}
[root@puppetmaster yaml]#
</pre>

This only works if all the matching keys contains a nested mapping (aka hashtable). ]]></Content>
		<Date><![CDATA[2015-01-20]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Vagrant -  Create your own Vagrant box]]></Title>
		<Content><![CDATA[http://docs-v1.vagrantup.com/v1/docs/base_boxes.html]]></Content>
		<Date><![CDATA[2015-01-22]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[vagrant|VirtualBox]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Vagrant]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - What is RSpec]]></Title>
		<Content><![CDATA[<a href="http://rspec.info/">RSpec is a BDD unit testing framework</a> that is designed for testing ruby code. 

When we run a RSpec test scripts against our puppet code we are working on, the following takes place:

<ol>
	<li>RSpec temporarily generates it's own site.pp file, and then instructs the puppetmaster to use make use of that site.pp file rather than the normal site.pp file. </li>

	<li> RSpec also generates the modules folder which houses the main puppet module (that is to be tested) along with any puppet modules that it depends on. It then instructs the puppetmaster to use the new modules folder rather than the old one</li>

	<li>RSpec masquarades as a puppet agent and submits a catalog request to the master.</li>
	<li>The puppetmaster then requests facter data from the puppet agent (ie, rspec)</li>
       <li>The puppetmaster then generates the catalog and sends it to the puppet agent (i.e., RSpec)</li>
	<li>After RSpec retrieves the catalog, it then analyses it and checks whether it contains what it should contain. </li>
	<li>After that RSpec deletes the catalog, get the puppetmaster to point back to the original site.pp and modules folder again. Then, finally, it reports back the tests results. </li>

</ol>


As you can see from the above flow, the catalog never actually get's applied to any agents. Hence in RSpec, we actually do "dry runs". This means that rspec testing are much faster to run, compared to other types of tests, e.g. acceptance testing. 







RSpec’s normal behaviour is to instantiate an “object” and check whether it holds the correct property values. This is only suitable if you are developing something in the context of the Ruby (object-oriented-programming) language, where a "class" is something that you can instantiate an object from.  


Hence, Rspec straight out of the box, is not natively designed to test puppet code. That's because Puppet code is written using <a href="https://docs.puppetlabs.com/puppet/latest/reference/lang_summary.html">Puppet's own Domain Specific Language (DSL)</a>, rather than Ruby. The DSL has it's own concepts such as puppet resources, puppet classes, puppet modules....etc. 

However the good news is that there are a few gems available that can bridge this gap between rspec and puppet, and allow us to use rspec to test puppet code, the main gems are:

<ul>
	<li><a href="https://rubygems.org/gems/rspec-puppet/">rspec-puppet</a> - introduces the rspec syntax for writing puppet rspec tests. </li>
	<li><a href="https://rubygems.org/gems/puppetlabs_spec_helper">puppetlabs_spec_helper</a> - provides a mechanism to run the rspec tests from the command line. </li>
</ul>

We'll cover installation of these gems in the next lesson. 

RSpec itself, like all ruby related tools, is available in the form of the<a href="https://rubygems.org/gems/rspec">RSpec ruby gem</a>, which makes it really easy to install.

These gems in turn are dependant on other gems which are also installed as part of install rspec e.g.:

<pre>

[root@puppetmaster demo_module]# gem dependency puppetlabs_spec_helper
Gem puppetlabs_spec_helper-0.8.2
  mocha (>= 0)
  puppet-lint (>= 0)
  puppet-syntax (>= 0)
  rake (>= 0)
  rspec (>= 0)
  rspec-puppet (>= 0)

[root@puppetmaster demo_module]# 

</pre>

We'll cover how to install these gems in the next lesson. 

]]></Content>
		<Date><![CDATA[2015-01-22]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>rspec-puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Setting up RSpec]]></Title>
		<Content><![CDATA[<h2>Install RVM and Bundler</h2>
RSpec tests needs to be run in a tightly controlled environment. This will help ensure that the RSpec tests that you write/run in on your machine will also run on other people's machine too. The environment has to be controlled in the in the following context:


<ul>
	<li>RSpec test needs to be run under a known version of Ruby. The exact version is not that important, as long as we know which version our tests are contained in. The RVM gem is specifically designed for this purpose.</li>
	<li>We need to contain the exact gems (along with gem versions) that the Rspec tests has access to. The Bundler gem is specifically designed to meet this purpose.</li>
	<li>The Rspec tests needs to be executed by a non-root user. This will help to keep the rspec tests contained. The RVM and Bundler gem also requires you to run in non-root mode in order for them to properly work</li>
</ul>

Here are 2 gems that needs to be installed by the root user, at the machine level:

<ul>
	<li>rvm</li>
	<li>bundler</li>
</ul>



Both of these gems are installed at the user's machine level, i.e. from the command line we simply run:

<pre>
gem install rvm
</pre>
and:
<pre>
gem install bundler
</pre>

We have separate tutorials covering for configuring both RVM and Bundler. 

Using RVM and bundler, is not essential but it introduces a lot of versatility in managing your environment more effectively. 


All the other gems that we'll use (including the main RSpec gem) will be used enabled temporarily in order to run the rspec tests, and then disabled again.  



<h2>Making gems available to your puppet project</h2>
As mentioned earlier, RSpec is not designed to natively support testing puppet classes, resources, modules...etc. RSpec's normal behaviour is to instantiate an "object" and check whether it holds the correct property values.  This is only suitable if you are developing something in the context of the Ruby (object-oriented-programming) language. 

However there are a few gems that can be utilised to bridge this gap and allow us to use RSpec for puppet testing to great effect.

The 3 main gems you need are:

<ul>
	<li><a href="https://rubygems.org/gems/rspec-puppet/">rspec-puppet</a></li>
	<li><a href="https://rubygems.org/gems/puppetlabs_spec_helper">puppetlabs_spec_helper</a></li>
	<li><a href="https://rubygems.org/gems/puppet">puppet</a></li>

</ul>

Note, there are lot of other essential gems that we also need, e.g. rake, and rspec itself. However these gems are depenancies to the above gems so you don't need to explicitly cater for them, and they'll still get loaded in. 

This time you don't install these gems in the way you did for bundler and rvm, earlier. Instead you make these gems available to your puppet project, via the Gemfile, which we'll come to shortly.  

<h2>Prepare your Puppet module</h2>

A lot of these gems that we'll be using will require config files set up control how they work. 

The approach we're going to take is to create these config files but leave them all blank initially. Then try to run an rspec "hello world" test. This will result in various error messages showing up, and we will then fix these error messages by adding configuration data to our config files, one by one, until we eliminate all the error message and eventually get a success message. The reason I'm taking this approach is so to help you understand what the various error messages means which in turn will shed light on what config data needs to be added to the config files in order to resolve the error messages, and that consequently make you understand the roles the various config files play, and how they all fit into the bigger picture.  


then you can quickly create one by running the following command while you are in your module's folder:

<pre>puppet module generate</pre>

In our demos, we have created a new module called "demo-module"


A lot of the gems we mentioned so far have dependencies on certain config files/folders existing within your puppet module. If they are not present, then you need to create them yourself. The four config files you need in your puppet module are:

<pre>

[root@puppetmaster demo_module]# tree
.
├── .fixtures.yml           # this file (which is a hidden file)
├── Gemfile                 # this file 
├── Gemfile.lock
├── manifests
│   └── init.pp
├── Modulefile
├── Rakefile                # this file 
├── README
├── spec
│   └── spec_helper.rb      # this file 
└── tests
    └── init.pp

11 directories, 13 files
[root@puppetmaster demo_module]# 


</pre>
Note: There is a top folder called "tests" but actually doesn't house any rspec tests. Instead it is just used for <a href="https://docs.puppetlabs.com/learning/modules1.html#the-other-subdirectories">example code</a>.

Let's now take a look at each of these config files in turn:

<h3>Gemfile</h3>
This config file is used by the bundler to determine which gems (as well as which version) will be made availabe to Rspec. You then make these gems available by first downloading these gems:

<pre>
bundle install      
</pre>

The gems you download are likely to have dependencies on other gems, and hence all the dependant gems are downloaded and made available as well. 


You then force Rspec to only access these gems by prefixing your rspec-execution-command command with, "bundle exec", i.e.:

<pre>
bundle exec {rspec-execution-command}
</pre>
  

As a minimum, you can add the following into your Gemfile:

<pre>
[vagrant@puppetmaster rspecdemo]$ cat Gemfile
source 'https://rubygems.org'

gem 'rspec-puppet', :require => false
gem 'puppetlabs_spec_helper',  :require => false
gem 'puppet',  :require => false
[vagrant@puppetmaster rspecdemo]$ 
</pre>


<h4>A bit more background info about Gemfile</h4>
This section gives some more background info that you can read up on after you have managed to successfully pass your first rspec test. If you haven't passed your first test, then skip this section and come back to it later. 


However most of the official puppetlab modules such as <a href="https://github.com/puppetlabs/puppetlabs-ntp/blob/master/Gemfile">Puppetlab's ntp module</a> has something like this:


<pre>
$ cat Gemfile
source 'https://rubygems.org'
group :development, :test do
gem 'rake', :require => false
gem 'rspec-puppet', :require => false
gem 'puppetlabs_spec_helper', :require => false
gem 'rspec-system', :require => false
gem 'rspec-system-puppet', :require => false
gem 'rspec-system-serverspec', :require => false
gem 'serverspec', :require => false
gem 'puppet-lint', :require => false
end
if puppetversion = ENV['PUPPET_GEM_VERSION']
  gem 'puppet', puppetversion, :require => false
else
  gem 'puppet', :require => false
end
# vim:ft=ruby
</pre>

Notice the if-else statement. This makes it possible to use a particular gem version we want to use, by simply creating an linux environment variable called "PUPPET_GEM_VERSION" before we start the puppet run. E.g. from the command line we first do:

<pre>
$ export PUPPET_GEM_VERSION="3.7.2" 
$ env | grep "PUPPET_GEM_VERSION"
PUPPET_GEM_VERSION=3.7.2
</pre>

Inclusion of these kinds of if-else statements in the gemfile effectively means that we have more <a href="http://codingbee.net/tutorials/ruby/ruby-gemfile/" title="Ruby – GemFile">dynamic gemfile</a> which can be used in more versatile ways.  Here's an example of a <a href="https://github.com/puppetlabs/puppetlabs-ntp/blob/master/Gemfile">dynamic gemfile</a>. 

For instance can also set this value via a continuous integration tool such as Jenkins, in which case we do this is in the form of build parameters.  

Another cool alternative to CI is called Travis. This tool can accept a .travis.yml which can list multiple combinations of parameter sets, and each git commit, can result in multiple job runs, one for each parameter combo. This is a good way to test your puppet module across several combinations of gem versions and ruby versions (via rvm)

However there is a jenkins plugin that can mimic this travis feature. 

We will cover more about about automating puppet unit testing via Jenkins, later. 



<h3>Rakefile</h3>

RSpec internally runs a predefined set of tasks when trigger a test run. While these tasks work fine for a Ruby based project, they won't work for a Puppet based project. 

Therefore to bridge this gap between Rspec and puppet, we provide our custom set of tasks for Rspec to follow instead. We provide this task via the "rake" gem, which is used for task management. Hence rake reads the rakefile to see what custom sets of tasks are available for use. we can then force rspec to use our custom (puppet compatible) task rather than the default task-set. 

You then force Rspec to only run our custom (puppet compatible) tasks by prefixing your rspec-execution-command command with, "rake", i.e.:


<pre>
rake {rspec-execution-task}
</pre>

Note: "rspec-execution-task" are a series of tasks that we will make available to rake, a little further down.  

However to also need to take into account our predefined gems (i.e. those listed in the gemfile), therefore we need to prefix our rake command with "bundle exec":

<pre>
bundle exec rake {rspec-execution-task}
</pre>

However before we can run the above command we first need to tell rake where to locate the puppet-rspec specific rake tasks. Luckily puppetlabs have already created a set of predefined rake task, which it has packaged into the "puppetlabs_spec_helper" gem. Therefore all we need to do is tell rake to look inside the puppetlabs_spec_helper gem for the rake tasks. We tell this to rake by adding the following lines in the rakefile: 

<pre>
[vagrant@puppetmaster rspecdemo]$ cat rakefile 
require 'rubygems'
require 'puppetlabs_spec_helper/rake_tasks'
[vagrant@puppetmaster rspecdemo]$ 
</pre>  

Rake should now have access to rspec-puppet specific rake tasks (as well as a few other tasks), you can check this by simply running the the following:

<pre>
[vagrant@puppetmaster rspecdemo]$ bundle exec rake
rake beaker            # Run beaker acceptance tests
rake beaker_nodes      # List available beaker nodesets
rake build             # Build puppet module package
rake clean             # Clean a built module package
rake coverage          # Generate code coverage information
rake help              # Display the list of available rake tasks
rake lint              # Check puppet manifests with puppet-lint
rake spec              # Run spec tests in a clean fixtures directory
rake spec_clean        # Clean up the fixtures directory
rake spec_prep         # Create the fixtures directory
rake spec_standalone   # Run spec tests on an existing fixtures directory
rake syntax            # Syntax check Puppet manifests and templates
rake syntax:hiera      # Syntax check Hiera config files
rake syntax:manifests  # Syntax check Puppet manifests
rake syntax:templates  # Syntax check Puppet templates
rake validate          # Check syntax of Ruby files and call :syntax
[vagrant@puppetmaster rspecdemo]$ 
</pre>

Since we didn't following up the rake command, with a task name, it automatically listed out all the available tasks, that have been made available thanks to the puppetlabs_spec_helper gem. 




<h3>.fixtures.yaml</h3>
This file lists all the other puppet modules that your module is dependant on. During a rspec test run, these modules gets download to your modules spec/fixtures/modules folder. The puppetmaster then use this folder as a temporary pseudo modules folder (rather than using /etc/puppet/modules folder) in order to generate the catalog.

The fixtures file also tell's rspec the location of the puppet module that's under test. 

As a side-note, If you don't create the fixtures.yaml file, and just run the rspec test anyway, you'll get an error message taht looks something like this:

<pre>
[vagrant@puppetmaster rspecdemo]$ bundle exec rake spec
/usr/local/rvm/rubies/ruby-1.9.3-p551/bin/ruby -S rspec spec/classes/init_spec.rb --color
F

Failures:

  1) rspecdemo The following classes should be present in the catalog should compile into a catalogue without dependency cycles
     Failure/Error: it { should compile }                # this is the test to check if it compiles.
       error during compilation: <strong>Could not find class rspecdemo for puppetmaster.ordsvy.gov.uk on node puppetmaster.ordsvy.gov.uk</strong>
     # ./spec/classes/init_spec.rb:6:in `block (3 levels) in <top (required)>'

Finished in 0.03425 seconds
1 example, 1 failure

Failed examples:

rspec ./spec/classes/init_spec.rb:6 # rspecdemo The following classes should be present in the catalog should compile into a catalogue without dependency cycles
/usr/local/rvm/rubies/ruby-1.9.3-p551/bin/ruby -S rspec spec/classes/init_spec.rb --color failed
[vagrant@puppetmaster rspecdemo]$ 
</pre> 
Note, in my case my module is called "rspecdemo"


Now as minimum, you can add the following into your fixtures.yml:

<pre>
[vagrant@puppetmaster rspecdemo]$ cat .fixtures.yml 
fixtures:
 forge: "https//forge.puppetlabs.com"
 symlinks:
   rspecdemo: "#{source_dir}"
[vagrant@puppetmaster rspecdemo]$ 
</pre>

However if your module has dependencies to particular version of other puppet modules, then you need to specify these dependencies here too. Here are some <a href="https://github.com/puppetlabs/puppetlabs_spec_helper#fixtures-examples">sample .fixtures.yml files</a> to help understand how to do this. 

    

<h3>spec_helper.rb</h3>



This is a rspec specific config file. This is used to adjust any of the rspec core settings. What we need to do here is to make the core rspec software aware of the rspec-puppet specific syntax. Luckily there is some code in the puppetlabs_spec_helper module that bridges that gap for us. Therefore at a minimum the only think we need to add this file is:  

<pre>
require 'puppetlabs_spec_helper/module_spec_helper'
</pre>


<h3>Create folders</h3>
Under the spec folder you also have to create the following folders to keep all your rspec tests organised. 
This is accordance to the <a href="https://github.com/rodjek/rspec-puppet#naming-conventions">puppet-rspec convention</a>. Therefore in our case we create the following folders:

 
<pre>
[vagrant@puppetmaster demo_module]$ cd spec/
[vagrant@puppetmaster spec]$ mkdir classes defines functions hosts
[vagrant@puppetmaster spec]$ cd ..
[vagrant@puppetmaster demo_module]$ tree
.
├── manifests
│   └── init.pp
├── metadata.json
├── Rakefile
├── README.markdown
├── spec
│   ├── classes          # new folder
│   ├── defines          # new folder
│   ├── functions        # new folder
│   ├── hosts            # new folder
│   ├── spec_helper.rb
│   └── spec.opts
└── tests
    └── init.pp
7 directories, 7 files
[vagrant@puppetmaster demo_module]$
</pre>


We just have to create the folders for the time being since the rb files are the actual rspec tests, which we'll be writing in the next tuturial. 





<h2>Verify setup</h2>

Do:

<pre>
bundle install    # verifies that bundler is installed and gemfile is working. 

bundle exec gem list   # also verifies that bundler is installed and gemfile is working. 

rake help       # verifies that puppet compatible rake tasks are available

bundle exec rake spec     # this should pass, because no tests written yet, hence nothing to fail
</pre>


If the above preliminary checks all passes, then you can then go ahead and start writing puppet tests, which we'll do in the next lesson.  








See also:

http://rspec-puppet.com/setup/

http://terrarum.net/blog/puppet-testing-part-1.html


https://rubygems.org/gems/puppetlabs_spec_helper

http://blog.nicksieger.com/articles/2007/01/02/customizing-rspec/ (gives info about the spec_helper.rb file)

]]></Content>
		<Date><![CDATA[2015-01-22]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>rspec-puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Testing Puppet Modules using RSpec]]></Title>
		<Content><![CDATA[RSpec is all about unit testing. That means we test the class without the need for changing the state of any puppet agents, i.e. it's a bit like a dry run. When we run an rspec tests, all that happens is that the rspec test provides a set of dummy (facter/puppet-variables/hiera) data to the puppetmaster via the "let" statement. The puppetmaster then generates the catalog using these inputs and then it sends it to RSpec instead of puppet-agent. RSpec then reviews the catalog to check that it contains what it "should" contain. 

If you use this approach then you can nearly write your entire puppet module without needing to run the module against a puppet agent, which is best to do much later on as part of <a href="http://codingbee.net/tutorials/puppet/puppet-testing-puppet-modules-using-beaker/" title="Puppet – Testing Puppet Modules using Beaker">acceptance testing using Beaker</a>. 


<h2>RSpec test writing approach</h2>

We will take the following (BDD) approach to writing rspec tests:

<ol>
	<li>write the rspec code</li>
	<li>Fail the Rspec test</li>
	<li>Update the puppet code</li>
	<li>Re-run the rspec test (to confirm that it now passes)</li>
</ol>

If step 4 fails, then we need to go back to step 1 or 3 to try to fix the issue. That's because a failing test could be caused by issues with the rspec or code. 

This is also the BDD approach to software development, i.e. you don't write your whole puppet module first, and then write your rspec tests as an afterthought. Instead you write your rspec code first and then follow it up with your puppet code. We will follow this process in the upcoming walkthroughs.


<h2>Preparing the puppet module for Rspec testing</h2>
The best way to explain how to write rspec tests is by walking through a series of Rspec tests in action. In order to do this, we'll first create a dummy puppet module called "demo_module".

<pre>
puppet module generate codingbee-demo_module</pre>
 

If you have done everything as expected, then your module should look something like this:

<pre>
[vagrant@puppetmaster demo_module]$ tree
.
├── manifests
│   └── init.pp
├── metadata.json
├── Rakefile
├── README.markdown
├── spec                    # This is the folder that will house all rspec tests.
│   ├── spec_helper.rb
│   └── spec.opts
└── tests                   # you can ignore this folder and it's content.
    └── init.pp
3 directories, 7 files
[vagrant@puppetmaster user_account]$
</pre>
 

Note: There is a top folder called "tests" but it actually doesn't house any rspec  tests. Instead it is just used for example code.

 

Now the first thing you need to set up the folder tree structure under spec, according to the <a href="https://github.com/rodjek/rspec-puppet">puppet-rspec folder convention</a>. We just have to create the folders for the time being since the .rb files are the actual rspec tests, which we'll be writing later on. 

 

Therefore in our case we create the following folders:

<pre> 
[vagrant@puppetmaster user_account]$ cd spec/
[vagrant@puppetmaster spec]$ mkdir classes defines functions hosts
[vagrant@puppetmaster spec]$ cd ..
[vagrant@puppetmaster user_account]$ tree
.
├── manifests
│   └── init.pp
├── metadata.json
├── Rakefile
├── README.markdown
├── spec
│   ├── classes          # new folder
│   ├── defines          # new folder
│   ├── functions        # new folder
│   ├── hosts            # new folder
│   ├── spec_helper.rb
│   └── spec.opts
└── tests
    └── init.pp
7 directories, 7 files
[vagrant@puppetmaster user_account]$
</pre>
 

At this point we are now ready to write our first rspec test!


<h2>Learning from examples</h2>

The best way to learn how to write rspec tests is by walking through a series of RSpec examples, and introduce things rspec concepts and syntax along the way. In order to do this, we’ll first create a dummy puppet module called “demo_module”.


<strong>Test 01:</strong> It should compile
<strong>Test 02:</strong> Check for main class
<strong>Test 03:</strong> Check for subclass
<strong>Test XX:</strong> Check for subclass in another folder
<strong>Test 04:</strong> Check the number of classes
<strong>Test 05:</strong> Check for a resource
<strong>Test 06:</strong> Check the number of resources
<strong>Test 07:</strong> Check for resource attributes 
<strong>Test 08:</strong> Check for class parameters
 
 

We will be using the same demo_module throughout, that means that as we progress through the above test cases, our rspec and puppet code bases will get progressively bigger. However as long as we stick to the rspec convention, everything should remain manageable/maintainable. 







<h3>Test 01: It should compile</h3>
This is the <a href="http://rspec-puppet.com/matchers/">most basic test</a> you can run. Essentially it checks whether the puppet master can process all the manifests (.pp files) and at the very minimum, generate a catalog, irrespective of what catalog's content is. 

Let's say the init.pp file's content is:
 

<pre>
[root@puppetmaster demo_module]# cat spec/classes/init_spec.rb
require 'spec_helper'                
describe 'demo_module' do
  context 'The following classes should be present in the catalog' do
    
    it { should compile }                # this is the test to check if it compiles. 
  
  end
 
end
[root@puppetmaster demo_module]# 
</pre>

We've introduced a few new things here, lets break all this down:

<ul>
	<li><code>require 'spec_helper'</code> - In order to use rspec to test puppet code, all your rspec tests (i.e. "_spec.rb" files ) needs have this line at the very top</li>
	<li><code>it { .... }</code> - This is an actual low level thing that we want to test. In Rspe, this it-block is also referred to as an "example" of something specific we want to test.</li>
	<li><code>context 'a description' do...end</code> - this is used to group together set of related tests, aka it-blocks, to help keep things organised. It also provides an human friendly description. </li>
</ul>



now let's say our init.pp file shows:

<pre>
[root@puppetmaster demo_module]# cat manifests/init.pp
class demo_module {

  xxxx

}
[root@puppetmaster demo_module]# 
</pre>

Note: For the purpose of this demo we added "xxx" in order to purposely make the test fail. 

<pre>
[root@puppetmaster demo_module]# bundle exec rake spec
/usr/local/rvm/rubies/ruby-1.9.3-p551/bin/ruby -S rspec spec/classes/init_spec.rb --color
F

Failures:

  1) demo_module The following classes should be present in the catalog 
     Failure/Error: should compile
     Puppet::Error:
       Syntax error at '}' at /git_source/puppet/environments/dev_virtualbox_v2/modules/demo_module/spec/fixtures/modules/demo_module/manifests/init.pp:5 on node puppetmaster.ordsvy.gov.uk
     # (eval):3:in `_racc_yyparse_c'
     # (eval):3:in `yyparse'
     # ./spec/classes/init_spec.rb:5:in `block (3 levels) in <top (required)>'

Finished in 0.03787 seconds
1 example, 1 failure

Failed examples:

rspec ./spec/classes/init_spec.rb:4 # demo_module The following classes should be present in the catalog 
/usr/local/rvm/rubies/ruby-1.9.3-p551/bin/ruby -S rspec spec/classes/init_spec.rb --color failed
[root@puppetmaster demo_module]# 

</pre>

Now if we remove the "xxx" typo from init.pp and then rerun the test, we get:

<pre>
[root@puppetmaster demo_module]# bundle exec rake spec
/usr/local/rvm/rubies/ruby-1.9.3-p551/bin/ruby -S rspec spec/classes/init_spec.rb --color
.

Finished in 0.0702 seconds
1 example, 0 failures
[root@puppetmaster demo_module]# 
</pre>
 









<h3>Test 02: - Check for the presence of the main class</h3>

The main class, is the class that's defined in the module's init.pp file. Hence in this case we want to test for the presence of the "demo_module" class.  

Now since our manifest is called "init.pp" and we are testing for the existance of a class, then by following <a href="https://github.com/rodjek/rspec-puppet#naming-conventions">rspec-puppet's naming convention</a>, we need to name and place our rspec test....:


<pre>
[sher@puppetmaster demo_module]$ tree
.
├── Gemfile
├── manifests
│   └── init.pp
├── metadata.json
├── Rakefile
├── README.md
├── spec
│   ├── classes
│   │   └── init_spec.rb     # Here
│   ├── defines
│   ├── functions
│   ├── hosts
│   └── spec_helper.rb
└── tests
    └── init.pp

7 directories, 8 files
[sher@puppetmaster demo_module]$

</pre>


We have now added the following test:


<pre>
[root@puppetmaster demo_module]# cat spec/classes/init_spec.rb
require 'spec_helper'       
describe 'demo_module' do

  context 'The catalog should at the very least compile' do
    it { 
         should compile
    }
  end

  context 'The main class should be present in the catalog' do
    it { 
         should contain_class('demo_module') 
    }
  end
 
end
[root@puppetmaster demo_module]# 


</pre>

For the purpose of this demo, lets add the following "_xxx" typo error in our init.pp file:

<pre>
[root@puppetmaster demo_module]# cat manifests/init.pp
class demo_module_xxx {

}
[root@puppetmaster demo_module]# 

</pre>


now we run the test:


<pre>

[root@puppetmaster demo_module]# bundle exec rake spec
/usr/local/rvm/rubies/ruby-1.9.3-p551/bin/ruby -S rspec spec/classes/init_spec.rb --color
FF

Failures:

  1) demo_module The catalog should at the very least compile 
     Failure/Error: should compile
     Puppet::Error:
       Could not find class demo_module for puppetmaster.ordsvy.gov.uk on node puppetmaster.ordsvy.gov.uk
     # ./spec/classes/init_spec.rb:6:in `block (3 levels) in <top (required)>'

  2) demo_module The main class should be present in the catalog 
     Failure/Error: should contain_class('demo_module')
     Puppet::Error:
       Could not find class demo_module for puppetmaster.ordsvy.gov.uk on node puppetmaster.ordsvy.gov.uk
     # ./spec/classes/init_spec.rb:12:in `block (3 levels) in <top (required)>'

Finished in 0.07075 seconds
2 examples, 2 failures

Failed examples:

rspec ./spec/classes/init_spec.rb:5 # demo_module The catalog should at the very least compile 
rspec ./spec/classes/init_spec.rb:11 # demo_module The main class should be present in the catalog 
/usr/local/rvm/rubies/ruby-1.9.3-p551/bin/ruby -S rspec spec/classes/init_spec.rb --color failed
[root@puppetmaster demo_module]# 

</pre>

As you can we the puppet master can't even compile the catalog let alone test whether the main class is included in the catalog. That's because we introduced a fundamental error, which is that the main class name is not mirroring the modules name. If we remove the typo and rerun the test, we get:


<pre>

[root@puppetmaster demo_module]# bundle exec rake spec
/usr/local/rvm/rubies/ruby-1.9.3-p551/bin/ruby -S rspec spec/classes/init_spec.rb --color
..

Finished in 0.07467 seconds
2 examples, 0 failures
[root@puppetmaster demo_module]# 

</pre>



<h3>Test 03: Check presence of a subclass</h3>

Let's now say we want to create a new class called "subclass" which should also be present in the class. Before we write this new class, let's write the rspec test for it first, and confirm that it fails.

hence we create the new subclass_rspec.rb rspec testsript:
<pre>
[vagrant@puppetmaster demo_module]$ touch spec/classes/subclass_spec.rb
[vagrant@puppetmaster demo_module]$ tree
.
├── Gemfile
├── Gemfile.lock
├── manifests
│   └── init.pp
├── Modulefile
├── Rakefile
├── README
├── spec
│   ├── classes
│   │   ├── init_spec.rb
│   │   └── subclass_spec.rb         # Here's the new test script
│   ├── defines
│   ├── fixtures
│   │   ├── manifests
│   │   └── modules
│   ├── functions
│   ├── hosts
│   └── spec_helper.rb
└── tests
    └── init.pp
10 directories, 11 files
[vagrant@puppetmaster demo_module]$
</pre>

 

Where the content of the subclass_spec.rb script is:

 
<pre>[vagrant@puppetmaster demo_module]$ cat spec/classes/subclass_spec.rb
require 'spec_helper' 
describe 'demo_module::subclass' do                                    # notice we always use the fqdn
  context 'The following subclass should be in the catalog' do
    it {
         should contain_class('demo_module::subclass')                 # notice we always use the fqdn
    }
  end
end
[vagrant@puppetmaster demo_module]$
</pre>
 

Now if we re-run the test:

 
<pre>[vagrant@puppetmaster demo_module]$ bundle exec rake spec
/usr/local/rvm/rubies/ruby-1.9.3-p551/bin/ruby -S rspec spec/classes/init_spec.rb spec/classes/subclass_spec.rb --color
.F
 
Failures:
 
  1) demo_module::subclass The following subclass should be in the catalog
     Failure/Error: should contain_class('demo_module::subclass')
     Puppet::Error:
       Could not find class demo_module::subclass for puppetmaster.ordsvy.gov.uk on node puppetmaster.ordsvy.gov.uk
     # ./spec/classes/subclass_spec.rb:6:in `block (3 levels) in <top (required)>'
 
Finished in 0.0687 seconds
2 examples, 1 failure
 
Failed examples:
 
rspec ./spec/classes/subclass_spec.rb:5 # demo_module::subclass The following subclass should be in the catalog
/usr/local/rvm/rubies/ruby-1.9.3-p551/bin/ruby -S rspec spec/classes/init_spec.rb spec/classes/subclass_spec.rb --color failed
[vagrant@puppetmaster demo_module]$
</pre>
 


As you can see above, as expected the test has failed, since we haven't created the class yet. Let's create the missing class/manifest now:

 
<pre>[vagrant@puppetmaster demo_module]$ touch manifests/subclass.pp
[vagrant@puppetmaster demo_module]$ tree
.
├── Gemfile
├── Gemfile.lock
├── manifests
│   ├── init.pp
│   └── subclass.pp                # Here we have created the new file.
├── Modulefile
├── Rakefile
├── README
├── spec
│   ├── classes
│   │   ├── init_spec.rb
│   │   └── subclass_spec.rb
│   ├── defines
│   ├── fixtures
│   │   ├── manifests
│   │   └── modules
│   ├── functions
│   ├── hosts
│   └── spec_helper.rb
└── tests
    └── init.pp
10 directories, 12 files
[vagrant@puppetmaster demo_module]$</pre>

 

Now let's add the following content to the subclass.pp file:

 
<pre>[vagrant@puppetmaster demo_module]$ cat manifests/subclass.pp
class demo_module::subclass {
 
}
[vagrant@puppetmaster demo_module]$</pre>

 

As you can this is another empty shell of a class. However this test should still pass because at the moment our tests are simply checking the inclusion of the class names only. so if we now do run the tests, we get:

 
<pre>
[vagrant@puppetmaster demo_module]$ bundle exec rake spec
/usr/local/rvm/rubies/ruby-1.9.3-p551/bin/ruby -S rspec spec/classes/init_spec.rb spec/classes/subclass_spec.rb --color
..
Finished in 0.07104 seconds
2 examples, 0 failures
[vagrant@puppetmaster demo_module]$
</pre>
 



<h3>Test 03 - Check for class parameter values</h3>


Let's now see how we can pass in some class parameter values into a class. We'll use the "file" resource as an example. so in our rspec test we include:

 
<pre>[root@puppetmaster demo_module]# cat spec/classes/init_spec.rb
require 'spec_helper'      
describe 'demo_module' do
  context 'The catalog should at the very least compile' do
    it {
         should compile
    }
  end
  context 'The catalog should contain exactly 2 resources' do
    it {
         should have_resource_count(2)
    }
  end
  context 'The files name should be @/tmp/testfile.txt@ and the files content should be @hello and goodbye@' do
    let(:params){
      {
        :filename    => '/tmp/testfile.txt',
        :filecontent => 'hello and goodbye'
      }
    }
    it {
         should contain_file('/tmp/testfile.txt').with({
           'content' => 'hello and goodbye',
         })
    }
  end
  
end
[root@puppetmaster demo_module]#</pre>

 

Here we have introduced the "let" statement. In software testing terminology, this let statement can be thought of as a "test driver". We also used rspec's ":params" setting to set and pass in the class parameter values. 

 
<pre>[root@puppetmaster demo_module]# bundle exec rake spec
/usr/local/rvm/rubies/ruby-1.9.3-p551/bin/ruby -S rspec spec/classes/init_spec.rb --color
..F
 
Failures:
 
  1) demo_module The files name should be /tmp/testfile.txt and the files content should be goodbye should contain File[/tmp/testfile.txt] with content => "hello and goodbye"
     Failure/Error: })
       expected that the catalogue would contain File[/tmp/testfile.txt] with content set to "hello and goodbye" but it is set to "hello world"
     # ./spec/classes/init_spec.rb:24:in `block (3 levels) in <top (required)>'
 
Finished in 0.32012 seconds
3 examples, 1 failure
 
Failed examples:
 
rspec ./spec/classes/init_spec.rb:21 # demo_module The files name should be /tmp/testfile.txt and the files content should be goodbye should contain File[/tmp/testfile.txt] with content => "hello and goodbye"
/usr/local/rvm/rubies/ruby-1.9.3-p551/bin/ruby -S rspec spec/classes/init_spec.rb --color failed
[root@puppetmaster demo_module]#</pre>

The error message quite easy to understand.

Now let's rewrite our class so that it can now accept the necessary class parameters:

 
<pre>[root@puppetmaster demo_module]# cat manifests/init.pp
class demo_module ($filename = "/tmp/testfile.txt", $filecontent = "hello world") {
  user {'homer':
  }
  file {$filename:
    content => "$filecontent",
  }
}
[root@puppetmaster demo_module]#</pre>

 

Note, we also defined default values for the class parameters. This isn't strictly necessary, however if for some reason the "test drivers" fails to pass in the data to the puppetmaster, then the puppetmaster would still be able to compile the class and give a more meaningful error message, rather than the  highlevel "could not compile" error message. 

 

Now if we re-run the test it passes:

 
<pre>
[root@puppetmaster demo_module]# bundle exec rake spec
/usr/local/rvm/rubies/ruby-1.9.3-p551/bin/ruby -S rspec spec/classes/init_spec.rb --color
...
Finished in 0.34118 seconds
3 examples, 0 failures
[root@puppetmaster demo_module]#
</pre>
 









<h3>Test 05: Check presence of a resource</h3>

So far we have only included empty classes, and then tested that those classes are present in the catalog. Now we are going to go the next level down, and start writing rspec tests that checks for the inclusion of the resources.

In our example, let's say that we want a new file resource, /tmp/testfile.txt as part of the main class. In that case let's first write the test:

 
<pre>
[vagrant@puppetmaster demo_module]$ cat spec/classes/init_spec.rb
require 'spec_helper'      
describe 'demo_module' do
  context 'The following classes should be present in the catalog' do
    it {
         should contain_class('demo_module')                 
    }
  end
 
  context 'the file /tmp/testfile.txt should exist' do       # Here is the code block to test for the existance of the new file resource.
    it {
         should contain_file('/tmp/testfile.txt')        
    }
  end
  
end
[vagrant@puppetmaster demo_module]$
</pre>

 

Now lets run the test:

 
<pre>
[vagrant@puppetmaster demo_module]$ bundle exec rake spec
/usr/local/rvm/rubies/ruby-1.9.3-p551/bin/ruby -S rspec spec/classes/init_spec.rb spec/classes/subclass_spec.rb --color
.F.
 
Failures:
 
  1) demo_module the file /tmp/testfile.txt should exist should contain File[/tmp/testfile.txt]
     Failure/Error: should contain_file('/tmp/testfile.txt')
       expected that the catalogue would contain File[/tmp/testfile.txt]
     # ./spec/classes/init_spec.rb:11:in `block (3 levels) in <top (required)>'
 
Finished in 0.15251 seconds
3 examples, 1 failure
 
Failed examples:
 
rspec ./spec/classes/init_spec.rb:10 # demo_module the file /tmp/testfile.txt should exist should contain File[/tmp/testfile.txt]
/usr/local/rvm/rubies/ruby-1.9.3-p551/bin/ruby -S rspec spec/classes/init_spec.rb spec/classes/subclass_spec.rb --color failed
[vagrant@puppetmaster demo_module]$
</pre>
 

This failed, as expected. Now let's add the resource to the manifest:

 
<pre>
[vagrant@puppetmaster demo_module]$ cat manifests/init.pp
class demo_module {
  file {'/tmp/testfile.txt':
  }
}
[vagrant@puppetmaster demo_module]$
</pre>

 

Notice, that for the time being we declared a file resource but without any attributes whatsoever, we will look at attributes later on. 

 

Now let's rerun the test:

 
<pre>
[vagrant@puppetmaster demo_module]$ bundle exec rake spec
/usr/local/rvm/rubies/ruby-1.9.3-p551/bin/ruby -S rspec spec/classes/init_spec.rb spec/classes/subclass_spec.rb --color
...
Finished in 0.14835 seconds
3 examples, 0 failures
[vagrant@puppetmaster demo_module]$ cat manifests/init.pp
class demo_module {
  file {'/tmp/testfile.txt':
  }
}
</pre>

 

Now, let's see what happens when we add another file resource to the main class:

 
<pre>
[vagrant@puppetmaster demo_module]$ cat manifests/init.pp
class demo_module {
  file {'/tmp/testfile.txt':
  }
  file {'/tmp/anothertestfile.txt':
  }
}
[vagrant@puppetmaster demo_module]$
</pre>
 









==================================================================







Nesting manifests

All manifests are stored under the manifests folders. However if your modules has a lot of manifests then you may want to create your own folder structure beneath the manifests folder and organise your manifests into these folders. If you do this then your rspec tests will fail because it won't be able to locate the manifests, hence you then need to update the rspec tests. Let's say we want to move the subclass.pp manifest to a new folder called "other", then in the spirit of TDD/BDD, let's update the rspec code first:

 
[vagrant@puppetmaster demo_module]$ cat spec/classes/subclass_spec.rb
require 'spec_helper' 
describe 'demo_module::other::subclass' do                        # notice the fqdn now contains "other"
  context 'The following subclass should be in the catalog' do
    it {
         should contain_class('demo_module::other::subclass')     # notice the fqdn now contains "other"
    }
  end
end
[vagrant@puppetmaster demo_module]$

 

Now let's run the test:

 
[vagrant@puppetmaster demo_module]$ bundle exec rake spec
/usr/local/rvm/rubies/ruby-1.9.3-p551/bin/ruby -S rspec spec/classes/init_spec.rb spec/classes/subclass_spec.rb --color
.F
 
Failures:
 
  1) demo_module::other::subclass The following subclass should be in the catalog
     Failure/Error: should contain_class('demo_module::other::subclass')     # notice the fqdn now contains "other"
     Puppet::Error:
       Could not find class demo_module::other::subclass for puppetmaster.ordsvy.gov.uk on node puppetmaster.ordsvy.gov.uk
     # ./spec/classes/subclass_spec.rb:5:in `block (3 levels) in <top (required)>'
 
Finished in 0.06494 seconds
2 examples, 1 failure
 
Failed examples:
 
rspec ./spec/classes/subclass_spec.rb:4 # demo_module::other::subclass The following subclass should be in the catalog
/usr/local/rvm/rubies/ruby-1.9.3-p551/bin/ruby -S rspec spec/classes/init_spec.rb spec/classes/subclass_spec.rb --color failed
[vagrant@puppetmaster demo_module]$

 

This time it has failed as expected. Now let's create the "other" folder and move the subclass.pp into it:

 
[vagrant@puppetmaster demo_module]$ mkdir manifests/other
[vagrant@puppetmaster demo_module]$ mv manifests/subclass.pp manifests/other/subclass.pp
[vagrant@puppetmaster demo_module]$ tree
.
├── Gemfile
├── Gemfile.lock
├── manifests
│   ├── init.pp
│   └── other                  # have now created this folder.
│       └── subclass.pp        # have now place this manifest into here.
├── Modulefile
├── Rakefile
├── README
├── spec
│   ├── classes
│   │   ├── init_spec.rb
│   │   └── subclass_spec.rb
│   ├── defines
│   ├── fixtures
│   │   ├── manifests
│   │   └── modules
│   ├── functions
│   ├── hosts
│   └── spec_helper.rb
└── tests
    └── init.pp
11 directories, 12 files
[vagrant@puppetmaster demo_module]$

 

We also have to update the fqdn in the subclass.pp manifest to include "other" too:

 
[vagrant@puppetmaster demo_module]$ cat manifests/other/subclass.pp
class demo_module::other::subclass {                                
}

 

Now lets rerun the test:

 
[vagrant@puppetmaster demo_module]$ bundle exec rake spec
/usr/local/rvm/rubies/ruby-1.9.3-p551/bin/ruby -S rspec spec/classes/init_spec.rb spec/classes/subclass_spec.rb --color
..
Finished in 0.06801 seconds
2 examples, 0 failures
[vagrant@puppetmaster demo_module]$

 

Now it has passed.
Testing for the existance of a resource

So far we have only included empty classes, and then tested that those classes are present in the catalog. Let's  now test introduce a new "user" resource into one of our class and tests that it exists. So let's go ahead




<h3>Test 06: Check the number of resources </h3>

We want to test that the catalog contains x number of resources. In our case, let's say 3 resources. 






<pre>
[root@puppetmaster demo_module]# cat spec/classes/init_spec.rb
require 'spec_helper'      
describe 'demo_module' do
  context 'The catalog should at the very least compile' do
    it {
         should compile
    }
  end
  context 'The catalog should contain exactly 2 resources' do
    it {
         should have_resource_count(2)
    }
  end
end
[root@puppetmaster demo_module]#
</pre>














See also:

https://relishapp.com/rspec/docs/gettingstarted



























]]></Content>
		<Date><![CDATA[2015-01-23]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>rspec-puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Testing Puppet Modules using Beaker]]></Title>
		<Content><![CDATA[Within your beaker test script you actually write a manifest and store that manifest into a variable called "pp"

This "pp" variable is a multiline variable. 

This pp is then sent to the test-machine using the "apply_manifest" This function requires 2 parameters, the first one is "pp" and the second one is a mode. 

<h2>Puppet Exit codes</h2>
0: This means no changes
1: This means that puppetmaster was unable to generate the catalog, maybe due to a parse failure.
2: puppet agents has made changes to itself after receiving the catalog. I.e. resource changes
4: pupppet agent tried to make changes to itself but failed, i.e. resource failure. e.g. trying to create a new file in a non existance directory. 
6: (2+4), which means resource changes as well as resource failures. 

You can use beaker's "<a href="https://github.com/puppetlabs/beaker/wiki/The-Beaker-DSL-API#shell-run_script-apply_manifest-fact-etc-using-the-default-host">apply_manifests()</a>" function. You can find out more about these exitcodes using puppet's help command:


<pre>
[sher@puppetmaster ~]$ puppet help apply
.
.
* --detailed-exitcodes:
  Provide transaction information via exit codes. If this is enabled, an exit
  code of '2' means there were changes, an exit code of '4' means there were
  failures during the transaction, and an exit code of '6' means there were both
  changes and failures.

.
.
</pre>

We have a function called "apply_manifest()" This is the function we used to tell beaker whether we want are doing negative or positive testing. 

:catch_failures   - This means that we are expecting this test to pass (i.e. 0 or 2), but if anything else, i.e. 1,4, or 6, then fail the test and report the failure details.  

:catch_changes - This means that we don't expect any resource changes, i.e. we are only expecting 0, but if there are any changes, then catch them. 

:expect_failures

:expect_changes




 








https://www.google.co.uk/search?q=beaker+&ie=utf-8&oe=utf-8&gws_rd=cr&ei=1XLDVKTKHoXvUrrGgqAP#safe=off&q=beaker+puppet]]></Content>
		<Date><![CDATA[2015-01-24]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - What is Beaker]]></Title>
		<Content><![CDATA[beaker lets you create an environment apply the manifests, reports the results and trash the environment again. In preparation for the next test. This means that you are always using a consistent and known starting (i.e. a fresh environment).

This saves the need to force your test bed env exactly back to how it was before.

beaker can test:
- one module and all it's indiviual components.
- bunch of modules
- 2 sets of module, one to set up a db server and another for a web server and then get them together to one another.
- can test all this in a test environment with real machines.

drawbacks - it is relativel slow.

https://github.com/puppetlabs/beaker/wiki/How-to-Write-a-Beaker-Test-for-a-Module

All the following modules contains beaker tests:

&nbsp;

&nbsp;
<ul>
	<li>puppetlabs-apache</li>
	<li>puppetlabs-mysql</li>
	<li>puppetlabs-ntp</li>
	<li>puppetlabs-concat</li>
	<li>puppetlabs-inifile</li>
</ul>

along with all the rest of the puppetlab modules. 

Note: you could have a go at running one of the above modules beaker test just to practice. 

To do a beaker run, you run the following command:

<pre>
bundle install
bundle exec rspec spec/acceptance</pre>
</pre>

Note, unlike puppet-rspec unit tests which are executed via "rake", beaker tests are actually run directly using the "rspec" command, and hence there are no rake-tasks and consequently rake is not used either for running beaker tests.  


By default the test will run on whichever nodeset that's called "default"


In your modules folder, all your beaker stuff resides in a new folder called "acceptance" which resides under spec:

<pre>
[sher@puppetmaster demo_module]$ mkdir spec/acceptance
[sher@puppetmaster demo_module]$ tree
.
├── Gemfile
├── Gemfile.lock
├── manifests
│   └── init.pp
├── metadata.json
├── orig-Gemfile
├── Rakefile
├── README.md
├── spec
│   ├── acceptance          # Here's the new folder. 
│   ├── classes
│   │   └── init_spec.rb
│   ├── defines
│   ├── fixtures
│   ├── functions
│   ├── hosts
│   └── spec_helper.rb
└── tests
    └── init.pp
</pre>





for example in ntp module, you have specs to check:

- that you can managed the service
- can install packages correctly
- ntp_config_spec.rb - this checks the configuration stuff. 
- ntp_parameters_spec.rb - this where most of the stuff are. It basically runs tests for every single parameter that ntp module accepts. e.g. can I pass in servers. 




when you do run a beaker test, what actually happens is:

<ol>
	<li>test vm gets booted</li>
	<li>waits for boot to complete</li>
	<li>ssh into the vm</li>
	<li>run the spec tests. </li>
</ol>


There are also <a href="https://github.com/puppetlabs/beaker/wiki/How-to-Write-a-Beaker-Test-for-a-Module#supported-env-variables">some environment level variables settings</a> that you can apply, by simply creating environment variables of a certain name. Alternatively you can pass these settings directly into your beaker-test-run command. This means that the environment variables are set up only temporarily while the beaker tests are running, and disappears after it has finished. This is done by prefixing the variable definition to the main beaker commands, e.g.:


<pre>
BEAKER_SET="a-value" bundle exec rspec spec/acceptance 
</pre>




also beaker quite a general tool and is not specific to puppet, i.e. you can write beaker tests for chef. 




Serverspec - this is just used as a checking tool. It logs into a machine (which can be a production machine) and does lots and lots of checks. Then it returns the results. It doesn't, or better yet, can't make any changes to server. It only checks, and does'nt actually make any changes to the machine. Hence you can think of it like a "server unit testing framework". This makes it perfect for inspecting production servers. But you might not be able to use it to generate any reports about versions of stuff installed, becuase you have to tell it check for a specific version of a thing. 

Therefore, Serverspec is a only a subset of what Beaker does, Where Beaker does the following:

<ol>
	<li>spins up a new machine</li>
	<li>applies the manifests</li>
	<li>check the outcome - which is the only part that serverspec on its own does</li>
	<li>destroys the used vm</li>
</ol>

Because of this, our beaker test scripts actually ends up including serverspec code in them too.  



To set up your module you need to have the following things in place in your module's folder:

<ul>
	<li>Gemfile - it includes the beaker gems. The main beaker specific gems are beaker, beaker-rspec, and </li>
	<li>spec/acceptance/  -  all beaker _spec.rbv files are placed here. </li>
	<li>spec/acceptance/nodesets/*  - list of files for each target testbed vm. one vm per file.</li>
	<li>spec/spec_helper_acceptance.rb - this mainly is unchanged but  </li>
</ul>







 















]]></Content>
		<Date><![CDATA[2015-01-24]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - How to write a  gem]]></Title>
		<Content><![CDATA[http://stackoverflow.com/questions/2194547/ruby-how-to-write-a-gem/2196833#2196833]]></Content>
		<Date><![CDATA[2015-01-24]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RVM - Installing a version of ruby]]></Title>
		<Content><![CDATA[You are now ready to <a href="https://www.ruby-lang.org/en/downloads/">install a version of ruby</a> to run on top of RMV. 

Before you install any new version of ruby to run on top of rvm, let's first confirm that there are no versions of ruby running on top of rvm to begin with:


<pre>
[sher@puppetmaster ~]$ rvm list

rvm rubies


# No rvm rubies installed yet. Try 'rvm help install'.
</pre>

Note: the version of ruby that was on the machine before RVM was installed, is not managed by ruby. More about this later. 

Now let's see what versions of ruby are available to run on inside rvm:

<pre>

[sher@puppetmaster bin]$ rvm list known | head -15
# MRI Rubies
[ruby-]1.8.6[-p420]
[ruby-]1.8.7[-head] # security released on head
[ruby-]1.9.1[-p431]
[ruby-]1.9.2[-p330]
[ruby-]1.9.3[-p551]
[ruby-]2.0.0[-p598]
[ruby-]2.1.4
[ruby-]2.1[.5]
[ruby-]2.2.0
[ruby-]2.2-head
ruby-head

</pre>

Now let's install 2.2.0, which we do using the rvm "install" action (see "rvm help" for more info about other available actions):


<pre>
[sher@puppetmaster /]$ rvm install 2.2.0
Searching for binary rubies, this might take some time.
No binary rubies available for: centos/6/x86_64/ruby-2.2.0.
Continuing with compilation. Please read 'rvm help mount' to get more information on binary rubies.
Checking requirements for centos.
Requirements installation successful.
Installing Ruby from source to: /home/sher/.rvm/rubies/ruby-2.2.0, this may take a while depending on your cpu(s)...
ruby-2.2.0 - #downloading ruby-2.2.0, this may take a while depending on your connection...
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 12.6M  100 12.6M    0     0   959k      0  0:00:13  0:00:13 --:--:--  834k
ruby-2.2.0 - #extracting ruby-2.2.0 to /home/sher/.rvm/src/ruby-2.2.0....
ruby-2.2.0 - #applying patch /home/sher/.rvm/patches/ruby/2.2.0/fix_installing_bundled_gems.patch.
ruby-2.2.0 - #configuring.........................................................
ruby-2.2.0 - #post-configuration..
ruby-2.2.0 - #compiling....................................................................................
ruby-2.2.0 - #installing............................
ruby-2.2.0 - #making binaries executable..
Rubygems 2.4.5 already available in installed ruby, skipping installation, use --force to reinstall.
ruby-2.2.0 - #gemset created /home/sher/.rvm/gems/ruby-2.2.0@global
ruby-2.2.0 - #importing gemset /home/sher/.rvm/gemsets/global.gems...........................................................
ruby-2.2.0 - #generating global wrappers........
ruby-2.2.0 - #gemset created /home/sher/.rvm/gems/ruby-2.2.0
ruby-2.2.0 - #importing gemsetfile /home/sher/.rvm/gemsets/default.gems evaluated to empty gem list
ruby-2.2.0 - #generating default wrappers........
ruby-2.2.0 - #adjusting #shebangs for (gem irb erb ri rdoc testrb rake).
Install of ruby-2.2.0 - #complete
Ruby was built without documentation, to build it run: rvm docs generate-ri
[sher@puppetmaster /]$

</pre>

As soon as this has finished all your "gem environment", "ruby --version", and "gem list" has changed!:

<pre>
[sher@puppetmaster /]$ ruby --version
ruby 2.2.0p0 (2014-12-25 revision 49005) [x86_64-linux]
[sher@puppetmaster /]$ gem environment
RubyGems Environment:
  - RUBYGEMS VERSION: 2.4.5
  - RUBY VERSION: 2.2.0 (2014-12-25 patchlevel 0) [x86_64-linux]
  - INSTALLATION DIRECTORY: /home/sher/.rvm/gems/ruby-2.2.0
  - RUBY EXECUTABLE: /home/sher/.rvm/rubies/ruby-2.2.0/bin/ruby
  - EXECUTABLE DIRECTORY: /home/sher/.rvm/gems/ruby-2.2.0/bin
  - SPEC CACHE DIRECTORY: /home/sher/.gem/specs
  - SYSTEM CONFIGURATION DIRECTORY: /home/sher/.rvm/rubies/ruby-2.2.0/etc
  - RUBYGEMS PLATFORMS:
    - ruby
    - x86_64-linux
  - GEM PATHS:
     - /home/sher/.rvm/gems/ruby-2.2.0
     - /home/sher/.rvm/gems/ruby-2.2.0@global
  - GEM CONFIGURATION:
     - :update_sources => true
     - :verbose => true
     - :backtrace => false
     - :bulk_threshold => 1000
  - REMOTE SOURCES:
     - https://rubygems.org/
  - SHELL PATH:
     - /home/sher/.rvm/gems/ruby-2.2.0/bin
     - /home/sher/.rvm/gems/ruby-2.2.0@global/bin
     - /home/sher/.rvm/rubies/ruby-2.2.0/bin
     - /home/sher/.rvm/bin
     - /usr/lib64/qt-3.3/bin
     - /usr/local/bin
     - /bin
     - /usr/bin
     - /usr/local/sbin
     - /usr/sbin
     - /sbin
     - /home/sher/bin
[sher@puppetmaster /]$ gem list

*** LOCAL GEMS ***

bigdecimal (1.2.6)
bundler (1.7.12)
bundler-unload (1.0.2)
executable-hooks (1.3.2)
gem-wrappers (1.2.7)
io-console (0.4.3)
json (1.8.1)
minitest (5.4.3)
power_assert (0.2.2)
psych (2.0.8)
rake (10.4.2)
rdoc (4.2.0)
rubygems-bundler (1.4.4)
rvm (1.11.3.9)
test-unit (3.0.8)
[sher@puppetmaster /]$

</pre>

If you want to revert back to your original setup, then don't panic, you just need to switch to the system version of ruby, which you can do using the rvm's "use" action:

<pre>
[sher@puppetmaster /]$ rvm use system
Now using system ruby.
[sher@puppetmaster /]$ ruby --version
ruby 1.8.7 (2013-06-27 patchlevel 374) [x86_64-linux]
[sher@puppetmaster /]$ gem list

*** LOCAL GEMS ***

hiera (1.3.4)
json (1.5.5)
json_pure (1.8.1)
[sher@puppetmaster /]$ gem environment
RubyGems Environment:
  - RUBYGEMS VERSION: 1.3.7
  - RUBY VERSION: 1.8.7 (2013-06-27 patchlevel 374) [x86_64-linux]
  - INSTALLATION DIRECTORY: /usr/lib/ruby/gems/1.8
  - RUBY EXECUTABLE: /usr/bin/ruby
  - EXECUTABLE DIRECTORY: /usr/bin
  - RUBYGEMS PLATFORMS:
    - ruby
    - x86_64-linux
  - GEM PATHS:
     - /usr/lib/ruby/gems/1.8
     - /home/sher/.gem/ruby/1.8
  - GEM CONFIGURATION:
     - :update_sources => true
     - :verbose => true
     - :benchmark => false
     - :backtrace => false
     - :bulk_threshold => 1000
  - REMOTE SOURCES:
     - http://rubygems.org/
[sher@puppetmaster /]$


</pre>

The <code>rvm use system</code> effectively disables rvm. 

If you want to switch to another (rvm managed) version of ruby then first find the version of ruby currently installed:


<pre>
[sher@puppetmaster /]$ rvm list

rvm rubies

 * ruby-2.2.0 [ x86_64 ]

# => - current
# =* - current && default
#  * - default

[sher@puppetmaster /]$

</pre>

And then just switch to it:

<pre>
[sher@puppetmaster /]$ rvm use 2.2.0
Using /home/sher/.rvm/gems/ruby-2.2.0
[sher@puppetmaster /]$ ruby --version
ruby 2.2.0p0 (2014-12-25 revision 49005) [x86_64-linux]
[sher@puppetmaster /]$
</pre>

As you can see it is now really easy to switch between different version of ruby, and their respective gem setups. 

You can also set a particular version of ruby as the default version, every time you start a new terminal session. For example to make version 2.2.0 as the default, you do:

<pre>
[sher@puppetmaster ~]$ rvm use --default 2.2.0
Using /home/sher/.rvm/gems/ruby-2.2.0

</pre>

Finally, as you can see above, our system version of ruby is 1.8.7, but if you do:

<pre>
[sher@puppetmaster ~]$ rvm use 1.8.7
ruby-1.8.7-head is not installed.
To install do: 'rvm install ruby-1.8.7-head'
[sher@puppetmaster ~]$
</pre>

This means that the system version of ruby isn't managed by rvm, however if you want to run this version of ruby inside rvm, then we do seperate rvm-install and then run it:



See also:


misc: RMV comes with a concepts called <a href="http://stackoverflow.com/questions/4689180/why-should-i-care-about-rvms-gemset-feature-when-i-use-bundler">"gemsets" this is a competitor to "bundler" and can be ignored</a>. ]]></Content>
		<Date><![CDATA[2015-01-24]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RVM - Deleting a Ruby version]]></Title>
		<Content><![CDATA[Any gems that you install while using an RVM's ruby version, is self contained in that version. However there may come a time when you no longer want to use a particular ruby version and want to delete it along with all it's gems. Then this can be done using the "remove" command. 


Let's say you have:

<pre>
[sher@puppetmaster ~]$ ruby --version
ruby 2.0.0p598 (2014-11-13 revision 48408) [x86_64-linux]
[sher@puppetmaster ~]$ rvm list

rvm rubies

   ruby-1.8.7-head [ x86_64 ]
   ruby-1.9.3-p551 [ x86_64 ]
=> ruby-2.0.0-p598 [ x86_64 ]
 * ruby-2.2.0 [ x86_64 ]

# => - current
# =* - current && default
#  * - default

[sher@puppetmaster ~]$

</pre>

Now let's say we want to delete version 1.8.7, in that case we simply do:


<pre>
[sher@puppetmaster ~]$ rvm remove 1.8.7
ruby-1.8.7-head - #removing src/ruby-1.8.7-head..
ruby-1.8.7-head - #removing rubies/ruby-1.8.7-head..
ruby-1.8.7-head - #removing gems....
ruby-1.8.7-head - #removing wrappers....
ruby-1.8.7-head - #removing environments....
[sher@puppetmaster ~]$ rvm list

rvm rubies

   ruby-1.9.3-p551 [ x86_64 ]
   ruby-2.0.0-p598 [ x86_64 ]
=* ruby-2.2.0 [ x86_64 ]

# => - current
# =* - current && default
#  * - default

[sher@puppetmaster ~]$
</pre>












]]></Content>
		<Date><![CDATA[2015-01-25]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RVM - Using rvmsudo instead of sudo]]></Title>
		<Content><![CDATA[There are times when you need to run a ruby related command to install something, e.g. "bundle install" or "gem install gem-name". If you are an rvm managed ruby version, then it is bad/messy practice to do this. because doing so will install the gem system-wide, rather then ruby version level. This all comes down to an environment variable called "rvm_path". When using sudo, path no longer becomes available. That's why to get around this, we use rvmsudo instead, which retains the user's environment data while in privileged mode

<pre> 
[sher@puppetmaster ~]$ env | grep "^rvm_path"
rvm_path=/home/sher/.rvm
[sher@puppetmaster ~]$ sudo env | grep "^rvm_path"      # notice that there's no match here.
[sher@puppetmaster ~]$ rvmsudo env | grep "^rvm_path"
rvm_path=/home/sher/.rvm
[sher@puppetmaster ~]$
</pre>

Hence the gem is installed system wide. 


]]></Content>
		<Date><![CDATA[2015-01-25]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Setting up RSpec - private notes]]></Title>
		<Content><![CDATA[Also see page 200 of puppet pro book


Here's some background info:

This gem is located in the <a href="http://codingbee.net/tutorials/ruby/ruby-gems/" title="Ruby – Gems">gem folder</a>. This gem defines a number of rake tasks that rake can make use of. Actually rake tasks are defined within the gem, in a file called rake_task.rb, which is located here:

<pre>
[root@puppetmaster /]# gem environment | grep -A 1 "GEM PATHS"
  - GEM PATHS:
     - /usr/local/rvm/gems/ruby-1.9.3-p551
[root@puppetmaster /]# cd /usr/local/rvm/gems/ruby-1.9.3-p551
[root@puppetmaster ruby-1.9.3-p551]# cd gems/
[root@puppetmaster gems]# cd puppetlabs_spec_helper-0.8.2/
[root@puppetmaster puppetlabs_spec_helper-0.8.2]# tree
.
├── CHANGELOG
├── Gemfile
├── lib
│   └── puppetlabs_spec_helper
│       ├── module_spec_helper.rb           
│       ├── puppetlabs_spec
│       │   ├── files.rb
│       │   ├── fixtures.rb
│       │   ├── matchers.rb
│       │   └── puppet_internals.rb
│       ├── puppetlabs_spec_helper.rb
│       ├── puppet_spec_helper.rb
│       ├── rake_tasks.rb                   # rake tasks defined here. 
│       └── version.rb
├── LICENSE
├── puppetlabs_spec_helper.gemspec
├── puppetlabs_spec_helper.rb
├── puppet_spec_helper.rb
├── Rakefile
├── README.markdown
└── spec
    ├── lib
    │   └── puppet
    │       └── type
    │           └── spechelper.rb
    ├── unit
    │   ├── puppetlabs_spec_helper
    │   │   └── puppetlabs_spec
    │   │       └── puppet_internals_spec.rb
    │   └── spechelper_spec.rb
    └── watchr.rb

10 directories, 21 files
[root@puppetmaster puppetlabs_spec_helper-0.8.2]# 

</pre>
 
to make these tasks available in your puppet module, you need to create "Rakefile" and teh "spec_help.rb" file in the following sections:

<pre>
[root@puppetmaster test_module]# tree
.
├── Gemfile
├── Gemfile.lock
├── manifests
│   └── init.pp
├── metadata.json
├── Rakefile              # create this file
├── README.markdown
├── spec
│   ├── classes
│   │   └── init_spec.rb
│   ├── fixtures
│   │   ├── manifests
│   │   └── modules
│   ├── spec_helper.rb     # create this file. 
│   └── spec.opts
└── tests
    └── init.pp

7 directories, 10 files
[root@puppetmaster test_module]# 
</pre> 


After that you can add the content to these 2 files as described here:

http://www.rubydoc.info/gems/puppetlabs_spec_helper   (see the usage section)


Once you have done that, you can then list all the available rake tasks by running the following command while in the directory containing the "Rakefile":

<pre>
[root@puppetmaster test_module]# ls
Gemfile  Gemfile.lock  manifests  metadata.json  Rakefile  README.markdown  spec  tests
[root@puppetmaster test_module]# rake help
rake beaker            # Run beaker acceptance tests
rake beaker_nodes      # List available beaker nodesets
rake build             # Build puppet module package
rake clean             # Clean a built module package
rake coverage          # Generate code coverage information
rake help              # Display the list of available rake tasks
rake lint              # Check puppet manifests with puppet-lint
rake spec              # Run spec tests in a clean fixtures directory
rake spec_clean        # Clean up the fixtures directory
rake spec_prep         # Create the fixtures directory
rake spec_standalone   # Run spec tests on an existing fixtures directory
rake syntax            # Syntax check Puppet manifests and templates
rake syntax:hiera      # Syntax check Hiera config files
rake syntax:manifests  # Syntax check Puppet manifests
rake syntax:templates  # Syntax check Puppet templates
rake validate          # Check syntax of Ruby files and call :syntax
[root@puppetmaster test_module]# 

</pre> 

Out of the above tasks, the main once of interest is the "spec" task. 

Now, if you have installed the above gems, created the config files, and you have all your rspec tests in place, then you can run the RSpec tests by simply running the following command:

<pre>
rake spec
</pre>


Now, this won't work at the moment. Because we first need to install the above gems. It is possible to install the gem using the gem command:

<pre>
gem install rspec-puppet
gem install puppetlabs_spec_helper
</pre>


However by doing this the <a href="http://stackoverflow.com/questions/12307217/rvm-list-all-gems-in-current-gemset-ignoring-global-default">gems are installed globally</a>. Whereas you want the gems installed locally. 



Note, if you want to configure how rspec itself behaves, then you do this via the spec/spec_helper.rb file, here's the <a href="https://relishapp.com/rspec">spec_helper.rb documentation</a>. 

Note, spec_helper.rb is an rspec specific config file. Hence in this file, we have to link it to the puppetlabs_spec_helper gem, which we do by insert either of the <a href="https://github.com/puppetlabs/puppetlabs_spec_helper#initializing-puppet-for-testing">puppetlabs_spec_helper library</a>.   

<pre>
require 'puppetlabs_spec_helper/puppet_spec_helper'

# or 

require 'puppetlabs_spec_helper/module_spec_helper'
</pre>


These in turn points to one of the two rb files in the puppetlabs_spec_helper gem's "lib" folder:

<pre>

[root@puppetmaster puppetlabs_spec_helper-0.8.2]# tree
.
├── CHANGELOG
├── Gemfile
├── lib
│   └── puppetlabs_spec_helper
│       ├── module_spec_helper.rb            # this one, or.....
│       ├── puppetlabs_spec
│       │   ├── files.rb
│       │   ├── fixtures.rb
│       │   ├── matchers.rb
│       │   └── puppet_internals.rb
│       ├── puppetlabs_spec_helper.rb
│       ├── puppet_spec_helper.rb           # this one. 
│       ├── rake_tasks.rb                    
│       └── version.rb
├── LICENSE
├── puppetlabs_spec_helper.gemspec
├── puppetlabs_spec_helper.rb
├── puppet_spec_helper.rb
├── Rakefile
├── README.markdown
└── spec
    ├── lib
    │   └── puppet
    │       └── type
    │           └── spechelper.rb
    ├── unit
    │   ├── puppetlabs_spec_helper
    │   │   └── puppetlabs_spec
    │   │       └── puppet_internals_spec.rb
    │   └── spechelper_spec.rb
    └── watchr.rb

10 directories, 21 files
[root@puppetmaster puppetlabs_spec_helper-0.8.2]# 
</pre>





http://rspec-puppet.com/setup/

http://terrarum.net/blog/puppet-testing-part-1.html


https://rubygems.org/gems/puppetlabs_spec_helper

]]></Content>
		<Date><![CDATA[2015-01-29]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[private]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>rspec-puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Setting up Beaker]]></Title>
		<Content><![CDATA[To do a beaker run, you run the following command:

<pre>
bundle install
bundle exec rspec spec/acceptance</pre>
</pre>


By default the test will run on whichever nodeset that's called "default". Hence a file called default.yml needs to exists. which we'll cover later. 


<h2>Create the "acceptance" and "nodesets" folder</h2>
In your modules folder, all your beaker stuff resides in a new folder called "acceptance" which resides under spec:

<pre>
[sher@puppetmaster demo_module]$ mkdir spec/acceptance
[sher@puppetmaster demo_module]$ mkdir spec/acceptance/nodsets
[sher@puppetmaster demo_module]$ tree
.
├── Gemfile
├── Gemfile.lock
├── manifests
│   └── init.pp
├── metadata.json
├── orig-Gemfile
├── Rakefile
├── README.md
├── spec
│   ├── acceptance        # Here's the new "acceptance"folder. 
│   │   └── nodsets       # Here's the new "nodesets"folder.
│   ├── classes
│   │   └── init_spec.rb
│   ├── defines
│   ├── fixtures
│   ├── functions
│   ├── hosts
│   └── spec_helper.rb
└── tests
    └── init.pp
</pre>

The acceptance folder will house all our beaker tests. 
The nodesets folder will house one or more yml file. Each of these yml files will contain information about the testbed environment that beaker will need to provision and run the tests on. 


<h2>Install the necessary Gems</h2>

To run beaker tests you need to install a few gems, They are:

<ul>
	<li><a href="https://rubygems.org/gems/beaker">beaker</a></li>
	<li><a href="https://rubygems.org/gems/beaker-rspec">beaker-rspec</a></li>
	<li><a href="https://rubygems.org/gems/serverspec">serverspec</a></li>
</ul>


As discussed earlier, we can install these gems from the command line using "gem install gem-name". But it is better to take the bundler approach, and get these added to your gemfile. 

Also you should add these gems to the gemfile that you created when you used for rspec testing, hence your gemfile should now look like:


<pre>
[sher@puppetmaster demo_module]$ cat Gemfile
source 'https://rubygems.org'

gem 'puppet',                 :require => false
gem 'puppetlabs_spec_helper', :require => false
gem 'rspec-puppet',           :require => false
gem 'rake', '~>10.4.2',       :require => false

# beaker related gems
gem 'beaker-rspec', :require => false
gem 'serverspec',   :require => false

[sher@puppetmaster demo_module]$
</pre>


Note: In most gemfiles like the <a href="https://github.com/puppetlabs/puppetlabs-apache/blob/master/Gemfile">apache puppet module's gemfile</a>, the beaker gem isn't explicitly declared, that's because the beaker gem is a dependency of the beaker-rspec gem. Hence beaker will get installed/used as part of the "bundle install" and "bundle exec..." commands. 


<h2>Create the "spec_helper_acceptance.rb" file</h2>


<ul>
	<li>Gemfile - it includes the beaker gems. The main beaker specific gems are beaker, beaker-rspec, and </li>
	<li>spec/acceptance/  -  all beaker _spec.rbv files are placed here. </li>
	<li>spec/acceptance/nodesets/*  - list of files for each target testbed vm. one vm per file.</li>
	<li>spec/spec_helper_acceptance.rb - this mainly is unchanged but  </li>
</ul>







See also:
http://puppetlabs.com/presentations/beaker-automated-cloud-based-acceptance-testing-alice-nodelman-puppet-labs
http://www.rubydoc.info/github/puppetlabs/beaker/Beaker/DSL/InstallUtils:install_puppet
http://www.rubydoc.info/github/puppetlabs/beaker/Beaker/Vagrant
https://github.com/puppetlabs/beaker/wiki/The-Beaker-DSL-API#install_pe-installing-pe

http://www.xkyle.com/getting-started-puppet-acceptance-tests-with-beaker/












]]></Content>
		<Date><![CDATA[2015-01-31]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Available types of nodesets]]></Title>
		<Content><![CDATA[<ul>
	<li>Amazon Elastic Compute Cloud</li>
	<li>Google Compute Engine</li>
	<li>Openstack</li>
	<li>Docker (local option)</li>
	<li>Vsphere (in-house)</li>
	<li>Fusion</li>
	<li>Vagrant (local option)</li>
</ul>
&nbsp;]]></Content>
		<Date><![CDATA[2015-01-31]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Beaker and Vagrant]]></Title>
		<Content><![CDATA[After test has ended, do:

&nbsp;

$ vagrant global-status

&nbsp;

&nbsp;

this will list all the vagrant machines created by beaker. Then use the id, to destroy these vagrant vms:

&nbsp;

$ vagrant destroy {id}

&nbsp;]]></Content>
		<Date><![CDATA[2015-01-31]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Gerrit - Installation and setup]]></Title>
		<Content><![CDATA[<h2>Intro</h2>
<a href="https://code.google.com/p/gerrit/">Gerrit</a> is a code review tool. To learn more about it, check out the <a href="http://www.amazon.co.uk/gp/product/1783289473/ref=as_li_qf_sp_asin_il_tl?ie=UTF8&camp=1634&creative=6738&creativeASIN=1783289473&linkCode=as2&tag=codi0f-21">The Gerrit Book</a>. This books has a lot of useful guides including how to automate code review using <a href="http://www.amazon.co.uk/gp/product/1784390089/ref=as_li_qf_sp_asin_il_tl?ie=UTF8&camp=1634&creative=6738&creativeASIN=1784390089&linkCode=as2&tag=codi0f-21">Jenkins</a>. 

This guide covers how to install/setup gerrit on Linux distro, CentOS.


<h2>Set up the database</h2>
Gerrit by default will create the "h2" database for you as part of it's installation process. However if you want to use an alternative/seperate db technology , then this db needs to be created beforehand. In my case I will be using a postgresql db, so I will be referring to postgres for the rest of this article.

You can create install/setup postgresql in a seperate server, or install/setup in the same server that gerrit will be installed on. For resilience purposes it is best practice to have a seperate gerrit-server, and a seperate postgresql-server.

From Puppet automation perspective, the postgresql server needs to be built before the gerrit server. I don't think puppet allows "node ordering", but we can insert a check in the gerrit-puppet-module to check connectivity with postgresql db and error out if that fails, which would happen if postgres server doesn't exist, or if postgres exists but gerrit-server hasn't been authenticated to communicated with the postgres server.

Therefore before I can move on to the next section, <a title="PostgreSQL – Install PostgreSQL, and then create a DB and User Account" href="http://codingbee.net/tutorials/postgresql/postgresql-install-postgresql-and-then-create-a-db-and-user-account/">I need to first install PostgreSQL on a separate server, create a new db, create a new db user account, and finally the db user full privelege to the db.</a>
<h2>Create the Gerrit user</h2>
You need to install gerrit using a user called "gerrit".

You can create the gerrit user in the same way as any other user "using the useradd", command, However it best practice to set the home directory of this user to:
<pre>/opt/gerrit   # this will get created automatically as part of the useradd command. 
</pre>
You need to set a password for the gerrit user, this is not essential but let's you login as gerrit user directly rather than doing "su -" from root user.

Hence, lets create a new user called "gerrit" and set it's home directory to be /opt/gerrit:
<pre>[root@puppetagent02 ~]# useradd -d /opt/gerrit gerrit
[root@puppetagent01 /]#  cat etc/passwd | grep "gerrit"
gerrit:x:501:502::/opt/gerrit:/bin/bash

</pre>
Note, this will create the "/opt" folder along with the "/opt/gerrit" folder.

As root user, you can then run "passwd gerrit" to set a password for gerrit, but we won't bother with that.
<h2>Install dependant packages</h2>
Before you can install gerrit, your machine must have:
<h4>java jdk v1.8</h4>
You can check which version you have like this:
<pre>[root@puppetagent02 etc]$ java -version
openjdk version "1.8.0_31"
OpenJDK Runtime Environment (build 1.8.0_31-b13)
OpenJDK 64-Bit Server VM (build 25.31-b07, mixed mode)
</pre>
if not installed install it via the command line:
<pre>[root@puppetagent02 etc]$ yum install java-1.8.0-openjdk.x86_64</pre>
<h4>Git v1.6+</h4>
You can check this like this:
<pre>[gerrit@puppetagent02 etc]$ git --version
git version 1.7.1
[gerrit@puppetagent02 etc]$
</pre>
If you don't have this, the you can use yum:
<pre>$ yum install git
</pre>
<h4>gitweb</h4>
This is done using:
<pre>$ yum install gitweb
</pre>
Installing gitweb will result in the following file becoming available:

<pre>
/var/lib/gitweb.cgi    # Not sure if this path is correct.
</pre>

You need to keep a note of this so that you can have <a href="https://review.openstack.org/Documentation/config-gitweb.html#_internal_managed_gitweb">gitweb managed internally by gerrit</a>. You then configure <a href="https://git.eclipse.org/r/Documentation/config-gerrit.html#gitweb">gitweb in the gerrit.config</a> file by inserting the following:

<pre>
[gitweb]
    cgi = /var/lib/gitweb.cgi
</pre>

Note: you don't need to change any gitweb specific configurations. 

<h4>Linux utilities</h4>
This is automatically true if installing Gerrit on Linux, but not so on Windows. Hence if you are installing gerrit on a linux machine, then you can ignore this requirement.
<h2>Download the gerrit installation file</h2>
login as the gerrit user:
<pre>[root@puppetagent02 ~]# su - gerrit
[gerrit@puppetagent02 ~]$
</pre>
and downlaod file from:

https://gerrit-releases.storage.googleapis.com/index.html

by using curl:
<pre>[gerrit@puppetagent02 ~]$ pwd
/opt/gerrit
[gerrit@puppetagent02 ~]$ curl https://gerrit-releases.storage.googleapis.com/gerrit-2.10.war -o gerrit.war
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 39.4M  100 39.4M    0     0   853k      0  0:00:47  0:00:47 --:--:-- 1042k
[gerrit@puppetagent02 ~]$ ls -l
total 40392
-rw-rw-r-- 1 gerrit gerrit 41360250 Feb 24 12:23 gerrit.war
</pre>
Note, we download this war into the gerrit user's home directory.
<h2>Install Gerrit</h2>
Note, if you already have a pre-prepared gerrit.config and secure.config, then you can do a silent or semin-automated install. In which case you can skip this section.

You need to be logged in as the gerrit user to Install this war:
<pre>[gerrit@puppetagent01 ~]$ id
uid=501(gerrit) gid=502(gerrit) groups=502(gerrit)
[gerrit@puppetagent01 ~]$ pwd
/opt/gerrit
[gerrit@puppetagent01 ~]$ ls
gerrit.war
[gerrit@puppetagent01 ~]$ java -jar gerrit.war
Gerrit Code Review
usage: java -jar gerrit.war command [ARG ...]

The most commonly used commands are:
  init           Initialize a Gerrit installation
  reindex        Rebuild the secondary index
  daemon         Run the Gerrit network daemons
  gsql           Run the interactive query console
  version        Display the build version number

  ls             List files available for cat
  cat FILE       Display a file from the archive
</pre>
Here we there are a number of options of which we are going to use init:

Note: run the "init" command as the gerrit user.
<pre>[gerrit@puppetagent01 ~]$ java -jar gerrit.war init     # Run the "init" command as the gerrit user. 

*** Gerrit Code Review 2.10
***


*** Git Repositories
***

Location of Git repositories   [git]:

</pre>
If you accept the "git" defaults, then it will create the following folder:
<pre>[root@puppetagent01 gerrit]# tree
.
├── bin
├── data
├── etc
│   └── mail
├── gerrit.war
├── lib
├── logs
├── plugins
├── static
└── tmp

9 directories, 1 file
[root@puppetagent01 gerrit]# tree
.
├── bin
├── data
├── etc
│   └── mail
├── gerrit.war
├── git                       # this folder is now created. 
├── lib
├── logs
├── plugins
├── static
└── tmp

10 directories, 1 file
[root@puppetagent01 gerrit]#

</pre>
Next we need to do accept the default "h2":

This will result in the "db" folder being created:
<pre>[root@puppetagent01 gerrit]# tree
.
├── bin
├── data
├── db             # this folder gets created. 
├── etc
│   └── mail
├── gerrit.war
├── git
├── lib
├── logs
├── plugins
├── static
└── tmp

11 directories, 1 file
[root@puppetagent01 gerrit]#
</pre>
Now we accept the default "LUCENE licence". This doesnt create any new files/folders.

The same is true of "OPENID", note this includes "?" option which might prompt you to try a few things.

Keep accepting defaults, when you reach and say yes to the following...:
<pre>Copy gerrit.war to /opt/gerrit/bin/gerrit.war [Y/n]? Y
</pre>
You get the following new item:
<pre>[root@puppetagent01 gerrit]# tree
.
├── bin
│   └── gerrit.war          # new file here. 
├── data
├── db
├── etc
│   └── mail
├── gerrit.war
├── git
├── lib
├── logs
├── plugins
├── static
└── tmp

11 directories, 2 files
[root@puppetagent01 gerrit]#
</pre>
Also for now, select "no" for the "bouncy castle" option. By selecting no, this will create the following folder:
<pre>[root@puppetagent01 gerrit]# tree
.
├── bin
│   └── gerrit.war
├── data
├── db
├── etc
│   ├── mail
│   └── ssh_host_key   # this file is created. 
├── gerrit.war
├── git
├── lib
├── logs
├── plugins
├── static
└── tmp

11 directories, 3 files
</pre>
Now it will give you the option to install/uninstall various plugins. These are temporarily placed in the local tmp folder:
<pre>
[root@puppetagent01 gerrit]# tree
.
├── bin
│   └── gerrit.war
├── cache
├── data
├── db
├── etc
│   ├── mail
│   └── ssh_host_key
├── gerrit.war
├── git
├── lib
├── logs
├── plugins
├── static
└── tmp
    ├── plugin_download-commands_150203_1501_1069983237733257196.jar
    ├── plugin_replication_150203_1501_3947389358378156958.jar
    ├── plugin_reviewnotes_150203_1501_7127427235158661758.jar
    └── plugin_singleusergroup_150203_1501_439039537351044470.jar

12 directories, 7 files


</pre>
Now when we select no to each of these, they will get deleted from this directory.

However if we say yes to any of them, then they get moved to the plugins folder:
<pre>[root@puppetagent01 gerrit]# tree
.
├── bin
│   └── gerrit.war
├── cache
├── data
├── db
├── etc
│   ├── mail
│   └── ssh_host_key
├── gerrit.war
├── git
├── lib
├── logs
├── plugins
│   └── download-commands.jar    # we said yes to this plugin. That's why it is here. 
├── static
└── tmp
    ├── plugin_replication_150203_1501_3947389358378156958.jar
    ├── plugin_reviewnotes_150203_1501_7127427235158661758.jar
    └── plugin_singleusergroup_150203_1501_439039537351044470.jar

12 directories, 7 files
[root@puppetagent01 gerrit]#


</pre>
Here are the values I entered during the interactive prompts of my install:
<pre>[gerrit@puppetagent01 ~]$ java -jar gerrit.war init   # Run the "init" command as the gerrit user. 

*** Gerrit Code Review 2.10
***


*** Git Repositories
***

Location of Git repositories   [git]:

*** SQL Database
***

Database server type           [h2]:

*** Index
***

Type                           [LUCENE/?]:

The index must be rebuilt before starting Gerrit:
  java -jar gerrit.war reindex -d site_path

*** User Authentication
***

Authentication method          [OPENID/?]:

*** Review Labels
***

Install Verified label         [y/N]? y

*** Email Delivery
***

SMTP server hostname           [localhost]:
SMTP server port               [(default)]:
SMTP encryption                [NONE/?]:
SMTP username                  :

*** Container Process
***

Run as                         [gerrit]:
Java runtime                   [/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.31-1.b13.el6_6.x86_64/jre]:
Copy gerrit.war to /opt/gerrit/bin/gerrit.war [Y/n]? Y
Copying gerrit.war to /opt/gerrit/bin/gerrit.war

*** SSH Daemon
***

Listen on address              [*]:
Listen on port                 [29418]:

Gerrit Code Review is not shipped with Bouncy Castle Crypto SSL v149
  If available, Gerrit can take advantage of features
  in the library, but will also function without it.
Download and install it now [Y/n]? n
Generating SSH host key ... rsa(simple)... done

*** HTTP Daemon
***

Behind reverse proxy           [y/N]?
Use SSL (https://)             [y/N]?
Listen on address              [*]:
Listen on port                 [8080]:
Canonical URL                  [http://puppetagent01.co.uk:8080/]:

*** Plugins
***

Install plugin commit-message-length-validator version v2.10 [y/N]? N
Install plugin download-commands version v2.10 [y/N]? y
Install plugin replication version v2.10 [y/N]? n
Install plugin reviewnotes version v2.10 [y/N]? N
Install plugin singleusergroup version v2.10 [y/N]? N

Initialized /opt/gerrit
[gerrit@puppetagent01 ~]$

</pre>
All the info that you entered via the interactive mode (with exception of password data), is used to generate a the following config files:
<pre>[gerrit@puppetagent02 etc]$ pwd
/opt/gerrit/etc
[gerrit@puppetagent02 etc]$ ls -l
total 16
-rw-rw-r-- 1 gerrit gerrit  650 Feb 13 07:07 gerrit.config      # this file
drwxrwxr-x 2 gerrit gerrit 4096 Feb 11 11:52 mail
-rw------- 1 gerrit gerrit  215 Feb 11 11:52 secure.config      # this stores the ldab and db credentials
-rw------- 1 gerrit gerrit 1219 Feb 11 11:52 ssh_host_key
[gerrit@puppetagent02 etc]$
</pre>
These 2 files are useful if you want install gerrit without any interactive prompts (i.e. a silent gerrit install), or semi-automated install, which involves just accepting the defaults.

There is a huge amount various configurations that you can set in the gerrit.config file, to see them, checkout the <a href="https://gerrit-review.googlesource.com/Documentation/config-gerrit.html">gerrit.config documentation</a>.
<h2>Silent or Semi-automated Gerrit Instalation</h2>
To do the silent install, You first need to place your gerrit.war and pre-prepared gerrit.config file in the following folder:
<pre>[gerrit@puppetagent02 ~]$ pwd
/opt/gerrit
[gerrit@puppetagent02 ~]$ ls -l
total 40400
-rwxrwxr-x 1 gerrit gerrit      650 Feb 24 12:55 gerrit.config
-rw-rw-r-- 1 gerrit gerrit 41360250 Feb 24 12:54 gerrit.war
-rwxrwxr-x 1 gerrit gerrit      215 Feb 24 12:55 secure.config
</pre>
Now, if you are doing the installation on a VM, then now is a good time to take a <strong>VM snapshot</strong>!!!

After that you need to go into that directory and run the following to do a silent install:
<pre>java -jar gerrit.war init --batch --site-path /opt/gerrit
</pre>
Note: you omit "--batch" if you want to do a semi-automated install. Where you can see each of them settings and default values.

Here's an example of this:
<pre>[gerrit@puppetagent02 ~]$ pwd
/opt/gerrit
[gerrit@puppetagent02 ~]$ ls -l
total 40396
-rwxr-xr-x 1 root root      488 Feb 10 14:11 gerrit.config
-rw-r--r-- 1 root root 41360250 Feb  4 09:09 gerrit.war
-rw-r--r-- 1 root root      215 Feb  4 09:09 secure.config
[gerrit@puppetagent02 ~]$ java -jar gerrit.war init --batch --site-path /opt/gerrit
Generating SSH host key ... rsa(simple)... done
Initialized /opt/gerrit
</pre>
After that has run, you should find your config files here now:
<pre>
[gerrit@puppetagent02 etc]$ pwd
/opt/gerrit/etc
[gerrit@puppetagent02 etc]$ ls -l
total 16
-rwxrwxr-x 1 gerrit gerrit  650 Feb 24 12:55 gerrit.config
drwxrwxr-x 2 gerrit gerrit 4096 Feb 24 13:05 mail
-rw------- 1 gerrit gerrit  170 Feb 24 13:06 secure.config
-rw------- 1 gerrit gerrit 1219 Feb 24 13:05 ssh_host_key

</pre>
Note: The content on the secure.config may have changed during the installation, if so then overwrite the secure.config file with your pre-prepared config file.
<pre>[gerrit@puppetagent02 etc]$ pwd
/opt/gerrit/etc
[gerrit@puppetagent02 etc]$ ls -l
total 16
-rwxrwxr-x 1 gerrit gerrit  650 Feb 24 12:55 gerrit.config
drwxrwxr-x 2 gerrit gerrit 4096 Feb 24 13:05 mail
-rw------- 1 gerrit gerrit  170 Feb 24 13:06 secure.config
-rw------- 1 gerrit gerrit 1219 Feb 24 13:05 ssh_host_key
[gerrit@puppetagent02 etc]$ cp /tmp/secure.config .
[gerrit@puppetagent02 etc]$ ls -l
total 16
-rwxrwxr-x 1 gerrit gerrit  650 Feb 24 12:55 gerrit.config
drwxrwxr-x 2 gerrit gerrit 4096 Feb 24 13:05 mail
-rw------- 1 gerrit gerrit  215 Feb 24 13:09 secure.config
-rw------- 1 gerrit gerrit 1219 Feb 24 13:05 ssh_host_key
[gerrit@puppetagent02 etc]$
</pre>
In my case I had kept a backup of my secure.config file in the /tmp/ folder.
<h2>Convert the Gerrit shell script into a service</h2>
Return back to the root user:
<pre>[gerrit@puppetagent02 etc]$ exit
logout
[root@puppetagent02 ~]#
</pre>
This was done by first running the following commands to create symbolic links:

Note: creating symbolic link isn't the best approach. So please ignore this.
<pre>sudo ln -snf /opt/gerrit/bin/gerrit.sh /etc/init.d/gerrit   # ignore this
</pre>
Note: I think you need to do this as the root user.

Instead of creating the symbolic link, as shown above, actually make a copy of this file (and get rid of the ".sh" suffix):
<pre>[root@puppetagent02 /]# cp /opt/gerrit/bin/gerrit.sh /etc/init.d/gerrit
</pre>
Make sure it has execute permission:
<pre>[root@puppetagent02 init.d]# ls -l gerrit
total 248
-rwxr-xr-x  1 root root 14638 Feb 24 15:09 gerrit
</pre>
Next run:
<pre>[root@puppetagent02 init.d]# chkconfig --add gerrit
</pre>
After that you should be able to do:
<pre>[root@puppetagent02 init.d]# chkconfig --list gerrit
gerrit          0:off   1:off   2:on    3:on    4:on    5:on    6:off
</pre>
Then start up on all run-levels, you do:
<pre>[root@puppetagent02 init.d]# chkconfig --level 0123456 gerrit on
[root@puppetagent02 init.d]# chkconfig --list gerrit
gerrit          0:on    1:on    2:on    3:on    4:on    5:on    6:on
[root@puppetagent02 init.d]#
</pre>
Now let's try using the service command, and you might get the following error message:
<pre>[gerrit@puppetagent02 etc]$ service gerrit status
** ERROR: GERRIT_SITE not set
[gerrit@puppetagent02 etc]$ 
</pre>
If you look inside the actual gerrit.sh script, it advises you to create the file, /etc/default/gerritcodereview, and store any gerrit specific environment variables in it, i.e.:
<pre>[root@puppetagent02 default]# pwd
/etc/default
[root@puppetagent02 default]# ls -l
total 8
-rw-r--r--. 1 root root 1756 Nov 21  2013 nss
-rw-------. 1 root root  119 Oct 13  2011 useradd
[root@puppetagent02 default]# echo "GERRIT_SITE=/opt/gerrit" &gt; /etc/default/gerritcodereview
[root@puppetagent02 default]# ls -l
total 12
-rw-r--r--  1 root root   24 Feb 24 13:16 gerritcodereview
-rw-r--r--. 1 root root 1756 Nov 21  2013 nss
-rw-------. 1 root root  119 Oct 13  2011 useradd
[root@puppetagent02 default]#

</pre>
Note: you need to do this as the root user, since the gerrit user only has read permissions for this folder.

Now if we try the service command again:
<pre>[root@puppetagent02 default]# service gerrit status
Checking arguments to Gerrit Code Review:
  GERRIT_SITE     =  /opt/gerrit
  GERRIT_CONFIG   =  /opt/gerrit/etc/gerrit.config
  GERRIT_PID      =  /opt/gerrit/logs/gerrit.pid
  GERRIT_TMP      =  /opt/gerrit/tmp
  GERRIT_WAR      =  /opt/gerrit/bin/gerrit.war
  GERRIT_FDS      =  1024
  GERRIT_USER     =  gerrit
  JAVA            =  /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.31-1.b13.el6_6.x86_64/jre/bin/java
  JAVA_OPTIONS    =
  RUN_EXEC        =  /usr/bin/perl -e '$x=$ENV{JAVA};exec $x @ARGV;die $!' -- GerritCodeReview
  RUN_ARGS        =  -jar /opt/gerrit/bin/gerrit.war daemon -d /opt/gerrit

[root@puppetagent02 default]#
</pre>
Success!!!

Now we need to start the service:
<pre>[root@puppetagent02 default]# service gerrit start
Starting Gerrit Code Review: FAILED
</pre>
However if it fails to start, for example above, then check gerrit's error log, which you can find here:
<pre>[gerrit@puppetagent02 logs]$ pwd
/opt/gerrit/logs
[gerrit@puppetagent02 logs]$ ls error_log
error_log
[gerrit@puppetagent02 logs]$
</pre>
A common error you might have is to do with reindexing, which we'll cover next:
<h2>do the reindexing</h2>
This step is optional and you may need to do this if your gerrit service failed to start. Check your gerrit logfile:
<pre>cat /opt/gerrit/logs/error_log</pre>
If it contains the following error message:
<pre>1) No index versions ready; run Reindex
</pre>
Then you fix this by doing a <a href="https://gerrit-documentation.storage.googleapis.com/Documentation/2.10/pgm-reindex.html">reindexing</a>, which is done by simply running the following command:
<pre>[gerrit@puppetagent02 ~]$ java -jar gerrit.war reindex
</pre>
Then try starting the service again:
<pre>
[gerrit@puppetagent02 ~]$ service gerrit start
Starting Gerrit Code Review: OK
[gerrit@puppetagent02 ~]$ service gerrit status
Checking arguments to Gerrit Code Review:
  GERRIT_SITE     =  /opt/gerrit
  GERRIT_CONFIG   =  /opt/gerrit/etc/gerrit.config
  GERRIT_PID      =  /opt/gerrit/logs/gerrit.pid
  GERRIT_TMP      =  /opt/gerrit/tmp
  GERRIT_WAR      =  /opt/gerrit/bin/gerrit.war
  GERRIT_FDS      =  1024
  GERRIT_USER     =  gerrit
  JAVA            =  /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.31-1.b13.el6_6.x86_64/jre/bin/java
  JAVA_OPTIONS    =
  RUN_EXEC        =  /usr/bin/perl -e '$x=$ENV{JAVA};exec $x @ARGV;die $!' -- GerritCodeReview
  RUN_ARGS        =  -jar /opt/gerrit/bin/gerrit.war daemon -d /opt/gerrit

Gerrit running pid=10885
[gerrit@puppetagent02 ~]$


</pre>
Success!!!
<h2>Start Gerrit at boot time for all Run Levels</h2>
We can also now manage the gerrit service via the service command:
<pre>[gerrit@puppetagent02 bin]$ service gerrit stop
Stopping Gerrit Code Review: OK             # after this the gerrit web insteface also stops working.
[gerrit@puppetagent02 bin]$ service gerrit start
Starting Gerrit Code Review: OK
</pre>
One final check we need to do is check that the gerrit service is registered correctly with chkconfig:
<pre>[gerrit@puppetagent02 bin]$ chkconfig
auditd          0:off   1:off   2:on    3:on    4:on    5:on    6:off
blk-availability        0:off   1:on    2:on    3:on    4:on    5:on    6:off
cgconfig        0:off   1:off   2:off   3:off   4:off   5:off   6:off
cgred           0:off   1:off   2:off   3:off   4:off   5:off   6:off
crond           0:off   1:off   2:on    3:on    4:on    5:on    6:off
docker          0:off   1:off   2:on    3:on    4:on    5:on    6:off
<strong>gerrit          0:off   1:off   2:off   3:on    4:off   5:off   6:off</strong>
ip6tables       0:off   1:off   2:on    3:on    4:on    5:on    6:off
iptables        0:off   1:off   2:on    3:on    4:on    5:on    6:off
lvm2-monitor    0:off   1:on    2:on    3:on    4:on    5:on    6:off
lxc             0:off   1:off   2:off   3:off   4:off   5:off   6:off
mcollective     0:off   1:off   2:on    3:on    4:on    5:on    6:off
netconsole      0:off   1:off   2:off   3:off   4:off   5:off   6:off
netfs           0:off   1:off   2:off   3:on    4:on    5:on    6:off
network         0:off   1:off   2:on    3:on    4:on    5:on    6:off
ntpd            0:off   1:off   2:off   3:off   4:off   5:off   6:off
ntpdate         0:off   1:off   2:off   3:off   4:off   5:off   6:off
postfix         0:off   1:off   2:on    3:on    4:on    5:on    6:off
puppet          0:off   1:off   2:off   3:off   4:off   5:off   6:off
puppetmaster    0:off   1:off   2:off   3:off   4:off   5:off   6:off
rdisc           0:off   1:off   2:off   3:off   4:off   5:off   6:off
restorecond     0:off   1:off   2:off   3:off   4:off   5:off   6:off
rhnsd           0:off   1:off   2:on    3:on    4:on    5:on    6:off
rsyslog         0:off   1:off   2:on    3:on    4:on    5:on    6:off
saslauthd       0:off   1:off   2:off   3:off   4:off   5:off   6:off
sshd            0:off   1:off   2:on    3:on    4:on    5:on    6:off
udev-post       0:off   1:on    2:on    3:on    4:on    5:on    6:off
vboxadd         0:off   1:off   2:on    3:on    4:on    5:on    6:off
vboxadd-service 0:off   1:off   2:on    3:on    4:on    5:on    6:off
vboxadd-x11     0:off   1:off   2:off   3:on    4:off   5:on    6:off
[gerrit@puppetagent02 bin]$
</pre>
According to this, the gerrit service should start up automatically when the machine is rebooted (and starts up under run level 3).

let's now reboot the machine. After the reboot, let's check whether the gerrit service is running:
<pre>[gerrit@puppetagent02 ~]$ service gerrit status
Checking arguments to Gerrit Code Review:
  GERRIT_SITE     =  /opt/gerrit
  GERRIT_CONFIG   =  /opt/gerrit/etc/gerrit.config
  GERRIT_PID      =  /opt/gerrit/logs/gerrit.pid
  GERRIT_TMP      =  /opt/gerrit/tmp
  GERRIT_WAR      =  /opt/gerrit/bin/gerrit.war
  GERRIT_FDS      =  1024
  GERRIT_USER     =  gerrit
  JAVA            =  /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.31-1.b13.el6_6.x86_64/jre/bin/java
  JAVA_OPTIONS    =
  RUN_EXEC        =  /usr/bin/perl -e '$x=$ENV{JAVA};exec $x @ARGV;die $!' -- GerritCodeReview
  RUN_ARGS        =  -jar /opt/gerrit/bin/gerrit.war daemon -d /opt/gerrit

Gerrit running pid=1312

</pre>
Success!!

Just in case the machine is reboot at a different run level, it may be best to ensure that gerrit starts up in all run levels, which can ensure by running the following command (as root user):
<pre>[root@puppetagent02 ~]# chkconfig | grep gerrit
gerrit          0:off   1:off   2:off   3:on    4:off   5:off   6:off
[root@puppetagent02 ~]# chkconfig --level 0123456 gerrit on
[root@puppetagent02 ~]# chkconfig | grep gerrit
gerrit          0:on    1:on    2:on    3:on    4:on    5:on    6:on
[root@puppetagent02 ~]#
</pre>
This is so that the script can be managed via the linux "service" command, e.g. "service gerrit status" and also this will let you configure gerrit to start-up during boot-time.
<h2>install Gerrit plugins</h2>
There are a bunch of official plugins available here:

https://gerritcodereview-plugins.storage.googleapis.com/index.html

If you want to install/load any of these plugins, you just need to copy the jar files into the following folder:
<pre>[gerrit@puppetagent02 plugins]$ pwd
/opt/gerrit/plugins/
</pre>
After that restart the gerrit service:
<h2>start the gerrit service</h2>
Now start the gerrit service:
<pre>/opt/gerrit/bin/gerrit.sh start
</pre>
Confirm that service is running:
<pre>[gerrit@puppetagent01 bin]$ /opt/gerrit/bin/gerrit.sh status
Checking arguments to Gerrit Code Review:
  GERRIT_SITE     =  /opt/gerrit
  GERRIT_CONFIG   =  /opt/gerrit/etc/gerrit.config
  GERRIT_PID      =  /opt/gerrit/logs/gerrit.pid
  GERRIT_TMP      =  /opt/gerrit/tmp
  GERRIT_WAR      =  /opt/gerrit/bin/gerrit.war
  GERRIT_FDS      =  1024
  GERRIT_USER     =  gerrit
  JAVA            =  /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.31-1.b13.el6_6.x86_64/jre/bin/java
  JAVA_OPTIONS    =
  RUN_EXEC        =  /usr/bin/perl -e '$x=$ENV{JAVA};exec $x @ARGV;die $!' -- GerritCodeReview
  RUN_ARGS        =  -jar /opt/gerrit/bin/gerrit.war daemon -d /opt/gerrit

Gerrit running pid=15768
[gerrit@puppetagent01 bin]$

</pre>
Here's a different init setup that I also tried out:
<pre>[gerrit@puppetagent02 ~]$ java -jar gerrit.war init   # Note: run the "init" command as the gerrit user. 

*** Gerrit Code Review 2.10
***


*** Git Repositories
***

Location of Git repositories   [git]:

*** SQL Database
***

Database server type           [h2]:

*** Index
***

Type                           [LUCENE/?]: ?
       Supported options are:
         lucene
         solr
Type                           [LUCENE/?]: solr
Solr Index URL                 [localhost:9983]:

The index must be rebuilt before starting Gerrit:
  java -jar gerrit.war reindex -d site_path

*** User Authentication
***

Authentication method          [OPENID/?]: ?
       Supported options are:
         openid
         openid_sso
         http
         http_ldap
         client_ssl_cert_ldap
         ldap
         ldap_bind
         custom_extension
         development_become_any_account
Authentication method          [OPENID/?]: development_become_any_account

*** Review Labels
***

Install Verified label         [y/N]? y

*** Email Delivery
***

SMTP server hostname           [localhost]:
SMTP server port               [(default)]:
SMTP encryption                [NONE/?]: ?
       Supported options are:
         none
         ssl
         tls
SMTP encryption                [NONE/?]:
SMTP username                  : gerrit
gerrit's password              :
              confirm password :

*** Container Process
***

Run as                         [gerrit]: ?
Java runtime                   [/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.31-1.b13.el6_6.x86_64/jre]:
Copy gerrit.war to /opt/gerrit/bin/gerrit.war [Y/n]? Y
Copying gerrit.war to /opt/gerrit/bin/gerrit.war

*** SSH Daemon
***

Listen on address              [*]:
Listen on port                 [29418]:

Gerrit Code Review is not shipped with Bouncy Castle Crypto SSL v149
  If available, Gerrit can take advantage of features
  in the library, but will also function without it.
Download and install it now [Y/n]? Y
Downloading http://www.bouncycastle.org/download/bcpkix-jdk15on-149.jar ... OK
Checksum bcpkix-jdk15on-149.jar OK

Gerrit Code Review is not shipped with Bouncy Castle Crypto Provider v149
** This library is required by Bouncy Castle Crypto SSL v149. **
Download and install it now [Y/n]? Y
Downloading http://www.bouncycastle.org/download/bcprov-jdk15on-149.jar ... OK
Checksum bcprov-jdk15on-149.jar OK
Generating SSH host key ... rsa... dsa... done

*** HTTP Daemon
***

Behind reverse proxy           [y/N]? N
Use SSL (https://)             [y/N]? y
Listen on address              [*]:
Listen on port                 [8443]:
Canonical URL                  [https://puppetagent02.co.uk:8443/]:
Create new self-signed SSL certificate [Y/n]? Y
Certificate server name        [puppetagent02.co.uk]:
Certificate expires in (days)  [365]:

*** Plugins
***

Install plugin commit-message-length-validator version v2.10 [y/N]?
Install plugin download-commands version v2.10 [y/N]?
Install plugin replication version v2.10 [y/N]?
Install plugin reviewnotes version v2.10 [y/N]?
Install plugin singleusergroup version v2.10 [y/N]?

Initialized /opt/gerrit
[gerrit@puppetagent02 ~]$


</pre>
<h3>Supported Databases</h3>
If you want to see a list of supported databases. then you can view them during the (init) installation process by typing "?" when prompted to choose a db:
<pre>[root@puppetagent02 gerrit]# java -jar gerrit.war init

*** Gerrit Code Review 2.10
***


*** Git Repositories
***

Location of Git repositories   [git]:

*** SQL Database
***

Database server type           [h2]: asdfasdf
error: 'asdfasdf' is not a valid choice
       Supported options are:
         h2
         jdbc
         maxdb
         mysql
         oracle
         postgresql
Database server type           [h2]: ?
       Supported options are:
         h2
         jdbc
         maxdb
         mysql
         oracle
         postgresql
Database server type           [h2]:

</pre>
<pre>LDAP</pre>
here are the prompts you get when choosing the ldap option:
<pre>*** User Authentication
***

Authentication method          [OPENID/?]: ?
       Supported options are:
         openid
         openid_sso
         http
         http_ldap
         client_ssl_cert_ldap
         ldap                                # I selected this one
         ldap_bind
         custom_extension
         development_become_any_account
Authentication method          [OPENID/?]: ldap
LDAP server                    [ldap://localhost]:    # I think need to also include port number.
LDAP username                  : test
test's password                :
              confirm password :
Account BaseDN                 : test
Group BaseDN                   [test]: test



</pre>
Here we have

Note: the db password details are not stored in the gerrit.config, instead it is stored a file called secure.config, which sits alongside the gerrit.config:
<pre>[root@puppetagent02 etc]# pwd
/opt/gerrit/etc
[root@puppetagent02 etc]# ls -l
total 16
-rw-r--r-- 1 root root  539 Feb  5 07:34 gerrit.config
drwxr-xr-x 2 root root 4096 Feb  5 07:34 mail
-rw------- 1 root root  191 Feb  5 07:34 secure.config
-rw------- 1 root root 1220 Feb  5 07:33 ssh_host_key
[root@puppetagent02 etc]#

</pre>
Automating Gerrit install:

In the above examples, we had an interactive prompt which we had to enter values into. In the end, this resulted in the gerrit installation as well as creation of the gerrit.config file. This config file contains all the information that you entered during the interactive prompts.

If you already have a gerrit.config file, then it is possible to do a "silent install", i.e. automate the installation using an existing gerrit.config file.

Then start gerrit (you might need to reindex again):
<pre>[gerrit@puppetagent02 ~]$ /opt/gerrit/bin/gerrit.sh start
Starting Gerrit Code Review: OK
</pre>
Next, you can check that the gerrit process is running under the gerrit user:
<pre>[gerrit@puppetagent02 ~]$ ps -ef | grep "GerritCodeReview" | grep -v "grep"
gerrit   10853     1 14 14:37 pts/0    00:00:16 GerritCodeReview -jar /opt/gerrit/bin/gerrit.war daemon -d /opt/gerrit --run-id=1423579028.10832
[gerrit@puppetagent02 ~]$

</pre>
This process id (pid) should match up with:
<pre>[gerrit@puppetagent02 ~]$ /opt/gerrit/bin/gerrit.sh status
Checking arguments to Gerrit Code Review:
  GERRIT_SITE     =  /opt/gerrit
  GERRIT_CONFIG   =  /opt/gerrit/etc/gerrit.config
  GERRIT_PID      =  /opt/gerrit/logs/gerrit.pid
  GERRIT_TMP      =  /opt/gerrit/tmp
  GERRIT_WAR      =  /opt/gerrit/bin/gerrit.war
  GERRIT_FDS      =  1024
  GERRIT_USER     =  gerrit
  JAVA            =  /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.31-1.b13.el6_6.x86_64/jre/bin/java
  JAVA_OPTIONS    =
  RUN_EXEC        =  /usr/bin/perl -e '$x=$ENV{JAVA};exec $x @ARGV;die $!' -- GerritCodeReview
  RUN_ARGS        =  -jar /opt/gerrit/bin/gerrit.war daemon -d /opt/gerrit

Gerrit running pid=10853
[gerrit@puppetagent02 ~]$
</pre>
Tip:

when testing your gerrit web interface on your local machine, you will need to add the following to your window's host file:
<pre>C:\Windows\System32\drivers\etc\hosts</pre>
Here you add something like:
<pre>{gerrit-server-ip-number}   hostname.co.uk 
</pre>
see also:

http://gerrit-documentation.googlecode.com/svn/Documentation/2.7/install.html - this explains how to convert the gerrit script into a service.

https://review.gerrithub.io/Documentation/install.html
https://gerrit-documentation.storage.googleapis.com/Documentation/2.10/cmd-index.html
http://gerrit-documentation.googlecode.com/svn/Documentation/2.7/install.html

https://gerrit.libreoffice.org/Documentation/install-quick.html

https://gerrit-review.googlesource.com/Documentation/#_tutorial

http://stackoverflow.com/questions/8584660/how-to-login-gerrit-as-administrator

http://www.google.co.uk/search?hl=en-GB&amp;ie=UTF-8&amp;source=android-browser&amp;q=gerrit+rest&amp;gfe_rd=cr&amp;ei=ozTdVJWfAcnBWJTMgJgJ#safe=off&amp;hl=en-GB&amp;q=gerrit+rest+ssh

https://gerrit-documentation.storage.googleapis.com/Documentation/2.10/cmd-plugin-install.html#_scripting

https://gerrit-documentation.storage.googleapis.com/Documentation/2.10/cmd-index.html

https://gerrit-documentation.storage.googleapis.com/Documentation/2.10/cmd-plugin-reload.html

https://gerrit-documentation.storage.googleapis.com/Documentation/2.10/cmd-plugin-ls.html

https://gerrit-review.googlesource.com/Documentation/config-gerrit.html#plugins

https://gerrit-documentation.storage.googleapis.com/Documentation/2.10/cmd-plugin-install.html

https://gerrit-review.googlesource.com/Documentation/config-gerrit.html]]></Content>
		<Date><![CDATA[2015-02-03]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[gerrit]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>gerrit]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PostgreSQL - Install PostgreSQL, and then create a DB and User Account]]></Title>
		<Content><![CDATA[<h2>Introduction</h2>

This guide gives you a crash course on:

<ol>
	<li>Installing the PostgreSQL software on your server</li>
	<li>Create your very first PostgreSQL DB - in our case we are going to create a db called "reviewdb"</li>
	<li>Create a new DB user account and give it full access to the new db</li>
	<li>Connect to the new db using the new db user account</li>
	<li>Remotely connect to the new db using the new db user account</li>
	<li>Making remote connections more secure</li> 
</ol>

 
<h2>Installing the PostgreSQL software on your server</h2>

First download the rpm, using curl/wget. First decide on which <a href="http://yum.postgresql.org/repopackages.php">postgresql rpm package</a> you want. e.g.: 

<pre>
curl http://yum.postgresql.org/9.3/redhat/rhel-6-x86_64/pgdg-oraclelinux93-9.3-1.noarch.rpm -o postgresql.rpm
</pre>

then, while in that directory, install it using yum:

<pre>
yum install postgresql.rpm
</pre>

Alternatively we can combine the above curl+yum commands into a single command, and run the following command instead of the above 2 commands:

<pre>yum install http://yum.postgresql.org/9.3/redhat/rhel-6-x86_64/pgdg-oraclelinux93-9.3-1.noarch.rpm </pre>

This hasn't actually installed postgres yet, it only installed the postgres yum repo, which in my case is the yum repo for postgres 9.3, which is the version I am interested in:

<pre>
[root@puppetmaster tmp]# ls -l /etc/yum.repos.d/ | grep "pgdg"
-rw-r--r--  1 root root  442 Apr 21  2014 pgdg-93-oraclelinux.repo
</pre>

In my case, the I want to install postresql v9.3, so that's what I'll search for that. 

<pre>
[root@puppetmaster tmp]# yum search "postgresql93"
Loaded plugins: refresh-packagekit
========================================================================================== N/S Matched: postgresql93 ==========================================================================================
postgresql93-debuginfo.x86_64 : Debug information for package postgresql93
postgresql93-jdbc-debuginfo.x86_64 : Debug information for package postgresql93-jdbc
postgresql93-odbc-debuginfo.x86_64 : Debug information for package postgresql93-odbc
postgresql93-python-debuginfo.x86_64 : Debug information for package postgresql93-python
postgresql93.x86_64 : PostgreSQL client programs and libraries
<strong>postgresql93-contrib.x86_64 : Contributed source and binaries distributed with PostgreSQL</strong>
postgresql93-devel.x86_64 : PostgreSQL development header files and libraries
postgresql93-docs.x86_64 : Extra documentation for PostgreSQL
postgresql93-jdbc.x86_64 : JDBC driver for PostgreSQL
postgresql93-libs.x86_64 : The shared libraries required for any PostgreSQL clients
postgresql93-odbc.x86_64 : PostgreSQL ODBC driver
postgresql93-plperl.x86_64 : The Perl procedural language for PostgreSQL
postgresql93-plpython.x86_64 : The Python procedural language for PostgreSQL
postgresql93-pltcl.x86_64 : The Tcl procedural language for PostgreSQL
postgresql93-python.x86_64 : Development module for Python code to access a PostgreSQL DB
<strong>postgresql93-server.x86_64 : The programs needed to create and run a PostgreSQL server</strong>
postgresql93-test.x86_64 : The test suite distributed with PostgreSQL

  Name and summary matches only, use "search all" for everything.
[root@puppetmaster tmp]#

</pre>

The two rpms we have shown in bold above are the ones we need. Also there is another one listed above which is called "postgresql93.x86_64", this installs the client side only which is useful for testing whether you can connect to your postgresql db from another machine. However the above two will also install the client side utility as well. 


Hence we go ahead and install them:

<pre>[root@puppetagent01 ~]# yum install postgresql93-server postgresql93-contrib
pgdg93                                                                         | 3.7 kB     00:00
pgdg93/primary_db                                                              | 147 kB     00:01
Setting up Install Process
Resolving Dependencies
--> Running transaction check
---> Package postgresql93-contrib.x86_64 0:9.3.6-1PGDG.rhel6 will be installed
--> Processing Dependency: postgresql93 = 9.3.6 for package: postgresql93-contrib-9.3.6-1PGDG.rhel6.x86_64
--> Processing Dependency: libpq.so.5()(64bit) for package: postgresql93-contrib-9.3.6-1PGDG.rhel6.x86_64
--> Processing Dependency: libossp-uuid.so.16()(64bit) for package: postgresql93-contrib-9.3.6-1PGDG.rhel6.x86_64
---> Package postgresql93-server.x86_64 0:9.3.6-1PGDG.rhel6 will be installed
--> Running transaction check
---> Package postgresql93.x86_64 0:9.3.6-1PGDG.rhel6 will be installed
---> Package postgresql93-libs.x86_64 0:9.3.6-1PGDG.rhel6 will be installed
---> Package uuid.x86_64 0:1.6.1-10.el6 will be installed
--> Finished Dependency Resolution

Dependencies Resolved

======================================================================================================
 Package                     Arch          Version                     Repository                Size
======================================================================================================
Installing:
 postgresql93-contrib        x86_64        9.3.6-1PGDG.rhel6           pgdg93                   486 k
 postgresql93-server         x86_64        9.3.6-1PGDG.rhel6           pgdg93                   4.1 M
Installing for dependencies:
 postgresql93                x86_64        9.3.6-1PGDG.rhel6           pgdg93                   1.0 M
 postgresql93-libs           x86_64        9.3.6-1PGDG.rhel6           pgdg93                   191 k
 uuid                        x86_64        1.6.1-10.el6                public_ol6_latest         53 k

Transaction Summary
======================================================================================================
Install       5 Package(s)

Total download size: 5.8 M
Installed size: 23 M
Is this ok [y/N]: y
Downloading Packages:
(1/5): postgresql93-9.3.6-1PGDG.rhel6.x86_64.rpm                               | 1.0 MB     00:03
(2/5): postgresql93-contrib-9.3.6-1PGDG.rhel6.x86_64.rpm                       | 486 kB     00:00
(3/5): postgresql93-libs-9.3.6-1PGDG.rhel6.x86_64.rpm                          | 191 kB     00:00
(4/5): postgresql93-server-9.3.6-1PGDG.rhel6.x86_64.rpm                        | 4.1 MB     00:05
(5/5): uuid-1.6.1-10.el6.x86_64.rpm                                            |  53 kB     00:00
------------------------------------------------------------------------------------------------------
Total                                                                 348 kB/s | 5.8 MB     00:17
Running rpm_check_debug
Running Transaction Test
Transaction Test Succeeded
Running Transaction
  Installing : postgresql93-libs-9.3.6-1PGDG.rhel6.x86_64                                         1/5
  Installing : postgresql93-9.3.6-1PGDG.rhel6.x86_64                                              2/5
  Installing : uuid-1.6.1-10.el6.x86_64                                                           3/5
  Installing : postgresql93-contrib-9.3.6-1PGDG.rhel6.x86_64                                      4/5
  Installing : postgresql93-server-9.3.6-1PGDG.rhel6.x86_64                                       5/5
  Verifying  : uuid-1.6.1-10.el6.x86_64                                                           1/5
  Verifying  : postgresql93-9.3.6-1PGDG.rhel6.x86_64                                              2/5
  Verifying  : postgresql93-server-9.3.6-1PGDG.rhel6.x86_64                                       3/5
  Verifying  : postgresql93-libs-9.3.6-1PGDG.rhel6.x86_64                                         4/5
  Verifying  : postgresql93-contrib-9.3.6-1PGDG.rhel6.x86_64                                      5/5

Installed:
  postgresql93-contrib.x86_64 0:9.3.6-1PGDG.rhel6    postgresql93-server.x86_64 0:9.3.6-1PGDG.rhel6

Dependency Installed:
  postgresql93.x86_64 0:9.3.6-1PGDG.rhel6         postgresql93-libs.x86_64 0:9.3.6-1PGDG.rhel6
  uuid.x86_64 0:1.6.1-10.el6

Complete!

</pre>

Here's a quick check that postgresql's command line client at least has been installed successfully:


<pre>
[root@puppetagent01 ~]# psql --version
psql (PostgreSQL) 9.3.6
</pre>




Now do:

<pre>
[root@puppetmaster tmp]#  service postgresql-9.3 status
postgresql-9.3 is stopped
[root@puppetmaster tmp]#  service postgresql-9.3 initdb       # this starts the db
Initializing database:                                     [  OK  ]
[root@puppetmaster tmp]# chkconfig postgresql-9.3 on          # this starts this service at boot-time
[root@puppetmaster tmp]#
</pre>

Next, you start the postgres service:

<pre>
[root@puppetmaster tmp]# service postgresql-9.3 start
Starting postgresql-9.3 service:                           [  OK  ]

</pre>


The above installation, has made a bunch of <a href="http://www.postgresql.org/docs/9.3/static/reference-client.html">psql commandline utilities</a> available.


After that you need to <a href="http://stackoverflow.com/questions/11919391/postgresql-error-fatal-role-username-does-not-exist">su to the postgres user</a>:

The "postgres" account has been automatically created during the Postgres installation. Not only is the "postgres" account a Linux account, but it is also a db-user account, which is also automatically created during the postgres installation. In fact the "postgres" user account is Postgres's equivalent of the linux's "root".      


<pre>
[root@puppetmaster tmp]# sudo -u postgres -i
-bash-4.1$ psql                                # then enter the postgresql command prompt. 
psql (9.3.5)
Type "help" for help.

postgres=#

</pre>
Notice here that there is link between the postgres linux account and the postgres db account. Which is that if you run "psql" under the postgres linux user, then postgres, automatically assumes you want to connect using the corresponding db user account. 

Notice here that we used the <a href="http://www.postgresql.org/docs/9.3/static/app-psql.html">psql command-line utility</a>. 



<h2>Create your very first PostgreSQL DB</h2>
Before we create our first db, Let's first <a href="http://dba.stackexchange.com/questions/1285/how-do-i-list-all-databases-and-tables-using-psql">view a list of all PostgreSQL databases</a> that Postgres comes with out of the box, using psql's "\l" command.:

<pre>
postgres=# \l
                                  List of databases
   Name    |  Owner   | Encoding |   Collate   |    Ctype    |   Access privileges
-----------+----------+----------+-------------+-------------+-----------------------
 postgres  | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 |
 template0 | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 | =c/postgres          +
           |          |          |             |             | postgres=CTc/postgres
 template1 | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 | =c/postgres          +
           |          |          |             |             | postgres=CTc/postgres
(3 rows)

</pre>



Now we <a href="http://www.postgresql.org/docs/9.3/static/tutorial-createdb.html">create our own PostgreSQL DB</a>. To do this you need to do it from the bash-prompt (while still logged in as the postgres user), and not the psql prompt:

<pre>
-bash-4.1$ createdb reviewdb
</pre>
In our case, we created a db called "reviewdb"

Note if you want to, you can also <a href="http://www.postgresql.org/docs/8.2/static/sql-dropdatabase.html">delete a postgres db</a>. 


Let's now confirm that it has been created:

<pre>
-bash-4.1$ psql
psql (9.3.5)
Type "help" for help.

postgres=# \l
                                  List of databases
   Name    |  Owner   | Encoding |   Collate   |    Ctype    |   Access privileges
-----------+----------+----------+-------------+-------------+-----------------------
 postgres  | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 |
 reviewdb  | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 |
 template0 | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 | =c/postgres          +
           |          |          |             |             | postgres=CTc/postgres
 template1 | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 | =c/postgres          +
           |          |          |             |             | postgres=CTc/postgres
(4 rows)

postgres=#

</pre>





<h2>Create a PostgreSQL user account</h2>

Before we create a new db user account, we should first check what db user account currently exists. There are 2 ways that you can check this, either run a sql query, or use the "\du" option:

<pre>
[root@puppetagent01 ~]# su - postgres
-bash-4.1$ psql
psql (9.3.6)
Type "help" for help.

postgres=# SELECT rolname FROM pg_roles;
 rolname
----------
 postgres
 gerrit
(2 rows)

postgres=# \du
                                List of roles
 Role name    |                   Attributes                   | Member of
--------------+------------------------------------------------+-----------
 gerritdbuser |                                                | {}
 postgres     | Superuser, Create role, Create DB, Replication | {}

postgres=#

</pre>

Note, you can find more info of the "\du" option like this:


<pre>
-bash-4.1$ psql
psql (9.3.6)
Type "help" for help.

postgres=# help
You are using psql, the command-line interface to PostgreSQL.
Type:  \copyright for distribution terms
       \h for help with SQL commands
       \? for help with psql commands
       \g or terminate with semicolon to execute query
       \q to quit
postgres=# \?
General
  \copyright             show PostgreSQL usage and distribution terms
  \g [FILE] or ;         execute query (and send results to file or |pipe)
  \gset [PREFIX]         execute query and store results in psql variables
  \h [NAME]              help on syntax of SQL commands, * for all commands
  \q                     quit psql
  \watch [SEC]           execute query every SEC seconds
.
.
.
  \du[+]  [PATTERN]      list roles
.
.
.
</pre>

In our case, we created a user called "gerritdbuser" and set the password "admin123" and we have granted full access priveleges to the "reviewdb" db. 


Now let's create a new user (which in our case will have the username "gerritdbuser") To do this we need to connect to one of postgres's pre-existing db, called "template1", in order to do this:


<pre>
-bash-4.1$ id
uid=26(postgres) gid=26(postgres) groups=26(postgres)
-bash-4.1$ psql template1
psql (9.3.5)
Type "help" for help.

template1=# CREATE USER gerritdbuser WITH PASSWORD 'admin123';
CREATE ROLE
template1=# GRANT ALL PRIVILEGES ON DATABASE "reviewdb" to gerritdbuser;
GRANT
template1=#
</pre>

Now let's check that the gerritdbuser account now exists:


<pre>
This is a placeholder. 
</pre>



Next we need to assign full control to the reviewdb db that we have created. We will do this in the next section. 


<h2>Grant full DB access privelege to the new user account</h2>

This is done like this:

<pre>
-bash-4.1$ id
uid=26(postgres) gid=26(postgres) groups=26(postgres)
-bash-4.1$ psql template1
psql (9.3.5)
Type "help" for help.

template1=# GRANT ALL PRIVILEGES ON DATABASE "reviewdb" to gerritdbuser;
GRANT
template1=#
</pre>


Now we can check that this has been successful by viewing the last column of the "\l" command:


<pre>
This is a placeholder. 
</pre>





<h2>Connect to the new db with the new db user account</h2>
At this point you will have realized that when you run psql on it's own, then by default, psql will assume that you want to connect to the db with the same db-account-name as the linux account name. This means that psql will fail if you try to run psql under a linux account that doesn't have an equivalent db account with a matching name. For example we haven't created a db account called "root" hence    

  



At this point you should now be able to run the following command as postgress/root user:


<pre>
-bash-4.1$ psql -h 127.0.0.1 -U gerritdbuser -d reviewdb
psql: FATAL:  Ident authentication failed for user "gerritdbuser"
</pre>

To fix this we need to edit the following file:



<pre>[root@puppetmaster data]# ls /var/lib/pgsql/9.3/data/pg_hba.conf
/var/lib/pgsql/9.3/data/pg_hba.conf</pre>


In this file, you will find the following line:

<pre>
host    all             all             127.0.0.1/32            ident

</pre>

Here you simply replace "ident" with "trust", and the restart the postgres service.


Now if you repeat the psql call, you get:

<pre>
[root@puppetagent01 ~]# psql -h 127.0.0.1 -U gerritdbuser -d reviewdb
psql (9.3.6)
Type "help" for help.

reviewdb=>
</pre>

Success!



<h2>
Connecting to db server remotely
</h2>


Now let's try connection using the machine ip address instead:


<pre>
[root@puppetagent01 ~]# psql -h 10.1.172.11 -U gerritdbuser -d reviewdb
psql: could not connect to server: Connection refused
        Is the server running on host "10.1.172.11" and accepting
        TCP/IP connections on port 5432?
[root@puppetagent01 ~]# 
</pre>

To fix this, you need to first edit the following file:

<pre>
[root@puppetagent01 ~]# vi /var/lib/pgsql/9.3/data/postgresql.conf

</pre>

On this file, we have the listen_addresses setting:


<pre>
.
.
#------------------------------------------------------------------------------
# CONNECTIONS AND AUTHENTICATION
#------------------------------------------------------------------------------

# - Connection Settings -

#listen_addresses = 'localhost'    # what IP address(es) to listen on;
.
.
</pre>

uncomment this line and set this to: '*', i.e.:


<pre>
.
.
#------------------------------------------------------------------------------
# CONNECTIONS AND AUTHENTICATION
#------------------------------------------------------------------------------

# - Connection Settings -

#listen_addresses = 'localhost'    # what IP address(es) to listen on;
listen_addresses = '*'    # what IP address(es) to listen on;
.
.
</pre>

Note: in my case I duplicated this line before editing it. 

Then restart the postgres service and try again. You should now get the following error message:


<pre>
[root@puppetagent01 ~]# psql -h 10.1.172.11 -U gerritdbuser -d reviewdb
psql: FATAL:  no pg_hba.conf entry for host "10.1.172.11", user "gerritdbuser", database "reviewdb", SSL off
</pre>

Now let's edit our pg_hba.conf and add in an entry for our ip address, which in this case is "10.1.172.11". Hence we add the following line:

<pre>
host    all             all             10.1.172.11/32            trust
</pre>

Note: this is nearly the same as our earlier 127.0.0.1 entry. 

Now restart the postgres service and try again:

<pre>
[root@puppetagent01 ~]# psql -h 10.1.172.11 -U gerritdbuser -d reviewdb
psql (9.3.6)
Type "help" for help.

reviewdb=> \q
[root@puppetagent01 ~]#

</pre>


Success!!!



Also see: http://dba.stackexchange.com/questions/14740/how-to-use-psql-with-no-password-prompt for better security. 
=======================================================================


On the client-machine run:

<pre>

yum install http://yum.postgresql.org/9.3/redhat/rhel-6-x86_64/pgdg-oraclelinux93-9.3-1.noarch.rpm 

</pre>

Next, we just want to install the postgres client software rather than the whole postgres db software:

<pre>
[root@puppetagent03 ~]# yum search "postgresql93" | grep "^postgres" | grep "client"
postgresql93.x86_64 : PostgreSQL client programs and libraries
[root@puppetagent03 ~]#
</pre>


So let's go ahead and do this now:

<pre>
[root@puppetagent03 ~]# psql --version
-bash: psql: command not found
[root@puppetagent03 ~]# yum install postgresql93
Setting up Install Process
Resolving Dependencies
--> Running transaction check
---> Package postgresql93.x86_64 0:9.3.6-1PGDG.rhel6 will be installed
--> Processing Dependency: postgresql93-libs = 9.3.6-1PGDG.rhel6 for package: postgresql93-9.3.6-1PGDG.rhel6.x86_64
--> Processing Dependency: libpq.so.5()(64bit) for package: postgresql93-9.3.6-1PGDG.rhel6.x86_64
--> Running transaction check
---> Package postgresql93-libs.x86_64 0:9.3.6-1PGDG.rhel6 will be installed
--> Finished Dependency Resolution

Dependencies Resolved

===============================================================================================================================================================================================================
 Package                                                Arch                                        Version                                                  Repository                                   Size
===============================================================================================================================================================================================================
Installing:
 postgresql93                                           x86_64                                      9.3.6-1PGDG.rhel6                                        pgdg93                                      1.0 M
Installing for dependencies:
 postgresql93-libs                                      x86_64                                      9.3.6-1PGDG.rhel6                                        pgdg93                                      191 k

Transaction Summary
===============================================================================================================================================================================================================
Install       2 Package(s)

Total download size: 1.2 M
Installed size: 5.8 M
Is this ok [y/N]: y
Downloading Packages:
(1/2): postgresql93-9.3.6-1PGDG.rhel6.x86_64.rpm                                                                                                                                        | 1.0 MB     00:01
(2/2): postgresql93-libs-9.3.6-1PGDG.rhel6.x86_64.rpm                                                                                                                                   | 191 kB     00:00
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total                                                                                                                                                                          163 kB/s | 1.2 MB     00:07
Running rpm_check_debug
Running Transaction Test
Transaction Test Succeeded
Running Transaction
  Installing : postgresql93-libs-9.3.6-1PGDG.rhel6.x86_64                                                                                                                                                  1/2
  Installing : postgresql93-9.3.6-1PGDG.rhel6.x86_64                                                                                                                                                       2/2
  Verifying  : postgresql93-libs-9.3.6-1PGDG.rhel6.x86_64                                                                                                                                                  1/2
  Verifying  : postgresql93-9.3.6-1PGDG.rhel6.x86_64                                                                                                                                                       2/2

Installed:
  postgresql93.x86_64 0:9.3.6-1PGDG.rhel6

Dependency Installed:
  postgresql93-libs.x86_64 0:9.3.6-1PGDG.rhel6

Complete!
[root@puppetagent03 ~]# psql --version
psql (PostgreSQL) 9.3.6
[root@puppetagent03 ~]#

</pre>



Now if we try to connect, we get the following error message:


<pre>
[root@puppetagent03 ~]# psql -h 10.1.172.11 -U gerritdbuser -d reviewdb
psql: FATAL:  no pg_hba.conf entry for host "10.1.172.13", user "gerritdbuser", database "reviewdb", SSL off
[root@puppetagent03 ~]# 
</pre>

This means that our client server managed to <a href="http://www.postgresql.org/docs/9.3/static/client-authentication-problems.html">successfully contact the db server, but the db server refused the connection</a>. 

To fix this we once again edit the db-server's, pg_hba.conf file. This time we insert a line representing the client server's ip address. In my case my client's ip address is "10.1.172.13", therefore insert:


<pre>
host    all             all             10.1.172.13/32            trust
</pre>

This essentially add's the client server to the whitelist. 






































Next insert the following line in the pg_hba.conf file:

<pre>
pg_hba.conf
</pre>






 
 

Now lets

- Insert the following line 

<pre>
host    all             all             {vm-ip-number}/32            trust
host    all             all             127.0.0.1/32           trust

</pre>





The next few things I think we need to do :







- Into the pg_hba.conf file:
[root@puppetmaster data]# ls /var/lib/pgsql/9.3/data/pg_hba.conf
/var/lib/pgsql/9.3/data/pg_hba.conf







- is <a href="http://www.cyberciti.biz/tips/howto-iptables-postgresql-open-port.html">open up IP tables to allow posgresql</a>:
  
<pre>
iptables -A INPUT -p tcp -s 0/0 --sport 1024:65535 -d <strong>{db-server-ip-number}</strong> --dport 5432 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -p tcp -s <strong>{db-server-ip-number}</strong> --sport 5432 -d 0/0 --dport 1024:65535 -m state --state ESTABLISHED -j ACCEPT
</pre>



Note, as soon as postgres finds a match it will stop processing this file any further. Therefore you need to ensure this file doesn't contain a match earlier, if so then comment this out. 

- restart postres service to take this change into account. 




<h2>automating postgresql installation and setup with puppet</h2>

All the above can be automated using puppet which. We have a seperate article for this. 





Note: 


http://www.cyberciti.biz/tips/howto-iptables-postgresql-open-port.html




 

Useful links:

http://www.postgresql.org/docs/9.3/static/tutorial-createdb.html
http://www.thegeekstuff.com/2009/04/15-practical-postgresql-database-adminstration-commands/

http://www.cyberciti.biz/faq/psql-fatal-ident-authentication-failed-for-user/

http://www.postgresql.org/docs/9.3/static/client-authentication.html  

http://www.cyberciti.biz/faq/postgresql-remote-access-or-connection/

http://www.depesz.com/2007/10/04/ident/


http://www.postgresql.org/docs/9.3/static/auth-pg-hba-conf.html

http://www.cyberciti.biz/faq/psql-fatal-ident-authentication-failed-for-user/

http://www.postgresql.org/docs/9.3/static/client-authentication-problems.html

http://www.postgresql.org/docs/9.3/static/app-psql.html

http://www.postgresql.org/docs/9.3/static/reference-client.html (these are all the commandline utilities that comes with postgres)

https://wiki.postgresql.org/wiki/YUM_Installation

http://www.postgresql.org/download/linux/redhat/

http://yum.postgresql.org/repopackages.php 

http://www.postgresql.org/docs/9.3/static/tutorial.html
]]></Content>
		<Date><![CDATA[2015-02-04]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PostgreSQL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PostgreSQL]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell -  Using WinSCP to copy files to/from Linux machines]]></Title>
		<Content><![CDATA[It is possible to copy files to/from a linux machine using Powershell. This can be done using a free tool called <a href="http://winscp.net/eng/index.php">winscp</a>. 

Winscp is actually a gui based tool, but you can use it from 




# I created this script using the following as an inspiration:
	# http://winscp.net/eng/docs/library_powershell#installing_the_assembly 
	
	# Load WinSCP .NET assembly
	[Reflection.Assembly]::LoadFrom("c:\path\to\WinSCPnet.dll") | Out-Null
	
	
	
	# Setup session options
	$sessionOptions = New-Object WinSCP.SessionOptions
	$sessionOptions.Protocol = [WinSCP.Protocol]::Sftp
	$sessionOptions.HostName = $ServerName
	$sessionOptions.UserName = $Username
	$sessionOptions.Password = $password
	#$sessionOptions.SshHostKeyFingerprint = "ssh-rsa 1024 xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx" # this feature is desabled in the next line. 
	# Note I disabled the above line, and enabled the line below using instructions from:
	# http://winscp.net/eng/docs/library_sessionoptions
	$sessionOptions.GiveUpSecurityAndAcceptAnySshHostKey = $true
	
	
	$session = New-Object WinSCP.Session
	# note for more info, see http://winscp.net/eng/docs/library_session
	
	# Connect
	$session.Open($sessionOptions)
	
	
	# Set transfer mode to binary
	# note, for more info, see:  http://winscp.net/eng/docs/library_transferoptions
	$transferOptions = New-Object WinSCP.TransferOptions
	$transferOptions.TransferMode = [WinSCP.TransferMode]::Binary



Now to send files to the linux machine, do:

<pre>
$session.PutFiles("$SourcePath", "$DestinationPath", $False, $transferOptions)
</pre>

And to retrieve files, do:

<pre>
$session.GetFiles("$SourcePath", "$DestinationPath", $False, $transferOptions)
</pre>


Putting it altogether, we have:


<pre>
[Reflection.Assembly]::LoadFrom(“C:\path\to\winscp-dll\WinSCPnet.dll”) | Out-Null

$sessionOptions = New-Object WinSCP.SessionOptions
$sessionOptions.Protocol = [WinSCP.Protocol]::Sftp
$sessionOptions.HostName = "linuxservername" 
$sessionOptions.UserName = "username"
$sessionOptions.Password = "password"

$sessionOptions.GiveUpSecurityAndAcceptAnySshHostKey = $true

$session = New-Object WinSCP.Session

$session.Open($sessionOptions)


$transferOptions = New-Object WinSCP.TransferOptions
$transferOptions.TransferMode = [WinSCP.TransferMode]::Binary

$SourcePath = "C:\Temp\testfile.txt"
$DestinationPath = "/tmp/testfile.txt"

$session.PutFiles("$SourcePath", "$DestinationPath", $False, $transferOptions)
</pre>

The folder that contains your WinSCPnet.dll file will also need to contain the WinSCP.exe file as well. You can download my copy of <a href="http://codingbee.net/wp-content/uploads/2015/02/winscp-dll.zip">both of these files</a>.  


<strong>Useful links:</strong>

http://winscp.net/eng/docs/library_sessionoptions#properties

http://winscp.net/eng/docs/library_powershell
]]></Content>
		<Date><![CDATA[2015-02-09]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell - Active Directory]]></Title>
		<Content><![CDATA[Active Directory is a type of LDAP. 

You can query active directory using  set of powershell commands. 

To do this you need to first turn on the AD windows feature. You do this on windows like this: 

start -> control panel -> programs -> Program and Features -> Turn windows features on or off (left hand side column)





  


import the active directory module. 



See Also:

<a href="http://codingbee.net/tutorials/redhat3/linux-ldap/" title="Linux – LDAP">Run LDAP query on Linux</a>

http://blogs.msdn.com/b/rkramesh/archive/2012/01/17/how-to-add-active-directory-module-in-powershell-in-windows-7.aspx 

http://stackoverflow.com/questions/16247552/easy-way-to-test-an-ldap-users-credentials
]]></Content>
		<Date><![CDATA[2015-02-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Virtualbox - Using ping]]></Title>
		<Content><![CDATA[For pinging to work, you need to have the following <a href="http://i.imgur.com/ZyGdCMi.png">virtualbox setup</a><a href="http://codingbee.net/wp-content/uploads/2015/02/virtualbox-nat-for-pinging.png"><img src="http://codingbee.net/wp-content/uploads/2015/02/virtualbox-nat-for-pinging.png" alt="" width="660" height="418" class="alignnone size-full wp-image-3108" /></a>. 


<a href="http://codingbee.net/wp-content/uploads/2015/02/u8Lpq3u.png"><img src="http://codingbee.net/wp-content/uploads/2015/02/u8Lpq3u.png" alt="" width="773" height="774" class="alignnone size-full wp-image-3114" /></a>



http://i.imgur.com/RWJavXx.png]]></Content>
		<Date><![CDATA[2015-02-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[VirtualBox]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>VirtualBox]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Networking - Network Addressing Translation (NAT)]]></Title>
		<Content><![CDATA[http://netfilter.org/documentation/HOWTO/NAT-HOWTO.html]]></Content>
		<Date><![CDATA[2015-02-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[nat]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Functions]]></Title>
		<Content><![CDATA[https://docs.puppetlabs.com/references/latest/function.html

https://docs.puppetlabs.com/puppet/latest/reference/lang_functions.html

]]></Content>
		<Date><![CDATA[2015-02-14]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Custom Functions]]></Title>
		<Content><![CDATA[https://docs.puppetlabs.com/guides/custom_functions.html
]]></Content>
		<Date><![CDATA[2015-02-14]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Install and setup Gerrit using Puppet]]></Title>
		<Content><![CDATA[<h2>Create the gerrit user</h2>

This can be done using the following resources:

<pre>
file{'/opt':
    ensure => directory,
  }

#  file{'/opts/gerrit':            # this might not be needed. 
#    ensure => directory,
#  }

user{'gerrit':
    ensure => present,
    home => '/opt/gerrit',
    managehome => true
  }

</pre>



need to also have java jdk installed :

<pre>
  package {'java-1.8.0-openjdk':
    ensure => present,
  }
</pre>


, or use the following puppet resource:

<pre>
  package {'git':
    ensure => present,
  }
}

</pre>





Alternatively this could be done via <strong>puppet</strong> using the file resource:


<pre>
create a file resource, with title=/opt/gerrit/gerrit.war, ensure=present, source=http://gerrit-releases.storage.googleapis.com/gerrit-2.10.war  
</pre>

Note, I replaced "https" with "http" because Puppet struggles to download the file using https. Luckily http also works.   



This pre-prepared gerrit.config can also be used to automate gerrit install via puppet, which we'll cover next:



<h3>Perform the (init) installation</h3>
With the puppet approach, you need to already have the following pre-prepared files:


<pre>
[gerrit@puppetagent02 etc]$ pwd
/opt/gerrit/etc
[gerrit@puppetagent02 etc]$ ls -l
total 16
-rw-rw-r-- 1 gerrit gerrit  611 Feb 11 11:52 gerrit.config       # this one...
drwxrwxr-x 2 gerrit gerrit 4096 Feb 11 11:52 mail
-rw------- 1 gerrit gerrit  215 Feb 11 11:52 secure.config       # ...and this one. 
-rw------- 1 gerrit gerrit 1219 Feb 11 11:52 ssh_host_key       # not sure about this one though. 
[gerrit@puppetagent02 etc]$

</pre>


Best way to get a hold of these files is to do a manual POC (proof of concept) installation and then use the files generated from the POC, for the puppet automation. You can can keep these file hardcoded in the files folder, or convert them into parameterised erb-templates. 

For now let's assumes we will use static files, in which case we need 2 file resources:

<pre>
file resource with title=/opt/etc/gerrit.config, source=puppet:///gerrit/file/gerrit.config
file resource with title=/opt/etc/secure.config, source=puppet:///gerrit/file/secure.config
</pre>
  
Once we have those in place, we can then run the following exec resource:

<pre>
exec resource with command="java -jar gerrit.war init --batch --site-path /opt/gerrit", and user=gerrit, cwd=/opt/gerrit
</pre>


For puppet automation of installing plugins, it's best to do this by creating a parametised defined-type for installing plugin, and this defined-type contains an "exec" resource that runs the above curl command. this defined type can then be called multiple times within the node-definition in the site.pp, or via ENC/hiera.  
]]></Content>
		<Date><![CDATA[2015-02-17]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Gerrit - rough info]]></Title>
		<Content><![CDATA[<h2>Setting up Private/Public key authentication (optional and not required)</h2>
Before we do the next task, it is worth taking time to realise that it is possible to interact directly with the actual "gerrit service". I.e. you can send commands to the gerrit deamon. 

In that respect the gerrit service (daemon) is a bit like a SOAP/REST web service. In otherwords it's a bit like an API. You can interact with this API via the ssh protocol at port "29418". That's because this is the port that gerrit by default listen's to. If you want, you can change this port in the gerrit.config file.

The cool thing is that it is possible to create a connection to this gerrit daemon from the command line, and then pass in a "gerrit command" to the service. These Gerrit commands are only commands that Gerrit service itself understands, so you can run them as a standalone on the bash-terminal. Instead the command has to be fed directly through to the gerrit daemon by first creating an ssh-29418 session. 

The Gerrit daemon only accepts ssh connections from already private-public key pre-authenticated users. We can do a simple test for a none authenticated user like this:

<pre>
[rayyan@puppetagent02 ~]$ ssh -p 29418 randomuser@localhost
Permission denied (publickey)
</pre>

To get this working we need to first to generate an ssh the id_rsa.pub for a user, like this:

<pre>
[schowdhury@puppetagent02 ~]$ pwd
/home/schowdhury
[schowdhury@puppetagent02 ~]$ ls -la
total 20
drwx------  2 schowdhury schowdhury 4096 Feb 12 16:03 .
drwxr-xr-x. 6 root       root       4096 Feb 12 16:03 ..
-rw-r--r--  1 schowdhury schowdhury   18 Jul 18  2013 .bash_logout
-rw-r--r--  1 schowdhury schowdhury  176 Jul 18  2013 .bash_profile
-rw-r--r--  1 schowdhury schowdhury  124 Jul 18  2013 .bashrc
[schowdhury@puppetagent02 ~]$ ssh-keygen
Generating public/private rsa key pair.
Enter file in which to save the key (/home/schowdhury/.ssh/id_rsa):
Created directory '/home/schowdhury/.ssh'.
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
Your identification has been saved in /home/schowdhury/.ssh/id_rsa.
Your public key has been saved in /home/schowdhury/.ssh/id_rsa.pub.
The key fingerprint is:
4e:2e:81:06:34:38:d7:28:c5:5c:7d:37:0f:57:99:0c schowdhury@puppetagent02.ordsvy.gov.uk
The key's randomart image is:
+--[ RSA 2048]----+
| =++..      E+.o |
|+.=.. . . + . +  |
| +.    . . =     |
|   . .      .    |
|    o . S        |
|   .   =         |
|      . o        |
|       .         |
|                 |
+-----------------+
[schowdhury@puppetagent02 ~]$ ls -la
total 24
drwx------  3 schowdhury schowdhury 4096 Feb 12 16:05 .
drwxr-xr-x. 6 root       root       4096 Feb 12 16:03 ..
-rw-r--r--  1 schowdhury schowdhury   18 Jul 18  2013 .bash_logout
-rw-r--r--  1 schowdhury schowdhury  176 Jul 18  2013 .bash_profile
-rw-r--r--  1 schowdhury schowdhury  124 Jul 18  2013 .bashrc
drwx------  2 schowdhury schowdhury 4096 Feb 12 16:05 .ssh
[schowdhury@puppetagent02 ~]$ cd .ssh/
[schowdhury@puppetagent02 .ssh]$ ls -l
total 8
-rw------- 1 schowdhury schowdhury 1675 Feb 12 16:05 id_rsa
<strong>-rw-r--r-- 1 schowdhury schowdhury  420 Feb 12 16:05 id_rsa.pub</strong>
[schowdhury@puppetagent02 .ssh]$
</pre>
note: you can simply hit return, for all the prompts. 

Now you need to copy the contents of id_rsa.pub. Now go to the gerrit's web interface, log into gerrit using the username you want to ssh with. 

then at the top right select:

<pre> 
{username} -> settings -> SSH Public Keys 
</pre>

paste in the content (be careful to not to include an unnecessary whitespace). then click add. 


================ Detour start ==============
Note: I think this web interaction can be automated using Gerrit's <a href="https://gerrit-documentation.storage.googleapis.com/Documentation/2.10/rest-api.html">REST API</a> and the "curl" command.


Also see how to authenticate here:

https://gerrit-review.googlesource.com/Documentation/dev-rest-api.html

Note, the password you use to authenticate is not your ldap (gerrit web login) password, but the HTTP password. You can generate this password 




=============== Detour end =================






Then return to the command line, and test your connection like this:


<pre>
[schowdhury@puppetagent02 ~]$ ssh -p 29418 schowdhury@localhost

  ****    Welcome to Gerrit Code Review    ****

  Hi Sher Chowdhury, you have successfully connected over SSH.

  Unfortunately, interactive shells are disabled.
  To clone a hosted Git repository, use:

  git clone ssh://schowdhury@puppetagent02.ordsvy.gov.uk:29418/REPOSITORY_NAME.git

Connection to localhost closed.
[schowdhury@puppetagent02 ~]$
</pre>

Success!!! this is what we expected when connection is successful!


Note, part of this test will also do a one time activity of creating a known_hosts file in the user's .ssh folder. 

Also see <a href="https://gerrit-documentation.storage.googleapis.com/Documentation/2.10/user-upload.html#test_ssh">Gerrit's testing ssh connection section</a> for more info. 

Note, if you still have problems then there is some <a href="http://gerrit-documentation.googlecode.com/svn-history/r8/Documentation/2.1.7/error-permission-denied.html#_test_ssh_authentication">gerrit test authentication troubleshooting guide</a>. 

There is a full <a href="https://gerrit-documentation.storage.googleapis.com/Documentation/2.10/cmd-index.html">list of gerrit commands</a> that you can pass into the gerrit via the ssh-prefixed-command-line.

 


However the command that we'll be looking at next is the "<a href="https://gerrit-documentation.storage.googleapis.com/Documentation/2.10/cmd-plugin-install.html">gerrit plugin install</a>" command]]></Content>
		<Date><![CDATA[2015-02-17]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[gerrit]]></Tags>
		<Status><![CDATA[private]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - External Node Classifiers (ENC)]]></Title>
		<Content><![CDATA[If you have stored all your puppet resources/logic/classes into modules, and you have placed all your configuration data (i.e. class parameter data) into hiera, then your site.pp file should now be just contain a series of node defintion, where each node definition is just a series include statements. 


When ever a puppet agent requests for a new catalog, the puppetmaster will first ask the agent to provide all facter+builtin data. It then looks for the agent's node definition in the site.pp file. However you can configure the puppetmaster to look for the node definition from an alternative source, this source is known as an ENC. In this situation, the puppetmaster is designed to retrieve the node definition in the form of yaml code, rather than puppet code as in the site.pp file. This means that when the puppetmaster queries the ENC, and the ENC must give any output in the form of yaml code. In reality, the ENC can be any kind executable, e.g. python/ruby/shell/java script/executables. The script doesn't need to have complex code to generate the node definition in yaml form, instead it can simply locate a existing yaml file that we manually created earlier, whic already holds the appropriate node definition and then output it back to the puppetmaster. We'll now show you how to create your very own enc now.     



We have already abstracted away most of the code already by fully utilizing the power of puppet modules and hiera. This has left the site.pp file to only contain a series of node definitions, and each node definition contains a series of include statements. 

The key thing to realise at this point is that we can now represent each of our node definitions in the form of yaml syntax. Hence we can create a yaml file equivalent of each node definition. For example if our site.pp contains just 2 node definitions:

<pre>
[root@puppetmaster manifests]# cat /etc/puppet/manifests/site.pp
node 'puppetmaster' {
  include user_account
}

node 'puppetagent' {
  include user_account
}
</pre>


Then we can create yaml file for each of them, and let's name each yaml file after the agent's fqdn. In my case I have placed these files in /tmp/node-definitions, i.e.:


<pre>
[root@puppetmaster ~]# cd /tmp/node-definitions/
[root@puppetmaster node-definitions]# ls -l
total 8
-rw-r--r--. 1 root root 32 Feb 22 12:53 puppetagent1.codingbee.dyndns.org.yaml
-rw-r--r--. 1 root root 32 Feb 22 13:49 puppetmaster.codingbee.dyndns.org.yaml
[root@puppetmaster node-definitions]# cat puppetmaster.codingbee.dyndns.org.yaml
---

classes:
  - user_account

[root@puppetmaster node-definitions]# cat puppetagent1.codingbee.dyndns.org.yaml
---

classes:
  - user_account

[root@puppetmaster node-definitions]#

</pre>

Now we can delete all the contents of site.pp, so that it becomes an empty file:

<pre>
[root@puppetmaster /]# cd /etc/puppet/manifests/
[root@puppetmaster manifests]# echo "" > site.pp
[root@puppetmaster manifests]# cat site.pp

[root@puppetmaster manifests]#
</pre>

Next we need to create a script that the puppetmaster can query to retrieve the node definition in yaml form. In my case I have created a script called /etc/puppet/enc.sh. This script is just made up of 3 lines:

<pre>
[root@puppetmaster ~]# cat /etc/puppet/enc.sh
#!/bin/bash

node_def="/tmp/node-definitions/${1}.yaml"

[ -f $node_def ] && cat $node_def
[root@puppetmaster ~]#

</pre>

As you can see, this script only makes use of one script parameter, ${1}, which is used to derived a possible path to a yaml file that could contain the node-definition in yaml form. The script then checks if the yaml file exists, if so then it output's it's content.  

Now that we have the external node classifier (enc.sh) in place along, we now need to <a href="https://docs.puppetlabs.com/guides/external_nodes.html#connecting-an-enc">tell the puppetmaster to start using the enc</a>. We do this by configuring 2 settings in the puppet.conf file, the <a href="https://docs.puppetlabs.com/references/latest/configuration.html#nodeterminus">node_terminus</a> setting and the <a href="https://docs.puppetlabs.com/guides/external_nodes.html">external_nodes</a> setting. These are puppetmaster setting and are defined under the master section. In our case we add the following to our puppet.conf file:

<pre>
[master]
  node_terminus = exec
  external_nodes = /etc/puppet/enc.sh
</pre>
  
Now we restart the puppetmaster to load the new puppet.conf settings into memory:

<pre>
[root@puppetmaster puppet]# service puppetmaster restart
Stopping puppetmaster:                                     [  OK  ]
Starting puppetmaster:                                     [  OK  ]
[root@puppetmaster puppet]#
</pre>

Now here's a summary of our puppetmaster setup for the agent, puppetagent1:

<pre>
##
## Here we determine the location of the node defintition in yaml form:
##

[root@puppetmaster puppet]# cat /etc/puppet/enc.sh
#!/bin/bash

node_def="/tmp/node-definitions/${1}.yaml"

[ -f $node_def ] && cat $node_def


##
## Here we determine the classes that are included in the node definition:
##

[root@puppetmaster puppet]# cat /tmp/node-definitions/puppetagent1.codingbee.dyndns.org.yaml
---

classes:
  - user_account

##
## Here we determine what class parameters the class needs:
##

[root@puppetmaster puppet]# cat /etc/puppet/modules/user_account/manifests/init.pp
class user_account ($username = "tempuser"){

  user { "$username":
    ensure => present,
    shell  => '/bin/bash',
  }


  file {"/tmp/$username.txt":
    ensure => file,
    content => $var,
  }

}



##
## Here we determine where the yaml datasource files are kept:
##

[root@puppetmaster puppet]# cat /etc/hiera.yaml
 ---
:backends:
  - yaml
:yaml:
  :datadir: /etc/puppet/hieradata/yaml
:hierarchy:
  - %{hostname}
  - filaA
  - fileB
  - common


##
## Here we determine what class parameter value will be used:
##
[root@puppetmaster puppet]# cat /etc/puppet/hieradata/yaml/puppetagent1.yaml
---

user_account::username: Tom
[root@puppetmaster puppet]#

</pre>

Let's now go ahead and do the puppet run:

<pre>
[root@puppetagent1 ~]# cat /etc/passwd | grep "Tom"
[root@puppetagent1 ~]# ls -l /tmp/Tom.txt
ls: cannot access Tom.txt: No such file or directory
[root@puppetagent1 ~]# puppet agent -t
Info: Retrieving pluginfacts
Info: Retrieving plugin
Info: Caching catalog for puppetagent1.codingbee.dyndns.org
Info: Applying configuration version '1424625777'
Notice: /Stage[main]/User_account/User[Tom]/ensure: created
Notice: /Stage[main]/User_account/File[/tmp/Tom.txt]/ensure: created
Notice: Finished catalog run in 0.10 seconds
[root@puppetagent1 ~]# cat /etc/passwd | grep "Tom"
Tom:x:505:506::/home/Tom:/bin/bash
[root@puppetagent1 ~]# ls -l /tmp/Tom.txt
-rw-r--r--. 1 root root 0 Feb 22 17:22 /tmp/Tom.txt
[root@puppetagent1 ~]#

</pre>

Success!!!

</a>See also:

http://www.cyberciti.biz/faq/unix-linux-test-existence-of-file-in-bash/

https://docs.puppetlabs.com/guides/external_nodes.html

https://docs.puppetlabs.com/references/latest/configuration.html#nodeterminus]]></Content>
		<Date><![CDATA[2015-02-22]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Format of the ENC's output]]></Title>
		<Content><![CDATA[<h2>Intro</h2>
The yaml data outputted by ENC can only have 3 high level keys, <code>classes</code>, <code>parameters</code>, and <code>environment</code>. We'll now take a look at each of these in turn. 


  
<h2>The "classes" key</h2>
We have already come across this in the previous lesson. This basically is where you write all your node definition's include statements in yaml form. However you can add class parameter values here as well, e.g.:


<pre> 
[root@puppetmaster yaml]# cat /tmp/node-definitions/puppetagent1.codingbee.dyndns.org.yaml
---

classes:
  user_account:
    username: james

[root@puppetmaster yaml]#
</pre>

This is basically the equivalent of doing a resource-like class declaration, but in yaml form. By specifying your class parameters here, it means that the puppetmaster will no longer refer to hiera to lookup the value for the "username" class parameter. 

However this approach isn't good practice because Hiera is much better suited at keeping your class parameter data organized compared to using an ENC for this.  

<h2>The "parameters" key</h2>
This basically let's you attach variables to your puppet agent. These are facter, but in this case the puppet agent's don't derive the values, instead these values are defined by the ENC. 

One possible use for this, is that you can use this way to attach metadata to your puppet agent (although <a href="http://codingbee.net/tutorials/puppet/puppet-external-facts/">external facts</a> might be a better way of doing this). For example your puppet agent may be provisioned for use in IT project called "Genesis" in order to build an IT system called "Skynet". In that case you can attach these metadata to your puppet agent like this:

<pre> 
[root@puppetmaster yaml]# cat /tmp/node-definitions/puppetagent1.codingbee.dyndns.org.yaml
---

classes:
  - user_account:

parameters:
  - project_name: Genesis
  - system_name:  Skynet    
[root@puppetmaster yaml]#
</pre>

These paremeters are then available to the puppetmaster in the same style as any order facter data. Hence you can use them as part of things like puppet if-else statements, and generating contents for files.  

for example if the user_account module's init.pp contains:

<pre>
[root@puppetmaster yaml]# cat /etc/puppet/modules/user_account/manifests/init.pp
class user_account ($username = "tempuser"){

  user { "$username":
    ensure => present,
    shell  => '/bin/bash',
  }


  file {"/tmp/testfile.txt":
    ensure => file,
    content => "The project name is: $project_name and the IT system name is $system_name. \n",
  }

}
[root@puppetmaster yaml]#
</pre>   

In that case, if we do a puppet run, we get:

<pre>
[root@puppetagent1 tmp]# cat /tmp/testfile.txt
cat: /tmp/testfile.txt: No such file or directory
[root@puppetagent1 tmp]# puppet agent -t
Info: Retrieving pluginfacts
Info: Retrieving plugin
Info: Caching catalog for puppetagent1.codingbee.dyndns.org
Info: Applying configuration version '1424636188'
Notice: /Stage[main]/User_account/File[/tmp/testfile.txt]/ensure: defined content as '{md5}4a0f48e055fa3de603280a5e268d4c26'
Notice: Finished catalog run in 0.04 seconds
[root@puppetagent1 tmp]# cat /tmp/testfile.txt
The project name is: Genesis and the IT system name is Skynet.
[root@puppetagent1 tmp]#

</pre>


<h2>The "environment" key</h2>
This is linked related to the <a href="https://docs.puppetlabs.com/references/latest/configuration.html#environment">Puppet agent's environment config setting</a>, under the [main] section. 

This setting is specifically used for setting up a powerful feature called <a href="https://docs.puppetlabs.com/puppet/latest/reference/environments.html#about-directory-environments">directory environments</a>.

Normally you set this in the puppet agent's puppet.conf file, e.g.: 

<pre>
[root@puppetagent1 tmp]# cat /etc/puppet/puppet.conf
[main]
    # The Puppet log directory.
    # The default value is '$vardir/log'.
    logdir = /var/log/puppet

    # Where Puppet PID files are kept.
    # The default value is '$vardir/run'.
    rundir = /var/run/puppet

    # Where SSL certificates are kept.
    # The default value is '$confdir/ssl'.
    ssldir = $vardir/ssl

    server=puppetmaster.codingbee.dyndns.org
<strong>
    environment=test                              # here</strong>

[agent]
    # The file in which puppetd stores a list of the classes
    # associated with the retrieved configuratiion.  Can be loaded in
    # the separate ``puppet`` executable using the ``--loadclasses``
    # option.
    # The default value is '$confdir/classes.txt'.
    classfile = $vardir/classes.txt

    # Where puppetd caches the local configuration.  An
    # extension indicating the cache format is added automatically.
    # The default value is '$confdir/localconfig'.
    localconfig = $vardir/localconfig
[root@puppetagent1 tmp]#

</pre>

So that:

<pre>
[root@puppetagent1 /]# puppet config print environment
test
[root@puppetagent1 /]#
</pre>

However individually logging into all the agents to do this can be tedious. Hence, it's easier to set this centrally.  

]]></Content>
		<Date><![CDATA[2015-02-22]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - What is Foreman]]></Title>
		<Content><![CDATA[Foreman is a web based puppet management that has 3 core functions:


<ul>
	<li>provision VMs</li>
	<li>ENC for Puppet</li>
	<li>reporting</li>
</ul>

]]></Content>
		<Date><![CDATA[2015-02-22]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PostgreSQL - Automate PostgreSQL installation/setup using Puppet]]></Title>
		<Content><![CDATA[This guide will walk you through how to use puppet to automate the steps we took manually when we followed the <a href="http://codingbee.net/tutorials/postgresql/postgresql-install-postgresql-and-then-create-a-db-and-user-account/" title="PostgreSQL – Install PostgreSQL, and then create a DB and User Account">postgreSQL installation and setup guide</a>.


based on the manual guide, we need to write puppet code to:

1. Install the correct version of postgres, in my case that is postgreSQL 9.3.
2. Create a new DB. 
3. Create a user. 
4. Assign user full access privileges to the db
5. enable remote access. 

All of the above can be achieved with the puppetlab's <a href="https://forge.puppetlabs.com/puppetlabs/postgresql">postgresql puppet module</a> and the following node definition in the site.pp file:


<pre>

node puppetagent01 {

  # This sets some of the defaults..
  class {'postgresql::globals':
    version => '9.3',
    manage_package_repo => true,
  }
  ->

  # This installs the postgres software for the version specified above..
  class { 'postgresql::server':.
    # ip_mask_allow_all_users    => '0.0.0.0/0',
    listen_addresses           => '*',
  }
  ->
  # This creates a new db called "reviewdb" and at the same time also creates a new db account called "gerritdbuser".
  # which is automatically granted full access to the new db.
  postgresql::server::db { 'reviewdb':
    user     => 'gerritdbuser',
    password => postgresql_password('gerritdbuser', 'admin123'),
  }

  postgresql::server::pg_hba_rule { 'allow application network to access app database':
    description => "Open up postgresql for access from 0.0.0.0/0",
    type => 'host',
    database => 'reviewdb',
    user => 'gerritdbuser',
    address => '0.0.0.0/0',
    auth_method => 'trust',
  }


}

</pre>



 
See also:

http://blog.2ndquadrant.com/devops-testing-postgresql-with-vagrant-on-your-computer-part-two/]]></Content>
		<Date><![CDATA[2015-02-23]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PostgreSQL|Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PostgreSQL]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Linux - The pam_tally2 command]]></Title>
		<Content><![CDATA[http://www.tecmint.com/use-pam_tally2-to-lock-and-unlock-ssh-failed-login-attempts/]]></Content>
		<Date><![CDATA[2015-02-25]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[pam_tally2]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Git - git stash]]></Title>
		<Content><![CDATA[temporary save files then do git pull

git stash   # car park currently changed files. 

git pull

git stash pop   # overwrite pulled files. ]]></Content>
		<Date><![CDATA[2015-02-26]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[git]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>git]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Linux - Pluggable Authentication Modules (PAM)]]></Title>
		<Content><![CDATA[<a href="http://www.linux-pam.org/Linux-PAM-html/Linux-PAM_SAG.html">The Linux-PAM System Administrators Guide</a>

http://linux.die.net/man/5/system-auth-ac
http://linux.die.net/man/8/authconfig

http://www.linuxgeek.net/documentation/authentication.phtml

https://www.google.co.uk/search?q=authconfig+enablewinbindauth+&ie=utf-8&oe=utf-8&aq=t&rls=org.mozilla:en-GB:official&client=firefox-a&channel=sb&gfe_rd=cr&ei=0XHwVKC5C4OP-wap0oCADA#safe=off&rls=org.mozilla:en-GB:official&channel=sb&q=edit+%2Fetc%2Fpam.d%2Fsystem-auth


]]></Content>
		<Date><![CDATA[2015-02-27]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[pam]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Linux - Download a file using "curl" and "wget"]]></Title>
		<Content><![CDATA[To download a file from the internet using "curl" command :

=&gt; Copy the url for the file needs to be downloaded.

In the shell, type the command "curl", then right click. This will paste the url copied earlier. In the same line type the command "-o". This command assign the url to a filename. Then enter the filename in the "filename. extention".

<pre>Curl url -o "filename.ext"</pre>

&nbsp;

=&gt; copy the url of the file from the internet.

In the shell, type the command "wget". Then type the command "-o" which will assign the url to a filename.  Then type the "filename.ext". then right click which will paste the  url in the shell.

wget -o "filename.ext" url

&nbsp;

P.S: .ext means extention. it define what the file is. suppose text.txt or zipper.zip

&nbsp;]]></Content>
		<Date><![CDATA[2015-02-28]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[curl|wget]]></Tags>
		<Status><![CDATA[private]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Linux - Zip and unzip files]]></Title>
		<Content><![CDATA[To zip a single file a in linux
<pre>gzip filename</pre>

To unzip a single file in linux 
<pre> gunzip filename </pre>

To compress a directory the command is 
<pre> tar -cf filename.tar filename</pre>
filename.tar will be the new name of zipped file for filename.
-cf means creat a file
When compressing a directory it is important not to be in the directory. The command will not work otherwise.

To extract the filename.tar file the command is 
<pre> tar -xf goat.tar</pre>
-xf means extracting a file.
]]></Content>
		<Date><![CDATA[2015-02-28]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Centos|RHEL|unzip|zip]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - The Augeas command line utility]]></Title>
		<Content><![CDATA[So far we have covered how to ensure the content of a whole file, by using static sample-files which are housed in a puppet module#s "files" folder, or by using .erb files which are stored in the module's "templates" folder. 

However there will be times when you don't want to ensure the content of the whole file, instead you just want to ensure the content of part of a file. For example you may just want to add a line to the /etc/hosts file. 

This is especially the case when dealing with config files that are part of the OS. For example /etc/hosts and /etc/fstab. Other Sys admins may make manual changes to these files. so if you ensure the state of this file using static-files/templates, then it will end up constantly over-riding manual changes made by the system administrators. 

Hence the need to ensure a file's state at a more granular line/section level rather than at a file level. 

Controlling the state of a certain line or (group of lines) is present in a given file is possible in puppet thanks to the <a href="https://docs.puppetlabs.com/references/latest/type.html#augeas">augeas resource type</a>.   


<a href="http://augeas.net/index.html">Augeas</a> is actually a standalone tool, and you can use it completely seperately from Puppet. However Augeas is so powerful and useful, that Puppetlabs decided to develop the <a href="https://docs.puppetlabs.com/references/latest/type.html#augeas">Augeas resource type</a> in order for puppet to natively support it.

<h2>Using Augeas on it's own</h2>
For the time being let's forget about Puppet, and just look at Augeas on it's own. 

To begin with, you first need to install augeas:

<pre>
yum install augeas
</pre> 

The main Augeas standalone tool is a commandline shell, called "augtool":

 

<pre>
[root@puppetmaster /]# rpm -ql augeas
/usr/bin/augparse
/usr/bin/augtool
/usr/bin/fadot
/usr/share/man/man1/augparse.1.gz
/usr/share/man/man1/augtool.1.gz
/usr/share/vim/vimfiles/ftdetect/augeas.vim
/usr/share/vim/vimfiles/syntax/augeas.vim
[root@puppetmaster augeas]# 
</pre>

Hence to start a new augtool, we do:

<pre>
[root@puppetmaster /]# augtool
augtool>
</pre>

You can exit the augtool terminal using the quit command. 


<pre>
[root@puppetmaster /]# augtool
augtool>
</pre>


Use the "help" command to see what other commands are available under augtool:


<pre>
[root@puppetmaster /]# augtool
augtool> help

Admin commands:
  help       - print help
  load       - (re)load files under /files
  quit       - exit the program
  retrieve   - transform tree into text
  save       - save all pending changes
  store      - parse text into tree
  transform  - add a file transform

Read commands:
  dump-xml   - print a subtree as XML
  get        - get the value of a node
  label      - get the label of a node
  ls         - list children of a node
  match      - print matches for a path expression
  print      - print a subtree
  span       - print position in input file corresponding to tree

Write commands:
  clear      - clear the value of a node
  clearm     - clear the value of multiple nodes
  ins        - insert new node
  insert     - insert new node (alias of 'ins')
  mv         - move a subtree
  move       - move a subtree (alias of 'mv')
  rename     - rename a subtree label
  rm         - delete nodes and subtrees
  set        - set the value of a node
  setm       - set the value of multiple nodes
  touch      - create a new node

Path expression commands:
  defnode    - set a variable, possibly creating a new node
  defvar     - set a variable

Type 'help <command>' for more information on a command

augtool> 

</pre>

The augeas tool view files in the form of an object. It can view this object in the form of a hierarchial tree structure. 


Augeas (aka augtool) is used for querying and editing config files from the command line. Augtool lets you navigate/drill-down to a particular part of a config file. 

In our guide, we will use the /etc/hosts file as an example. In our example, the hosts file currently contains:

<pre>
[root@puppetmaster /]# cat /etc/hosts
127.0.0.1 localhost                                            
127.0.1.1 puppetmaster.codingbee.net puppetmaster
10.1.172.10 puppetmaster.codingbee.net puppet puppetmaster
10.1.172.11 puppetagent01.codingbee.net puppetagent01
10.1.172.12 puppetagent02.codingbee.net puppetagent02
10.1.172.13 puppetagent03.codingbee.net puppetagent03
[root@puppetmaster /]# 
</pre>
Note, as you can see above, we have 6 entries. 



However augeas cannot query/edit any config file, but only those that it is has a schema (aka lens) for. There are a set of <a href="http://augeas.net/stock_lenses.html">stock lenses</a> that comes with augeas by default. These lens are stored in:


<pre>
[root@puppetmaster dist]# pwd
/usr/share/augeas/lenses/dist
[root@puppetmaster dist]# ls -l 
total 840
-rw-r--r-- 1 root root  3550 Nov  2  2012 access.aug
-rw-r--r-- 1 root root  1485 Feb 10 10:51 activemq_conf.aug
-rw-r--r-- 1 root root   841 Feb 10 10:51 activemq_xml.aug
-rw-r--r-- 1 root root  2231 Nov  2  2012 aliases.aug
-rw-r--r-- 1 root root  2564 Nov  2  2012 anacron.aug
.
.
.
...etc
[root@puppetmaster dist]# 
</pre>

Let's just take at the hosts.aug lens file:

<pre>
[root@puppetmaster dist]# cat hosts.aug 
(* Parsing /etc/hosts *)

module Hosts =
  autoload xfm

  let sep_tab = Util.del_ws_tab
  let sep_spc = Util.del_ws_spc

  let eol = Util.eol
  let indent = Util.indent

  let comment = Util.comment
  let comment_or_eol = Util.comment_or_eol
  let empty   = [ del /[ \t]*#?[ \t]*\n/ "\n" ]

  let word = /[^# \n\t]+/
  let record = [ seq "host" . indent .
                              [ label "ipaddr" . store  word ] . sep_tab .
                              [ label "canonical" . store word ] .
                              [ label "alias" . sep_spc . store word ]*
                 . comment_or_eol ]

  let lns = ( empty | comment | record ) *

  let xfm = transform lns (incl "/etc/hosts")
[root@puppetmaster dist]# 
</pre>

Don't worry too much about what all this means, but notice that there are 3 labels defined here, "ipaddr", "canonical", and "alias". These labels are used to label different parts of a config file's line/section. 

Now let's take a look at how to query values in the /etc/hosts file:









To do this we use the augtool's "ls" command.

The "ls" command actually let's you navigate to a particular file you want to change, and then lets you drill down further to a particular line of that file:

<pre>
[root@puppetmaster /]# augtool
augtool> ls /
augeas/ = (none)
files/ = (none)
</pre>

Now since we want to query a file, we drill down into files:

<pre>
augtool> ls /files
etc/ = (none)
boot/ = (none)
augtool> 

</pre>


Since the hosts file resides in /etc folder, we now go there:


<pre>

augtool> ls /files/etc/
puppet/ = (none)
hosts/ = (none)      # this is the one we are interested in. 
.
.
.
....etc
augtool> 

</pre>

Notice that the "hosts/" appears as a directory. That's augeas way of telling us that we can drill down further into the file itself. 

Now let's drill down into hosts file:

<pre>
[root@puppetmaster /]# cat /etc/hosts
augtool> ls /files/etc/hosts
1/ = (none)
2/ = (none)
3/ = (none)
4/ = (none)
5/ = (none)
6/ = (none)
augtool> 
</pre>

The 1-6 entries matches up with the lines in the /etc/hosts file that we saw earlier. 

Now we can pick a line to drill down to a particular line, e.g. let's pick line 3:


<pre>
augtool> ls /files/etc/hosts/3
ipaddr = 10.1.172.10
canonical = puppetmaster.codingbee.net
alias[1] = puppet
alias[2] = puppetmaster
augtool> 

</pre>

At this point we have drilled down to a particular node, and hence we now see labels along with the value it houses. We can now use the "get" command to display a particular label's value:


<pre>
augtool> get /files/etc/hosts/3/ipaddr
/files/etc/hosts/3/ipaddr = 10.1.172.10
augtool> 
</pre>

We can now edit this value using the set command, e.g. let's change it from "10.1.172.10" to "8.8.8.8". We do this using the "set" command:

<pre>
augtool> set /files/etc/hosts/3/ipaddr 8.8.8.8
augtool> get /files/etc/hosts/3/ipaddr
/files/etc/hosts/3/ipaddr = 8.8.8.8
augtool> ls /files/etc/hosts/3
ipaddr = 8.8.8.8
canonical = puppetmaster.codingbee.net
alias[1] = puppet
alias[2] = puppetmaster
augtool> 
</pre>

Now that we have checked the change, and are happy with it, we can then save it and exit:


<pre>
augtool> save
Saved 1 file(s)
augtool> quit
[root@puppetmaster /]# 
</pre>


Now if we check the content of the /etc/hosts file:

<pre>
[root@puppetmaster /]# cat /etc/hosts
127.0.0.1 localhost
127.0.1.1 puppetmaster.codingbee.net puppetmaster
8.8.8.8 puppetmaster.codingbee.net puppet puppetmaster         
10.1.172.11 puppetagent01.codingbee.net puppetagent01
10.1.172.12 puppetagent02.codingbee.net puppetagent02
10.1.172.13 puppetagent03.codingbee.net puppetagent03
[root@puppetmaster /]# 
</pre>

As you can see the 3 config item's ip address has now been changed to 8.8.8.8.

In this example, we just edited an existing line. You can also <a href="http://augeas.net/tour.html">append a new line</a>, delete lines, insert lines....etc. 

Another useful command is the print command. This let's you view a file in the form of an object:

<pre>
augtool> print /files/etc/hosts
/files/etc/hosts
/files/etc/hosts/1
/files/etc/hosts/1/ipaddr = "127.0.0.1"
/files/etc/hosts/1/canonical = "localhost\r"
/files/etc/hosts/2
/files/etc/hosts/2/ipaddr = "127.0.1.1"
/files/etc/hosts/2/canonical = "puppetmaster.codingbee.net"
/files/etc/hosts/2/alias = "puppetmaster\r"
/files/etc/hosts/3
/files/etc/hosts/3/ipaddr = "10.1.172.10"
/files/etc/hosts/3/canonical = "puppetmaster.codingbee.net"
/files/etc/hosts/3/alias[1] = "puppet"
/files/etc/hosts/3/alias[2] = "puppetmaster\r"
/files/etc/hosts/4
/files/etc/hosts/4/ipaddr = "10.1.172.11"
/files/etc/hosts/4/canonical = "puppetagent01.codingbee.net"
/files/etc/hosts/4/alias = "puppetagent01\r"
/files/etc/hosts/5
/files/etc/hosts/5/ipaddr = "10.1.172.12"
/files/etc/hosts/5/canonical = "puppetagent02.codingbee.net"
/files/etc/hosts/5/alias = "puppetagent02\r"
/files/etc/hosts/6
/files/etc/hosts/6/ipaddr = "10.1.172.13"
/files/etc/hosts/6/canonical = "puppetagent03.codingbee.net"
/files/etc/hosts/6/alias = "puppetagent03\r"

</pre>
 





]]></Content>
		<Date><![CDATA[2015-03-02]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - The Augeas resource type]]></Title>
		<Content><![CDATA[In the previous demo, we saw how to makd a change via the command line. We used the "/etc/hosts" file as the example. We'll now show you how to make the same change, but this time with the <a href="https://docs.puppetlabs.com/references/latest/type.html#augeas">augeas puppet resource</a>. 


You can therefore do the same thing as via the command line using the following resource:

<pre>
  augeas{"hostfile" :
    context    => "/files/etc/hosts/3/",
    changes  => "set ipaddr 8.8.8.8",
  }
</pre>


If you want to append to a file, then you need to create a new node. You can achieve this using the <a href="https://github.com/hercules-team/augeas/wiki/Adding-nodes-to-the-tree">last()</a> function. 
 

<h3>
the "/etc/sudoers" file</h3>
Let's say you want to add the following line to your /etc/sudoers file:

<pre>
Defaults:tom !requiretty
tom ALL = (ALL) ALL
</pre>

This can be achieved using the following resource:

<pre>
  augeas { "sudotom":
    context => "/files/etc/sudoers",
    changes => [
      "set Defaults[type=':tom']/type :tom",
      "set Defaults[type=':tom']/requiretty/negate ''",
      "set spec[user = 'tom']/user tom",
      "set spec[user = 'tom']/host_group/host ALL",
      "set spec[user = 'tom']/host_group/command ALL",
      "set spec[user = 'tom']/host_group/command/runas_user ALL",
    ],
  }
</pre> 






See also:
https://docs.puppetlabs.com/guides/augeas.html 


]]></Content>
		<Date><![CDATA[2015-03-02]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Jenkins - Automate the creation of new Jenkins jobs]]></Title>
		<Content><![CDATA[It is possible to create a jenkins job that when run:

- scans git for any newly created repo. 
- creates a new jenkins job for new repos. 


This is achieved using the the following plugin:

https://wiki.jenkins-ci.org/display/JENKINS/Job+DSL+Plugin

as well as writing some groovy script. 

This jenkins job can then be run as an hourly cron job, or manually, as and when needed. 

Also see:


https://github.com/jenkinsci/job-dsl-plugin/wiki/Tutorial---Using-the-Jenkins-Job-DSL

https://wiki.jenkins-ci.org/display/JENKINS/Jenkins+CLI

]]></Content>
		<Date><![CDATA[2015-03-06]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Jenkins]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Git - Installing Git]]></Title>
		<Content><![CDATA[On Redhat or CentOS:

<pre>yum install git</pre>


On windows you need to go here:

http://git-scm.com/download/win

alternatively you can install github for windows which comes prepackaged with git:

https://windows.github.com/

Also see:
http://git-scm.com/download/linux
]]></Content>
		<Date><![CDATA[2015-03-07]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[git]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>git]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Git - Setting up Git and respective config files]]></Title>
		<Content><![CDATA[Right after installing Git, you need to configure git before you can start using it.

There are 3 files where git configuration settings are stored. These files actually stores the same info but at different scopes, repo scope, user scope, and machine level scope:


<ul>
	<li><code>/etc/gitconfig</code> - machine level</li>
	<li><code>~/.gitconfig</code> or <code>~/.config/git/config</code>  - user level</li>
	<li> config file in a Git directory (i.e., <code>.git/config</code>) of whatever repository you’re currently using. Specific to that single repository. Repo level</li>
</ul>

Note: These locations will be different on a windows machine. 

These config files have over-ride feature, where the repo-level settings has the highest power, followed by user level (aka global), and then finally machine level.  

You can read/edit these files straight from the command line. For example to get a list of all overall active git settings you do:

<pre>
[root@localhost ~]# git config --list        # this lists settings after all over-rides are evaluated.
fatal: error processing config file(s)
[root@localhost ~]# git config --list --global
fatal: unable to read config file '/root/.gitconfig': No such file or directory
[root@localhost ~]# git config --list --system
fatal: unable to read config file '/etc/gitconfig': No such file or directory
[root@localhost ~]#
</pre> 

As you can see all of the above commands have failed. That's because none of these config files exists yet. however they get generated when apply setting from the command line. The two main settings that you need to set before you can start using git are what your name and email address is. Git needs this info so that git tags all the commits with these details for everyone to keep track of who commited what. You can set the name and email-address setting like this:


<pre>
[vagrant@localhost ~]$ git config --global user.name "Sher Chowdhury"
[vagrant@localhost ~]$ git config --global user.email "Sher.Chowdhury@codingbee.net"
[vagrant@localhost ~]$
</pre>

Running the above commands generate the following file in the user's home directory where these info are stored:

<pre>
[vagrant@localhost ~]$ cat .gitconfig
[user]
        name = Sher Chowdhury
        email = Sher.Chowdhury@codingbee.net
[vagrant@localhost ~]$

</pre>


Now we can view this info from the command line:

<pre>
[vagrant@localhost ~]$ git config --list
user.name=Sher Chowdhury
user.email=Sher.Chowdhury@codingbee.net
[vagrant@localhost ~]$ git config --list --global
user.name=Sher Chowdhury
user.email=Sher.Chowdhury@codingbee.net
[vagrant@localhost ~]$ 
</pre>

Note, if you omit the "global" scope setting, then git will automatically apply the setting at the repo level, if it exists, otherwise it fails:

<pre>
[vagrant@localhost ~]$ git config user.name "Sher Chowdhury"
error: could not lock config file .git/config: No such file or directory
[vagrant@localhost ~]$
</pre>

Now here's how you do it at the system level:

<pre>
[vagrant@localhost ~]$ sudo git config --system user.email "Sher.Chowdhury@codingbee.net"
[vagrant@localhost ~]$ cat /etc/gitconfig
[user]
        email = Sher.Chowdhury@codingbee.net
[vagrant@localhost ~]$ git config --list --system
user.email=Sher.Chowdhury@codingbee.net
[vagrant@localhost ~]$

</pre>]]></Content>
		<Date><![CDATA[2015-03-07]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[git]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>git]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Git - The Git command line]]></Title>
		<Content><![CDATA[The best way to use git is from the command line. There are also a bunch of gui git clients available. These gui clients are essentially a wrapper around the git command line. However none of the gui clients fully utilise the git command line. Hence it is best to invest the time in learning how to use the git command line so that you have full control over git. 

On windows, you can use git from the powershell terminal. However the <a href="http://git-scm.com/downloads">git installation</a> also comes with it's own git terminal called "git-bash" which enhance your experience in using git (e.g. syntax highlighting). 


Here's git's main help info:


<pre>
[vagrant@localhost ~]$ git help
usage: git [--version] [--exec-path[=GIT_EXEC_PATH]] [--html-path]
           [-p|--paginate|--no-pager] [--no-replace-objects]
           [--bare] [--git-dir=GIT_DIR] [--work-tree=GIT_WORK_TREE]
           [--help] COMMAND [ARGS]

The most commonly used git commands are:
   add        Add file contents to the index
   bisect     Find by binary search the change that introduced a bug
   branch     List, create, or delete branches
   checkout   Checkout a branch or paths to the working tree
   clone      Clone a repository into a new directory
   commit     Record changes to the repository
   diff       Show changes between commits, commit and working tree, etc
   fetch      Download objects and refs from another repository
   grep       Print lines matching a pattern
   init       Create an empty git repository or reinitialize an existing one
   log        Show commit logs
   merge      Join two or more development histories together
   mv         Move or rename a file, a directory, or a symlink
   pull       Fetch from and merge with another repository or a local branch
   push       Update remote refs along with associated objects
   rebase     Forward-port local commits to the updated upstream head
   reset      Reset current HEAD to the specified state
   rm         Remove files from the working tree and from the index
   show       Show various types of objects
   status     Show the working tree status
   tag        Create, list, delete or verify a tag object signed with GPG

See 'git help COMMAND' for more information on a specific command.
[vagrant@localhost ~]$

</pre>

However this doesn't list all the commands, just the most commonly used git commands. If you do "git help help", then you'll discover there is an "--all" option, which if you try, you'll discover a lot more git commands:

<pre>
[vagrant@localhost ~]$ git help --all
usage: git [--version] [--exec-path[=GIT_EXEC_PATH]] [--html-path]
           [-p|--paginate|--no-pager] [--no-replace-objects]
           [--bare] [--git-dir=GIT_DIR] [--work-tree=GIT_WORK_TREE]
           [--help] COMMAND [ARGS]

available git commands in '/usr/libexec/git-core'
-------------------------------------------------
  add                 filter-branch       merge-subtree       rev-list
  add--interactive    fmt-merge-msg       merge-tree          rev-parse
  am                  for-each-ref        mergetool           revert
  annotate            format-patch        mktag               rm
  apply               fsck                mktree              send-pack
  archive             fsck-objects        mv                  shell
  bisect              gc                  name-rev            shortlog
  bisect--helper      get-tar-commit-id   notes               show
  blame               grep                pack-objects        show-branch
  branch              hash-object         pack-redundant      show-index
  bundle              help                pack-refs           show-ref
  cat-file            http-backend        patch-id            stage
  check-attr          http-fetch          peek-remote         stash
  check-ref-format    http-push           prune               status
  checkout            imap-send           prune-packed        stripspace
  checkout-index      index-pack          pull                submodule
  cherry              init                push                symbolic-ref
  cherry-pick         init-db             quiltimport         tag
  clean               instaweb            read-tree           tar-tree
  clone               log                 rebase              unpack-file
  commit              lost-found          rebase--interactive unpack-objects
  commit-tree         ls-files            receive-pack        update-index
  config              ls-remote           reflog              update-ref
  count-objects       ls-tree             relink              update-server-info
  describe            mailinfo            remote              upload-archive
  diff                mailsplit           remote-ftp          upload-pack
  diff-files          merge               remote-ftps         var
  diff-index          merge-base          remote-http         verify-pack
  diff-tree           merge-file          remote-https        verify-tag
  difftool            merge-index         repack              web--browse
  difftool--helper    merge-octopus       replace             whatchanged
  fast-export         merge-one-file      repo-config         write-tree
  fast-import         merge-ours          request-pull
  fetch               merge-recursive     rerere
  fetch-pack          merge-resolve       reset

See 'git help COMMAND' for more information on a specific command.
[vagrant@localhost ~]$

</pre>


]]></Content>
		<Date><![CDATA[2015-03-07]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[git]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>git]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Git - Create your first local git repo]]></Title>
		<Content><![CDATA[There's 2 ways that you can get started with using git:

<ul>
	<li>Work on an existing project that is already being version-controlled with git.</li>
	<li>Create a new project that you want to version-control and share with others using Git. </li>
</ul>

A project in this context, can be anything, e.g. the source code for a java based project, or a c# project, or an android project....etc. 
  



If you are creating a brand new project, then you need to first create your git repo. If you want to work on an existing git-managed project, then you need to clone that project from the git-server, which is covered in the next lesson. 

To create a git repo, you use the "init" command

<pre>[vagrant@localhost ~]$ mkdir demo1
[vagrant@localhost ~]$ cd demo1/
[vagrant@localhost demo1]$ ls -la | grep ".git"
[vagrant@localhost demo1]$ git init
Initialized empty Git repository in /home/vagrant/demo1/.git/
[vagrant@localhost demo1]$ ls -la | grep ".git"
drwxrwxr-x  7 vagrant vagrant 4096 Mar  8 13:12 .git
[vagrant@localhost demo1]$
</pre>

At this stage this git repo doesn't know which files needs to be tracked by this repo. Hence git is not tracking anything yet. Git also will not automatically track any files/folders that sites alongside the .git folder. It is up to you to tell what files/folders needs to track. As a result you see the following when you check your git repo's status:


<pre>
[vagrant@localhost demo1]$ git status
# On branch master
#
# Initial commit
#
nothing to commit (create/copy files and use "git add" to track)
[vagrant@localhost demo1]$

</pre>





Once you have version controlled your project, you can share your project with others. The best way to do this is by pushing your repo to your companies git-server, or if you want to share it publically, then push it so a public git-service, e.g. github.com. 



]]></Content>
		<Date><![CDATA[2015-03-08]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[git]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>git]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Git - Clone an existing repo]]></Title>
		<Content><![CDATA[Let's say that there is an existing git repo (that is on a git server) that you want to work with. In that case you can make an entire copy of that repo using the "clone" command. 

For example, let's say I want to work with the code that is in the following repo:


https://github.com/puppetlabs/puppetlabs-ntp


On this page, you'll find the following link:

https://github.com/puppetlabs/puppetlabs-ntp.git


This is the link you need to make clone the repo. So to clone this repo, we do the following:


<pre>
[vagrant@localhost Documents]$ ls -l
total 0
[vagrant@localhost Documents]$ git clone https://github.com/puppetlabs/puppetlabs-ntp.git
Initialized empty Git repository in /home/vagrant/Documents/puppetlabs-ntp/.git/
remote: Counting objects: 1688, done.
remote: Total 1688 (delta 0), reused 0 (delta 0), pack-reused 1688
Receiving objects: 100% (1688/1688), 343.95 KiB | 350 KiB/s, done.
Resolving deltas: 100% (826/826), done.
[vagrant@localhost Documents]$ ls -l
total 4
drwxr-xr-x 8 vagrant vagrant 4096 Mar  9 14:02 puppetlabs-ntp
</pre>

As you can see the clone command downloads the whole git repo in directory by the same name. Note if you want the folder to have some other name, e.g. "ntp", then do:

<pre>
$ git clone https://github.com/puppetlabs/puppetlabs-ntp.git ntp
</pre>

In the above example we use the https protocol, but there are other protocals, e.g. ssh that you can also use, if access to a repo requires authentication. 


Now let's take a look inside this folder:

<pre>
[vagrant@localhost Documents]$ cd puppetlabs-ntp/
[vagrant@localhost puppetlabs-ntp]$ ls -la
total 104
drwxr-xr-x 8 vagrant vagrant  4096 Mar  9 14:02 .
drwxr-xr-x 3 vagrant vagrant  4096 Mar  9 14:02 ..
-rw-rw-r-- 1 vagrant vagrant  5781 Mar  9 14:02 CHANGELOG.md
-rw-rw-r-- 1 vagrant vagrant  7965 Mar  9 14:02 CONTRIBUTING.md
-rw-rw-r-- 1 vagrant vagrant   131 Mar  9 14:02 .fixtures.yml
-rw-rw-r-- 1 vagrant vagrant   900 Mar  9 14:02 Gemfile
drwxrwxr-x 8 vagrant vagrant  4096 Mar  9 14:02 .git
-rw-rw-r-- 1 vagrant vagrant    83 Mar  9 14:02 .gitignore
drwxrwxr-x 3 vagrant vagrant  4096 Mar  9 14:02 lib
-rw-rw-r-- 1 vagrant vagrant 11346 Mar  9 14:02 LICENSE
drwxrwxr-x 2 vagrant vagrant  4096 Mar  9 14:02 manifests
-rw-rw-r-- 1 vagrant vagrant  1941 Mar  9 14:02 metadata.json
-rw-rw-r-- 1 vagrant vagrant   753 Mar  9 14:02 .nodeset.yml
-rw-rw-r-- 1 vagrant vagrant   495 Mar  9 14:02 Rakefile
-rw-rw-r-- 1 vagrant vagrant  8697 Mar  9 14:02 README.markdown
drwxrwxr-x 6 vagrant vagrant  4096 Mar  9 14:02 spec
-rw-rw-r-- 1 vagrant vagrant    43 Mar  9 14:02 .sync.yml
drwxrwxr-x 2 vagrant vagrant  4096 Mar  9 14:02 templates
drwxrwxr-x 2 vagrant vagrant  4096 Mar  9 14:02 tests
-rw-rw-r-- 1 vagrant vagrant   518 Mar  9 14:02 .travis.yml
[vagrant@localhost puppetlabs-ntp]$

</pre>

Here we have all project files (as of the latest commit). Hence we see the following when we check the status:


<pre>
[vagrant@localhost puppetlabs-ntp]$ git status
# On branch master
nothing to commit (working directory clean)

</pre>

The really cool thing about the .git folder is that it house the git repo's entire history. One of the things this means is that you can disconnect your workstation from the internet, delete the project files. , but still recover them again because all the project files are stored in the ".git" folder (in compressed form), along with all it's past history. 



In other word's the .git folder essentially acts as the project's time machine. This means you can use git to roll back your project to how it looked like at some point in the past. E.g. roll back to commit number 2. ]]></Content>
		<Date><![CDATA[2015-03-08]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[git]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>git]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Git - File states]]></Title>
		<Content><![CDATA[Each of the files in the working directory are in one of two high-level states:


<ol>
	<li><strong>tracked files</strong> - These are files that were in the last snapshot (aka); they can be in one of the following sub states:
<ol>

	<li><strong>unmodified</strong> - These are any files that haven't been modified since the last commit. They will still be included in the next commit, but remain as is.</li>
	<li><strong>modified</strong> - These are files that have been modified since the last commit (we probably modified these as part of bug fixes). These files will be included in the next commit, but will be included in thier respective new form. </li>
	<li><strong>staged</strong> - These are files that are either not present in the last commit (e.g. newly created files) or are "modified" files that we tell git to include in the next commit. Files are added to the staging using git's "add" command.  </li>
</li>
</ol>

	<li><strong>untracked files</strong> - these are any files that are not being tracked, i.e. git isn't of the existance of these files. You can change the state of these files to "tracked - staged" by using git's "add" command</li>
</ol>

<h3>
In the case of cloning an exisiting git repo</h3>
Now if you clone a git repo, then all the project files start off in the "unmodified" state
<pre>
                  |----------------------Tracked----------------------------| 
                  |                                                         |
Untracked             Unmodified             Modified                Staged 
    |                      |                     |                     |
    |                      |                     |                     |
    |                      |                     |                     |
    |                      |                     |                     |
    |                      |                     |                     |
    |                      |                     |                     |
    |                      |                     |                     |
    |                      |                     |                     |
    |                      |                     |                     |
    |                      |                     |                     |
    |                      |                     |                     |
</pre> 

If you edit any of these files (e.g using a text editor such as vim), then these file's state changes to "modified"

<pre>

                  |----------------------Tracked----------------------------| 
                  |                                                         |
Untracked             Unmodified                 Modified                Staged 
    |                      |                      |                     |
    |                      |                      |                     |
    |                      |                      |                     |
    |                      | edit a tracked file  |                     |
    |                      | -------------------> |                     |
    |                      |                      |                     |
    |                      |                      |                     |
    |                      |                      |                     |
    |                      |                      |                     |
    |                      |                      |                     |
    |                      |                      |                     |

</pre>


At this stage, if you do a git "commit", the edited file will not get committed, that's because modified files first needs to be "staged" in order to be included in the next commit. This is done using git's "add" command:


<pre>

                  |----------------------Tracked----------------------------| 
                  |                                                         |
Untracked             Unmodified                 Modified                Staged 
    |                      |                      |                     |
    |                      |                      |                     |
    |                      |                      |                     |
    |                      | edit a tracked file  |                     |
    |                      | -------------------> |                     |
    |                      |                      | "add" modified files|
    |                      |                      | ------------------->|
    |                      |                      |                     |
    |                      |                      |                     |
    |                      |                      |                     |
    |                      |                      |                     |

</pre>


You also use git's "add" command to have new files "staged" so that they are included in the the next commit:


<pre>

                  |----------------------Tracked----------------------------| 
                  |                                                         |
Untracked             Unmodified               Modified               Staged 
    |                      |                      |                     |
    | Add new files to be included in the next commit                   |
    | ----------------------------------------------------------------->|
    |                      |                      |                     |
    |                      | edit tracked files   |                     |
    |                      | -------------------> |                     |
    |                      |                      | "add" modified files|
    |                      |                      | ------------------->|
    |                      |                      |                     |
    |                      |                      |                     |
    |                      |                      |                     |
    |                      |                      |                     |

</pre>

After that, once you are happy with everything, you then take a snapshot using git's "commit" command. This results in the files from the previous commit being overlayed with the newly staged files in order to create a new snapshot (aka commit). 

You then repeat the cycle again. 

Another thing you may want to do is remove a tracked file from the project. This is done using git's rm command:


<pre>

                  |----------------------Tracked----------------------------| 
                  |                                                         |
Untracked             Unmodified               Modified               Staged 
    |                      |                      |                     |
    | Add new files to be included in the next commit                   |
    | ----------------------------------------------------------------->|
    |                      |                      |                     |
    |                      | edit tracked files   |                     |
    |                      | -------------------> |                     |
    |                      |                      | "add" modified files|
    |                      |                      | ------------------->|
    |  remove file         |                      |                     |
    | <------------------- |                      |                     |
    |                      |                      |                     |
    |                      |                      |                     |

</pre>
   


When you are making edit's to a tracked files, git behind the scenes of what files are being modified. Hence when we check the status of a newly cloned repo, we see:

<pre>
[vagrant@localhost puppetlabs-ntp]$ git status
# On branch master
nothing to commit (working directory clean)
[vagrant@localhost puppetlabs-ntp]$

</pre>

However when we edit a unmodified-tracked file and check the status again:

<pre>
[vagrant@localhost puppetlabs-ntp]$ ls
CHANGELOG.md     Gemfile  LICENSE    metadata.json  README.markdown  templates
CONTRIBUTING.md  lib      manifests  Rakefile       spec             tests
[vagrant@localhost puppetlabs-ntp]$ echo "hello" >> README.markdown
[vagrant@localhost puppetlabs-ntp]$ git status
# On branch master
# Changed but not updated:
#   (use "git add <file>..." to update what will be committed)
#   (use "git checkout -- <file>..." to discard changes in working directory)
#
#       modified:   README.markdown
#
no changes added to commit (use "git add" and/or "git commit -a")
[vagrant@localhost puppetlabs-ntp]$

</pre>

As you can see we also get prompts on how to stage this file in order to be included in the next commit.

Now let's create a new file that is going to be part of the project:


<pre>
[vagrant@localhost puppetlabs-ntp]$ echo "hello" >> testfile.txt
[vagrant@localhost puppetlabs-ntp]$ git status
# On branch master
# Changed but not updated:
#   (use "git add <file>..." to update what will be committed)
#   (use "git checkout -- <file>..." to discard changes in working directory)
#
#       modified:   README.markdown
#
# Untracked files:
#   (use "git add <file>..." to include in what will be committed)
#
#       testfile.txt
no changes added to commit (use "git add" and/or "git commit -a")
[vagrant@localhost puppetlabs-ntp]$
</pre> 


As you can see, git has also discovered this new file, and is marked it as a an untracked file, along with how to get this filed as tracked-staged so to have it included in the next commit. 

If we now go ahead and do a commit, you'll discover from git's status that nothing has actually happened, since nothing has been put into staging:


<pre>

[vagrant@localhost puppetlabs-ntp]$ git commit
# On branch master
# Changed but not updated:
#   (use "git add <file>..." to update what will be committed)
#   (use "git checkout -- <file>..." to discard changes in working directory)
#
#       modified:   README.markdown
#
# Untracked files:
#   (use "git add <file>..." to include in what will be committed)
#
#       testfile.txt
no changes added to commit (use "git add" and/or "git commit -a")
[vagrant@localhost puppetlabs-ntp]$ git status
# On branch master
# Changed but not updated:
#   (use "git add <file>..." to update what will be committed)
#   (use "git checkout -- <file>..." to discard changes in working directory)
#
#       modified:   README.markdown
#
# Untracked files:
#   (use "git add <file>..." to include in what will be committed)
#
#       testfile.txt
no changes added to commit (use "git add" and/or "git commit -a")
[vagrant@localhost puppetlabs-ntp]$


</pre>




Let's now follow the above instructions to stage these 2 files:
<pre>
[vagrant@localhost puppetlabs-ntp]$ git add README.markdown
[vagrant@localhost puppetlabs-ntp]$ git add testfile.txt
[vagrant@localhost puppetlabs-ntp]$ git status
# On branch master
# Changes to be committed:
#   (use "git reset HEAD <file>..." to unstage)
#
#       modified:   README.markdown
#       new file:   testfile.txt
#
[vagrant@localhost puppetlabs-ntp]$ git commit -m "this is a demo"
[master 62cdcc6] this is a demo
 2 files changed, 2 insertions(+), 0 deletions(-)
 create mode 100644 testfile.txt
[vagrant@localhost puppetlabs-ntp]$ git status
# On branch master
# Your branch is ahead of 'origin/master' by 1 commit.
#
nothing to commit (working directory clean)
[vagrant@localhost puppetlabs-ntp]$

</pre>

Now everything is committed. 

Note, at this point, our commit is only stored on our local workstation. We need to do a git "push" to get our commit uploaded.


Tips:

There will be times when you have a lot of modified files that you want staged. In which case, it can get tedious to stage them one-by-one. However you can use the following to stage all modified files:

<pre>
git add -u
</pre>

If you want to stage all modified files as well as all untracked files, then do:


<pre>
git add -A
</pre> 

Note, this command will also factor in any tracked files that have been set to untracked too. 

Really useful link: http://stackoverflow.com/questions/572549/difference-between-git-add-a-and-git-add

Note, there isn't any obvious way to only stage new files and ignore any modified files. 
 

<h3>
In the case of creating a new git repo</h3>


With the new git repo, there are no commits to begin with. Hence all exisitng/newly-written project files start out as untracked files. We then use the "add" command to tracked-staged them, before we do the commit. ]]></Content>
		<Date><![CDATA[2015-03-09]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[git]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>git]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Git - The .gitignore]]></Title>
		<Content><![CDATA[There are some untracked files in your project file that you want git to completely ignore, e.g. temporary files, log files, etc. You can do this by creating a special new file called .gitignore which sits alongside the .git folder. this file can list everything you want ignored, here's an example of one:

<pre>
[vagrant@localhost puppetlabs-ntp]$ ls -la | grep git
drwxrwxr-x 8 vagrant vagrant  4096 Mar  9 15:46 .git
-rw-rw-r-- 1 vagrant vagrant    83 Mar  9 14:02 .gitignore
[vagrant@localhost puppetlabs-ntp]$ cat .gitignore
pkg/
Gemfile.lock
vendor/
spec/fixtures/
.vagrant/
.bundle/
coverage/
.idea/
*.iml
[vagrant@localhost puppetlabs-ntp]$
</pre>


Tip: if you are using eclipse, then you may want to add ".project" in the above file too. 


]]></Content>
		<Date><![CDATA[2015-03-09]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[git]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>git]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Git - Using diff]]></Title>
		<Content><![CDATA[]]></Content>
		<Date><![CDATA[2015-03-09]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[git]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>git]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Git - Renaming or moving files]]></Title>
		<Content><![CDATA[The best way to move or rename a file is doing it a via git's mv command, e.g.:

<pre>$ git mv filename.txt filename2.txt</pre>

This will not only make git aware that the file has been moved (or in this case renamed), but it will actually rename the file in the working directory. 

You can still use the traditional mv command, but this won't get picked up git, and so you have to manually tell git of this, like this:


<pre>
mv filename.txt filename2.txt
git rm filename.txt
git add filename2.txt
</pre>


Hence the <code>git mv</code> command can do all of the above, but in a single line. 


Finally you have to commit the change to take a snapshot for the above to be captured in git's internal db:


<pre>
git commit -m "filename.txt has been renamed to filename2.txt"
</pre>]]></Content>
		<Date><![CDATA[2015-03-09]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[git]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>git]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Git - Viewing past commits]]></Title>
		<Content><![CDATA[Simply run:


<pre>
git log
</pre>


the log command has a huge number of options available:


git log -2 # means only show the last 2 entries


git log -p -2 # same as above but also show diff data. 


git stat    # shows info in a more summarised form. 


]]></Content>
		<Date><![CDATA[2015-03-09]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[git]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>git]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Git - Undoing things]]></Title>
		<Content><![CDATA[There are 3 common things you may want to undo when using git:


<ul>
	<li>undo a commit</li>
	<li>unstage a file</li>
	<li>unmodify a file</li>
</ul>



If you have done several commits and you want to roll back several commits, then do:


<pre>git log -p</pre>


This will list commit id's pick the id you want to roll back to, then do:

<pre>
git reset --hard e0e7671b58895cfa1bbca4016238c693efaa4fbe
</pre>

This will rollback to a commit with the id above. Be careful with this command, because you cannot undo this. 




If you wan to delete a remote branch:

http://stackoverflow.com/questions/2003505/delete-a-git-branch-both-locally-and-remotely

Rename a local and remote branch:

http://stackoverflow.com/questions/1526794/rename-master-branch-for-both-local-and-remote-git-repositories  


http://lrotherfield.com/blog/delete-remote-git-repo-to-specific-commit/ 



if you have unstaged changes of a folder and it's contents, and you want to undo all the changes you made in that folder then do:


<pre>
git checkout -- {folder-name}
</pre>]]></Content>
		<Date><![CDATA[2015-03-09]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[git]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>git]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Git - Intro to branches]]></Title>
		<Content><![CDATA[To view a list of all branches do:


<pre>
git branch -a
</pre>

the one that has an asterix next to it is the one that is currently checked out. 

]]></Content>
		<Date><![CDATA[2015-03-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[git]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>git]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Overview of the file system hierarchy]]></Title>
		<Content><![CDATA[<h2>System Hierarchy overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What is the command the displays help info about your machine's high level folder structures?"]
$ man hier
[/toggle]
[/accordion]

<hr/>

<h2>System Hierarchy</h2>
At it's most fundamental level, Everything in Linux is a file. 

The top most folder in linux is called the root directory. This folder contains all the other folders and files. In Linux the root directory and is represent by "/". So to navigate to the root folder we do:


<pre>
$ cd /
</pre>

This contains the following:

<pre>
$ ls -l
total 98
dr-xr-xr-x.  2 root root  4096 Mar  9 09:33 bin
dr-xr-xr-x.  5 root root  1024 Oct 15 13:14 boot
drwxr-xr-x  18 root root  3480 Mar 13 10:49 dev
drwxr-xr-x. 98 root root  4096 Mar 13 10:49 etc
drwxr-xr-x.  3 root root  4096 Oct 15 23:54 home
dr-xr-xr-x. 11 root root  4096 Dec 24 15:51 lib
dr-xr-xr-x.  9 root root 12288 Mar  9 09:33 lib64
drwx------.  2 root root 16384 Oct 15 13:03 lost+found
drwxr-xr-x.  2 root root  4096 Sep 23  2011 media
drwxr-xr-x.  2 root root  4096 Sep 23  2011 mnt
drwxr-xr-x.  4 root root  4096 Oct 15 23:55 opt
dr-xr-xr-x  98 root root     0 Mar 13 10:49 proc
dr-xr-x---.  3 root root  4096 Dec 24 16:16 root
dr-xr-xr-x.  2 root root 12288 Mar  8 12:07 sbin
drwxr-xr-x.  2 root root  4096 Oct 15 13:03 selinux
drwxr-xr-x.  2 root root  4096 Sep 23  2011 srv
drwxr-xr-x  13 root root     0 Mar 13 10:49 sys
drwxrwxrwt.  7 root root  4096 Mar 14 16:19 tmp
drwxr-xr-x. 13 root root  4096 Oct 15 13:05 usr
drwxr-xr-x   2 root root  4096 Dec 24 15:39 vagrant
drwxr-xr-x. 21 root root  4096 Dec 24 15:51 var
</pre>
 
To get a brief idea of what are stored in each of these folders, you can view the <code> hier</code> man pages:

<pre>
$ man hier
</pre>
]]></Content>
		<Date><![CDATA[2015-03-14]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Uncommenting a line in a config file]]></Title>
		<Content><![CDATA[http://stackoverflow.com/questions/11843506/uncomment-a-line-in-puppet




This could be better:

http://augeasproviders.com/documentation/examples.html#shellvar-provider

https://forge.puppetlabs.com/herculesteam

]]></Content>
		<Date><![CDATA[2015-03-17]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Git - resetting Git.]]></Title>
		<Content><![CDATA[If you want to drop all your local changes and commits, fetch the latest history from the server and point your local master branch at it like this
<pre>git fetch origin
git reset --hard origin/master</pre>


http://git-scm.com/blog/2011/07/11/reset.html

]]></Content>
		<Date><![CDATA[2015-03-18]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[git]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>git]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - The file_in resource type]]></Title>
		<Content><![CDATA[This isn't specifically related to augeas, but is a similar tool in case augeas is not appropriate to use:

https://puppetlabs.com/blog/module-of-the-week-puppetlabsstdlib-puppet-labs-standard-library]]></Content>
		<Date><![CDATA[2015-03-18]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Using Augeasproviders]]></Title>
		<Content><![CDATA[While the augeas resource type is really powerful, it is relatively complicated to craft an augeas resource types. There are a few puppet modules that run on top of augeas and simplifies the use of augeas by providing custom resource types for editing specific config files:


<a href="http://augeasproviders.com/documentation/examples.html">http://augeasproviders.com/documentation/examples.html</a>

I think the shellvar module is for editing any variable=value type config files.   

]]></Content>
		<Date><![CDATA[2015-03-19]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - About this Course]]></Title>
		<Content><![CDATA[This course covers everything you need to know in order to pass the official Red Hat Certified Systems Administrator (RHCSA) exam.

There are a lot of different Linux Operating Systems, but only a small number of them are enterprise-grade level that are used by large corporations. One of the most popular enterprise-grade Linux OS is called RHEL, and this OS is made by Red Hat. RHEL is short for "Red Hat Enterprise Linux".

RHEL is quite expensive to buy. However there are free alternatives such as <a href="http://www.centos.org/" rel="nofollow">CentOS</a> and <a href="https://www.scientificlinux.org/" rel="nofollow">Scientific Linux</a>. These alternatives are identical to RHEL in every way except that all the red hat logos and branding have been replaced.

<h2>Get Certified</h2>
The RHCSA certification is considered to be the gold started standard in validating your Linux skills in the Corporate IT world. Therefore this certification puts you in a strong position in the Linux and Devops job market.

There is also the next level up Red Had certification, which is RHCE.


<h2>Why CentOS</h2>
Throughout this course we will be using CentOS. But everything we do will also work in exactly the same way as RHEL.



<h2>How this course is structured</h2>
This course is based on the <a href="http://www.redhat.com/training/courses/ex200/examobjective" rel="nofollow">RHCSA exam objectives</a> course. This means that this series of tutorials covers everything you need to know to pass the RHCSA exam!


Each lesson is broken down into the following sections:
<ul>
	<li>Commands (that are introduced in this lesson)</li>
	<li>Config files (files that will be viewing/editing)</li>
	<li>Course notes (this is the lesson's main content)</li>
</ul>

In this course we will be primarily focused on showing how to do everything via the terminal because it is a much faster way to perform task and also we want you to get comfortable using the shell. However there are some tasks that are easier to do via the GUI.

In the next tutorial we list out all the RHCSA exam objectives along with what which tutorials cover each objective.  
]]></Content>
		<Date><![CDATA[2015-03-22]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - How to create Kickstart files]]></Title>
		<Content><![CDATA[<h2>Overview to creating Kickstart files</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="How many sections are there in a kickstart file and what are they called?"]
There are 3 sections, they are:
- settings section
- pre-scripts section
- packages section
- post-scripts
[/toggle]
[toggle title="How many approaches are there to create a kickstart file and what are they?"]
There are 3 approaches:
- Perform an initial manual installation, to generate the kickstart
- Use the gui tool
- Manually write one
[/toggle]
[toggle title="What is the command to install to the GUI kickstart editor?"]
$ yum install system-config-kickstart
[/toggle]
[toggle title="If you have manually created/edited a kickstart file, then what command can you run to check that the syntax is correct?"]
$ ksvalidator /path/to/kickstart-file
[/toggle]
[toggle title="Which package do you need to install to get access ksvalidor?"]
$ yum install system-config-kickstart
[/toggle]
[toggle title="What is the command to locate a help guide for creating kickstart files?"]
$ rpm -qd pykickstart | grep "txt$"
/usr/share/doc/pykickstart-1.99.43.17/kickstart-docs.txt
[/toggle]
[/accordion]

<hr/>




<h2>Anatomy of a Kickstart file</h2>

Once you understand the contents of a kickstart file, it then becomes quite easy to customize it. The kickstart file is arranged into 4 sections. 


<ul>
	<li>Configuration commands</li>
	<li>pre section for writing preinstall scripts</li>
	<li>packages section</li>
	<li>post section</li>
</ul>

These sections neeeds to be declared in the same order as listed above. 


A lot of the stuff in the kickstart file is self explanatory, such as:

<pre>keyboard us        # This sets to keyboard to United States layout</pre>


The <strong>packages</strong> section. The packages section lists all the software that are to be installed during the OS install process:

<ul>
	<li>Lines beginning with an "@" symbol, Indicates a package group that is installed.</li>
	<li>Lines with no symbol represents individual packages that are installed.</li>
	<li>Lines beginning with an "-" symbol, indicates packages that will be excluded from the install.</li>
</ul>



By default a Kickstart file will only create the root user. However you can create other users during the install prcocess by inserting the following entry:


<pre>
user --name=vagrant --plaintext --password vagrant --groups=vagrant,wheel
</pre>

In this example we created a user called "vagrant". 

Note, For security purpose, you should always &lt;l=>encrypt the password before inserting into the kickstart file.

You might find the partition section is commented out, that's because machines can have different hardware in terms of number/capacity of hdds. Therefore as a precaution, this bit is commented out so that you do this manually. But if all your machines have the same HDD setups (and you want all of them to have the same partition/LV setup) then you can uncomment this bit as well. Once you are happy with your kickstart file, you can then start to use kickstart to automate Centos installations.





<h2>Creating a kickstart file</h2>

A Kickstart file automatically gets created by the “anaconda installer” at the end of a RHEL installation. It is saved at the root user’s home directory, and has the name <code>/root/anaconda-ks.cfg</code>





There are main 3 ways to create a kickstart file:

<h3>Approach 1 - Carry out a normal Centos/RHEL installation</h3>
The chance are that you have unknowing already created a ks file the last time you install Centos. It was automatically generated using the answers you selected at the time of the install. You should find the ks file under the root directory, and it has the name "anaconda-ks.cfg". You can use this file as a template to create your own kickstart files.

<h3>Approach 2 - Write it from scratch</h3>
The kickstart file is a simple text file and a lot of it is quite intuitive. There is a long list of kickstart options that you can set. Some of the settings are manadatory. The easiest way to create a kickstart file is by using an existing kickstart file as a template.  



<h3>Approach 3 - Use the GUI tool</h3>
There is a very handy little tool called system-config-kickstart which lets you create your own custom ks files, using a simple GUI. First you need to install the tool:

<pre>
$ yum install system-config-kickstart
</pre>

After that, you can open the tool via the command line:

<pre>
$ system-config-kickstart
</pre>

or from the desktop go to:

<em>Applications => system tools => Kickstart</em>


This will result in the following gui popping up:

<a href="http://codingbee.net/wp-content/uploads/2015/06/hMAroOQ.png"><img src="http://codingbee.net/wp-content/uploads/2015/06/hMAroOQ.png" alt="" width="725" height="598" class="alignnone size-full wp-image-4516" /></a>

Note: this gui tool hasn't been updated in a long time, and hence some powerful features unavailable, e.g. the ability to specify logical volumes, instead this tool is only limited to specifying partitions.






<h2>Validating your kickstart file</h2>

If you manually edit the ks file, be sure to use the <code>ksvalidator</code> command to check that your kickstart file is valid.


<pre>$ vksvalidator /path/to/ks-file</pre>

This tool will find error likes:

<pre>keyxaboard us    # Notice "keyboard" has been mispelt.</pre>

However it won't identify errors relating to:
- URL paths
- package names or groups
- %post or %pre

ksvalidator is part of the <code>system-config-kickstart</code> package. So you need to install this package in order to use ksvalidator.


<h2>Kickstart User Guide</h2>
You can find help info for kickstart by running the following:


<pre>
$ rpm -qd pykickstart
/usr/share/doc/pykickstart-1.99.43.17/COPYING
/usr/share/doc/pykickstart-1.99.43.17/ChangeLog
/usr/share/doc/pykickstart-1.99.43.17/README
<strong>/usr/share/doc/pykickstart-1.99.43.17/kickstart-docs.txt   # Open this in vim.</strong>
/usr/share/doc/pykickstart-1.99.43.17/programmers-guide
/usr/share/man/man1/ksflatten.1.gz
/usr/share/man/man1/ksshell.1.gz
/usr/share/man/man1/ksvalidator.1.gz
/usr/share/man/man1/ksverdiff.1.gz

 
</pre>

Note: pykickstart is a software package that writes/reads kickstart files. 

<h2>Further reading</h2>

<a href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Installation_Guide/chap-kickstart-installations.html">Redhat Kickstart documentations</a>


<a href="https://access.redhat.com/knowledge/docs/en-US/Red_Hat_Enterprise_Linux/6/html/Installation_Guide/ch-redhat-config-kickstart.html">RedHat’s Kickstart GUI tool guide</a>


<a href="https://academy.redhat.com/instructor/guide/instructorguide_ks.html">An example kickstart file</a>
]]></Content>
		<Date><![CDATA[2015-03-22]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Redhat|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Automating RHEL installations using Kickstart]]></Title>
		<Content><![CDATA[<h2>Kickstart installation overview</h2>
The <a href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Installation_Guide/chap-kickstart-installations.html" rel="nofollow">kickstart</a> system lets you automate the installation of the RHEL/Centos Operating system.  

By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="Where can you find the default kickstart files that was created after installing the RHEL Operating System?"]
/root/anaconda-ks.cfg
[/toggle]
[toggle title="What GUI tool can you use to create and edit kickstart files?"]
system-config-kickstart
[/toggle]
[toggle title="What naming convention should the kickstart file have?"]
You can call it anyting you like. 
[/toggle]
[toggle title="Where can you host kickstart files?"]
- http  
- ftp   
- nfs
- hd
- cdrom
[/toggle]
[toggle title="What are the syntax for each of the above locations, where path to ks file is '/dir/file' and where appropriate file is stored on a remote server called 'server'?"]
- ks=http://server/dir/file
- ks=ftp://server/dir/file
- ks=nfs:server:/dir/file
- ks=hd:device:/dir/file            # e.g. device is 'sda1'
- ks=cdrom:/dir/file
[/toggle]

[/accordion]

<hr/>





<h2>What is kickstart</h2>
When you do a normal installation of a RHEL Operating System, the install process walks you through a series of <a rel="nofollow" href="http://www.tecmint.com/centos-7-installation/">installation screens</a>, prompting you for input such as timezone, language, country....etc. This manual interactive process can get quite tedious if you have to installations on a lot of machines. That's where Kickstart comes to the rescue. Kickstart is used for automating the the RHEL/Centos install process.

The kickstart system lets you feed in a "kickstart" config file into the installation process. This file contains the answers to all the install questions. Therefore the install process will automatically take answers from the kickstart file rather than prompting the user to manually type in the answers. As a result the installation becomes an automated process and doesn't require human interaction. 

By default, a kickstart file is automatically generated when you perform a manual RHEL installation. This kickstart contains the answers that you provided during the manual installation (along with a lot more info). This file is called <strong>anaconda-ks.cfg</strong> and is located in the root user's home directory.  

<pre>
$ ls -l /root/anaconda-ks.cfg
-rw-------. 1 root root 1298 Mar 14 19:24 /root/anaconda-ks.cfg
</pre>

Here's what it looks like:

<pre>
$ cat /root/anaconda-ks.cfg
#version=RHEL7
# System authorization information
auth --enableshadow --passalgo=sha512

# Use CDROM installation media
cdrom
# Run the Setup Agent on first boot
firstboot --enable
ignoredisk --only-use=sda
# Keyboard layouts
keyboard --vckeymap=uk --xlayouts='gb'
# System language
lang en_GB.UTF-8

# Network information
network  --bootproto=dhcp --device=enp0s3 --ipv6=auto --activate
network  --hostname=localhost.localdomain
# Root password
rootpw --iscrypted $6$VN2iccVGyceNgeUG$ieJtErIOc/E/Lfak/DRHNiMm4nVryVqPngCVTztfnSFda/V7BIMCfeKVTAzIbOXQ0JbnHaYwMEdHXEUEjlYP01
# System timezone
timezone Europe/London --isUtc
user --homedir=/home/mchowdhury --name=mchowdhury --password=$6$dfa35FQlLz54y4jU$OrPER9VxI9o/vQQY1kkG2jgEFlk2qVJWtOfqdR3vd/I4NW1rjt.CjO3U0tM8W5JbeOProTYO3PiZ/lg.ufkue/ --iscrypted --gecos="Mirfath Chowdhury"
# X Window System configuration information
xconfig  --startxonboot
# System bootloader configuration
bootloader --location=mbr --boot-drive=sda
autopart --type=lvm
# Partition clearing information
clearpart --none --initlabel

%packages
@base
@core
@desktop-debugging
@dial-up
@directory-client
@fonts
@gnome-desktop
@guest-agents
@guest-desktop-agents
@input-methods
@internet-browser
@java-platform
@multimedia
@network-file-system-client
@print-client
@x11

%end

</pre>

We'll cover more about a creating/editing kickstart files in the next article. But for now we'll cover how to make use of a kickstart file. 
 
<h2>Feeding the Kickstart file into the install process</h2>

Installing Centos using kickstart is a 3 step process

1. Create the kickstart file (if you don't already have one)
2. make the kickstart file available to the boot process (e.g. put kickstart file on a web server)
3. Access the boot prompt during the Centos installation and then tell the boot prompt where your kickstart file is located.


You then go ahead and start the installation process, after a few seconds the very first screen you see during the install process is:

<a href="http://codingbee.net/wp-content/uploads/2015/06/YuvKYPj.png"><img src="http://codingbee.net/wp-content/uploads/2015/06/YuvKYPj.png" alt="" width="578" height="398" class="alignnone size-full wp-image-4518" /></a>

In order to automate the install using kickstart, you press the tab key, which leads to:

<a href="http://codingbee.net/wp-content/uploads/2015/06/tcWzWBw.png"><img src="http://codingbee.net/wp-content/uploads/2015/06/tcWzWBw.png" alt="" width="635" height="355" class="alignnone size-full wp-image-4520" /></a>

We then append the following "ks" setting to the bottom line, which is used for specifying the location of the kickstart file (highlighted in yellow):

<a href="http://codingbee.net/wp-content/uploads/2015/06/tNb9kcg.png"><img src="http://codingbee.net/wp-content/uploads/2015/06/tNb9kcg.png" alt="" width="636" height="357" class="alignnone size-full wp-image-4522" /></a>

Note: You can call the kickstart file anything you like, and even give it any suffix as well. 

Here our kickstart file is stored on a web server, but here are other possible options:

<pre>
ks=http://server/path/to/file  
ks=ftp://server/path/to/file   
ks=nfs:server:/path/to/file    
ks=hd:sda1:/path/to/file 
ks=cdrom:/path/to/file         
</pre>


Now to start the kickstart install:

1. First start your machine and access your BIOS to set the boot order to cd-rom.
2. Place the OS install cd into the cd-rom.
3. Then wait for the Installation boot screen to appear.
4. Hit the tab key (while the first menu option is highlighted). This will display the following line:
=> vmlinuz initrd=initrd.img
5. Append to this line by adding a space followed by one of the following:
ks=http://server/path/to/file        # This is if the file is stored on a web server
ks=ftp://server/path/to/file        # This is if the file is stored on a ftp server
ks=nfs:server:/path/to/file            # This is if the file is stored on a nfs server
ks=hd:device:/path/to/file            # This is if the file is stored on a locally attached hdd.
ks=cdrom:/path/to/file                # This is if the file is stored in a cd.

<h2>Alternative scenario: machine already has centos installed on it</h2>
In this case you don't need change the BIOS settings. Instead wait for the following screen and press enter when prompted. Then type "autoboot" at the prompt and this will take you to the boot screen.

<h2>Alternative scenario: (you want to do a network install instead of using a cd-rom)</h2>
It is actually possible to install centos by accessing the centos iso image via a network instead of using a installation cd. In this scenario, you perform the  <strong>network install</strong> and once you reached the boot installation screen, select tab, and enter the ks entry like before.

This works by the kickstart file containing the url to the iso image. ]]></Content>
		<Date><![CDATA[2015-03-22]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Redhat|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Git - The 3 main parts of git]]></Title>
		<Content><![CDATA[A git project is made up of 3 parts:


<ol>
	<li><strong>Working directory</strong> - This is the directory where all your project files and folders reside (along with the .git folder). Each of your files within this directory is in 1 of possible states, untracked, unmodified, modified, staged. Will cover more about <a href="http://codingbee.net/tutorials/git/git-file-states/" title="Git – File states">file states</a> later.</li>
	<li><strong>staging area</strong> - This is a hypothetical layer which sit's on top of the last commit's layer. When you run the "git commit" command the 2 layers get's merged. Any files that are in the commit layer, that has a newer file directly above it (in the "staging layer"), will get over-written by the newer (staged) file. Note, that a file's content are tracked so that you can roll back to how the file looked like in any previous (commits) snapshots. Note the staging area is also referred to as  the "index"</li>
	<li><strong>.git directory</strong> - this is basically like your git repo's database. </li>
</ol>



<pre>
 Working                 Staging             .git directory 
Directory                 area                (Repository)
    |                       |                     |
    |                       |                     |
    |                       |                     |
    |                       |                     |
    |                       |                     |
    |                       |                     |
    |                       |                     |
    |                       |                     |
    |                       |                     |
    |                       |                     |
    |                       |                     |
</pre> 


If you want to get a new file tracked by the git repository, then you do the following: 

<ol>
	<li>Use the "git checkout" command to checkout the project. This will pull out the latest commit (snapshot) from the .git repositorie's database (for a particular branch) and place it into the working directory. All the newly checked out files have the file state "unmodified" (more about file states later).

<pre>
 Working                 Staging             .git directory 
Directory                 area                (Repository)
    |                       |                     |
    |      git checkout     |                     |
    | <========================================== |
    |                       |                     |
    |                       |                     |
    |                       |                     |
    |                       |                     |
    |                       |                     |
    |                       |                     |
    |                       |                     |
    |                       |                     |
    |                       |                     |
</pre> 


</li>
        <li>add/create the file somewhere inside the working-directory. This will make git aware of the existence of this file it won't keep track of this file. i.e. the file's state is "untracked" (more about file states later).</li>
	<li>Use the "git add" command to place this file in the staging area, waiting to be merged into the previous commit (snapshot). This will change the file's state to "staged" (more about file states later).

<pre>
 Working                 Staging             .git directory 
Directory                 area                (Repository)
    |                       |                     |
    |      git checkout     |                     |
    | <========================================== |
    |                       |                     |
    |     git add           |                     |
    | =====================>|                     |
    |                       |                     |
    |                       |                     |
    |                       |                     |
    |                       |                     |
    |                       |                     |
    |                       |                     |
</pre> 



</li>
	<li>Use the "git commit" command to add the file, to create a new snapshot which is made up from the previous commit "layer" along with the files in the "staged layer". This will change the file's state to "unmodified" (more about file states later).

<pre>
 Working                 Staging             .git directory 
Directory                 area                (Repository)
    |                       |                     |
    |      git checkout     |                     |
    | <========================================== |
    |                       |                     |
    |     git add           |                     |
    | =====================>|                     |
    |                       |                     |
    |                       |   git commit        |
    |                       | ===================>|
    |                       |                     |
    |                       |                     |
    |                       |                     |
</pre> 


</li>
</ol>


 Here's another way to look at the above:

<a href="http://codingbee.net/wp-content/uploads/2015/03/Ne1EPj7.png"><img src="http://codingbee.net/wp-content/uploads/2015/03/Ne1EPj7.png" alt="" width="1024" height="768" class="alignnone size-full wp-image-3529" /></a>











<pre>
                  |----------------------Tracked----------------------------| 
                  |                                                         |
Untracked             Unmodified             Modified                Staged 
    |                      |                     |                     |
    |                      |                     |                     |
    |                      |                     |                     |
    |                      |                     |                     |
    |                      |                     |                     |
    |                      |                     |                     |
    |                      |                     |                     |
    |                      |                     |                     |
    |                      |                     |                     |
    |                      |                     |                     |
    |                      |                     |                     |
</pre> 
]]></Content>
		<Date><![CDATA[2015-03-22]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[git]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>git]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Git - Removing files]]></Title>
		<Content><![CDATA[If there is file that is being tracked by git, but is a file you now want to delete. Then the best way to do it is by running the following git command:

<pre>
$ git remove filename.txt
</pre>
 
This command does 2 things, it first changes the file's state to untrack, and then it actually deletes the file from the working directory altogether, i.e. it does the equivalent of "rm filename.txt". 


You can still remove the file using the conventional way of "rm filename.txt", but you would then have to stage the deletion of the file as well, hence you have to take a extra step with this approach.

Irrespective of the approach you take, you will then need to commit this to confirm the deletion. 


If you just want to remove a file from being tracked, but still keep the actual file, i.e. you want to change a file's state from staged/unmodified/modified to untracked. then you use the "cached" option:


 
<pre>
$ git remove --cached filename.txt
</pre>


]]></Content>
		<Date><![CDATA[2015-03-22]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[git]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>git]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - r10k.yaml]]></Title>
		<Content><![CDATA[https://docs.puppetlabs.com/pe/latest/r10k_yaml.html

r10k comes with it's own config file which is usually in:



<pre>
$ cd /etc/puppet              # note you have to cd into this folder first. 
$ cat /etc/puppet/r10k.yaml.         # note, you have to create this file if it doesn't already exist. 
:cachedir: '/var/cache/r10k'

:sources:
 puppet:
    remote: http://{github url that contains puppetfile}.git
    basedir: '/etc/puppet/environments'    
    prefix: false #This defaults to false
 hiera:
    remote: http://{github url that contains yaml files}/hiera.git     
    basedir: '/etc/puppet/hiera'
    prefix: false

</pre>

The puppet repo above needs to have this structure:

<pre>
[root@puppetmaster platform_provision]# tree
.
├── environment.conf
├── Gemfile
├── manifests
│   └── site.pp
├── modules
└── Puppetfile      # important. 
</pre>

Where the Gemfile contains:

<pre>
$ cat Gemfile
source 'https://rubygems.org'
</pre>

And environment.conf contains:

<pre>
$ cat environment.conf
manifest  = manifests/site.pp
moduledir = modules
</pre>

As for the hiera repo, you simply place your .yaml datasource in a folder structure to your liking.  

Note: the site.pp file here can be left empty if you are using an enc like foreman. Alternatively you could have a "default-node" definition to include class that you want installed in all vms. 


This config file is used by r10k when you run the following command:


<pre>
$ r10k deploy environment -p {environmentName} --verbose
</pre>

Where "{environmentName}" should be set to <code>prefix_branchname</code>. The prefix is something you can choose, e.g. you can name it the application you are working on. 

Now let's say you have a project, called "projectA", and this project requires several seperate directory-environments (e.g. called dev,test,prod...etc), in which case you might want r10k to create the following directory-environments:


<pre>
$ ls -l /etc/puppet/environments/
total 44
drwxr-xr-x 5 root root 4096 Dec 11 12:47 projectA_dev
drwxr-xr-x 5 root root 4096 Apr 15 16:03 projectA_test
drwxr-xr-x 5 root root 4096 Apr 24 13:40 projectA_prod
</pre>

As well as the corresponding hiera data sources:

<pre>
$ ls -l /etc/puppet/hiera/
drwxr-xr-x 5 root root 4096 Dec 11 12:47 projectA_dev
drwxr-xr-x 5 root root 4096 Apr 15 16:03 projectA_test
drwxr-xr-x 5 root root 4096 Apr 24 13:40 projectA_prod
</pre>

In order to achieve the above scenario, all you need to do is to achieve this, is to ensure that git branches by the names "dev", "test", and "prod" exists. 

Then during an r10k run, r10k will create a folder from each repo branch. then it will populate the modules folder with each branches puppetfile. 


Now we have limitation with this approach. What if projectB comes along, which also has "dev", "test", and "prod" environments, which needs their own directory environments. You can achieve this by adding multiple entres in your r10k.yaml file. 


<pre>
$  cat /etc/r10k.yaml
:cachedir: '/var/cache/r10k'

:sources:
 puppet_ProjectA:
    remote: http://{github url that contains puppetfile}.git
    basedir: '/etc/puppet/environments'
    prefix: 'ProjectA'
 hiera_ProjectA:
    remote: http://{github url that contains yaml files}.git
    basedir: '/etc/puppet/hiera'
    prefix: 'ProjectA'
 puppet_ProjectB:
    remote: http://{github url that contains puppetfile}.git
    basedir: '/etc/puppet/environments'
    prefix: 'ProjectB'
 hiera_ProjectB:
    remote: http://{github url that contains yaml files}.git
    basedir: '/etc/puppet/hiera'
    prefix: 'ProjectB'

:purgedirs:
  - /etc/puppet/environments
  - /etc/puppet/hiera

</pre>


Notice here we used the "prefix" setting. thats because if we didn't then r10k will create a folder called "dev" in the environment folder twice, which could cause errors.  

<pre>
$ ls -l /etc/puppet/environments/
total 44
drwxr-xr-x 5 root root 4096 Dec 11 12:47 ProjectA_dev
drwxr-xr-x 5 root root 4096 Apr 15 16:03 ProjectA_test
drwxr-xr-x 5 root root 4096 Apr 24 13:40 ProjectA_prod
drwxr-xr-x 5 root root 4096 Dec 11 12:47 ProjectB_dev
drwxr-xr-x 5 root root 4096 Apr 15 16:03 ProjectB_test
drwxr-xr-x 5 root root 4096 Apr 24 13:40 ProjectB_prod
</pre>

Now you need update all your puppet agent's puppet.conf file with an environment setting, e.g.:


<pre>
[root@puppetagent1 ~]# cat /etc/puppet/puppet.conf | grep env
environment     = ProjectB_dev
[root@puppetagent2 ~]# cat /etc/puppet/puppet.conf | grep env
environment     = ProjectA_prod
</pre>

That way the puppetmaster will know which directory environment to use to generate the catalog. 



]]></Content>
		<Date><![CDATA[2015-03-27]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Linux - rhn-channel and Spacewalk]]></Title>
		<Content><![CDATA[The Spacewalk server is your custom rpm server which hosts all your rpm packages. If you want to connect to an Spacewalk server, you need to viers install the following package:

<pre>
$ yum install rhn-setup
</pre> 


https://access.redhat.com/documentation/en-US/Red_Hat_Network_Satellite/5.5/html/Channel_Management_Guide/chap-Red_Hat_Network_Satellite-Channel_Management_Guide-Introduction_to_RHN_Channels.html 


http://www.spacewalkproject.org/

the following lists all channels a machine is subscribed to:
<pre># rhn-channel --list
</pre>

Alternatively do:

<pre>
$ spacewalk-channel --list
</pre>




to register a client to a spacewalk server, you use the <a href="http://linux.die.net/man/8/rhnreg_ks">rhnreg_ks</a> command:


<pre>
$ rhnreg_ks --activationkey {channel name}
</pre>



To subscribe to a new channel, we do:

https://access.redhat.com/solutions/57504 

See also:

https://access.redhat.com/solutions/57504


also run the following command to check what repos you're connected to:

<pre>
$ yum repolist
</pre>]]></Content>
		<Date><![CDATA[2015-03-31]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[spacewalk]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Standard Outputs, Inputs, and Errors]]></Title>
		<Content><![CDATA[<h2>Overview of standard Outputs/Inputs/Errors</h2>

By the end of this article you should be able to answer the following questions:

[expand title="How many types of outputs can a command give out, and what are they called?"  alt=" "]
Two types, They are called, "standard output" and "standard error". 
[/expand]

<hr/>


<h2>Standard Outputs and Standard Errors</h2>

In Linux, a command can return 2 types of outputs, they are called <strong>Standard Outputs</strong> and <strong>Standard Errors</strong>. Redirection gives you a further level of management control when redirecting Standard Outputs and Standard Errors.   


The <strong>standard output</strong> is also known as <strong>Type 1</strong>) outputs. Similarly <strong>standard error</strong> outputs is known as <strong>Type 2</strong>. Standard errors, are more commonly recognized as error messages. By default all outputs, regardless of the type, are outputted to the screen. In most cases it is quite easy to identify the output type e.g.: 

<pre>
$ ls -l testfile.txt
ls: cannot access testfile.txt: No such file or directory
</pre>

The above shows a "Standard Error" type message. A more definitive way to determine what type output a command outputted is by view a command's "exit status", which is stored in the special parameter, <code>$?</code>:

<pre>
$ ls -l testfile.txt
ls: cannot access testfile.txt: No such file or directory
$ echo $?
2
</pre>


If the return value is "0", then the last command ran gave out type 1 messages. Otherwise it is a type 2 message. 

<pre>
$ touch testfile.txt
$ ls -l testfile.txt
-rw-r--r--. 1 root root 0 Oct 19 20:38 testfile.txt
$ echo $?
0
</pre>

Note: All the available special parameters are documented in <code>man bash</code>, just search for "Special Parameters"


<h2>Redirection</h2>
In some cases, you might want to capture type 1 messages messages into a file. This is acheived by  redirecting the standard output outputs to a file:

<pre>
$ ls -l testfile.txt > info.log
$ cat info.log
-rw-r--r--. 1 root root 0 Oct 19 20:38 testfile.txt
</pre> 

However this doesn't automatically redirect standard errors (type 2 outputs). That's because the above command is an implicit version of the following command:

<pre>$ ls -l testfile 1>>info.log</pre>

The "1>>info.log" basically means <italic>only redirect type1 outputs to the file called info.log<italic>. The <code>>></code> means append to this file, whereas a single <code>></code> means set the content of this file. In both cases it will create the file if it doesn't already exist.  

Therefore type 2 outputs are outputed to the screen, e.g.: 

<pre>
$ ls -l i-dont-exits.txt > info.log
ls: cannot access i-dont-exits.txt: No such file or directory
$ echo $?
2
$ cat info.log
$
</pre>



If you want to also redirect type2 outputs to the same place as type 1, then do:

<pre>
$ ls -l testfile123 >info.log 2>&1
</pre>

Here, "2>&1" means <italic>redirect type 2 outputs to where ever type1 outputs are being sent to<italic>

To summarise, There are actually several ways you can write the above command:

<pre>
$ {command} >>info.log 2>&1
$ {command} 1>>info.log 2>>info.log
$ {command} 2>>info.log 1>&2
$ {command} 2>>info.log >&2
</pre>

You can also display type1 on screen and just keep a log of type2, like this:

<pre>
$ {command} 2>>info.log
</pre>


Another popular option is to store type1 and type2 outputs in seperate files:

<pre>
$ {command} 1>>info.log 2>>error.log
</pre>


<h2>Discarding Outputs</h2>
Sometimes you might want to ignore outputs and don't display/store them anywhere. To do this, you need to redirect the output to a special file, <code>/dev/null</code>. For example if you want to discard any type2 outputs, but display type1 on screen, then do:

<code>
$ {command} 2>>/dev/null
</code>

<code>/dev/null</code> is a special file that's solely used for discarding outputs. It is a bit like Microsoft window's recycle bin. Some people also refer to it as the "black hole". Once you have sent an output to the <code>/dev/null</code>, thats it, you wont be able to recover it again.  

Discarding type2 outputs is not a good idea, since they can be informative and can help you resolve issues. However sometimes it can be useful, such as when <l>using the find command<l>

<h2>Spacing doesnt matter</h2> 
In the above examples we use a file called "info.log". But it doesn't really matter what you call the files whatever you like. e.g. you can call them log.txt, or just info
You can also specify the whole path of the file, to make sure the files are located where you want them. So instead of "info.log", you can have them as "/tmp/info.log"
Anywhere that you see a redirection which is not preceded with a digit, e.g. " >info.log", then you should read this as the "1" is implied. Hence " >info.log" can also be written as " 1>info.log". People often leave out the "1" because it saves them from extra typing. 






<h2>Useful links</h2>
http://www.cyberciti.biz/faq/redirecting-stderr-to-stdout/
http://stackoverflow.com/questions/2342826/how-to-pipe-stderr-and-not-stdout 







By default all output messages (which is 1 and 2) are sent to the standard output which is the command prompt screen. However this can be edited. Here is an example:

find / -type f -name “*.ora” 2=>/dev/null 1=>&2

This means:

2=>/dev/null   :  any error output messages is ignored

1=>&2   :          any standard (success) output gets redirected to wherever standard error is sent to.

The following has the same effect (i.e. nothing is outputted):

find / -type f -name “*.ora” 2=>/dev/null 1=>/dev/null

You can also send error and standard info messages to different files. for example:

find / -type f -name “*.ora” 2=>error.log 1=>info.log

or

find / -type f -name “*.ora” 1=>info.log 2=>error.log

or you could leave out the “1”

find / -type f -name “*.ora” =>info.log 2=>error.log

This creates two files in the current directory and sends and saves error messages in one and info messages in the other.

Here is another example:

find / -type f -name “*.ora” =>log

This will save all success messages to the log whereas all error messages (e.g. permission denied messages) get displayed on the screen. so if you want to also save error messages to the log and not show them on screen, then you can do:

find / -type f -name “*.ora” =>log 2=>&1

or a shorthand way to write this is:

find / -type f -name “*.ora” &=>log

or

find / -type f -name “*.ora” =>log 2=>log

The above actually doesn’t work because error message overwrites success messages. The following also doesn’t work for other reasons:

find / -type f -name “*.ora” =>log 2=>=>log

The ordering of the redirection is also important, here is an example:

<pre>$ find / -type f -name “*.ora” =>/dev/null 2=>&1 | wc -l

0
</pre>

Here, the first redirection is straight to null which is why all error messages are straight away ignored.



<pre>$ find / -type f -name “*.ora” 2=>&1 =>/dev/null | wc -l
160
</pre>

The second command outputs error messages to screen before the redirection to null takes place.

Tip: In shell scripts, near to the top of the shell script, you can add:


<pre>
exec 1>> /path/to/logfile.txt 2>&1
</pre>

This means that from this point forward, all of the scripts output will get redirected to the log file. 

<h2>
See also:</h2> 
http://www.cyberciti.biz/faq/redirecting-stderr-to-stdout/

You can also create a third file descriptor, which you can use to swap round how error and standard messages are treated, see:

http://stackoverflow.com/questions/2342826/how-to-pipe-stderr-and-not-stdout]]></Content>
		<Date><![CDATA[2015-04-03]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Piping and Redirection]]></Title>
		<Content><![CDATA[skip this

<h2>Piping and Redirection
</h2>
Piping and redirection are techniques that gives the linux command line an incredible amount of power and versatility. Piping and Redirection works by essentially controlling the flow of standard inputs and standard outputs. 


<h2>Piping</h2>

Piping is a technique that lets you use Linux commands as building blocks to build your own custom commands. To understand how piping works, you need to see it in action. Lets assume we have a list of files:


<pre>
$ ls -l
total 0
-rw-r--r--. 1 root root 0 Oct 20 19:22 file1
-rw-r--r--. 1 root root 0 Oct 20 19:22 file2
-rw-r--r--. 1 root root 0 Oct 20 19:22 file3
drwxr-xr-x. 2 root root 6 Oct 20 19:22 folder1
drwxr-xr-x. 2 root root 6 Oct 20 19:22 folder2
drwxr-xr-x. 2 root root 6 Oct 20 19:22 folder3
</pre>
 

Now lets say you want to only list files and no directories. Then one way to do this is by using piping:

<pre>
$ ls -l | grep "^-"
-rw-r--r--. 1 root root 0 Oct 20 19:22 file1
-rw-r--r--. 1 root root 0 Oct 20 19:22 file2
-rw-r--r--. 1 root root 0 Oct 20 19:22 file3
</pre>



Here we used the pipe character, "|", to redirect the output of of "ls -l", into the grep command.
The way it works is that The first part "ls -l" generates the output that would normally would show up on the screen (aka <l>standard output<l>). However due to the pipe, the standard output is instead redirected (i.e. piped) as the input (aka <l>standard input<l>) for the next command (grep). The "^-" instructs grep to only display lines that begins with "-". This means that it filters out any lines starting with "d" (i.e. directories). The grep's standard output is then displayed on the screen.

If instead you want to count the number of files there are (rather than a list of the files), then you can do this by adding another pipe:

<pre>
$ ls -l | grep "^-" | wc -l
3
</pre>

Here, the standard output from the grep command is redirected into the (w)ord(c)ount command. The -l option instructs wc to only do a line count. Hence the resulting output is equal to the number of files. 

This is just a simple example of how powerful piping can be. 


<pre>Redirection 
</pre>
Redirection is a technique that essentially allows commands to either read data from a text file, or save the output to text files. In other words it lets you redirect a command's standard output to a file rather than displaying it on the screen. The best way to understand how piping works, is to see it in action. Lets say we have a file1 which contains the following content:

<pre>
$ cat file1
apple
102
cakes
drinks
bananas
500
301
</pre>

Now we want to sort the lines in this file in alphabetical order, and then store the ordered lines into a file called file2. To start with, lets first check that we can order the content using the "sort" command:

<pre>
$ sort file1
102
301
500
apple
bananas
cakes
drinks
</pre>

Now we need to redirect the standard output to a file instead of outputting it to the screen:

<pre>
$ sort file1 > file2
</pre>

Here we have employed the <mark>></mark> redirection operator to redirect the standard output from "sort file1" to a new file called file2. Lets now look at the contents of file2 to confirm that this has worked:

<pre>
$ cat file2
102
301
500
apple
bananas
cakes
drinks
</pre>

If you alredy have a file called file2 which contains data, then if redirect standard outputs to it using <code>></code> then it will overwrite file:

<pre>
$ echo "oops, I have now overwritten this file" > file2
$ cat file2
oops, I have now overwritten this file
</pre>

If you want to append to an existing file then you can do this using append redirection symbol, <code>>></code>:


<pre>
$ echo "this is a new line" >> file2
</pre>


Now check that this has worked:


<pre>
$ cat file2
oops, I have now overwritten this file
this is a new line
</pre>



If you try to append to a file that doesn't exist, then ">>" will simply create the file and then append the content to it. 

In Linux, the <code>>></code> operator is often used for adding entries to log files. 


<h2>Using redirection to take input from a file
</h2>If you want a command to take in the contents of a file, as it's input, then you use the read-in redirection symbol:

<pre>
$ sort < file1
102
301
500
apple
bananas
cakes
drinks
</pre>

Here, you are instructing "sort" to take in the content of file1, sort it, and then display the output. However the sort command already accepts a filename as a parameter. which means you can achieve the same results like this:

<pre>
$ sort file1
102
301
500
apple
bananas
cakes
drinks
</pre>

For that reason, the "<" isn't used as much since most commands by default already accepts a filename as a parameter. However some people prefer to use "<" for the sake of clarity, hence here are 2 commands that does gives the same result:   

<pre>
$ sort file1 > file2 
</pre>

Here the "sort" command reads file1 as its input, sorts it, and then sends the output to file2.


<pre>
$ sort <file1 >file2 
</pre>

This does the same thing as the previous command, but describes what it is happening more explicitly. 

 
<h2>Combining piping with redirection</h2>
There is a third way to write the above command, and that is by combining piping and redirection, like this:

<pre>
cat file1 | sort > file2 
</pre>

just to check that this has worked:

<pre>
$ cat file2
102
301
500
apple
bananas
cakes
drinks
</pre>

Combining piping and redirection in this way unleashes a whole new way of working on the Linux command line, and is heavily used when writing shell scripts.]]></Content>
		<Date><![CDATA[2015-04-03]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - The grep command]]></Title>
		<Content><![CDATA[<h2>Overview of grep</h2>
By the end of this article you should be able to answer the following questions:

[expand title="Question"  alt=" "]
<pre>

</pre>
[/expand]
<br />
[expand title="Question" alt=" "]
<pre>

</pre>
[/expand]
<br />
[expand title="Question" alt=" "]
<pre>

</pre>
[/expand]
<br />
[expand title="Question" alt=" "]
<pre>

</pre>
[/expand]
<br />

<hr/>



<h2>What is grep?</h2> 
The grep command is for scanning through texts for a particular search term (which can be in the form of a regex string) and return all lines that meets the match. As a result grep is one of the most commonly used Linux commands. Grep essentially acts as a filtering tool that's used for only outputting the lines we are interested in.   

To use grep, all you have to do is give it some content and tell it what search-term/regular-expression to serach for. Grep will then output all lines where it finds a match. 

For example, let's say we have the following file:

<pre>
$ cat testfile.txt
A list of fruits:
apples
bananas
oranges
more bananas
</pre>

Now if we grep for bananas we get:

<pre>
$ grep 'bananas' testfile.txt
bananas
more bananas
</pre>


Or we can grep for the letter "e":
<pre>
$ grep 'e' testfile.txt
apples
oranges
more bananas
</pre>


We can also pipe content into the grep command, here's how we do this when grepping for "pp":

<pre>
$ cat testfile.txt | grep 'pp'     
apples

</pre>


If grep can't find a match, then it won't return anything at all:

<pre>
$ grep peaches testfile.txt
$
</pre>



If your search-term contains any spaces, then you need to enclose the search term in single or double quotes:

<pre>
$ grep "list of"
A list of fruits:
</pre>


Grep is commonly used as a way to filter out content from other commands, via piping, here's an example:

<pre>
$ ls -l | grep testfile
-rw-r--r--. 1 root root     0 May 24 14:23 testfile1.txt
-rw-r--r--. 1 root root     0 May 24 14:23 testfile2.txt

</pre>

However the true power of grep is unleashed when you grep to find matches using regular expressions.

<h2>Understanding Regular Expressions</h2>
Sometimes you will want to search for a pattern rather than a static string. This is done by using grep with a "regular expression" as search term rather than a simple string.

Combining the power of grep and regular expressions, gives you a far more intelligent matching capabilities. To do this, all you have to do use the reqular expression as the search term when executing the grep. You also have to enclose the regular expression in double quotes instead of single quotes.

Here are the most commonly used Regex Operators: 

[table id=3 /]

The best way to understand how grep and regular expressions works, you need to take a look at a few examples. Here are 3 examples of patterns you might want to search for:


<ul>
	<li>Example 1 - grep for all lines that starts with the letter d.</li>
	<li>Example 2 - grep for all lines that contains two digits, followed by ":" followed by 2 digits, followed by either "am" or "pm" (e.g. 08:30pm)</li>
	<li>Example 3 - grep for all lines that contain a 2 digits, followed by 3 letters, followed by 4 digits (e.g. 23Apr2013, or 14Nov2010</li>
</ul>


In Linux, you can represent these examples in the form of a special syntax called "regular expressions" (and then use grep to search for regular expressions, which is covered later). Here are the three examples written in the form of regular expression. Lets look at each example in turn.

<strong>Example 1 - Search for any lines that starts with the letter d</strong>

The corresponding regular expression is:
<pre>
"^d"</pre>

In the world of regular expressions, the carat symbol, "^" means "start with". If we omitted the carat then grep return lines that that contains the letter d anywhere on the line,  rather than just the beginning.


<strong>Example 2 - Search for any lines that contains two digits, followed by ":" followed by 2 digits, followed by either "am" or "pm" (e.g. 08:30pm)</strong>

The corresponding regular expression is:

<pre>"[0123456789][0123456789]:[0123456789][0123456789][ap]m"</pre>

Here, for the first digit, we used square brackets to encase all the digits that the first digit is allowed to be (the square brackets here are being used as special regular expression notations, are not being search for themselve). We did the same for the
second digit. The third character is the symbol ":". The next two are like the first two are the same as the first 2, followed by a character that can be either a or p, (as indicated by the encased square brackets). Finally the last character is just the letter "m".

Writing a regular expression this long can be a little tedious, but fortunately if the square brackets encases a sequential range of characters, then you can use a shorthand notation:

<pre>"[0-9][0-9]:[0-9][0-9][ap]m"</pre>

Here "-" is being as a special regular expression notation to represent a range, and won't be something that is searched for itself. The "-" only works when being used inside a square bracket.

This scenario hints that its purpose is to match for time stamps based on the 12 hour clock (although it doesn't say so explicitly). However if you were to use it for timestamp matching then it won't work because it can match invalid values, such as "99:99am". To overcome this issues, requires a bit more tweaking :

<pre>"[0-2][0-9]:[0-6][0-9][ap]m"</pre>


<strong>Example 3 - Search for any lines that contain a 2 digits, followed by 3 letters, followed by 4 digits (e.g. 23Apr2013, or 14Nov2010</strong>

The corresponding regular expression is:

<pre>"[0-9][0-9][A-Za-z][A-Za-z][A-Za-z][0-9][0-9][0-9][0-9]</pre>

The third character in this example is represented by [A-Za-z]. This means that it can be any upper case letter in the range A-Z, or lower case letter in the range a-z.

<strong>Example 3 - List all pictures in the current directory</strong>

The "$" match is used to match the end of the line rather than anywhere else.

<pre>$ ls -l
$ ls -l | grep ".jpg$"  
</pre>

For help info for regular expressions, checkout:

<pre>
$ man 7 regex
</pre>


<h2>Useful grep options</h2>

There's a bunch of option that you can use to control grep's default behaviour. You can view all of them using <code>grep --help</code>. Here are some useful options: 


<ul>
	<li><code> -i, --ignore-case</code>: find matches irrespective of upper/lower case characters</li>
	<li><code> -v, --invert-match</code>: This does a reverse match.</li>
	<li><code>-R, --dereference-recursive</code>: This recursively search through all content.</li>
</ul>




<h2>Wildcards and Regular expressions</h2>
At first sight, wild cards and grep appears to do the same thing. But there is a subtle difference. For example if you have a file called "rwx" then do ls -l to list this file. The wild card option will give the correct option whereas grep will confuse it with the first column of ls -l, which are the permission.



<h2>Linux Grep OR, Grep AND, Grep NOT Operator</h2>

http://www.thegeekstuff.com/2011/10/grep-or-and-not-operators/

Let's say we have the following file:


<pre>
$ cat testfile.txt
apple pie
apple cake
banofee cake
orange soda
brown bread
cottage pie
chocolate cake
white bread
upside down pineapple cake
bread and butter pudding
</pre>

Now let's say we want to grep for any lines containing the word "cake" or "pie". There's a few ways to do this. The first way is by using the "-E" option along with the grep's pipe operator:

<pre>
$ grep -E "cake|pie" testfile.txt
apple pie
apple cake
banofee cake
cottage pie
chocolate cake
upside down pineapple cake
</pre>

Or you can pass in multiple regex expressions using the -e option:

<pre>
grep -e "cake" -e "pie" testfile.txt
apple pie
apple cake
banofee cake
cottage pie
chocolate cake
upside down pineapple cake
</pre>


If you want to search for all lines contain "apple", we do:


<pre>
$ grep "apple" testfile.txt
apple pie
apple cake
upside down pineapple cake
</pre>

However if you want to grep for all lines containing "apple" AND "cake", then we can simply achieve this through piping:

<pre>
$ grep "apple" testfile.txt | grep "cake"
apple cake
upside down pineapple cake
</pre>





<h2>Recursively scan contents of all files for a regex match</h2>


This is possible with the -R option:

<pre>
grep -R search-term
</pre>

This will search all sub directories as well. 

]]></Content>
		<Date><![CDATA[2015-04-03]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Accessing built-in help guides]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="How do you view the 'tar' command's usage info?"]
It is usually one of the following:
$ tar h
# or
$ tar -h
# or 
$ tar help
# or
$ tar --help
[/toggle]
[toggle title="How do you view the 'tar' command's reference manual entry?"]
$ man tar
[/toggle]
[toggle title="What other commands can you run instead of the above??"]
$ info tar
# or 
$ pinfo tar
[/toggle]
[toggle title="What's the command to locate the name of the man entry for the config file, <code>/etc/selinux/config</code>?"]
$ whatis config
[/toggle]
[toggle title="What's the command to see if there is an available man page for the <code>/etc/passwd</code> config file?"]
$ whatis passwd
# you should discover the man page is accessible like this:
$ man 5 passwd
[/toggle]
[toggle title="What is the command to run before performing a search on the man pages?"]
$ mandb
[/toggle]
[toggle title="What's the command to search through all the man title+brief descriptions for the word 'compress'?"]
$ man -k compress
[/toggle]
[toggle title="What's the command to search through the entire man pages for the word 'compress'?"]
$ man -K compress    # note this time it is a capital k. 
[/toggle]
[toggle title="What's the command to open up the GUI based help guide?"]
$ yelp
[/toggle]
[toggle title="What is the command to view help info in firefox?"]
$ firefox file:///usr/share/doc
[/toggle]
[toggle title="What are the various commands for locating files?"]
Any of the following:
$ locate {command}
$ whereis {command}
$ which {command}
$ type {command}
[/toggle]
[toggle title="What command should you run first before using the 'locate' command?"]
$ updatedb
[/toggle]
[/accordion]

<hr/>




While most people rely on google when they get stuck, there are actually a number of useful help resources available from the command line.

You can also access help info from both the desktop gui (via the desktop user guide), but we'll just focus on finding help at the command line.


<h2>Viewing usage info</h2>

The man pages are quite long a detailed, which can be time consuming to read through, if you want a quick guide on how to use a command. That's why a lot of commands have a shorter (usage) guide that gives a much briefer overview of all a given command's options. The usage guide is accessible in one of several ways depending on the command. 

For example, to access the "ls" command's help info we use the "--help" switch

<pre>
$ ls --help | head
Usage: ls [OPTION]... [FILE]...
List information about the FILEs (the current directory by default).
Sort entries alphabetically if none of -cftuvSUX nor --sort is specified.

Mandatory arguments to long options are mandatory for short options too.
  -a, --all                  do not ignore entries starting with .
  -A, --almost-all           do not list implied . and ..
      --author               with -l, print the author of each file
  -b, --escape               print C-style escapes for nongraphic characters
      --block-size=SIZE      scale sizes by SIZE before printing them; e.g.,

</pre>

If <code>--help</code> doesn't give the usage info, then try <code>-h</code>, <code>help</code>, <code>-help</code>, or just <code>h</code>.

Some commands don't provide usage info, in which case you have to view their man pages instead. 


<h2>The man command</h2>

The RHEL OS comes with a Reference Manual. You can access various pages and sections of this manual using the 
"man" command is short for "Reference (man)ual". 

The man command displays pages from the official reference manual for a command. For example:

<pre>
$ man ls 
</pre>

You can even use the man command to view the help for in the man command itself:

<pre>
$ man man 
</pre>

It can be a bit overwhelming to take in the content from man pages, but it gets easier once you take the time to understand man pages in greater detail.

<h2>The info command</h2>

The info command is just another source of help information:

<pre>
$ info ls
</pre>

In most cases the info command gives the same content as the man command. However occasionally it does contains additional info. So it is probably best to only use 'info' after going through the man pages.

While in info mode, press the "h" key to list the various navigation options. 

This command is particularly useful for really long man pages, e.g.:


<pre>
$ info bash
</pre>

<h2>The pinfo command</h2>

The pinfo command display the man pages in a more user friendly format. It essentially breaks down a man pages into sections which you can navigate to using your arrow keys. Whereas the man command displays everything in a single output. Whether you use pinfo or man is down to personal preference.

<pre>
$ info bash
</pre>




<h2>The whatis command</h2>

The whatis command gives a one line description of a command:

<pre>
whatis ls
</pre>

whatis essentially works by looking up a given command's man page, and displays the header line from that man page. The whatis command is useful if you just want a quick reminder of a command without bringing up the whole man page.



However the whatis command is more useful for finding help info for various config file. For example if you want to find more info about the file <code>/etc/selinux/config</code> then do::



<pre>
$ whatis config
Config (3pm)         - access Perl configuration information
config (5ssl)        - OpenSSL CONF library configuration files
selinux_config (5)   - The SELinux sub-system configuration file.
</pre>

Once you have located a man page, you can then view it using the man command, by specifying it's location:


<pre>
$ man 5 selinux_config
</pre>



<h2>Searching the man pages</h2>

Before you search through the man pages, you should first run:

<pre>
$ mandb
</pre>

This will create/update an internal db using info from all the man pages. 


Sometimes you want to perform a task but dont know the name of the command to use. In those situations you will want to do a keyword search against the entire reference manual and then outputs a list of all man pages where a match has been found. for example, lets say you want to compress a file, but don't know what command you should use, in that case can do a keyword search on the word "compress", is the man command's k option:

<pre>
$ man -k compress
bunzip2 (1)          - a block-sorting file compressor, v1.0.6
bzcat (1)            - decompresses files to stdout
bzcmp (1)            - compare bzip2 compressed files
bzdiff (1)           - compare bzip2 compressed files
bzgrep (1)           - search possibly bzip2 compressed files for a regular expression
bzip2 (1)            - a block-sorting file compressor, v1.0.6
bzless (1)           - file perusal filter for crt viewing of bzip2 compressed text
.
.
.
. ...etc
</pre>

Note if you want to do a deeper search then use capital K instead:

<pre>
$ man -k compress
</pre>




As you can see it search's all "whatis" entries for search matches. 


Some software packages don't come with man pages. Instead, the man pages have to be installed separately (not sure how to identify these packages, might be possible using yum or rpm)


<h2>Accessing help from the desktop GUI</h2>

While it is possible to access a vast collection help resources straight from the command line, it can also be worthwhile accessing help from the desktop. This is especially true for help material that are in the form of images and diagrams.

There are a number of ways to access help:

The first way is:


<a href="http://codingbee.net/wp-content/uploads/2015/06/B1dCHP5.png"><img src="http://codingbee.net/wp-content/uploads/2015/06/B1dCHP5.png" alt="" width="653" height="395" class="alignnone size-full wp-image-4815" /></a>

Which takes you to:

<a href="http://codingbee.net/wp-content/uploads/2015/06/bNLMBxR.png"><img src="http://codingbee.net/wp-content/uploads/2015/06/bNLMBxR.png" alt="" width="824" height="537" class="alignnone size-full wp-image-4816" /></a>


You can also launch this utility by simply running the yelp command:

<pre>$ yelp</pre>


<h2>Viewing help info in usr/share/doc</h2>

Another place that contains help material <code>/usr/share/doc</code>, which is best viewed via firefox. You can launch firefox via the command line.

<pre>
$ firefox file:///usr/share/doc
</pre>

Which results in:

<a href="http://codingbee.net/wp-content/uploads/2015/06/A8xYMVV.png"><img src="http://codingbee.net/wp-content/uploads/2015/06/A8xYMVV.png" alt="" width="947" height="595" class="alignnone size-full wp-image-4817" /></a>



<h2>Tracking down a command</h2>


<pre>
type {command}      # tells you if command is builtin or not, if not then shows the path.
which {command}     # shows command's file location (even for builtin ones).
                    # this command is specifically suited for finding binaries.  
whereis {command}   # shows command's file location (even for builtin ones).
locate {command}    # shows command's file location 
</pre>
Note: you need to run the "updatedb" to run the "locate" command. This is the equivelent to the "mandb" is to the man command's -k option. 



<h3>The "type" and "which" commands</h3>
Before we start, let's first lookup the whatis info for the "type" and "which" commands:


<pre>
$ whatis type
type (1)             - bash built-in commands, see bash(1)
type (1p)            - write a description of command type
$ whatis which
which (1)            - shows the full path of (shell) commands.
</pre>

The "type" command tells you whether a command is builtin, or not, if not then it shows the path. E.g. 

<pre>
$ type cd
cd is a shell builtin
</pre>

You can find a list of all builtin commands listed in the type command's man page:

<pre>
$ man 1 type
</pre>


However even built-in files are represented in linux as a file, hence in this situation you need to use the "which" command to locate the binary (aka executable) file for the "cd" command:

<pre>
$ which cd
/usr/bin/cd
</pre>
 

Both "type" and "which" can be used to locate non-builtin commands:

<pre>
$ type tar
tar is /usr/bin/tar
$ which tar
/usr/bin/tar
</pre>

<h2>Locating documentation with rpm</h2>

You can also view what documentations have been installed using the rpm command. 

<pre>
$ rpm -qd tree
/usr/share/doc/tree-1.6.0/LICENSE
/usr/share/doc/tree-1.6.0/README
/usr/share/man/man1/tree.1.gz
</pre>

More about this when we come to talk about rpm. 

]]></Content>
		<Date><![CDATA[2015-04-03]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Using the Linux find command]]></Title>
		<Content><![CDATA[skip this

The "find" command is the main command to use if you are trying to locate a particular file or folder. 

The find command is incredibly versatile and has lots of features. So the best way to understand how to use it is by taking a look at some examples:




<h2>Find a file by it's name</h2>

Let's say I want to find a file called "sshd_config" but have no idea which directory it might be in, in that case I do: 

<pre>
$ find / -name sshd_config -type f
/etc/ssh/sshd_config
</pre>


Let's break down whats happening here:

<ul>
	<li><code>/</code> - first argument that I specified is an absolute path, which in my case is the root directory, "/". This tells the find command where to search, which in my case is the entire linux filesystem. Hence this command may take several minutes to complete, as there's a lot of files/folders to search through. That's why you should specify a lower level folder in order to narrow the search scope and get a faster response. 

</li>
	<li><code>-name sshd_config</code> - The "name" option is used to tell the find command, to perform a search and return all results based on the file's (or folder's) name. In this case I specified that the name should be "sshd_config". You can also make use of wildcards here.</li>
	<li><code>-type f</code> - Here I specified that I am looking for files and not directories. This argument is optional and if I omit it, then the find command will return matches for both files and folders. </li>
</ul>


Note you can omit specifying a path, in which case the find command will default to searching through the cwd. Therefore the following:  

<pre>
$ find -name sshd_config -type f
</pre>

is the same as writing: 


<pre>
$ find . -name sshd_config  -type f
</pre>


<h2>Find a folder by it's name</h2>
For example, to locate all directories beginning with "ssh", and located somewhere in /etc, I do:

<pre>
$ find /etc -name ssh* -type d
</pre>

Note: you cannot be in your home directory when running the above command, because it won't work. I think this could be a bug. 

Here's I specified the type as "d", which mean's I want to see matches that are directories only.  

If I omit to specify a type, then the find command will return all matches, i.e. files and folders. 

<h2>Find a file that has been recently accessed</h2>
So far we matched files/folders based on the "-name" condition (aka test) being satisfied. However there are lots of other tests that are available, which you can view in the man page. E.g you can use "-amin -x" to list all files that have been accessed less than x minutes ago.  You can then combine these tests together like this:

<pre>
[root@localhost /]# find / -name ssh* -amin -50 -type d
/root/sshd_config

</pre>

<h2>Alternatives to the find command</h2>
The find command is really powerful. But there are other user-friendly (but less powerful) commands that you can use to locate files, they are the <a href="http://codingbee.net/tutorials/rhcsa/rhcsa-find-help/" title="RHCSA – Finding help on the commandline">which, locate, and whereis commands</a>.  



See also:

http://www.tecmint.com/35-practical-examples-of-linux-find-command/]]></Content>
		<Date><![CDATA[2015-04-03]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Compressing files and folders using gzip and tar]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What are the 3 main modes of tar?"]
- c: creating tar files
- x: extracting tar files
- t: displaying table of contents of a tar file
[/toggle]
[toggle title="What is the command to extract 'testfile.tar.gz'?"]
$ tar -xzvf testfile.tar.gz   
[/toggle]
[toggle title="What is the command to create a compressed archive for the folder '/tmp/testfolder' into a file called 'name.tar.gz'?"]
$ tar -cvzf name.tar.gz /tmp/testfolder     
[/toggle]
[toggle title="What is the command to compress the file 'testfolder.tar'?"]
$ gzip testfolder.tar    # this will rename this file to testfolder.tar.gz
[/toggle]
[toggle title="What is the command to uncompress the file '/tmp/testfolder.tar.gz'?"]
$ gunzip /tmp/testfolder.tar.gz
[/toggle]
[toggle title="What is the command to compress /tmp/testfile1.txt using bzip?"]
$ bzip2 /tmp/testfile1.txt    # this will rename file to testfile1.txt.bz2
[/toggle]
[toggle title="What is the command to uncompress /tmp/testfile1.txt.bz2 using bzip?"]
$ bunzip2 /tmp/testfile1.txt.bz2
[/toggle]
[toggle title="What is the command to list the contents of /tmp/testfolder.tar.gz?"]
$ tar -tf /tmp/testfolder.tar.gz
[/toggle]
[toggle title="What is the command to create the the gzipped tar file 'testfolder.tar.gz' from the folder, 'testfolder', using the star utility?"]
$ star -czv -f=testfolder.tar.gz testfolder
[/toggle]
[toggle title="What is the command to install the star utility?"]
$ yum install star
[/toggle]
[toggle title="What is the command to extract the file testfolder.tar.gz, using the star utility?"]
$ star -xzv -f=testfolder.tar.gz
[/toggle]
[toggle title="What is the star command to view the content of the file testfolder.tar.gz?"]
$ star -tv -f=testfolder.tar.gz
[/toggle]
[/accordion]

<hr/>


<h2>Compressing files</h2>
There are 2 main commands that you can use for compressing files <code>gzip</code> and <code>bzip2</code>. 





<h3>Using the gzip file compression utility</h3>
gzip is the more popular command out of the 2. gzip is actually really easy to use, here's an example. First let's create a file that we want to compress:


<pre>
$ echo "hello world" > testfile1.txt
$ file testfile1.txt
testfile1.txt: ASCII text
</pre>

Here we use the <code>file</code> command to confirm the type of file this is. We used the ".txt" for our benefit, and it as an indicator to us of what type of file this is. So it is best practice to always add a meaningful suffix to the files you create. E.g. if you create a shell script, you should give it a suffix such ".sh". However filename suffixes like ".txt" and ".sh" doesn't have any special meaning to Linux. Instead Linux determines a file's type by analyzing its content, in the same way that the <code>file</code> command does. Since a suffix is something that we assign to files ourselves, it means that a filename's can be wrong/inaccurate due to human error. Therefore if a file doesn't have a suffix, or we suspect that it could be wrong, then we can check a file's type using the <code>file</code> command.    


Now here's how to compress this file using gzip:


<pre>
$ gzip testfile1.txt
$ ls -l
total 4
-rw-r--r--. 1 root root 46 Oct 23 19:00 testfile1.txt.gz
$ file testfile1.txt.gz
testfile1.txt.gz: gzip compressed data, was "testfile1.txt", from Unix, last modified: Fri Oct 23 19:00:14 2015
</pre>

As you can see, the gzip command not only compressed the file, but also added it's own suffix to the resulting compressed file, ".gz". This again is for our benefit only and doesn't have any special meaning to Linux or to the gzip utility.  

Now to unzip, we can use the gzip's command's (d)ecompress option. Or we use the gunzip command:

<pre>
$ gunzip testfile1.txt.gz
$ ls -l
total 4
-rw-r--r--. 1 root root 12 Oct 23 19:00 testfile1.txt
$ file testfile1.txt
testfile1.txt: ASCII text
</pre>




<h3>Using the bzip2 file compression utility</h3>

Now here's how we compress a file using bzip2:

<pre>
$ bzip2 testfile1.txt
$ ls -l
total 4
-rw-r--r--. 1 root root 52 Oct 23 19:00 testfile1.txt.bz2
$ file testfile1.txt.bz2
testfile1.txt.bz2: bzip2 compressed data, block size = 900k
</pre>

Once again bzip2 has added the bz2 suffix for our benefit. Now to uncompress, we can use the bzip command's -d option, or we can use the <code>bunzip2</code> command:


<pre>
$ bunzip2 testfile1.txt.bz2
$ ls -l
total 4
-rw-r--r--. 1 root root 12 Oct 23 19:00 testfile1.txt
$ file testfile1.txt
testfile1.txt: ASCII text
</pre>



<h2>Compressing whole directories using tar</h2>

gzip and bzip2 on their own can't compress a directory. They can only compress files. However the tar command can convert (i.e. archive) a file into a single file, which is referred to as a "tar" file. For example let's say we have the following folder, called "testfolder":

<pre> 
$ pwd
/home/codingbee/testfolder
$ ls -l 
total 12
drwxr-xr-x. 5 root root    91 Oct 23 19:58 testfolder
$ file testfolder
testfolder: directory
$ tree
.
└── testfolder
    ├── file2
    ├── file3
    ├── folder1
    │   ├── testfile2.txt
    │   └── testfile3.txt
    ├── folder2
    ├── folder3
    │   └── testfile4.txt
    └── testfile.txt

4 directories, 6 files
</pre>

Now here's how we create a tar file from this folder:

<pre>
$ tar -cvf testfolder.tar testfolder
testfolder/
testfolder/file2
testfolder/file3
testfolder/folder1/
testfolder/folder1/testfile2.txt
testfolder/folder1/testfile3.txt
testfolder/folder2/
testfolder/folder3/
testfolder/folder3/testfile4.txt
testfolder/testfile.txt
$ ls -l
total 12
drwxr-xr-x. 5 root root    91 Oct 23 19:58 testfolder
-rw-r--r--. 1 root root 10240 Oct 23 20:03 testfolder.tar
$ file testfolder.tar
testfolder.tar: POSIX tar archive (GNU)
</pre>

You should read this tar command as 




<blockquote>
Create verbosely, a tar archive with filename of 'testfolder.tar' using contents from 'testfolder'.
</blockquote>



Note, don't use the full path to the testfolder. Instead cd into the directory and then run the tar command. 

Note that tar keeps the original directory intact, and just generates a tar equivalent for it. Also note that the tar command doesn't automatically add any suffixes to the resulting tar file, e.g. a .tar suffix. That's why we manually added this as shown above. 

However if you want to also compress a directory and not just create a tar file, then you can run the gzip command on the tar file:


<pre>
$ gzip testfolder.tar
$ ls -l
total 4
drwxr-xr-x. 5 root root  91 Oct 23 19:58 testfolder
-rw-r--r--. 1 root root 442 Oct 23 20:09 testfolder.tar.gz
$ file testfolder.tar.gz
testfolder.tar.gz: gzip compressed data, was "testfolder.tar", from Unix, last modified: Fri Oct 23 20:09:17 2015
</pre>

So to compress a folder, we had to first had to run the tar command followed by the gzip command. However a faster way to achieve the same result is to instruct the tar command to automatically gzip the tar file straight after it has been created. This is done using the tar command's -z option:


<pre>
$ ls -l
total 0
drwxr-xr-x. 5 root root 91 Oct 23 19:58 testfolder
$ tar -cvzf testfolder.tar.gz testfolder
tar: Removing leading `/' from member names
testfolder/
testfolder/file2
testfolder/file3
testfolder/folder1/
testfolder/folder1/testfile2.txt
testfolder/folder1/testfile3.txt
testfolder/folder2/
testfolder/folder3/
testfolder/folder3/testfile4.txt
testfolder/testfile.txt
$ ls -l
total 4
drwxr-xr-x. 5 root root  91 Oct 23 19:58 testfolder
-rw-r--r--. 1 root root 437 Oct 23 20:27 testfolder.tar.gz
$ file testfolder.tar.gz
testfolder.tar.gz: gzip compressed data, from Unix, last modified: Fri Oct 23 20:27:09 2015
</pre>

Note, if you want tar to use the bzip2 instead of gzip, then you simply using the tar command's "j" option instead of the "z" option.  


<h2>Uncompressing tar files back into directories</h2>




<pre>
$ ls -l
total 4
-rw-r--r--. 1 root root 427 Oct 23 20:51 testfolder.tar.gz
$ tar -xvzf testfolder.tar.gz
testfolder/
testfolder/file2
testfolder/file3
testfolder/folder1/
testfolder/folder1/testfile2.txt
testfolder/folder1/testfile3.txt
testfolder/folder2/
testfolder/folder3/
testfolder/folder3/testfile4.txt
testfolder/testfile.txt
</pre>

You should read this command as:



<blockquote>
Extract verbosely the gzipped tar archive of the filename testfolder.tar.gz
</blockquote>




Now let's check this has worked:

<pre>
$ ls -l
total 4
drwxr-xr-x. 5 root root  91 Oct 23 19:58 testfolder
-rw-r--r--. 1 root root 427 Oct 23 20:51 testfolder.tar.gz
$ tree testfolder
testfolder
├── file2
├── file3
├── folder1
│   ├── testfile2.txt
│   └── testfile3.txt
├── folder2
├── folder3
│   └── testfile4.txt
└── testfile.txt

3 directories, 6 files

</pre>

One thing to remember is that doing an extract like this will overwrite any files/folders that it will get extracted to, here's an example:


<pre>
$ cat testfolder/testfile.txt
hello wooooorld
$ tar -xvf testfolder.tar
testfolder/
testfolder/file2
testfolder/file3
testfolder/folder1/
testfolder/folder1/testfile2.txt
testfolder/folder1/testfile3.txt
testfolder/folder2/
testfolder/folder3/
testfolder/folder3/testfile4.txt
testfolder/testfile.txt
$ cat testfolder/testfile.txt
apple pie
apple cake
banofee cake
orange soda
brown bread
cottage pie
chocolate cake
white bread
upside down pineapple cake
bread and butter pudding
$
</pre>
 
One way to avoid accidantically overwrite 

<h2>Looking inside a tar file</h2>

You can look inside a tar file using the -t option:


<pre>
$ tar -tf testfolder.tar.gz
testfolder/
testfolder/file2
testfolder/file3
testfolder/folder1/
testfolder/folder1/testfile2.txt
testfolder/folder1/testfile3.txt
testfolder/folder2/
testfolder/folder3/
testfolder/folder3/testfile4.txt
testfolder/testfile.txt
</pre>




<h2>Compressing using the star utility</h2>

Standard Tape ARchiver, aka star, is actually a wrapper around tar, which offers some other features on top of tar, for example it avoids overwriting existing files/folders during extracting., but it isn't installed by default, so to use this utility you first have to install it:

<pre>
$ yum install star   
</pre>


Afte that, to create an archive you do:


<pre>
$ ls -l
total 0
drwxr-xr-x. 5 root root 91 Oct 23 19:58 testfolder
$ star -cz -f=testfolder.tar.gz testfolder/
star: 1 blocks + 0 bytes (total of 10240 bytes = 10.00k).
$ ls -l
total 4
drwxr-xr-x. 5 root root  91 Oct 23 19:58 testfolder
-rw-r--r--. 1 root root 458 Oct 23 21:26 testfolder.tar.gz
$ file testfolder.tar.gz
testfolder.tar.gz: gzip compressed data, from Unix, last modified: Fri Oct 23 21:26:32 2015
$
</pre>

Note: we stick with the same ".tar.gz" extension convention to keep this simple. star also doesn't have 


To extract, you do:

<pre>
$ star -x -z -f=testfolder.tar.gz
star: current 'testfolder/' newer.
star: current 'testfolder/file2' newer.
star: current 'testfolder/file3' newer.
star: current 'testfolder/folder1/' newer.
star: current 'testfolder/folder1/testfile2.txt' newer.
star: current 'testfolder/folder1/testfile3.txt' newer.
star: current 'testfolder/folder2/' newer.
star: current 'testfolder/folder3/' newer.
star: current 'testfolder/folder3/testfile4.txt' newer.
star: current 'testfolder/testfile.txt' newer.
star: 1 blocks + 0 bytes (total of 10240 bytes = 10.00k).

</pre>

Note, this process will ONLY overwrite if archive files are newer than those that are on the Operating System. If the OS has newer versions of the corresponding archive files, then star will skip extracting the older archived files. 

Also you can uncompress star files using tar as well. 


You can also use star to extract just a single file:


<pre>
$ tree
.
├── testfolder
│   ├── file2
│   ├── file3
│   ├── folder1
│   │   ├── testfile2.txt
│   │   └── testfile3.txt
│   ├── folder2
│   ├── folder3
│   │   └── testfile4.txt
│   └── testfile.txt              # lets first delete this file
└── testfolder.tar.gz

4 directories, 7 files
$ rm testfolder/testfile.txt
rm: remove regular file ‘testfolder/testfile.txt’? y
$ tree
.
├── testfolder
│   ├── file2
│   ├── file3
│   ├── folder1
│   │   ├── testfile2.txt
│   │   └── testfile3.txt
│   ├── folder2
│   └── folder3
│       └── testfile4.txt
└── testfolder.tar.gz

4 directories, 6 files
$ star -t -f=testfolder.tar.gz
star: WARNING: Archive is 'gzip' compressed, trying to use the -z option.
testfolder/
testfolder/folder1/
testfolder/folder1/testfile2.txt
testfolder/folder1/testfile3.txt
testfolder/folder2/
testfolder/folder3/
testfolder/folder3/testfile4.txt
testfolder/file2
testfolder/file3
testfolder/testfile.txt         # we want to just extract this file
star: 1 blocks + 0 bytes (total of 10240 bytes = 10.00k).
$ star -x -z -f=testfolder.tar.gz testfolder/testfile.txt
star: 1 blocks + 0 bytes (total of 10240 bytes = 10.00k).
$ tree
.
├── testfolder
│   ├── file2
│   ├── file3
│   ├── folder1
│   │   ├── testfile2.txt
│   │   └── testfile3.txt
│   ├── folder2
│   ├── folder3
│   │   └── testfile4.txt
│   └── testfile.txt
└── testfolder.tar.gz

4 directories, 7 files
$

</pre>

]]></Content>
		<Date><![CDATA[2015-04-03]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Secure Shell (SSH)]]></Title>
		<Content><![CDATA[We have covered how to use ssh in <a href="http://codingbee.net/tutorials/rhcsa/rhcsa-the-bash-terminal/">accessing the bash terminal</a> article. 

But now we're going explore other aspects of ssh. ]]></Content>
		<Date><![CDATA[2015-04-04]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - SSH fingerprint authentication]]></Title>
		<Content><![CDATA[Skip this. 

When you ssh to a linux machine, the first thing the remote machine does is repond by sending a fingerprint id to the requester:


<pre>
[sher@localhost .ssh]$ ssh localhost
The authenticity of host 'localhost (::1)' can't be established.
ECDSA key fingerprint is d7:2b:04:af:6f:29:72:7a:56:b7:de:80:b5:6a:e7:9c.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'localhost' (ECDSA) to the list of known hosts.
sher@localhost's password:
Last login: Sat Apr  4 02:09:43 2015 from powershellpc.codingbee.dyndns.org
[sher@localhost ~]$

</pre>


If this finger print id is not already listed in your local ~/.ssh/known_hosts file, then it will ask you to confirm that you trust the remote machine. Once you confirm it, the fingerprint will then get appended to the known_hosts file:   

<pre>
[sher@localhost .ssh]$ cat known_hosts
localhost ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBMNtomP4wf3baK/8nxXCYbIJxHxdDKPXHomYgYqMfQOuTV//LoVM6EXvhyZgpgebCupx+jDalnivqmtVM3kqy7A=
[sher@localhost .ssh]$

</pre>

Note this string is taken from the following file on the remote machine:


<pre>
$ cat /etc/ssh/ssh_host_ecdsa_key.pub
ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBMNtomP4wf3baK/8nxXCYbIJxHxdDKPXHomYgYqMfQOuTV//LoVM6EXvhyZgpgebCupx+jDalnivqmtVM3kqy7A=
</pre>

After that has happened, if you disconnect and then ssh back in again, then the remote machine will send the fingerprint again and this time your local machine will automatically trust the the target machine since there is an encrypted trust token in the known_hosts file for this fqdn/ip-number/hostname. Notice that the known_host file resides in the user's home directory. That means if you try to remotely login again (using the same remote credentials) but this time while logged in as a different user, then you'll need to to another 

However if the target machine has been rebuilt (i.e. has it's os reinstalled), but still retains the same ip-number and fqdn, then the rebuilt target machine will this time issue a different fingerprint id for any new incoming ssh requests. This time when you try to ssh into the machine you will get a warning message, indicating a possible man-in-the-middle attack. 



]]></Content>
		<Date><![CDATA[2015-04-04]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Create SSH session without typing a password (Public/Private Key Authentication)]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="01. Assume you are logged in as the user 'tom' on machine called 'LinuxA'. Assume there is another user account called 'tom' but on machine 'LinuxB'. what is the command to ssh into machine 'LinuxB' as the user tom (also assume private/public keys have not been set up yet)?"]
[tom@LinuxA /]$ ssh linuxB      
# you will then be prompted for password
[/toggle]
[toggle title="02. Assume LinuxB also has the user 'jerry'. In that case what is the command to ssh into the user 'jerry' instead of the user 'tom' (also assume private/public keys have not been set up yet)?"]
[tom@LinuxA /]$ ssh jerry@linuxB
# you will then be prompted for password
[/toggle]
[toggle title="03. What is the command to generate public/private keys?"]
$ ssh-keygen
[/toggle]
[toggle title="04. What files will the above command create?"]
- ~/.ssh/id_rsa     # You can think of this file as a key.
- ~/.ssh/id_rsa.pub # You can think of this file as a padlock.
[/toggle]
[toggle title="05. What is the command to copy the public key to user jerry on LinuxB from user tom?"]
[tom@LinuxA ~]$ ssh-copy-id jerry@LinuxB
[/toggle]
[toggle title="06. What file we get appended wiht the public key on the jerry user's account?"]
/home/jerry/.ssh/authorized_keys
[/toggle]
[toggle title="07. Assume you added a passphrase during the ssh-keygen command. What is the command to enable passphrase caching mode?"]
[tom@LinuxA ~]$ ssh-agent bash 
[/toggle]
[toggle title="08. What is the command, to add your passphrase to the cache?"]
[tom@LinuxA ~]$ ssh-add   # you'll then be prompted to add it to cache
[/toggle]
[/accordion]

<hr/>



If you are logged into one linux machine (lets call it LinuxA), but you now want to connect to another Linux machine (lets call it LinuxB) from LinuxA, then it is possible to do this using the "ssh" command. Here's an example of how this is done

<pre>
[Tom@LinuxA /]$ ssh linuxB
</pre> 


When you use ssh, you need to log in as a user that already exists on LinuxB. In the above case you will get 2 prompts, the first to enter a LinuxB username, followed by that user account's password. However you can reduce this to 1 prompt by specifying the username as part of the initial connection request:

<pre>
[Tom@LinuxA /]$ ssh Jerry@linuxB
</pre> 

This time, you only get prompted once, which is to enter the password for user "Jerry". 


Here I am logged in as user "Tom" on LinuxA but when I ssh'd into LinuxB I assumed the identity of a user called "Jerry". In order to establish the connection, I had to enter jerry's password. 

Once I have logged into LinuxB as the "Jerry" user, I can do anything i want on LinuxB as if you have logged into LinuxB directly as the "Jerry" user.


However it is also possible to suppress the password prompt request as well. This is done by setting up something that's called private/public key authentication.

Private/public keys are essentially a pair of files that resides in the user's home directory, in the .ssh folder. These 2 files are:


<ul>
	<li>~/.ssh/id_rsa        # You can think of this file as a key.</li>
	<li>~/.ssh/id_rsa.pub    # You can think of this file as a padlock.</li>
</ul>


If these files don't exist, or in fact the .ssh folder itself doesn't exist then it means that no public/private keys has not been created for this user yet. In which case you need to generate these keys.


<h2>Generate Private/Public Keys</h2>

To generate public/private keys you simply have to run the following command
    
<pre>$ ssh-keygen</pre>

This command will prompt you to enter some info, but you can simply hit return to all of them to accept the defaults. 

Here's what you'll see happening when you run the command:

<pre>
[tom@localhost ~]$ pwd
/home/tom
[tom@localhost ~]$ ls -la | grep .ssh
[tom@localhost ~]$ ssh-keygen
Generating public/private rsa key pair.
Enter file in which to save the key (/home/tom/.ssh/id_rsa):
Created directory '/home/tom/.ssh'.
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
Your identification has been saved in /home/tom/.ssh/id_rsa.
Your public key has been saved in /home/tom/.ssh/id_rsa.pub.
The key fingerprint is:
7e:78:9e:96:0b:94:d1:20:88:a2:b0:40:d4:92:2e:dc tom@localhost.localdomain
The key's randomart image is:
+--[ RSA 2048]----+
|ooo. .. .        |
|+o...  . o       |
|*oo     . .      |
|+o E     o       |
|.       S        |
|       o .       |
|        + o.     |
|         =o.     |
|         .+.     |
+-----------------+
[tom@localhost ~]$ ls -la | grep .ssh
drwx------. 2 tom  tom    36 Apr  6 13:55 .ssh
[tom@localhost ~]$ cd .ssh/
[tom@localhost .ssh]$ ls -la
total 12
drwx------. 2 tom tom   36 Apr  6 13:55 .
drwx------. 6 tom tom 4096 Apr  6 13:55 ..
-rw-------. 1 tom tom 1675 Apr  6 13:55 id_rsa
-rw-r--r--. 1 tom tom  407 Apr  6 13:55 id_rsa.pub
[tom@localhost .ssh]$

</pre>

As you can see that this command created the .ssh folder, it then generated the id_rsa and id_rsa.pub keys and placed them in this folder.

Now we need to tell the "Jerry" user to accept the "id_rsa.pub" as an "authorised" padlock that a user (in this case Tom) is allowed to unlock (using the id_rsa file) in order to login as the Jerry user. This is done by updating the Jerry user's authorized_keys file. 


<h2>Adding the id_rsa.pub to the authorized_keys file using the ssh-copy-id command</h2> 
We now need to make Jerry aware of the existence of the padlock (id_rsa.pub) as an authorised key that the Jerry user will publicly present this padlock to any incoming ssh requests. This is done by simply appending the id_rsa.pub file's content's to the Jerry user's authorized_keys file, which resides in the Jerry user's .ssh folder:


<pre>[jerry@localhost .ssh]$ pwd
/home/jerry/.ssh
[jerry@localhost .ssh]$ ls -l authorized_keys
-rw-------. 1 jerry jerry 815 Apr  6 14:12 authorized_keys
[jerry@localhost .ssh]$

</pre>

Note, this file might not exist yet, in which case you can manually create this file and copy+paste the id_rsa.pub into it. However doing this manually is quite error prone, which is why there is the <code>ssh-copy-id</code> command which is specifically used for this purpose. For example, here how to add Tom's id_rsa.pub file's content to Jerry's authorized_keys file:

<pre>
[tom@localhost ~]$ ssh-copy-id jerry@localhost
/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed
/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys
jerry@localhost's password:

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh 'jerry@localhost'"
and check to make sure that only the key(s) you wanted were added.

[tom@localhost ~]$
</pre>


Note, this command does prompt you for Jerry's password. 

Now you can ssh without a password prompt:


<pre>
[tom@localhost ~]$ ssh jerry@localhost
[jerry@localhost ~]$</pre>



<h2>Primary and Subprime numbers</h2>
the id_rsa.pub contains the subprimary key in encrypted from. Whereas the id_rsa contains the primary key. 


the id_rsa.pub and id_rsa files are not actually linked to the user account. Hence you can copy them to another user's .ssh folder if needed. 


<h2>Using the Passphrase</h2>

When running the <code>ssh-keygen</code> command to generate your public+private keys you get prompted to enter a "passphrase". This passphrase is used to create an encrypted private key. 

After that you will need to enter this passphrase everytime you want to ssh into a remote server. This at first sounds like it defeats the whole point of public+private keys, and especially makes running shell scripts impossible. But actually you can cache this passphrase into your terminal session, by running the following command:

<pre>
$ ssh-agent bash 
</pre>

This basically starts a new bash session but this time with the ssh-agent utility enabled. 

In our new bash session, we now use the ssh-add command to add our passphrase to the ssh-agent's cache memory:

<pre>
[root@puppetmaster ~]$ ssh-add
Enter passphrase for /root/.ssh/id_rsa:
Identity added: /root/.ssh/id_rsa (/root/.ssh/id_rsa)
[root@puppetmaster ~]$
[root@puppetmaster ~]$ ssh root@puppetagent01
Last login: Mon Oct 12 21:57:12 2015 from puppetmaster
[root@puppetagent01 ~]$
</pre>
  
This time we didn't get a password prompt. If we now exit out of our bash terminal, then ssh to the remote machine again, then we'll get a password prompt:

<pre>
[root@puppetmaster ~]# exit
exit
[root@puppetmaster ~]# ssh root@puppetagent01
Enter passphrase for key '/root/.ssh/id_rsa':
Last login: Mon Oct 12 22:09:24 2015 from puppetmaster
[root@puppetagent01 ~]#
</pre>









 


<strong>Public key / private key authentication</strong>


http://www.cyberciti.biz/faq/how-to-set-up-ssh-keys-on-linux-unix/
http://www.cyberciti.biz/faq/install-ssh-identity-key-remote-host/ 






See also:
https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Deployment_Guide/s2-ssh-configuration-keypairs.html
&nbsp;

http://nerderati.com/2011/03/17/simplify-your-life-with-an-ssh-config-file/
http://linux.die.net/man/5/ssh_config
https://blogs.aws.amazon.com/security/post/Tx3N8GFK85UN1G6/Securely-connect-to-Linux-instances-running-in-a-private-Amazon-VPC


bastion hosts:
https://10mi2.wordpress.com/2015/01/14/using-ssh-through-a-bastion-host-transparently/
http://stackoverflow.com/questions/7114990/pseudo-terminal-will-not-be-allocated-because-stdin-is-not-a-terminal  #use the -t option

]]></Content>
		<Date><![CDATA[2015-04-04]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Copying files from one server to another using SCP]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="Assume you are logged in as the user 'tom' on machine 'LinuxA'. Let’s say that there is a machine called 'LinuxB' and there is a user account called 'jerry' on machine 'LinuxB'. Also on LinuxB you have the file, '/tmp/testfile.txt'. Now what is the command to copy /tmp/testfile.txt from LinuxB to the tom’s home directory on LinuxA?"]
[tom@LinuxA ~]$ scp jerry@LinuxB:/tmp/testfile.txt /home/tom
[/toggle]
[toggle title="Following on from the above scenario, What is the command to copy /home/tom/testfile.txt to jerry's home directory on LinuxB?"]
[tom@LinuxA ~]$ scp /home/tom/testfile.txt jerry@LinuxB:/home/jerry/testfile.txt
[/toggle]
[/accordion]

<hr/>


What if you want to copy files from one machine to another machine. There are a few ways to do this, e.g. via ftp, nfs, cifs....etc. We will cover all these options later. But for now we'll cover the easiest file copying option, called scp, which comes as part of the ssh suite.  

The scp is like using the cp command, but this time you have to specify the remote machine's name, and username. 

For example, you are logged in as the user tom on machine LinuxA. Let's say that there is a machine called LinuxB and there is a user account called 'jerry' on machine LinuxB. Also on LinuxB you have the file, /tmp/testfile.txt. Now here's the command to copy /tmp/testfile.txt from LinuxB to the tom's home directory on LinuxA.:

<pre>
[tom@LinuxA ~]$ scp jerry@LinuxB:/tmp/testfile.txt /home/tom
jerry@LinuxB's password:
testfile.txt                                                                                      100%   12     0.0KB/s   00:00
$ ls -l
total 4
-rw-r--r--. 1 tom tom 12 Nov 14 13:06 testfile.txt

</pre>   

Note, here we got prompted to enter jerry's password. However if you have set up ssh private/public keys, as in the last article, then you won't get prompted to enter the password, since scp is essentially ssh with a bit of extra syntax. 

Now let's say you want to copy to file back to LinuxB but this time place it in the /home/jerry directory, then you do:


<pre>
[tom@LinuxA ~]$ scp /home/tom/testfile.txt jerry@LinuxB:/home/jerry/testfile.txt
jerry@puppetagent01's password:
testfile.txt                                                                                      100%   12     0.0KB/s   00:00
</pre>

Now let's check this has worked:

<pre>
[tom@LinuxA ~]$ ssh jerry@LinuxB
jerry@puppetagent01's password:
[jerry@puppetagent01 ~]$ ls -l /home/jerry/
total 4
-rw-r--r--. 1 jerry jerry 12 Nov 14 13:13 testfile.txt
</pre>


<h2>Further reading</h2>
If you are using a windows machine, and want to copy files to/from a linux machine, then you can still use scp, by firsting installing <a href="https://winscp.net/eng/download.php" rel="nofollow">winscp</a> on your windows machine. ]]></Content>
		<Date><![CDATA[2015-04-06]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - User Account properties]]></Title>
		<Content><![CDATA[Before you create a new user account, there's a few user account properties that you need to know:


<ul>
	<li>usernma</li>
	<li>password - in /etc/shodow, if it shows "!!" then it means that account doesn't have a password yet. You fix this by running the "passwd username" command as the root user</li>
	<li>uid</li>
	<li>guid - full list of guids are available in /etc/groups file</li>
	<li>home directory</li>
	<li>group name - a primary group is created at the same time as the user account, and it is named after the user account's username</li>
	<li>GECOS - a description for the user. </li>
	<li>home directory</li>
	<li>default shell</li>
	<li></li>
</ul>

]]></Content>
		<Date><![CDATA[2015-04-06]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Adding, modifying, and deleting users]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="01. What is the command to create a user called 'donald'?"]
$ useradd donald
[/toggle]
[toggle title="02. What is the command to view what default settings are applied when creating a new user?"]
$ useradd -D
[/toggle]
[toggle title="03. What is the command to view the donald user's uid+gid values along with correspoding primary+supplementary group names?"]
$ id donald
[/toggle]
[toggle title="04. What is the command to set a password for the user 'donald'?"]
$ passwd donald
[/toggle]
[toggle title="05. Which file stores each user's details?"]
/etc/passwd
[/toggle]
[toggle title="06. Which file stores each user's password in hash form?"]
/etc/shadow
[/toggle]
[toggle title="07. What is the command to edit the donald user account's comment section to 'sitting duck'?"]
$ usermod –c "sitting duck" donald 
[/toggle]
[toggle title="08. What is the command to lock donald's user account?"]
$ usermod –L donald
[/toggle]
[toggle title="09. What is the command to delete the 'donald' user account along with their home directory and mailbox?"]
$ userdel –r donald
[/toggle]
[/accordion]
<hr/>





<h2>Create a new user</h2>

Creating a new user account is done by using the <code>useradd</code> command. Here's the help info for this command:


<pre>

$ useradd --help
Usage: useradd [options] LOGIN
       useradd -D
       useradd -D [options]

Options:
  -b, --base-dir BASE_DIR       base directory for the home directory of the
                                new account
  -c, --comment COMMENT         GECOS field of the new account
  -d, --home-dir HOME_DIR       home directory of the new account
  -D, --defaults                print or change default useradd configuration
  -e, --expiredate EXPIRE_DATE  expiration date of the new account
  -f, --inactive INACTIVE       password inactivity period of the new account
  -g, --gid GROUP               name or ID of the primary group of the new
                                account
  -G, --groups GROUPS           list of supplementary groups of the new
                                account
  -h, --help                    display this help message and exit
  -k, --skel SKEL_DIR           use this alternative skeleton directory
  -K, --key KEY=VALUE           override /etc/login.defs defaults
  -l, --no-log-init             do not add the user to the lastlog and
                                faillog databases
  -m, --create-home             create the user's home directory
  -M, --no-create-home          do not create the user's home directory
  -N, --no-user-group           do not create a group with the same name as
                                the user
  -o, --non-unique              allow to create users with duplicate
                                (non-unique) UID
  -p, --password PASSWORD       encrypted password of the new account
  -r, --system                  create a system account
  -R, --root CHROOT_DIR         directory to chroot into
  -s, --shell SHELL             login shell of the new account
  -u, --uid UID                 user ID of the new account
  -U, --user-group              create a group with the same name as the user
  -Z, --selinux-user SEUSER     use a specific SEUSER for the SELinux user mapping

</pre>

Based on the above info, if you want to take a look at useradd's default settings do:

<pre>
$ useradd -D
GROUP=100
HOME=/home
INACTIVE=-1
EXPIRE=
SHELL=/bin/bash
SKEL=/etc/skel
CREATE_MAIL_SPOOL=yes
</pre>

If you want to adjust these default values, then you can do so by editing the <strong>/etc/login.defs</strong> file and the <code>/etc/default/useradd</code> file. 

These default values means that we don't need to write a really long command to explicitly define these when creating a new user. Therefore to create a new user called Donald, we can simply do:

<pre>
$ useradd donald
</pre>

It's also recommended to use the (c)omment flag (for setting a user's name) when creating a user, e.g.

<pre>
$ useradd –c “donald duck” donald
</pre>


Note, for security we can't pass in the password at this stage of creating a user. We'll cover setting password later on. But let's first check that the user now exist by checking the <code>/etc/passwd</code> file:

<pre>
$ cat /etc/passwd | grep donald
donald:x:1005:1005:donald duck:/home/donald:/bin/bash
</pre>  

In Linux, the <code>/etc/passwd</code> file acts as an official register for all the machine's user accounts. In other words, it's the main place that stores all the information for each user that exists on the machine. 

Another handy way to check whether a user exists is with the <strong>id</strong> command:

<pre>
$ id donald
uid=1005(donald) gid=1005(donald) groups=1005(donald)
</pre>

As you can see here, when you create a new user account, then a group (of the same) is also automatically created and the new user is automatically assigned to that group. This group is referred to as the <a href="http://codingbee.net/tutorials/rhcsa/rhcsa-group-administration/">primary group</a>. Therefore in this example, when we created  case a user called "donald" (using the useradd command), a group of the name "donald" was created automatically, and the new user account "donald" was automatically assigned to the "donald" group.     


<h3>User and Group IDs</h3>
Each user account is automatically assigned with a unique id, which is referred to as <strong>user id</strong> (aka uid). Similarly, each group is assigned with it's own unique id, which is referred to as <strong>group id</strong> (aka gid). 




By default the root user's uid and gid values are both 0 as indicated below:

<pre>
$ cat /etc/passwd | grep "^root"
root:x:0:0:root:/root:/bin/bash
$ id
uid=0(root) gid=0(root) groups=0(root) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023
</pre>
Note, the "context" refers to SELinux Security Context, which we'll cover later. 


The uid and gids for internal RedHat "system" user accounts ranges between 1-200. Files/Folders can be owned by these accounts. Also process can be run under these accounts. 

The 201-999 range is reserved for system user accounts that can run processes but don't own any files or folders. 









<h3>Primary and Supplementary Groups</h3>
Any file or folder must be owned by both a user and a group. This is why a user's primary group is so important, when a user create's a new file (or folder) then that user's primary group automatically assumes group ownership of that item. That's why a user must belong to exactly one primary group.   

However users can also be added to other groups as well, and these additional groups are referred to as <a href="http://codingbee.net/tutorials/rhcsa/rhcsa-group-administration/">supplementary groups</a>. 



<h2>Set a password</h2>

If we look at the <code>/etc/shadow</code> file for our new user:


<pre>
$ cat /etc/shadow | grep donald
donald:!!:16716:0:99999:7:::
</pre>

You'll see "!!". This indicates that a password has not been set yet for this user. So now we set the password for our new account, using the <strong>passwd</strong> command:

<pre>
$ passwd {username}
</pre>

You need to run the passwd as the root user. If you are logged in as a normal user, and just want to change your current user's password, then simply run passwd commands on it's own:

<pre>
$ passwd 
</pre>

Therefore, going back to our donald user, let's set the password:


<pre>
[root@localhost ~]# passwd donald
Changing password for user donald.
New password:
BAD PASSWORD: The password is shorter than 8 characters
Retype new password:
passwd: all authentication tokens updated successfully.
[root@localhost ~]#


</pre>

We may get "BAD PASSWORD" warnings, but since we are logged in as root, we can ignore these error messages.

If we check the shadow file again, we'll see that the "!!" is no longer there:

<pre>
[root@localhost ~]# cat /etc/shadow | grep "donald"
donald:$6$MJmDFGEe$GR/DkhS.ARMxS9LFHwB8yK28X5B7et6d9lQiqSyEE41pgQ3t4yUZSu8lMgR0NFmzU5aOGNC5nrpsIU4NiF1hh/:16531:0:99999:7:::
[root@localhost ~]#
</pre>

It is now replaced by an encrypted hash. 

Note this file also stores <a href="http://codingbee.net/tutorials/rhcsa/rhcsa-password-properties-passwd-and-chage-commands/">password policy</a> configurations, such as password expiry dates. We'll cover this later. 



<h2>Modifying a user account</h2>

To modifying an existing user account, you can simply use the usermod command:

<pre>
$ usermod --help
Usage: usermod [options] LOGIN

Options:
  -c, --comment COMMENT         new value of the GECOS field
  -d, --home HOME_DIR           new home directory for the user account
  -e, --expiredate EXPIRE_DATE  set account expiration date to EXPIRE_DATE
  -f, --inactive INACTIVE       set password inactive after expiration
                                to INACTIVE
  -g, --gid GROUP               force use GROUP as new primary group
  -G, --groups GROUPS           new list of supplementary GROUPS
  -a, --append                  append the user to the supplemental GROUPS
                                mentioned by the -G option without removing
                                him/her from other groups
  -h, --help                    display this help message and exit
  -l, --login NEW_LOGIN         new value of the login name
<mark>  -L, --lock                    lock the user account</mark>
  -m, --move-home               move contents of the home directory to the
                                new location (use only with -d)
  -o, --non-unique              allow using duplicate (non-unique) UID
  -p, --password PASSWORD       use encrypted password for the new password
  -R, --root CHROOT_DIR         directory to chroot into
  -s, --shell SHELL             new login shell for the user account
  -u, --uid UID                 new UID for the user account
  -U, --unlock                  unlock the user account
  -Z, --selinux-user SEUSER     new SELinux user mapping for the user account
</pre>


Based on this, to add/change a user account's comment field, we do:


<pre>
$ usermod –c “donald duck” donald 
</pre>

Or to lock the user, we do:

<pre>
$ usermod –L donald
</pre>

This command ends up simply prefixing an "!" at the beginning of the hash in the /etc/shadow file:

<pre>
$ cat /etc/shadow | grep donald
donald:!$6$LtEzIHPi$RTfF8d0R1lYyQVHDClyb/PFtUKdCaEXU4uCEomLql05IEVYF9.uPT1b1z6iWllXPq/q.L2Qw85lNIBFGkNvM/.:16716:0:99999:7:::
</pre>



<h2>Deleting user accounts</h2> 


We can delete users using the <strong>userdel</strong>. However this command doesn't delete a user’s:

<ul>
	<li>home directory</li>
	<li>mailbox</li>
</ul>

 

This is a precautionary measure in case there is something valuable in the user’s home directory or mailbox. However if you want to remove them as well, then do:

<pre>
$ userdel –r donald
</pre>]]></Content>
		<Date><![CDATA[2015-04-06]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Hard and Soft Links]]></Title>
		<Content><![CDATA[<h2>Overview of hard and soft links</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What is the command to view the inode value for the file /tmp/testfile.txt?"]
$ ls -li /tmp/testfile.txt
[/toggle]
[toggle title="What is the command to create a hard link for /tmp/testfile.txt called /tmp/anothertestfile.txt?"]
$ ln /tmp/testfile.txt /tmp/anothertestfile.txt
[/toggle]
[toggle title="What command can you run to confirm this has worked?"]
$ ls -li /tmp/testfile.txt     # third column should show "2"
[/toggle]
[toggle title="True or False, hard links stay intact even if you change the file’s name, or the hard link's name, or both of them?"]
True
[/toggle]
[toggle title="True or False, hard-links still stay intact if you move the file, or the hard-link to another folder as long as this new target folder is within the same files system?"]
True
[/toggle]
[toggle title="True or false, you can create hard-links for folders?"]
false
[/toggle]
[toggle title="What is the command to create a soft/symbolic link for /tmp/testfile.txt called /tmp/yetanothertestfile.txt??"]
$ ln -s /tmp/testfile.txt /tmp/yetanothertestfile.txt
[/toggle]
[toggle title="True or False, soft-links still stay intact if you move the file, to another folder?"]
False
[/toggle]
[toggle title="True or False, hard-links will stay intact if you move the file to another folder which happens to reside in a different files system?"]
False
[/toggle]
[/accordion]

<hr/>





In windows you'll have come across shortcuts, e.g. desktop shortcuts. In windows, shortcuts gives you access to the same thing but from a different folder. In Linux you can do the same thing, but with a feature called "links". Links allow users to edit the same file from different locations.  There are 2 types of links:

<ol>
	<li>soft links, aka symbolic links</li>
	<li>hard links</li>
</ol>

Both of these types are links are created by the "ln" command. 

To understand how symbolic and hard links differ from one another, you need to first understand inodes:


<h2>Hard links</h2>

An inode (aka “index node”) is an entry in a filesystem table that reference’s a location in a filesystem. In other words, it's like a reference book has an index at the back, containing a list of keys along with page numbers, a filesystem also has an index, but instead of the keys being keyword-terms, they are the absolute paths to files, and instead of page numbers, it lists inode values.

You can look-up the inode value for a particular file using the ls command's -i option:

<pre>
$ echo "hello world" > testfile.txt
$ ls -li /tmp/testfile.txt
102113012 -rw-r--r--. 1 root root 12 Apr  3 19:08 /tmp/testfile.txt
</pre>  
In this example, the key is "/tmp/testfile.txt" and it's inode value is "102113012"

Also notice the third column, which shows a "1". This says how many entries in the index points to the same inode value. 
&nbsp;

inode works a bit like the index of a reference book, where a file's name are entries in the index, and inodes are the corresponding page numbers, which points to a page (bit of data) in the book. This analogy is how hard-links works, if you imagine hard links as the index's entry, and inode values are the page numbers. Hence creating a hardlink is like creating a new entry in the index, but references the same inode value (aka page number).

 
<ul>
	<li>the hard link stays intact even if you change the file’s name, or the hard link's name, or both of them. This is analogous to amending the index key's but the inode values are left unchanged. </li>
	<li>The hard-link still stays intact if you move the file, or the hard-link to a different folder as long as this folder is within the same files system. This again is analogous to chnaging the index key's, since they are all written in absolute path form, but inode value stays the same. This analogy breaks down if you move the file to a different device-block, as this is analogous to tearing out the page from one book and placing it in another book.</li>
</ul>
&nbsp;

Basically both the file-name and hard-link have equal claim to the actual bit of data, and in that respect, they are basically the same thing....pointers to an inode. Therefore if you do move a file to another device block, then which ever pointer, you used to perform this task will remain intact (i.e. both the index key and value will be updated accordingly), whereas the other will no longer reference this file. 

However the link breaks when you move the file-or-link to a new block device (i.e. new filesytem). It’s basically like moving the page (bit of data) to a different book. In this scenario, as a failsafe, a copy of that file (with a new inode value) is generated on the new filesystem, so that the link/filename is still useable. just take caution, when making changes to one file, since it won’t be reflected in the other file.



Here’s how to create a hard-link:

<pre>
$ ln /tmp/testfile.txt /tmp/anothertestfile.txt
</pre>

Now let's confirm that both the original filename and the hardlink have the same inode:

<pre>
$ ls -li /tmp
total 8
102113012 -rw-r--r--. 2 root root 12 Apr  3 19:20 anothertestfile.txt
102113012 -rw-r--r--. 2 root root 12 Apr  3 19:20 testfile.txt
</pre>

Also see that the earlier "1" value has now incremented to "2". 

&nbsp;

By the way, you will find that all newly created directories always start with a inode value of “2”, that’s because of the “.” directory that’s automatically created within the folder, is actually an auto-generated hardlink.

&nbsp;

note: it is not possible to create a hard link for a folder....you will get an error message if you try.


Note: hard links have and actual paths always have the same ugo+rwo permissions, if you change the permissions using chmod for the hard-link it will also automatically get reflected on the main link too.  

&nbsp;
One final thing to note is that you can only create hard-links for files that you at least have read permissions for. This is for security reasons. Obviously if you are logged in as the root user, then you won't experience this restriction.  

<h2>soft links (aka symbolic links)</h2>

compared to hardlinks, soft links works more like how microsoft shortcuts.


Softlinks essentially redirects you to the source file. That means that soft links essentially gets broken if try to move the source-file to a different location, or rename this file.

You create symbolic links by using the ln command again, but this time with the -s switch enabled:

<pre>
ln -s /tmp/testfile.txt /tmp/yetanothertestfile.txt
</pre>



Now let's check that this has worked by by using the ls command again:

<pre>

[root@localhost tmp]# ls -li
total 8
102113012 -rw-r--r--. 2 root root 12 Apr  3 19:20 anothertestfile.txt
102113012 -rw-r--r--. 2 root root 12 Apr  3 19:20 testfile.txt
102113022 lrwxrwxrwx. 1 root root 17 Apr  3 19:34 yetanothertestfile.txt -> /tmp/testfile.txt
[root@localhost tmp]#


</pre>

Notice that symbolic links are denoted by "l" prefixed to the user-group-other permissions. Also notice that the inode value is different to that of the original file. Also notice that the filename has arrow pointing to the actual file. 

Also notice that by default symbolic links have 777 permissions. That's because the actual permissions are managed by the actual file that the soft-link is pointing to. 

Going back to our book analogy, when you lookup a certain term, you might that instead of it giving a page number, it prompts you too "see" another keyword term instead. This is effectively what symbolic links are. However the symbolic link also has a inode value, that's because in Linux, everything can be thought of as a file, that included folders and symbolic links as well, since they all use up disk space (or pages) to store them.     



Also this doesn’t cause the file’s link value (ls -l, 2nd column) increment by one, since soft-links are conceptually different things to hard-links, as indicated by the fact that softlinks have different inode values compared to the source-file’s inode value.

Since soft-links reference the key of another file, it means that the soft-link breaks if you rename or move the source file.

&nbsp;

However compared to hardlinks, softlinks have 2 advantages:

<ul>
	<li>soft links works across filesystems (hdd, partitions,...etc)</li>
	<li>soft links can be used to link to directories.</li>
</ul>


<strong>See also:</strong>

&nbsp;

<a href="http://www.thegeekstuff.com/2012/01/linux-inodes/">http://www.thegeekstuff.com/2012/01/linux-inodes/</a> - learn about inodes

&nbsp;

<a href="http://www.thegeekstuff.com/2009/07/unix-stat-command-how-to-identify-file-attributes/">http://www.thegeekstuff.com/2009/07/unix-stat-command-how-to-identify-file-attributes/</a> - learn about the “stat” command, which gives various info about a file.

&nbsp;

<a href="http://www.thegeekstuff.com/2010/10/linux-ln-command-examples/">http://www.thegeekstuff.com/2010/10/linux-ln-command-examples/</a> - 9 and 10 are quite good to know. 9 basically says softlinks lets you virtually increases a diskspace                                                                                                   and 10 says that with hardlinks, a file isn’t deleted when you delete the source, you have to delete

all the hardlinks as well.

&nbsp;

<a href="http://www.cyberciti.biz/tips/understanding-unixlinux-symbolic-soft-and-hard-links.html">http://www.cyberciti.biz/tips/understanding-unixlinux-symbolic-soft-and-hard-links.html</a>

&nbsp;]]></Content>
		<Date><![CDATA[2015-04-06]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Creating, modifying, and deleting groups]]></Title>
		<Content><![CDATA[<h2>Managing groups overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What is the command to display the 'donald' user's uid, gids, primary, and all supplementary groups?"]
$ id donald
[/toggle]
[toggle title="What's the command to view the primary+secondary groups that the user donald belongs to?"]
$ groups donald
[/toggle]
[toggle title="Which file acts as a register that lists all the available groups along with a comma separated list of all users that belong to each group?"]
/etc/group
[/toggle]
[toggle title="What's the command to create the group 'sales'?"]
$ groupadd sales
[/toggle]
[toggle title="What's the command to delete the group 'sales'?"]
$ groupdel sales
[/toggle]
[toggle title="What's the command to add the 'donald' user to the 'sales' group?"]
$ usermod  -aG sales donald
[/toggle]
[toggle title="What's the (gpasswd) command to remove the donald user from the sales group?"]
$ gpasswd -d donald sales
[/toggle]
[toggle title="What's the command to change the donald user's primary group to 'sales'?"]
$ usermod -g sales donald
[/toggle]
[toggle title="What are the commands to temporarily change donald's primary group to the 'sales' group?"]
$ su - donald
$ newgrp sales
$ groups
sales donald
[/toggle]
[toggle title="What is the command to back out of a temparary group?"]
$ exit   # this just exits the group but not the terminal session.
[/toggle]
[toggle title="What's the command to rename the 'sales' group to 'retail'?"]
$ groupmod -n retail sales
[/toggle]
[toggle title="What's the command to make the 'donald' user the group administrator for the group 'sales'?"]
$ gpasswd –A donald sales 
[/toggle]
[toggle title="What's the command that the group administrator, 'donald' can use to add the user goofy to the group 'sales'?"]
$ gpasswd -a goofy sales
[/toggle]
[/accordion]

<hr/>




<h2>Types of Groups</h2>

When you create a new user account, then a group (of the same) is also automatically created and the new user is automatically assigned to that group. This group is referred to as the <strong>primary group</strong>. Therefore when we create a user called "donald" (using the useradd command), a group called "donald" is then created automatically, and the new user "donald" is automatically assigned to the new "donald" group.     

The main reason for this because all files and folders created on a Linux machine must be owned by both a user and a group. This is why a user's primary group is so important, when a user create's a new file (or folder) then that user's primary group automatically assumes group ownership of that item. That's why a user must belong to exactly one primary group. A user's primary group is also known as a private-group (since only that user is a member of that group). 
  
However users can also be added to other groups as well, and these additional groups are referred to as <strong>supplementary groups</strong>. 

Having supplementary groups is useful. For example you can end up with lots of user accounts. One of things you will want to do is organize all these user accounts into various groups. For example you may have a several users who work in your company's sales department in which case you want to create a group called "sales" and add each user to that group. 


You can use the <code>id</code> to view what primary and supplementary groups a user belongs to. For example, for a user called "donald", we do:

<pre>
$ id donald
uid=1001(donald) gid=1001(donald) groups=1001(donald),1002(sales),1003(disney)
</pre>

Here we can see that donald's primary group is "donald", and this user also belongs to 2 supplementary groups, "sales" and "disney". You can also view this info using the <code>groups</code> command:

<pre>
$ groups donald
donald : donald sales disney
</pre>

Note: By default, the first group in the list is the primary group. 

The <code>/etc/group</code> is the main file that stores all the information about all primary and supplementary groups that exist on the machine:

<pre>
$ cat /etc/group
root:x:0:
bin:x:1:
daemon:x:2:
sys:x:3:
adm:x:4:
wheel:x:8:vagrant,mickey
.
.
...etc. 
</pre>


A new entry is added to this file each time a new primary or supplementary group is created. There are three fields per entry, they are, group name, group password, group id, and a comma seperated list of all the users that belong to the group (except for the group's primary user). For more info about this file, see: 

<pre>
$ man 5 group
</pre>




<h2>Creating and Deleting groups</h2>
 
Primary groups are created automatically when we create a new user. Therefore in practice, we usually need to only create supplementary groups. To create a new group, we use the <strong>groupadd</strong> command:


<pre>
$ groupadd sales
$ grep sales /etc/group
sales:x:1006:
</pre>

Similarly to delete a group we do:

<pre>
$ groupdel sales
$ grep sales /etc/group
$
</pre>




<h2>Add user to a group</h2> 

To add a user to a (supplementary) group, you use the <strong>usermod</strong> command:


<pre>
$ usermod --help
Usage: usermod [options] LOGIN

Options:
  -c, --comment COMMENT         new value of the GECOS field
  -d, --home HOME_DIR           new home directory for the user account
  -e, --expiredate EXPIRE_DATE  set account expiration date to EXPIRE_DATE
  -f, --inactive INACTIVE       set password inactive after expiration
                                to INACTIVE
  -g, --gid GROUP               force use GROUP as new primary group
  -G, --groups GROUPS           new list of supplementary GROUPS
  -a, --append                  append the user to the supplemental GROUPS
                                mentioned by the -G option without removing
                                him/her from other groups
  -h, --help                    display this help message and exit
  -l, --login NEW_LOGIN         new value of the login name
  -L, --lock                    lock the user account
  -m, --move-home               move contents of the home directory to the
                                new location (use only with -d)
  -o, --non-unique              allow using duplicate (non-unique) UID
  -p, --password PASSWORD       use encrypted password for the new password
  -R, --root CHROOT_DIR         directory to chroot into
  -s, --shell SHELL             new login shell for the user account
  -u, --uid UID                 new UID for the user account
  -U, --unlock                  unlock the user account
  -Z, --selinux-user SEUSER     new SELinux user mapping for the user account

</pre>




For example to add the user "donald" to the "sales" group, we do:


<pre>
$ usermod  -aG sales donald
</pre>

Now we use "id" commmand to confirm that donald is now a member of that group:

<pre>
$ id donald
uid=1005(donald) gid=1005(donald) groups=1005(donald),1006(sales)
</pre>


Note: the group id (gid) field shows what the primary group is, whereas the group fields shows an array of all groups (primary and supplementary groups) that the user belongs to. 


However to remove a user from a group, you can use the usermod command again, bit it's more fiddly, isntead it's easier to remove a user from a group suing the gpassswd command:

<pre>
$ gpasswd -d goofy disney
Removing user goofy from group disney
</pre>

We'll cover more about the gpasswd command later. 

If you want, you can view the "group" file to see who are members of a given group:

<pre>
$ grep sales /etc/group
</pre>


If you want to remove a user from all supplementary groups then you do:

<pre>
$ id donald
uid=1005(donald) gid=1005(donald) groups=1005(donald),1006(sales),1007(sales1)
$ usermod -G ""  donald
$ id donald
uid=1005(donald) gid=1005(donald) groups=1005(donald)
</pre>




<h2>Setting primary group</h2>
When a user creates a file, it automatically gets a group ownership assigned to it. This is usually the user's private group. That's because by default the private group is also the user's primary group. However if you want to change that, e.g. change it to the "sales" group then you can use the "-g" option. For example let's change the donald user's primary group to "sales":


<pre>
$ ls -l /home/donald
total 0
-rw-rw-r--. 1 donald donald 0 Apr  9 21:18 testfile
$ id donald
uid=1005(donald) gid=1005(donald) groups=1005(donald),1006(sales)
$ usermod  -g sales donald
$ id donald
uid=1005(donald) gid=1006(sales) groups=1006(sales)
$ ls -l /home/donald
total 0
-rw-rw-r--. 1 donald sales 0 Apr  9 21:18 testfile
</pre>

Notice that any files that were previously owned by the donald private group (since they were created by the "donald" user) have automatically switched to the new primary group!!! 

Note, changing the primary group using usermod is persitent, where is using the newgrp command isn't persistent. We'll cover newgrp next. 

<h2>temporary change primary group</h2>
Sometime you may want to change your primary group temporarily. This can be done using the <strong>newgrp</strong> command:

<pre>
[donald@localhost ~]$ id
uid=1005(donald) gid=1005(donald) groups=1005(donald),1006(sales) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023
[donald@localhost ~]$ newgrp sales
[donald@localhost ~]$ id
uid=1005(donald) gid=1006(sales) groups=1005(donald),1006(sales) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023
[donald@localhost ~]$ exit
exit
[donald@localhost ~]$ id
uid=1005(donald) gid=1005(donald) groups=1005(donald),1006(sales) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023

</pre>

The newgrp is kind of similar to the way the su command works, but instead it is for switching groups. This means that when you use this command, you actually go inside another nested bash shell, with your new primary group. So when you "exit", in effect this resets your primary group to it's default. Therefore you can think of the primary group as which group "your logged into".  



<h2>Rename a group</h2>

To rename a group, you need use the <code>groupmod</code> command:

<pre>
$ groupmod --help
Usage: groupmod [options] GROUP

Options:
  -g, --gid GID                 change the group ID to GID
  -h, --help                    display this help message and exit
  -n, --new-name NEW_GROUP      change the name to NEW_GROUP
  -o, --non-unique              allow to use a duplicate (non-unique) GID
  -p, --password PASSWORD       change the password to this (encrypted)
                                PASSWORD
  -R, --root CHROOT_DIR         directory to chroot into


</pre>


For example if we have a group called "sales" and we want to change it's name to "retail", then we do:

<pre>
$ groupmod -n retail sales
</pre>

All users that belonged to the "sales" group will now belong to the "retail" group instead. 


<h2>Setup group administrators</h2>


You can set up users as group admins for a given group. Group-admins are users who can add/remove members from a group. This is done using the gpasswd command:


<pre>
$ gpasswd --help
Usage: gpasswd [option] GROUP

Options:
  -a, --add USER                add USER to GROUP
  -d, --delete USER             remove USER from GROUP
  -h, --help                    display this help message and exit
  -Q, --root CHROOT_DIR         directory to chroot into
  -r, --delete-password         remove the GROUP's password
  -R, --restrict                restrict access to GROUP to its members
  -M, --members USER,...        set the list of members of GROUP
  -A, --administrators ADMIN,...
                                set the list of administrators for GROUP
Except for the -A and -M options, the options cannot be combined.

</pre>


Let's say we have a group called 'disney', so to view a list of members for this group, we can do:

<pre>
$ cat /etc/group | grep disney
disney:x:1005:donald,minnie
$ cat /etc/gshadow | grep disney
disney:!::donald,minnie
</pre>

Now let's say we want to make the user "mickey" a group administrator for the disney group, then we do:

<pre>
$ gpasswd -A mickey disney
</pre>

Note, only the root user can use the “A” option when running this command. 

Now 'let's chekc that this has worked:

<pre>
$ cat /etc/group | grep disney
disney:x:1005:donald,minnie
$ cat /etc/gshadow | grep disney
disney:!:mickey:donald,minnie
$ id mickey
uid=1003(mickey) gid=1004(mickey) groups=1004(mickey)
</pre>

As you can see, the user mickey, isn't a member of the group, but his is a group administrator for this group. The group administrator can then add/remove new members to the group:

<pre>
$ su - mickey
[mickey@puppetmaster ~]$ gpasswd -a goofy disney
Adding user goofy to group disney
</pre> 

Next we can confirm that this has worked like this:

<pre>
$ cat /etc/group | grep disney
disney:x:1005:donald,minnie,goofy
$ cat /etc/gshadow | grep disney
disney:!:mickey:donald,minnie,goofy
</pre>

see also:

https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Deployment_Guide/ch-Managing_Users_and_Groups.html]]></Content>
		<Date><![CDATA[2015-04-06]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Special File and Folder Permissions]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="01. What is the command to modify permission of /tmp/script1.sh so that any member of of the 'other' group can run this script as if it is being run by the script's owner?"]
$ chmod u+s /tmp/script1.sh
[/toggle]
[toggle title="02. What is the name for this type of permission?"]
Set User ID (SUID). 
It is very unlikely that you will ever need to set this type of permission on a file, but commonly used for applying on folders.
[/toggle]
[toggle title="03. What is the command to check that this has worked?"]
$ ls -l /tmp/script1.sh
# look for "s" under user's execute permission
[/toggle]
[toggle title="04. Alternatively what is the command to alter this script's permission, so to run this script as if it is being run by a member of the script's group-owner?"]
$ chmod g+s /tmp/script1.sh
Again, it is highly unlikely that you will need to set this type of permission. 
[/toggle]
[toggle title="05. What is the command to check that this has worked?"]
$ ls -l /tmp/script1.sh
# look for "s" under group's execute permission
[/toggle]
[toggle title="06. What is this type of permission called?"]
Set Group ID SGID - it is commonly used for folders.
[/toggle]
[toggle title="07. What is the command to create a group called 'accountants'?"]
$ groupadd accountants 
[/toggle]
[toggle title="08. What is the command to assign a user called 'tom' to this new group?"]
$ usermod –aG accountants tom
[/toggle]
[toggle title="09. What is the command to change the /tmp/finance folder's ownernship, so that no user owns it, and it's group ownership is set to 'accountants'?"]
$ chown nobody:accountants /tmp/finance
[/toggle]
[toggle title="10. Now what is the command to ensure only members of the accountants group has full priveleges and no one else has any priveleges at all?"]
$ chmod 070 /tmp/finance
[/toggle]
[toggle title="11. What is the command to apply permissions so that any files+folders that gets created in the '/tmp/finance' folder automatically gets adopted by the 'accountants' group?"]
$ chmod g+s /tmp/finance
[/toggle]
[toggle title="12. What is the command to apply a restriction so that members of the 'accountants' group can't delete files/folders that resides in the /tmp/finance folders, unless they are also the file's owner?"]
$ chmod +t /tmp/finance
[/toggle]
[toggle title="13. What is the command to check that this has worked?"]
$ ls -l /tmp | grep finance
# look for "t" under other's execute permissions
[/toggle]
[toggle title="14. What is this type of permission called?"]
The "sticky bit". It is indicated by a "T"
$ ls -l
d---rws--T. 2 nobody accountants 25 Apr 11 11:26 /tmp/finance
[/toggle]
[toggle title="What is the command?"]

[/toggle]
[toggle title="What is the command?"]

[/toggle]
[/accordion]

<hr/>








There will be times when the standard ugo and rwx permissions don't provide enough flexibility to allow a group of people to work collaboratively. That's why another set of permissions, called "special permissions" are available. Here's an overview of these permissions:


[table id=special-permissions-sticky-bit /]
 

<h2>Set User ID (SUID)</h2>

This permission only makes sense if you apply it to a file that is an executable (e.g. a shell script). 

You can apply this permission with chmod command and the "s" value:

<pre>
$ ls -l
total 4
-rwx---r-x. 1 root root 33 Apr 11 10:24 testscript.sh
$ chmod u+s testscript.sh
$ ls -l
total 4
-rws---r-x. 1 root root 33 Apr 11 10:24 testscript.sh
</pre>
 
The "s" under the user's permission means that if an "other" runs this script, then the script will run with the same level of privileges as who ever is the owner of this file, which in this case is the root user. Hence if an ordinary user runs this executable and this executable attempts to delete folders/files that only root has access to, then it will still be able to delete them. 

Note: the other permission still needs to be minimum of r-X in order for it to be able to run this executable. 

This is what makes this type of permission dangerous, especially if the file's owner is root. In the real world it is extremely unlikely that you will ever need to set this type of permission on an executable. 

The reason this permission type exists in the first place is because some internal system executables requires this permission, here is a list of them:

<pre>
$ find / -perm /4000 2>/dev/null
/usr/bin/fusermount
/usr/bin/su
/usr/bin/chage
/usr/bin/gpasswd
/usr/bin/mount
/usr/bin/newgrp
/usr/bin/chfn
/usr/bin/chsh
/usr/bin/umount
/usr/bin/ksu
/usr/bin/passwd      # this is a good example, e.g. when a user wants thier own password 
/usr/bin/pkexec
/usr/bin/crontab
/usr/bin/Xorg
/usr/bin/at
/usr/bin/staprun
/usr/bin/sudo
/usr/sbin/pam_timestamp_check
/usr/sbin/unix_chkpwd
/usr/sbin/userhelper
/usr/sbin/usernetctl
/usr/sbin/mount.nfs
/usr/lib/polkit-1/polkit-agent-helper-1
/usr/lib64/dbus-1/dbus-daemon-launch-helper
/usr/libexec/spice-gtk-x86_64/spice-client-glib-usb-acl-helper
/usr/libexec/pulse/proximity-helper
/usr/libexec/sssd/krb5_child
/usr/libexec/sssd/ldap_child
/usr/libexec/sssd/selinux_child
/usr/libexec/abrt-action-install-debuginfo-to-abrt-cache
/usr/libexec/qemu-bridge-helper
$
 
</pre>



<h2>Set Group ID (SGID)</h2>

This is a special permission that can be applied to files and folder.

 
<h3>Set Group ID (SGID) for files</h3>

You can apply SGID permission to a file using chmod along with the "s" value being attached to the group setting:

<pre>
$ chmod g+s testscript.sh
$ ls -l
total 4
-rwx--Sr-x. 1 root root 33 Apr 11 10:24 testscript.sh
</pre>  

This pretty much has the same affect as the SUID, but instead of executable being run by an ordinary user with the elevated privileges of the script's owner, it is being run as if run by one of the group's member. 

Therefore once again this type of permission on a file is very dangerous, and in the real world it is extremely unlikely that you will ever need to set this type of permission on an executable.



<h3>Set Group ID (SGID) for folders</h3>


Applying this type of permission to a folder, results in something different happening which actually has real world applications. 

It's best to see this with an example. 


Lets assume we want to set up a “team folder” for team collaboration. For example lets say we want to create a folder “research-team-folder” which is only to be used by the “research” team, So here is an example of this in practice.

First we create a group for all the team members to be added to:

<pre>
$ groupadd research 
</pre>
      
This creates a new group called "research" that all the members of the research team can belong to.


Now let's assign the all the research team's members to the research group:


<pre>
$ usermod –aG {group-name} {username}                              
</pre>

You need to do this for each member of the research team.


Next we create the team's folder

<pre>
$ mkdir research-team-folder
</pre>

Now let's change group ownership

<pre>
$ ls -l
total 0
drwxr-xr-x. 2 root root 6 Apr 11 11:04 research-team-folder
$ chown nobody:research research-team-folder
$ ls -l
total 0
drwxr-xr-x. 2 nobody research 6 Apr 11 11:04 research-team-folder
</pre>

Notice we used the special reserved word "nobody" here. This folder is now owned by the "research" group, and no user (u) owns this folder. There is also the "nogroup" special reserved word too. 


Next we change this folder's permisions to:


<pre>
$ chmod 070 research-team-folder
$ ls -l
total 0
d---rwx---. 2 nobody research 6 Apr 11 11:04 research-team-folder
</pre>


This is so that only members of the “research” group can access this folder and modify it’s content.


However at this point we encounter a problem. Each user's default primary group, is the user's respective private group. This means that all folder/file will automatically get adopted by the primary group (and default permissions assigned according to umask). So each time a user creates a file/folder, then they would also have to do at least one of the following in order for collaboration to work:

<ul>
	<li>use chmod to give others (o) rwx permissions</li>
	<li>use chgrp or chown to change file's/folder’s group ownership</li>
</ul>




However there is a better alternative to the above 2 options....which is to apply something that's known as <strong>special permissions</strong>.  

You can apply a special permission to a directory so that any content that is created within the research-team-folder, is automatically adopted by the “research” group, instead of the user’s primary group. That special permission is the SGID permission, which is applied as follows:


<pre>
[root@localhost scripts]# chmod g+s research-team-folder
[root@localhost scripts]# ls -l
total 0
d---rws---. 2 nobody research 6 Apr 11 11:04 research-team-folder
</pre>

Now let's switch to a different user and create a new file in this folder and see which group the newly created file belongs:

<pre>
[root@localhost /]# su - sher
Last login: Sat Apr 11 11:15:53 BST 2015 
[sher@localhost ~]$ id
uid=1002(sher) gid=1002(sher) groups=1002(sher),1008<strong>(research)</strong> context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023
[sher@localhost ~]$ cd /tmp/research-team-folder/
[sher@localhost research-team-folder]$ touch testfile.txt
[sher@localhost research-team-folder]$ ls -l
total 0
-rw-rw-r--. 1 sher <strong>research</strong> 0 Apr 11 11:26 testfile.txt
[sher@localhost research-team-folder]$
</pre>

Success!

This is the recommanded way to create a team folder that is used to be for collaboration. It is also what the SGID permission is primarily used for. We should also apply sticky bit permission too, which is covered next. 



<h2>Sticky bit</h2>


Now that we have created a shared team folder for collaboration, it means that users can create files and share it easily with each other by placing the files in the shared team folder. However it also means that team member can also delete each other's files. This is where a special permission called "sticky bit" comes to the rescue.

So in this case, we need to apply the sticky bit permission to our shared team folder, which we do by setting the "T" option with the chmod command:



<pre>
$ ls -l
total 0
d---rws---. 2 nobody research 25 Apr 11 11:26 research-team-folder
$ chmod +t research-team-folder/
$ ls -l
total 0
d---rws--<strong>T</strong>. 2 nobody research 25 Apr 11 11:26 research-team-folder
</pre>
  
Notice the "T", which means sticky bit has been applied. 

Now with this in place it means that a user can delete only the files that they have created inside the collaboration folder, but can't delete anyone else's files. However the collaboration folder's owner can still delete any files though. However in this case we set the folder owner to nobody, so no one has this privilege. In the real world, you may want to set the folder owner to the team's leader, or an administrator. In our example, only the root user can over-ride the sticky bit permission and delete any files. 


<h2>Using octal values to set permissions</h2>

So far we set special permissions using human friendly notation, however you can also use octal values.  


For example for setting SUID, we can do:

<pre>
$ chmod u+s {filename} 
</pre>

or octal equivalent:

<pre>chmod 4xxx {filename} </pre> 

Either of these results in something like this:

<pre>
$ ls -l | grep testfile
-rwsr----x   1 username usergroup         154 Nov 23 11:18 testfile
</pre>

However after experimenting I found that using octal values can get messy and it's better to stick to the human readable notation. 


<h2>Capital letters for permissions</h2>

Sometimes you will see:


<pre>
[root@localhost scripts]# ls -l
total 0
d---rwS--T. 2 nobody research 44 Apr 11 15:33 research-team-folder
</pre>

This happens if there was no "x" just before we applied the special permission. Hence in this example it means that the capital "S" is not sitting on top of an x. In practice this means that a member of the research team cannot cd into this directory and hence cannot create files in this directory:

<pre>
[sher@localhost scripts]$ cd research-team-folder/
-bash: cd: research-team-folder/: Permission denied
[sher@localhost scripts]$
</pre> 
   
Hence having a capital "S" is meaningless and is an indicator to you that something is configured incorrectly and needs to be fixed. 

The capital T also indicates a similar kind of problem. 


<h2>Setting consistent default rwx permisions for ugo</h2>

This can be achieved using <a href="http://codingbee.net/tutorials/rhcsa/rhcsa-access-control-list-acl/" title="RHCSA – Access Control List (ACL)">acl</a>. 


]]></Content>
		<Date><![CDATA[2015-04-06]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Password properties (passwd and chage commands)]]></Title>
		<Content><![CDATA[<h2>Overview of password policies</h2>
By the end of this article you should be able to answer the following questions:


[accordion multiopen="true"]
[toggle title="What's the command to change the 'donald' user's password?"]
$ passwd donald
[/toggle]
[toggle title="What are the 2 commands available to lock the 'donald' user's account?"]
$ passwd –l donald
# or
$ usermod -L donald 
[/toggle]
[toggle title="What's the command to unlock the 'donald' user's account"]
$ passwd –u donald
# or
$ usermod -U donald 
[/toggle]
[toggle title="Which file holds all user passwords stored in hash/encrypted form?"]
/etc/shadow
[/toggle]
[toggle title="What's the command to look up help info for this file?"]
$ man 5 shadow           # first do: 'whatis shadow' to locate this man page
[/toggle]
[toggle title="What command is used for configuring password policies?"]
chage
[/toggle]
[toggle title="What's the command to view user guide for chage?"]
$ man chage
[/toggle]
[toggle title="What's the command to view the password settings for the user 'donald'?"]
$ chage -l donald
[/toggle]
[toggle title="Which file stores password default password policy settings?"]
/etc/login.defs
[/toggle]
[toggle title="What is the command to prevent 'donald' from using the same password for more than 90 days?"]
$ chage -M 90 donald
[/toggle]
[toggle title="What is the command to warn the user donald to change 5 days before the password expires?"]
$ chage –W 5 donald
[/toggle]
[toggle title="What is the command to expire donald's account on 2016-05-25?"]
$ chage -E 2016-05-25 donald
[/toggle]
[toggle title="What's the command to view what the date will be 100 days from now?"]
$ date -d "+100days" +%F        # outputs date in format: YYYY-MM-DD
[/toggle]
[toggle title="What is the command to lock donald's account if donald doesn't change his password 10 days after it has expired?"]
$ chage -I 10 donald
[/toggle]
[/accordion]




<hr />

There are lots of settings associated to a user account's password. The <code>passwd</code> and <code>chage</code> are the 2 main commands for viewing/managing password. let's first take a look at the passwd command:
<pre>$ passwd --help
Usage: passwd [OPTION...] 
  -k, --keep-tokens   keep non-expired authentication tokens
  -d, --delete        delete the password for the named account (root only)
  -l, --lock          lock the password for the named account (root only)
  -u, --unlock        unlock the password for the named account (root only)
  -e, --expire        expire the password for the named account (root only)
  -f, --force         force operation
  -x, --maximum=DAYS  maximum password lifetime (root only)
  -n, --minimum=DAYS  minium password lifetime (root only)
  -w, --warning=DAYS  number of days warning users receives before password expiration (root
                      only)
  -i, --inactive=DAYS number of days after password expiration when an account becomes
                      disabled (root only)
  -S, --status        report password status on the named account (root only)
  --stdin             read new tokens from stdin (root only)

Help options:
  -?, --help          Show this help message
  --usage             Display brief usage message
</pre>
Here's some examples of how this command is used:
<pre>$ passwd {username}      # This changes the password for a given user 

$ passwd –l {username}   # this (l)ocks a user’s account 

$ passwd –u {username}   # this (u)nlocks a user’s account 
</pre>
Also you can lock/unlock as user's account using the <a href="http://codingbee.net/tutorials/rhcsa/rhcsa-adding-managing-and-removing-users/">usermod</a> command.
<h2>Configuring password policies</h2>
Most of the password policies are configured using the chage command:
<pre>$ chage --help
Usage: chage [options] LOGIN

Options:
  -d, --lastday LAST_DAY        set date of last password change to LAST_DAY
  -E, --expiredate EXPIRE_DATE  set account expiration date to EXPIRE_DATE
  -h, --help                    display this help message and exit
  -I, --inactive INACTIVE       set password inactive after expiration
                                to INACTIVE
  -l, --list                    show account aging information
  -m, --mindays MIN_DAYS        set minimum number of days before password
                                change to MIN_DAYS
  -M, --maxdays MAX_DAYS        set maximim number of days before password
                                change to MAX_DAYS
  -R, --root CHROOT_DIR         directory to chroot into
  -W, --warndays WARN_DAYS      set expiration warning days to WARN_DAYS

</pre>
The chage command essentially makes changes to the <code>/etc/shadow</code> file. So to learn more about the above options, you should check out the shadow's man page:
<pre>$ man 5 shadow  

# also checkout:

$ man chage
</pre>
Bot of these locations have really useful info.

Based on the above info, to view a a user's password policies, for example for a user accound called "donald" we do:
<pre>$ chage -l donald
Last password change                                    : Oct 24, 2015
Password expires                                        : never
Password inactive                                       : never
Account expires                                         : never
Minimum number of days between password change          : 0
Maximum number of days between password change          : 99999
Number of days of warning before password expires       : 7
</pre>
Here you'll see some default values e.g. "99999". These default values can be changed by editing the <code>/etc/login.defs</code> file.

By default, a user account doesn’t expire. However you can set it to expire after a specific period of time using the chage command. For example if you want donald's user account's password to expire after a (M)ax age of 90 days, then you run:
<pre>$ chage -M 90 donald
</pre>
This will mean that the user, donald, cannot use the same password for more than 90 days, and he will need to change his password in less then 90 days.

Now let's check that this command has worked:
<pre>$ chage -l donald
Last password change                                    : Oct 24, 2015
Password expires                                        : Jan 22, 2016
Password inactive                                       : never
Account expires                                         : never
Minimum number of days between password change          : 0
<strong>Maximum number of days between password change          : 90</strong>
Number of days of warning before password expires       : 7
</pre>
If you want give the donald user 5 days warning before the passoword max_day_age is reached, then run:
<pre>$ chage –W 5 donald
</pre>
Now let's check that this has worked:
<pre>$ chage -l donald
Last password change                                    : Oct 24, 2015
Password expires                                        : Jan 22, 2016
Password inactive                                       : never
Account expires                                         : May 25, 2016
Minimum number of days between password change          : 0
Maximum number of days between password change          : 90
<strong>Number of days of warning before password expires       : 5</strong>
</pre>
Straight after a user has changed his password, you might want prevent the user changing the password again straightaway, and instead let a few days pass before doing this. E.g. to prevent donald from change the password within 3 days of the last password change, you do:
<pre>$ chage -m 3 donald
</pre>
Let's check that this has worked:
<pre>$ chage -l donald
Last password change                                    : Oct 24, 2015
Password expires                                        : Jan 22, 2016
Password inactive                                       : never
Account expires                                         : May 25, 2016
<strong>Minimum number of days between password change          : 3</strong>
Maximum number of days between password change          : 90
Number of days of warning before password expires       : 5
</pre>
If you want to set an expiration date of "2016-05-25" for the donald user's account, then run:
<pre>$ chage -E 2016-05-25 donald
</pre>
This is useful for temporary employees who finishes their contract at a fixed date in the future. Let's check this has worked:
<pre>$ chage -l donald
Last password change                                    : Oct 24, 2015
Password expires                                        : Jan 22, 2016
Password inactive                                       : never
<strong>Account expires                                         : May 25, 2016</strong>
Minimum number of days between password change          : 0
Maximum number of days between password change          : 90
Number of days of warning before password expires       : 7
</pre>
The account doesn't actually get deleted. It just means that the account's password expires which effectively means that the user get's locked out of his account.

However let's say you want to set the Expiry date on 100 days from today. This can be a little tricky to figure out because you can only specify a date with the "-E" option. Luckily you can use the <code>date</code> command to work out what the date will be 100 days from now, like this:
<pre>$ date -d "+100days" +%F
2015-11-17
</pre>
If donald no longer plans to use the machine, and in fact forget's that the machine exists in the first place, then you want a way to lock their account after a period of inactivity. The most logical way to determining this is the period of time that has passed after an account's password's --maxdays age has been reached. This period is referred to as the "inactive" period. If you want to set the inactive period for 10 days for the donald user, you do:
<pre>$ chage -I 10 donald
</pre>
Now let's check that this has worked:
<pre>$ chage -l donald
Last password change                                    : Oct 24, 2015
Password expires                                        : Jan 22, 2016
<strong>Password inactive                                       : Feb 01, 2016</strong>
Account expires                                         : May 25, 2016
Minimum number of days between password change          : 3
Maximum number of days between password change          : 90
Number of days of warning before password expires       : 5
</pre>
This means that as soon as the donald user's password reaches 90 days of age, then donald has 10 days left log into the machine (he will automatically be prompted to change password at login time), if they still don't login during those 10 days then the account becomes locked.

<h2>The difference between account expiration and password expiration</h2>
There is a difference between <strong>account expiration</strong> and <strong>password expiration</strong>. When you set "--maxdays" you are effectively setting a password exipiration date. However the user can change their password at any point between the "--minday" and "--inactive" periods to keep their account active. But if the account passes the "Account expiration" date (which is set by the -E option) or the Inactive period, then the account becomes locked and only the root user can unlock the account again.]]></Content>
		<Date><![CDATA[2015-04-09]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - LDAP Overview]]></Title>
		<Content><![CDATA[<h2>Basic LDAP concepts</h2>
In big companies, it's best to store all employees login credentials centrally on a single server. This means that you can maintain them in a single place rather than lots of places. This central server is called an "LDAP server". LDAP is short for <strong>Lightweight Directory Access Protocol</strong>. There is a Microsoft alternative to LDAP, which works in a similar way to LDAP and is called "active directory".

<h3>What data does an ldap server store?</h3>
LDAP is used to store more than just usernames and passwords. It stores all kinds of information for a given user account, e.g.:

<ul>
	<li>username</li>
	<li>password</li>
	<li>job title</li>
	<li>first name</li>
	<li>surename</li>
	<li>email</li>
	<li>phone number</li>
	<li>group names</li>
	<li>...etc</li>
</ul>


However an ldap server isn't limited to just storing user account data, It can also store data relating to:

<ul>
	<li>servers</li>
	<li>printers</li>
	<li>profiles</li>
</ul>

<h3>How is data stored in an LDAP server?</h3>
LDAP organises all the data in a hierarchical structure. The linux directory tree structure is a good analogy to this. This means that you can can refer to a particular resource (e.g. user account) by referencing all the string of "nodes" that drills down to that given resource account. 

LDAP actually uses the ldap server's own fqdn to build the hierarchy's core tree structure. For example if the ldap server's fqdn is:

<pre>users.codingbee.co.uk</pre>

Based on this fqdn, in ldap terms we have:

 <ol>
	<li>the top level node is "uk". In LDAP, this node is referred to as a dc (domain component).</li>
 	<li>the next node is "co", which is referred to as a dc (domain component).</li>
	<li>then we have another dc called "codingbee"</li>
 	<li>Then we have the final, lowest-level node, "users". In LDAP this node is referred to as an "ou" (Organisational Unit). The ou is actually an extension that ldap has added onto the fqdn like tree.</li>
</ol>

There can be other ou's available, e.g.:

<pre>
- servers.codingbee.co.uk
- printers.codingbee.co.uk
</pre>

Note, these fqdns which are prefixed with the ou, don't actually exist. they are just used to help you understand what's happening. These ou's are the lowest level of the hierarchy. The resource data is stored under inside the ou. 

Now let's say that we have a user account called "david", then by following this convention, david's user account data should be located at:

<pre>david.users.codingbee.co.uk</pre>

Here this "david" prefix is referred to as a "cn" (common name). Once again this string doesn't actually exist, it is just being used to help you understand what's happening.

When the ldap client requests david's user account data, from the ldap server, it does so by sending the request in the following form:


<pre>
cn=david,ou=users,dc=codingbee,dc=co,dc=uk  
</pre>

This is the ldap specific syntax to represent a particular record, which in this case is the record of "david". In LDAP a record, e.g. "david" is referred to as an "entry". The above reference uniquely identifies an entry, and in LDAP this is referred to as "Distinguished Name", aka "dn", and the full ldap syntax is:

<pre>
dn: cn=david,ou=users,dc=codingbee,dc=co,dc=uk  
</pre>



Each entry can be thought of as a hash made up of key-value pairs. However in ldap, a key is referred to as an <strong>attribute type</strong> and a value is referred to as <strong>attribute value</strong>. An attribute for the user account "david", could be:

<pre>   
job title = accountant
</pre>



In big companies it is common to have a central openldap server that all the other Linux servers to authenticate user login credentials against. This kind of setup is known as "single sign-on", aka SSO.

For the RHCSA and RHCE exam, you only need to know how to set up your Linux machine as an LDAP client that can connect to an existing LDAP server. You don't need to know how to setup the LDAP server itself.

]]></Content>
		<Date><![CDATA[2015-04-09]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Binary files (aka executables)]]></Title>
		<Content><![CDATA[In Linux a lot of files, especially the underlying files represented by shell commands are in the form of a non-human readable binary file, which is also referred to as an executable. The humble "pwd" command is a typical example:


<pre>
$ type pwd
pwd is a shell builtin
$ which pwd
/usr/bin/pwd
</pre>

The above "pwd" file appears to be binary file, since it's in the (bin)ary folder. But this may not always be the case. A better way to determine whether a file is a binary/executable or not is by using the "file" command.   


<h2>The "file" command</h2>

<pre>
$ whatis file
file (1)             - determine file type
</pre>

Hence:

<pre>
$ file /usr/bin/pwd
/usr/bin/pwd: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked (uses shared libs), for GNU/Linux 2.6.32, BuildID[sha1]=0x9a42cda95a4835fcef88198eecf0bd7191e8d3dc, stripped
</pre> 



<h2>the "ldd" command</h2>

The people who have developed binary files, such as pwd, usually re-use code provided by others. These third party codes provided by others are usually packaged into "libraries".

The ldd (List Dynamic Dependencies) command displays the shared libraries required by each program:


<pre>
$ whatis ldd
ldd (1)              - print shared library dependencies
$ ldd --help
Usage: ldd [OPTION]... FILE...
      --help              print this help and exit
      --version           print version information and exit
  -d, --data-relocs       process data relocations
  -r, --function-relocs   process data and function relocations
  -u, --unused            print unused direct dependencies
  -v, --verbose           print all information

For bug reporting instructions, please see:
<http://www.gnu.org/software/libc/bugs.html>.

</pre>
 
Hence for pwd, we have:


<pre>$ ldd /usr/bin/pwd
    linux-vdso.so.1 =>  (0x00007fffadbfe000)
    libc.so.6 => /lib64/libc.so.6 (0x00007f5a9f34d000)
    /lib64/ld-linux-x86-64.so.2 (0x00007f5a9f721000)
</pre>



See also

http://www.cyberciti.biz/tips/linux-shared-library-management.html

http://www.ibm.com/developerworks/linux/library/l-dynamic-libraries/
]]></Content>
		<Date><![CDATA[2015-04-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Pluggable Authentication Module (PAM)]]></Title>
		<Content><![CDATA[The following guide is really useful:


https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Managing_Smart_Cards/PAM_Configuration_Files.html#PAM_Configuration_File_Format

This guide explains things better than anything else that I can find. 

There are a number of commandline utilities that needs to authenticate the user before it can provide service to it. For example we have the "login" utility:




Once user has successfully authenticated against the login utility, the login utility will then give the user access to the bash terminal. 

The login utility behind the scenes makes use of PAM to do the actual authentication. 

PAM let's you configure how authentication happens on a tool by tool basis. Each tool have respective pam config file, where you can do the configuration. All these config files can be found here:

<pre>
$ ls -l /etc/pam.d/
total 156
-rw-r--r--. 1 root root  272 Oct  7  2014 atd
-rw-r--r--. 1 root root  192 Mar  6 05:58 chfn
-rw-r--r--. 1 root root  192 Mar  6 05:58 chsh
-rw-r--r--. 1 root root  232 Mar  6 04:55 config-util
-rw-r--r--. 1 root root  293 Jul 30  2014 crond
-r--r--r--. 1 root root  146 Mar  5 22:51 cups
lrwxrwxrwx. 1 root root   19 Mar 14 19:23 fingerprint-auth -> fingerprint-auth-ac
-rw-r--r--. 1 root root  702 Mar 14 19:23 fingerprint-auth-ac
-rw-r--r--. 1 root root  590 Mar  5 23:41 gdm-autologin
-rw-r--r--. 1 root root  605 Mar  5 23:41 gdm-fingerprint
-rw-r--r--. 1 root root  341 Mar  5 23:41 gdm-launch-environment
-rw-r--r--. 1 root root  829 Mar  5 23:41 gdm-password
-rw-r--r--. 1 root root  844 Mar  5 23:41 gdm-pin
-rw-r--r--. 1 root root  597 Mar  5 23:41 gdm-smartcard
-rw-r--r--. 1 root root   70 Mar  6 01:41 ksu
-rw-r--r--. 1 root root   97 Mar 26 10:43 liveinst
-rw-r--r--. 1 root root  796 Mar  6 05:58 login
-rw-r--r--. 1 root root  154 Mar  6 04:55 other
-rw-r--r--. 1 root root  188 Jun 10  2014 passwd
lrwxrwxrwx. 1 root root   16 Mar 14 19:23 password-auth -> password-auth-ac
-rw-r--r--. 1 root root  974 Mar 14 19:23 password-auth-ac
-rw-r--r--. 1 root root  510 Mar  6 03:39 pluto
-rw-r--r--. 1 root root  155 Jun  9  2014 polkit-1
lrwxrwxrwx. 1 root root   12 Mar 14 19:23 postlogin -> postlogin-ac
-rw-r--r--. 1 root root  330 Mar 14 19:23 postlogin-ac
-rw-r--r--. 1 root root  144 Jun 10  2014 ppp
-rw-r--r--. 1 root root  681 Mar  6 05:58 remote
-rw-r--r--. 1 root root  143 Mar  6 05:58 runuser
-rw-r--r--. 1 root root  138 Mar  6 05:58 runuser-l
-rw-r--r--. 1 root root  145 Jun  9  2014 setup
lrwxrwxrwx. 1 root root   17 Mar 14 19:23 smartcard-auth -> smartcard-auth-ac
-rw-r--r--. 1 root root  752 Mar 14 19:23 smartcard-auth-ac
lrwxrwxrwx. 1 root root   25 Mar 14 19:20 smtp -> /etc/alternatives/mta-pam
-rw-r--r--. 1 root root   76 Jun 10  2014 smtp.postfix
-rw-r--r--. 1 root root  643 Mar  6 04:44 sshd
-rw-r--r--. 1 root root  540 Mar  6 05:58 su
-rw-r--r--. 1 root root  202 Mar  6 05:42 sudo
-rw-r--r--. 1 root root  187 Mar  6 05:42 sudo-i
-rw-r--r--. 1 root root  137 Mar  6 05:58 su-l
lrwxrwxrwx. 1 root root   14 Mar 14 19:23 system-auth -> system-auth-ac
-rw-r--r--. 1 root root 1015 Mar 14 19:23 system-auth-ac
-rw-r--r--. 1 root root  181 Mar 26 13:03 systemd-user
-rw-r--r--. 1 root root   84 Mar  6 03:11 vlock
-rw-r--r--. 1 root root  297 Mar 12 15:06 vmtoolsd
-rw-r--r--. 1 root root  163 Mar  6 06:11 xserver

</pre>

Most config files here have the same name as the utility that needs to make use of PAM. These commands are referred to as "PAM aware". Hence you can reconfigure one of these files in order to change the ways the utility uses pam to verify the user. 

Here you'll find a file called "login", this configures pam's set up when being called by the login binary, if we take a look at this:


<pre>

[root@localhost pam.d]# cat login
#%PAM-1.0
auth [user_unknown=ignore success=ok ignore=ignore default=bad] pam_securetty.so
auth       substack     system-auth
auth       include      postlogin
account    required     pam_nologin.so
account    include      system-auth
password   include      system-auth
# pam_selinux.so close should be the first session rule
session    required     pam_selinux.so close
session    required     pam_loginuid.so
session    optional     pam_console.so
# pam_selinux.so open should only be followed by sessions to be executed in the user context
session    required     pam_selinux.so open
session    required     pam_namespace.so
session    optional     pam_keyinit.so force revoke
session    include      system-auth
session    include      postlogin
-session   optional     pam_ck_connector.so
[root@localhost pam.d]#


</pre>


Notice that each line is made up of <a href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Managing_Smart_Cards/PAM_Configuration_Files.html#PAM_Configuration_File_Format">4 fields</a>, These fields have the following labels:

 
<pre>
module_interface    control_flag    module_name    module_arguments (optional)
</pre>


Note: service isn't used that much. instead the config file is named after the service instead. 


<h3>The module_interface field</h3>

The first field in the configuration file is the module-type indicating which of four general PAM management services that the module will provide to the application. This field is referred to the "type" field

PAM make 4 general pam services available to each application specific pam module:


<ul>
	<li><strong>auth</strong> - Determines whether the user is who they claim to be, usually by a password, or other means, such as biometrics.</li>
	<li><strong>account</strong> - Determines whether the user is allowed to access the service. This is more about user right's rather than authenticating user. </li>
	<li><strong>password</strong> - Provides a mechanism for user to change their password, this is typically using their current password</li>
	<li><strong>session</strong> - Things that are done before and/or after the user has been authenticated. This might included things such as mounting/unmounting the user home directory, logging their login/logout, and restricting/unrestricting the services available to the user.</li>
</ul>

Note,

<h3>The control_flag field</h3>

The second field is called the "<a href="http://www.linux-pam.org/Linux-PAM-html/sag-configuration-file.html">control</a>" field, it can take the following values:

<ul>
	<li><strong>required</strong> - The module result must be successful for authentication to continue. If the test fails at this point, the user is not notified until the results of all module tests that reference that interface are complete.</li>
	<li><strong>requisite</strong> -</li>
	<li><strong>sufficient</strong> - The module result is ignored if it fails. However, if the result of a module flagged sufficient is successful and no previous modules flagged required have failed, then no other results are required and the user is authenticated to the service.</li>
	<li><strong>optional</strong> -</li>
	<li><strong>include</strong> -</li>
	<li><strong>substack</strong> -</li>
	<li><strong>{complex}</strong> -</li>
</ul>







The type token tells PAM what type of authentication is to be used for this module (which in this example is the login module). Modules of the same type can be "stacked", requiring a user to meet multiple requirements to be authenticated. 


<h3>The modulepath field</h3>

You can find info for all the pam modules here:

<pre>
$ ls /usr/share/doc/pam-1.1.8/txts
README.pam_access     README.pam_ftp        README.pam_namespace   README.pam_succeed_if
README.pam_chroot     README.pam_group      README.pam_nologin     README.pam_tally
README.pam_console    README.pam_issue      README.pam_permit      README.pam_tally2
README.pam_cracklib   README.pam_keyinit    README.pam_postgresok  README.pam_time
README.pam_debug      README.pam_lastlog    README.pam_pwhistory   README.pam_timestamp
README.pam_deny       README.pam_limits     README.pam_rhosts      README.pam_tty_audit
README.pam_echo       README.pam_listfile   README.pam_rootok      README.pam_umask
README.pam_env        README.pam_localuser  README.pam_securetty   README.pam_unix
README.pam_exec       README.pam_loginuid   README.pam_selinux     README.pam_userdb
README.pam_faildelay  README.pam_mail       README.pam_sepermit    README.pam_warn
README.pam_faillock   README.pam_mkhomedir  README.pam_shells      README.pam_wheel
README.pam_filter     README.pam_motd       README.pam_stress      README.pam_xauth
</pre>

As you can see each module has it's own readme file. 

<pre>
less /usr/share/doc/pam-1.1.8/Linux-PAM_SAG.txt
</pre>

This <a href="http://linux-pam.org/Linux-PAM-html/Linux-PAM_SAG.html">pam guide</a> is also available online.

This guide covers all the various pam modules that are available. 








<a href="http://www.linux-pam.org/Linux-PAM-html/Linux-PAM_SAG.html">The Linux-PAM System Administrators Guide</a>

http://linux.die.net/man/5/system-auth-ac
http://linux.die.net/man/8/authconfig

http://www.linuxgeek.net/documentation/authentication.phtml

https://www.google.co.uk/search?q=authconfig+enablewinbindauth+&ie=utf-8&oe=utf-8&aq=t&rls=org.mozilla:en-GB:official&client=firefox-a&channel=sb&gfe_rd=cr&ei=0XHwVKC5C4OP-wap0oCADA#safe=off&rls=org.mozilla:en-GB:official&channel=sb&q=edit+%2Fetc%2Fpam.d%2Fsystem-auth






<h3>See also</h3>
https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Managing_Smart_Cards/Pluggable_Authentication_Modules.html]]></Content>
		<Date><![CDATA[2015-04-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Accessing remote NFS folders and CIFS folders]]></Title>
		<Content><![CDATA[By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="01. What is the command to install the nfs client side packages?"]
$ yum install nfs-utils
[/toggle]
[toggle title="02. What is the command to check what folders the server 'The-Server' is sharing via nfs?"]
$ showmount -e The-Server
[/toggle]
[toggle title="03. What is the command to install the CIFS/Samba client side packages?"]
$ yum install samba-client cifs-utils
[/toggle]
[toggle title="04. What is the command to check what folders the server 'The-Server' is sharing via CIFS/Samba by querying as the user 'vagrant'?"]
$ smbclient -L The-Server <span style="letter-spacing:0.1px">-</span>-user vagrant
[/toggle]
[toggle title="05. What is the command to create logical folder structure for an NFS mountpoint?"]
$ mkdir -p /nfs/{server-name}/{mountpoint}
[/toggle]
[toggle title="06. What is the command to manually mount onto the mountpoint, '/nfs/The-Server/Shared-Folder' the nfs share, '/nfs/Shared-Folder' that has been made available by the server 'The-Server'?"]
$ mount -t nfs The-Server:/nfs/Shared-Folder /nfs/The-Server/Shared-Folder
[/toggle]
[toggle title="07. What is the command to create logical folder structure for an CIFS mountpoint, assuming the nfs server is called 'The-Server' and the folder being shared out is called 'Shared-Folder'?"]
$ mkdir -p /cifs/The-Server/Shared-Folder
[/toggle]
[toggle title="08. What is the command to manually mount onto the mountpoint, '/cifs/The-Server/Shared-Folder' the cifs share, 'Shared-Folder' that has been made available by the server 'The-Server', where the smb username is 'vagrant', and corresponding password is 'admin'?"]
$ mount -t cifs -o user=vagrant,password=admin <mark>//</mark>The-Server/Shared-Folder /cifs/The-Server/Shared-Folder
# Note, you must use the double-slash syntax. 
[/toggle]
[toggle title="09. What two commands can you run to check the cifs or nfs mount has worked?"]
$ mount
# or 
$ df -h
[/toggle]
[/accordion]

<hr/>



<h2>Checking for any available NFS shares</h2>
In our scenario we know that The-Server is sharing folders via the NFS and CIFS/Samba technology. 

Before we can access The-Server's nfs shared folder, we first see what folders The-Server has made available for sharing via nfs. To do this we first need to install:

<pre>
$ yum install nfs-utils
</pre>



This package installs a few things including <code>showmount</code> command. This command is used to query nfs servers to see what folders it's sharing via nfs (if any). In our scenario we'll use it to check what folder The-Server has made available for sharing:

<pre>
$ showmount -e The-Server
Export list for The-Server:
/nfs/Shared-Folder *
</pre>

As indicated The-Server has made the folder "/nfs/Shared-Folder" available for sharing via nfs. We'll cover access to this Share-Folder later on. 



<h2>Check for any available CIFS shares</h2>

First you need to install the samba client:

<pre>
$ yum install samba-client cifs-utils
</pre>

Now to check for available CIF shares, you need to have a valid samba user credential. Let's assume we have valid credentials of a user called "vagrant", then to check if there are any available CIF shares we run the following command: 


<pre>
$ smbclient -L The-Server --user vagrant
Enter vagrant's password:
Domain=[MYGROUP] OS=[Unix] Server=[Samba 4.1.12]

        Sharename       Type      Comment
        ---------       ----      -------
<strong>        Shared-Folder   Disk</strong>
        IPC$            IPC       IPC Service (Samba Server Version 4.1.12)
        vagrant         Disk      Home Directories
Domain=[MYGROUP] OS=[Unix] Server=[Samba 4.1.12]

        Server               Comment
        ---------            -------

        Workgroup            Master
        ---------            -------
</pre>


<h2>Accessing shared folders by "mounting" them</h2>

So far we have determined that The-Server has made 2 folders available for sharing. The first folder is being shared via NFS, whereas the second folder is being shared via CIFS (aka samba). In both cases the folder called Share-Folder. 

We can now access these folders by "mounting" them. 




<h3>Configuring the The-Client machine to access the NFS Shared-Folder</h3>

Now that we know that The-Server is sharing folders via nfs, we now need to create a folder that will act as our local representation of the Shared-Folder. We'll call it:

<pre>
$ mkdir -p /nfs/The-Server/Shared-Folder
</pre>

Notice that we used a meaningful folder structure so to make it easy to track, this is best practice. If we take a look inside this folder then it should be empty: 

<pre>
$ cd /nfs/The-Server/Shared-Folder
$ ls -l
</pre>


This folder is referred to as a "mount-mount". Next we "mount" The-Server's Shared-Folder onto our mountpoint, which effectively makes the Shared-Folder's content accessible. This is done using the <code>mount</code> command. 

<pre>
$ mount -t nfs The-Server:/nfs/Shared-Folder /nfs/The-Server/Shared-Folder
</pre>

Now to check if this command has worked, we do:

<pre>
$ mount | grep The-Server
The-Server:/nfs/Shared-Folder on /nfs/The-Server/Shared-Folder type nfs4 (rw,relatime,vers=4.0,rsize=131072,wsize=131072,namlen=255,hard,proto=tcp,port=0,timeo=600,retrans=2,sec=sys,clientaddr=192.168.50.10,local_lock=none,addr=192.168.50.11)
</pre>

Another way to check this has worked, is by using the df command:


<pre>
$ df -h
Filesystem                            Size  Used Avail Use% Mounted on
/dev/mapper/centos_puppetmaster-root   38G  4.7G   33G  13% /
devtmpfs                              487M     0  487M   0% /dev
tmpfs                                 497M   80K  497M   1% /dev/shm
tmpfs                                 497M  6.9M  490M   2% /run
tmpfs                                 497M     0  497M   0% /sys/fs/cgroup
/dev/sda1                             497M  125M  373M  26% /boot
none                                  224G  135G   89G  61% /vagrant
<strong>The-Server:/nfs/Shared-Folder       38G  3.9G   34G  11% /nfs/The-Server/Shared-Folder
</strong>
</pre>



Now, lets navigate into this folder and see what it contains:

<pre>
$ cd /nfs/The-Server/Shared-Folder
$ ls -l
total 0
drwxr-xr-x. 2 root root 26 Oct 24 20:14 subfolder
-rw-r--r--. 1 root root  0 Oct 24 20:14 testfile1.txt
$ tree
.
├── subfolder
│   └── testfile2.txt
└── testfile1.txt

1 directory, 2 files

</pre>


As you can see, when you access the mount-point, you actually end up accessing the contents of The-Server's Shared-Folder.

However the above mounting isn't persistant, i.e. it won't survive a reboot. In the next article We'll cover how to make it persistent by setting up automounting during machine's boot up.   
 



<h3>Configuring the The-Client machine to access the CIFS Shared-Folder</h3>

Like nfs, we first create a mountpoint:

<pre>
$ mkdir -p /cifs/The-Server/Shared-Folder
</pre>

As before, we create a meaningful folder structure, to make things easier to track. 

Now we use the mount command again, but this time it looks like this:


<pre>
$ mount -t cifs -o user=vagrant,password=admin <mark>//</mark>The-Server/Shared-Folder /cifs/The-Server/Shared-Folder
</pre>

Note we must use this <code>//</code> syntax, otherwise it won't work. Also we don't need to know of the remote folder's fullpath, just the folder name itself is required.  


We can now confirm that this has worked using the mount command:


<pre>
$ mount | grep cifs
//The-Server/Shared-Folder on /cifs/The-Server/Shared-Folder type cifs (rw,relatime,vers=1.0,cache=strict,username=vagrant,domain=THE-SERVER,uid=0,noforceuid,gid=0,noforcegid,addr=192.168.50.11,unix,posixpaths,serverino,acl,rsize=1048576,wsize=65536,actimeo=1)
</pre>

You can also check with the df command:

<pre>
$ df -h
Filesystem                            Size  Used Avail Use% Mounted on
/dev/mapper/centos_puppetmaster-root   38G  4.7G   33G  13% /
devtmpfs                              487M     0  487M   0% /dev
tmpfs                                 497M   80K  497M   1% /dev/shm
tmpfs                                 497M  6.9M  490M   2% /run
tmpfs                                 497M     0  497M   0% /sys/fs/cgroup
/dev/sda1                             497M  125M  373M  26% /boot
none                                  224G  135G   89G  61% /vagrant
The-Server:/nfs/Shared-Folder       38G  3.9G   34G  11% /nfs/The-Server/Shared-Folder
<strong>//The-Server/Shared-Folder          38G  3.9G   34G  11% /cifs/The-Server/Shared-Folder</strong>
</pre>


Now let's go into the folder and see what it contains:

<pre>
$ cd /cifs/The-Server/Shared-Folder
$ ls -l
total 0
-rwxrwxrwx. 1 root root 0 Oct 24 22:31 hello-world-cifs.txt
drwxrwxrwx. 2 root root 0 Oct 24 22:31 subfolder
$ tree
.
├── hello-world-cifs.txt
└── subfolder
    └── another-testfile.txt

1 directory, 2 files
</pre>

However the above mounting isn't persistent, i.e. it won't survive a reboot. We'll cover how to make it persisting by setting up automounting during machine boot up.   






]]></Content>
		<Date><![CDATA[2015-04-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Automounting the home directory]]></Title>
		<Content><![CDATA[https://linuxacademy.com/cp/livelabs/view/id/61

need to write how to use the 

* and "&" notation as well as the extra ldap related mounting options. 


I.e. the auto.home will look something like this:


<pre>$ cat /etc/auto.home
*     -rw        nfsserver:/home/guests/&
</pre>

This special shorthand notation will save the need for writing hundreds of lines, one for each user's home directory. 

Here the "*" is a wildcard which points to any directory (which matches the user's username)

The "&" automatically gets resolved to the matching the username. This shorthand is specifically designed to help with automounting a user's home directory. 

Also I think for security reasons, the * and & are tied into the user's username. So that the user can't wonder into someone else's home directory.  

So when a user logs in, their cwd will automatically get directed to their home directory which will then trigger the automount. 


This solution makes it possible for user to have access to their personal files irrespective of which machine the log into, as long as the machine has home directory automounting configured. 


This special syntax structure is documented in the autofs man page. 


<pre>
$ man 5 autofs
</pre>

]]></Content>
		<Date><![CDATA[2015-04-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - File and Folder Permissions]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What file do you need to edit to make umask value persistant?"]
~/.bashrc
[/toggle]
[toggle title="What is the command to display umask values in human friendly format?"]
$ umask -S
[/toggle]
[toggle title="What is the command to make the file /tmp/testfile.txt have no user or group owns it?"]
$ chown nobody:nobody /tmp/testfile.txt
[/toggle]
[/accordion]

<hr/>


<h2>File Permissions</h2>
To understand files and Folder permissions, let's first create a new user, called "tom", switch into that user, then create a testfile:

<pre>
[root@puppetmaster ~]# useradd tom
[root@puppetmaster ~]# su - tom
[tom@puppetmaster ~]$ touch testfile.txt
</pre>

Now let's look at the testfile's properties:

<pre>
[tom@puppetmaster ~]$ ls -l
total 0
-rw-rw-r--. 1 <mark>tom</mark> tom 0 Oct 29 19:41 testfile.txt
</pre>

In Linux when you create a file, the file's creator automatically become's the file's owner as indicated above. 

When you create a new user, a group with the same name as the user's name is created, and then the user is automatically added as a member of this group. This group is called the user's <strong>private group</strong>.   

In Linux when you create a file, that file also has to be owned by a group, and in normal circumstance's the owner's private group, becomes this file's group owner:

<pre>
[tom@puppetmaster ~]$ ls -l
total 0
-rw-rw-r--. 1 tom <mark>tom</mark> 0 Oct 29 19:41 testfile.txt
</pre>
 

the first character indicates the kind of item this is. An "-" indicates that this is a file. A "d" indicates it is an directory, and an "l" indicates it is a symbolic link. In this case it is an file: 

<pre>
[tom@puppetmaster ~]$ ls -l
total 0
<mark>-</mark>rw-rw-r--. 1 tom tom 0 Oct 29 19:41 testfile.txt
</pre>

Now we have three kinds of users that can potentially interact with this file, the file's owner, a member of the file's group owner, and all the other Linux users (which is referred to simply as "other")

The level of interactions, aka permissions, that each of these three types of users can have, is indicated by the following:

<pre>
[tom@puppetmaster ~]$ ls -l
total 0
-<mark>rw-rw-r--</mark>. 1 tom tom 0 Oct 29 19:41 testfile.txt
</pre>

This string of characeters 9 characters is actually 3 sets of 3 characters. Each set indicates the file permission for the file's owner, the file's group, and "other", respectively. 

The first 3 characters shows the owner's permissions:


<pre>
[tom@puppetmaster ~]$ ls -l
total 0
-<mark>rw-</mark>rw-r--. 1 tom tom 0 Oct 29 19:41 testfile.txt
</pre>

Here's what each character means:

 
<ul>
	<li><strong>First character</strong>: This character can either be set to "r" or "-". "r" means that the file's owner has read access. So that they view the files content, with commands like, cat, less, view, vim....etc. If this is set to "-" then it means that the file's owner isn't allowed to view this file. Behind the scenes, the "r" setting is also represented by the number "4"</li>
	<li><strong>Second character</strong>: This character can either be set to "w" or "-". "w" means that the file's owner has write access. So that they can edit the file's content, using utility like vi and vim, or redirects like ">", ">>". If this is set to "-" then it means that the file's owner isn't allowed to edit this file. Behind the scenes, the "w" setting is also represented by the number "2"</li>
	<li><strong>Third character</strong>: This character can either be set to "x" or "-". "x" means that the file's owner has execute permissions. This means that if this file contains executable code, then the user can run this file by just typing it's full path in the command prompt. If this is set to "-", then it means that the file's owner can't execute this file.  Behind the scenes, the "x" setting is also represented by the number "1".</li>
</ul>

In our example the owner permission is "rw-". Since r=4 and w=1, it means you can represent "rw-" by single number, which is the sum of the permission, which in this case is 5 (4+1). Therefore "rw-" behind the scenes is represented by 5. These numbers are referred to as octal values. Here are all the possible octal values:

--- = 0
--x = 1
-w- = 2
-wx = 3
r-- = 4
r-x = 5
rw- = 6
rwx = 7




The next 3 shows the group's permissions:

<pre>
[tom@puppetmaster ~]$ ls -l
total 0
-rw-<mark>rw-</mark>r--. 1 tom tom 0 Oct 29 19:41 testfile.txt
</pre>


and the last three shows the other's permission.   

<pre>
[tom@puppetmaster ~]$ ls -l
total 0
-rw-rw-<mark>r--</mark>. 1 tom tom 0 Oct 29 19:41 testfile.txt
</pre>

These string of 9 characters van be represented in octal form, So for this example, the permissions "rw-rw-r--" can be represented by "664". 





<h2>Folder Permissions</h2>

The "rwx" have slightly different meanings in the context of folders. 

[table id=4 /]

<h2>Default Permissions</h2>

When we created the file, the permissions ended up defaulting too:


<pre>
[tom@puppetmaster ~]$ ls -l
total 0
-rw-rw-r--. 1 tom tom 0 Oct 29 19:41 testfile.txt
</pre>

Which in this case is 664.  

You might think why isn't the default "777" (rwxrwxrwx) so that everyone has full priveleges. That's because that would mean this file would be an executable which could be dangerous if the file contains malicious code. Based on this, then the default should now be, 666 (rwxrwxrwx). However that is not the case either. That's because there is something else that influences the default permission, and that is called the umask value:

<pre>    
[tom@puppetmaster ~]$ umask
0002
</pre>

This value is subtracted from 666 to leave with the default permission, 664

However if we create a folder we'll see that :

<pre>
[tom@puppetmaster ~]$ mkdir testfolder
[tom@puppetmaster ~]$ ls -l
total 0
drwxrwxr-x. 2 tom tom 6 Oct 30 19:21 testfolder
</pre>

Here the folder permission defaults to 775. That's because in the case of folders, it is safe for the starting point to be 777 (rwxrwxrwx), since execute has a different meaning when it comes to folders. There if we subtract the umask value from  777, we end up with 775.  

You can change the umask value, you can do:

<pre>
[tom@puppetmaster ~]$ umask 0007
[tom@puppetmaster ~]$ umask
0007
</pre> 

However this change will only last for the current sessions, to make it persistant, you need to add this command to the end of the following file:


<pre>
~/.bashrc
</pre>


You can also view your default settings in a more human readable form:

<pre>
[tom@puppetmaster ~]$ umask
0007
[tom@puppetmaster ~]$ umask -S
u=rwx,g=rwx,o=
</pre>

Note: the above is in the context of folders. 



<h2>Changing Permissions</h2>

You can change a file's or folder's permission using the <code>chmod</code> command. There chmod command can accept 2 forms of instructions, the first form is by octal values: 

<pre>
[tom@puppetmaster ~]$ ls -l
total 0
-rw-rw----. 1 tom tom 0 Oct 30 20:52 testfile.txt
[tom@puppetmaster ~]$ chmod 777 testfile.txt
[tom@puppetmaster ~]$ ls -l
total 0
-rwxrwxrwx. 1 tom tom 0 Oct 30 20:52 testfile.txt
</pre>

The second form is by human readable notation:

<pre>
[tom@puppetmaster ~]$ chmod ugo-x testfile.txt
[tom@puppetmaster ~]$ ls -l
total 0
-rw-rw-rw-. 1 tom tom 0 Oct 30 20:52 testfile.txt
</pre>

chmod is used exactly the same way for folders as well. However you can recursively change the permissions of all the contents of a particular folder using the "-R" setting:

<pre>
$ chmod -R g+w testfolder 
</pre>

<h2>Changing File and Folders ownerships</h2>

If you are the owner of a particular file, and you want to assign ownership to another user, then you can do this using the <code>chown</code> command: 

<pre>
$ ls -l
total 0
-rw-rw-rw-. 1 tom tom 0 Oct 30 20:52 testfile.txt
$ chown jerry:jerry testfile.txt
$ ls -l
total 0
-rw-rw-rw-. 1 jerry jerry 0 Oct 30 20:52 testfile.txt
</pre>

Here we changed both a file's and a group's ownership using the "{user}:{group}" notation.

If you just want to change the group only, but keep the owner unchanged, then you simply declare the existing owner, e.g., here we just want to change the group to "tom", but keep the owner as jerry, therefore we do:

<pre>
$ ls -l
total 0
-rw-rw-rw-. 1 jerry jerry 0 Oct 30 20:52 testfile.txt
$ chown jerry:tom testfile.txt
$ ls -l
total 0
-rw-rw-rw-. 1 jerry tom 0 Oct 30 20:52 testfile.txt
</pre>

You can also take the same approach of keeping the same group but changing the owner. 

With chown, there are also a special reserved word, <code>nobody</code> which is used to a change the owner or group to "nobody", e.g.:


<pre>
$ ls -l testfile.txt
-rw-r--r--. 1 root root 12 Oct 30 22:10 testfile.txt
$ chown nobody:nobody testfile.txt
$ ls -l testfile.txt
-rw-r--r--. 1 nobody nobody 12 Oct 30 22:10 testfile.txt
</pre>]]></Content>
		<Date><![CDATA[2015-04-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Access Control List (ACL)]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="01. Which filesystems support acl by default?"]
The xfs and ext4 filesystem
[/toggle]
[toggle title="02. Where can you find help info for acl?"]
$ man acl
[/toggle]
[toggle title="03. What is the command to give the user 'homer', read+write acl permissions to /tmp/testfile.txt?"]
$ setfacl -m user:homer:rw-  /tmp/testfile.txt
[/toggle]
[toggle title="04. What is the command to check that this has worked?"]
$ ls -l /tmp/testfile.txt
# look for "+" at the very end of the rwx string. 
[/toggle]
[toggle title="05. What is the command to view the special permissions for /tmp/testfile.txt?"]
$ getfacl /tmp/testfile.txt
[/toggle]
[toggle title="06. What is the command to remove all acl permissions from /tmp/testfile.txt for the user, homer?"]
$ setfacl -x u:homer /tmp/testfile.txt
[/toggle]
[toggle title="07. What is the command to remove all acl permissions from /tmp/testfile.txt?"]
$ setfacl <span style='letter-spacing:0.1px'>-</span>-remove-all /tmp/testfile.txt
[/toggle]
[/accordion]

<hr/>
<h2>What is ACL</h2>

Access Control List (ACL), is a feature that add’s even greater permission granularity on top of the standard ugo/rwx controls. ACL offers the following:

<ul>
	<li>an extra granularity of permissions-control. E.g. give a particular user account group/owner/or-other-custom permission levels, even though they fall in "others".</li>
	<li>Allows you to set default ugo+rwx setting for files and folders created in a specific directory. This essentially over-rides umask settings. This is handy when using it on a shared-team-folder, and complements nicely with SGID and Sticky bit. </li>
</ul>


<pre>
$ man acl
</pre>

It is also good practice to enable acl using tunefs. This will mean that acl is enabled on the filesystem itself. hence if hdd is moved to another pc, it will still have acl enabled even if other machine doesn't explicit specify the acl option in the /etc/fstab file. However for the XFS filesystem, the acl option is enabled by default. Which is one reason why XFS is a better alternative to the ext4 filesystem. 

  

<h2>Creating ACL rules for specific user or group</h2>
We use the getacl and setacl commands to manage acl settings for each file/folder:


<pre>
$ whatis getfacl
getfacl (1)          - get file access control lists
$ whatis setfacl
setfacl (1)          - set file access control lists
</pre>


ACL actually inherits standard permissions (ugo/rwx) and uses them as a starting point, and then let's you apply custom exceptions on top of this base.  Before we go any further it is important to realise that ACL is used differently in conjunction to files than to folders. 


<h2>Using ACL on files</h2>

Let's create a file and view it's acl info. We'll create this file as the root user:

<pre>
$ id
uid=0(root) gid=0(root) groups=0(root) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023
</pre>

Now let's create this file:

<pre>
$ touch /tmp/testfile.txt
$ ls -l /tmp/testfile.txt
-rw-r--r--. 1 root root 0 Apr 11 17:18 testfile.txt
</pre>

Now let's check this file's acl info:

<pre>
$ getfacl /tmp/testfile.txt
# file: testfile.txt
# owner: root
# group: root
user::rw-
group::r--
other::r--
</pre>

As you can see, getacl starts off by showing the standard rwx/ugo info in long format. ACL inherits the rwx/ugo settings in order to build on them. Now let's say we also want the user "homer" to also have write permission to this file, but no one else. At the moment, homer falls in other, and hence can't write to this file:


<pre>
[homer@localhost scripts]$ echo "hello world" > /tmp/testfile.txt
-bash: testfile.txt: Permission denied
</pre>



One approach to achieve this is to create a new group, then add both root and homer into this group, then use chown to change this files group to this new group. This approach is quite tedious and and long winded. The proper way to resolve this is to use setfacl to overlay a custom permission on /tmp/testfile.txt that's specific to homer only:

  
<pre>
[root@localhost scripts]# setfacl -m user:homer:rw-  /tmp/testfile.txt
</pre>

Here we used the (m)odify option to assign "homer", read+write (rw-) privileges for testfile.txt. Let's confirm that this file now has special permissions on top of the normal permissions:

<pre>
$ ls -l /tmp/testfile.txt
-rw-rw-r--<mark>+</mark> 1 root root 12 Apr 11 17:33 testfile.txt
</pre> 

The "ls -l" now shows a "+" at the end of the permission string. This is an indicator that the setacl command has been used on this file to apply special permissions. 

Now let's check what those special permissions actually are:

<pre>
[root@localhost scripts]# getfacl testfile.txt
# file: testfile.txt
# owner: root
# group: root
user::rw-
<strong>user:homer:rw-</strong>
group::r--
mask::rw-
other::r--
</pre>

As you can see a new entry has been added. 


Now homer has write permissions:

<pre>
[homer@localhost ~]$ echo "hello world" > /tmp/testfile.txt
[homer@localhost ~]$ cat /tmp/testfile.txt
hello world
</pre>


<h2>Deleting and Undoing ACL permissions</h2>

You can remove homer's special permissions like this:

<pre>
setfacl -x u:homer /tmp/testfile.txt
</pre>
 

However to delete all special permissions, you do:

<pre>
$ setfacl --remove-all /tmp/testfile.txt
</pre>

Running this command is like not running setfacl on this command in the first place. You can confirm that this has worked by making sure the "+" is no longer displayed:
<pre>
$ ls -l /tmp/testfile.txt
-rw-r--r--. 1 root root 12 Apr 11 17:33 testfile.txt
</pre>

Also you can check like this:

<pre>
$ getfacl /tmp/testfile.txt
# file: testfile.txt
# owner: root
# group: root
user::rw-
group::r--
other::r--
</pre>



<h2>Using setfacl for setting basic permissions</h2>
You can use setfacl instead of using chmod for setting basic ugo+rwx permissions.
This is done by not specifying a a group name or username. For example the following:


<pre>
$ setfacl -m user::rwx /tmp/testfile.txt
</pre>

Is equivalent to:

<pre>
$ chmod u=rwx testfile.txt
</pre>


<h2>Copying acl special permissions from one file to another</h2>
Another thing you can do is copy the acl permisions from one file to another file, this is done like this:

<pre>
$ getfacl /tmp/file1 | setfacl --set-file=<mark>-</mark> /tmp/file2
</pre>

The "-" in --set-file=- means take the standard input
 



]]></Content>
		<Date><![CDATA[2015-04-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Network Interface Card (NIC)]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="Which directory contains all the network connections config files?"]
/etc/sysconfig/network-scripts/
[/toggle]
[toggle title="What do you need to do to ensure that the interface, enp0s3 starts up during boot time?"]
# Set the entry:
ONBOOT=yes
# in the file:
/etc/sysconfig/network-scripts/ifcfg-enp0s3
[/toggle]
[toggle title="What is the nmcli command to apply the above setting?"]
# First locate the connection:
$ nmcli connection show
# Then apply the setting:
$ nmcli connection modify {connection-name} connection.autoconnect yes
[/toggle]
[/accordion]

<hr/>


In rhel, there are 3 different naming conventions available for labelling your machine NIC:

<ul>
	<li><strong>BIOS naming</strong> - names are based on hardware property of a given NIC. For example:
<ul>
	<li>em1, em2, ...etc - names for embedded NICs, i.e NICs that are integrated into the motherboard.</li>
	<li>p<slot-number>p<port-number> - e.g. p4p2, this mean's the fourth pci slot on the motherboard, and the second ethernet port on the NIC, since you can buy <a href="http://www.amazon.co.uk/s/ref=nb_sb_noss_2?url=node%3D430513031&field-keywords=4+port&rh=n%3A340831031%2Cn%3A428655031%2Cn%3A430513031%2Ck%3A4+port">NICs that contains upto 4 ports</a>, hence let's you plug in up 4 ethernet cables.</li>
</ul></li>
	<li><strong>udev naming</strong> - e.g. eth0, eth1. This naming convention is not that informative. </li>
	<li><strong>Physical naming</strong> - same as bios naming. </li>
	<li><strong>logical naming</strong> - vlan and alias configurations. Not important for RHCSA course</li>
</ul>



<h2>Autoconnect Interfaces at boot time</h2>

Your network interfaces can be configured to automatically start them up at boot time. 

In order to autostart an interface at boot time, you need to tell Network Manager which interfaces should be started up at boot time. This in turn is dictated by which connections (config files) are going to be activated at boot time. This is controlled by each connection's "ONBOOT" setting. 


So an easy way to identify which interfaces are going to be activated at boot time is by viewing the connection files, like this:


<pre>
$ grep -R "ONBOOT"  /etc/sysconfig/network-scripts/ifcfg-*
/etc/sysconfig/network-scripts/ifcfg-CustCon:ONBOOT=no
/etc/sysconfig/network-scripts/ifcfg-enp0s3:ONBOOT=yes
/etc/sysconfig/network-scripts/ifcfg-enp0s3-static:ONBOOT=no
/etc/sysconfig/network-scripts/ifcfg-enp0s8:ONBOOT=yes
/etc/sysconfig/network-scripts/ifcfg-enp0s8-1:ONBOOT=yes
/etc/sysconfig/network-scripts/ifcfg-lo:ONBOOT=yes
</pre>
 
Therefore for a given interface, if there is connection associated to it which contains the setting "ONBOOT=yes", then than interface will start up. But just to be sure, you also need to ensure that the connection you are interested in is also managed by NetworkManager:


<pre>
 nmcli -p con show
=============================================================================
                     NetworkManager connection profiles
=============================================================================
NAME           UUID                                  TYPE            DEVICE
-----------------------------------------------------------------------------
System enp0s3  ad2de184-098f-4eb8-92af-3e6c086f1186  802-3-ethernet  enp0s3
enp0s3-static  0d936343-5c8e-4545-9174-f295b6c9ae86  802-3-ethernet  --
enp0s8         7607287c-31fd-4cb3-b663-a4e4120ef265  802-3-ethernet  --
CustCon        991d8f70-7f92-481e-9202-c7afe5a434c0  802-3-ethernet  --
</pre>

And also that the Network Manager service is running and enabled:


<pre>
$ systemctl status NetworkManager
NetworkManager.service - Network Manager
   Loaded: loaded (/usr/lib/systemd/system/NetworkManager.service; enabled)
   Active: active (running) since Sun 2015-10-04 15:06:32 BST; 20min ago
 Main PID: 5496 (NetworkManager)
   CGroup: /system.slice/NetworkManager.service
           ├─5496 /usr/sbin/NetworkManager --no-daemon
           └─5506 /sbin/dhclient -d -q -sf /usr/libexec/nm-dhcp-helper -pf /var/run/dhclient-enp0s3....

Oct 04 15:06:32 puppetmaster.local dhclient[5506]: bound to 10.0.2.15 -- renewal in 32475 seconds.
Oct 04 15:06:32 puppetmaster.local NetworkManager[5496]: <info>  (enp0s3): device state change: ip...0]
Oct 04 15:06:32 puppetmaster.local NetworkManager[5496]: <info>  (enp0s3): device state change: se...0]
Oct 04 15:06:32 puppetmaster.local NetworkManager[5496]: <info>  NetworkManager state is now CONNE...AL
Oct 04 15:06:32 puppetmaster.local NetworkManager[5496]: <info>  NetworkManager state is now CONNE...TE
Oct 04 15:06:32 puppetmaster.local NetworkManager[5496]: <info>  NetworkManager state is now CONNE...AL
Oct 04 15:06:32 puppetmaster.local NetworkManager[5496]: <info>  (enp0s3): Activation: successful,...d.
Oct 04 15:07:03 puppetmaster.local NetworkManager[5496]: <info>  (enp0s3): Activation: Stage 4 of .....
Oct 04 15:07:03 puppetmaster.local NetworkManager[5496]: <info>  (enp0s3): Activation: Stage 4 of .....
Oct 04 15:07:03 puppetmaster.local NetworkManager[5496]: <info>  (enp0s3): Activation: Stage 4 of ...e.
Hint: Some lines were ellipsized, use -l to show in full.


</pre>


Now if there is a connection that has the ONBOOT=NO setting.

Let's say we have:

<pre>
$ cat /etc/sysconfig/network-scripts/ifcfg-enp0s3-static
TYPE=Ethernet
BOOTPROTO=dhcp
DEFROUTE=yes
PEERDNS=yes
PEERROUTES=yes
IPV4_FAILURE_FATAL=no
IPV6INIT=yes
IPV6_AUTOCONF=yes
IPV6_DEFROUTE=yes
IPV6_PEERDNS=yes
IPV6_PEERROUTES=yes
IPV6_FAILURE_FATAL=no
NAME=enp0s3-static
UUID=0d936343-5c8e-4545-9174-f295b6c9ae86
DEVICE=enp0s3
<strong>ONBOOT=no</strong>
</pre>


Let's say we want to enable this connection so that the interface autostarts at the next reboot with this connection, then you run the following command:


<pre>
$ nmcli connection modify enp0s3-static connection.autoconnect yes
</pre>

To confirm this command has worked we do:

<pre>
$ cat /etc/sysconfig/network-scripts/ifcfg-enp0s3-static | grep ONBOOT
ONBOOT=yes
</pre>

Another thing you need to is see if there are any other connections for this interface, if so then run the nmcli command to turn off onboot for them, so that you only have a singe connection for the given device, which has the setting "ONBOOT=yes".












 ]]></Content>
		<Date><![CDATA[2015-04-12]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Setting user password]]></Title>
		<Content><![CDATA[When you use the user resource, you'll notice that it contains a password attribute. This can only be used to enter an encrypted password, aka a hashed password. For security reasons, you cannot use a plain text password. 

Also for some reason, you can't use the passwd command to create hash value and retrieve that value from teh /etc/shadow file either. However Puppet passes the password supplied in the user type definition into the /etc/shadow file.

You have to use the openssl command to generate the hash:

<pre> 
#openssl passwd -1  
#Enter your password here 
Password: 
Verifying - Password: 
 $1$HTumvYUGYUGwsxQxCp3F/nGc4DCYM
</pre>

You then insert this password into your resource:

<pre>user { 'TestUser': 
  ensure   => present,
  password => '$1$HTumvYUGYUGwsxQxCp3F/nGc4DCYM',
}</pre>

]]></Content>
		<Date><![CDATA[2015-04-15]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Git - useful links]]></Title>
		<Content><![CDATA[http://stackoverflow.com/questions/292357/what-are-the-differences-between-git-pull-and-git-fetch
]]></Content>
		<Date><![CDATA[2015-04-15]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[git]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>git]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - The "ip" command]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What is the command to display all your interfaces along with their addressing info?"]
$ ip addr show
[/toggle]
[toggle title="What is the command to show routing info along with the default gateway?"]
$ ip route show
[/toggle]
[toggle title="What is the command to show network traffic?"]
$ ip -s link show
[/toggle]
[toggle title="What is the command?"]


[/toggle]
[toggle title="What is the command?"]


[/toggle]
[toggle title="What is the command?"]


[/toggle]
[toggle title="What is the command?"]


[/toggle]
[toggle title="What is the command?"]


[/toggle]
[toggle title="What is the command?"]


[/toggle]
[toggle title="What is the command?"]


[/toggle]
[/accordion]

<hr/>





The ip command is mainly used for querying your network setup. The ip command is made up lots of sub commands (which you can display using the tab+tab technique). One commonly used subcommand is <code>ip addr show</code>. This shows current interfaces along with their addressing info:


<pre>
$ ip addr show
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: enp0s3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP qlen 1000
    link/ether 08:00:27:82:83:fc brd ff:ff:ff:ff:ff:ff
    inet 192.168.1.124/24 brd 192.168.1.255 scope global dynamic enp0s3
       valid_lft 81874sec preferred_lft 81874sec
    inet6 fe80::a00:27ff:fe82:83fc/64 scope link
       valid_lft forever preferred_lft forever
</pre>


This shows that our machine's ip number is 192.168.1.124 (and it is part of a network that has a subnet mask of 24). However an easier way to output your machine's ip number is:


Now let's view the current routing table:


<pre>
$ ip route show
default via 192.168.1.1 dev enp0s3  proto static  metric 100
192.168.1.0/24 dev enp0s3  proto kernel  scope link  src 192.168.1.124  metric 100
</pre>

Here the router ip address is 192.168.1.1, which is the default gateway. Anything that falls in the 192.168.1.0/24 subnet will use the 192.168.1.1 (i.e. the router) as the default gateway. 



Any outgoing data will automatically get directed to the default gateway, potentially through a series of switches. To improve performance you may want to add a new route which helps data to bypass switches and reaches it's destination faster. This is done by adding a new route, like this:

To view data transfer statistics we do:


<pre>
$ ip -s link show
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    RX: bytes  packets  errors  dropped overrun mcast
    420        4        0       0       0       0
    TX: bytes  packets  errors  dropped carrier collsns
    420        4        0       0       0       0
2: enp0s3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT qlen 1000
    link/ether 08:00:27:82:83:fc brd ff:ff:ff:ff:ff:ff
    RX: bytes  packets  errors  dropped overrun mcast
    704513     7732     0       0       0       120
    TX: bytes  packets  errors  dropped carrier collsns
    266335     1391     0       0       0       0

</pre>
]]></Content>
		<Date><![CDATA[2015-04-18]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Connections config files]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="Which directory contains all the network connection files?"]
/etc/sysconfig/network-scripts
[/toggle]
[toggle title="What is a connections file's naming convention?"]
ifcfg-{interface-name}
[/toggle]
[toggle title="In the connections file, what setting do you need to apply to enable dhcp?"]
BOOTPROTO="dhcp"
[/toggle]
[toggle title="In the connections file, what setting do you need to apply to set an IP address?"]
IPADDR0={ip-number}

[/toggle]
[toggle title="In the connections file, what setting do you need to apply to set a Gateway?"]
GATEWAY0={ip-number}

[/toggle]
[toggle title="In the connections file, what setting do you need to apply to ensure that Network Manager doesn't manage this connection?"]
NM_CONTROLLED=no

[/toggle]
[toggle title="In the connections file, what setting do you need to apply to ensure that this connection is used when machine is started up?"]
ONBOOT='yes'

[/toggle]
[toggle title="In the connections file, what setting do you need to apply to set the netmask to 28?"]
PREFIX=28   

[/toggle]
[toggle title="In the connections file, what setting do you need to apply to set the DNS to 8.8.8.8?"]
DNS1=8.8.8.8              
[/toggle]
[toggle title="What is the command?"]


[/toggle]
[/accordion]

<hr/>



The following directory houses connection config files for every single interface. These names of these files all follow the convention <strong>ifcfg-{interface-name}</strong>, in our case conly 2 files fit that naming convention:

<pre>
$ ls -l /etc/sysconfig/network-scripts 
total 232
<strong>-rw-r--r--. 1 root root   321 Mar 14 19:23 ifcfg-enp0s3
-rw-r--r--. 1 root root   254 Jan 15 08:57 ifcfg-lo
</strong>lrwxrwxrwx. 1 root root    24 Apr  3 16:19 ifdown -> ../../../usr/sbin/ifdown
-rwxr-xr-x. 1 root root   627 Jan 15 08:57 ifdown-bnep
-rwxr-xr-x. 1 root root  5817 Jan 15 08:57 ifdown-eth
-rwxr-xr-x. 1 root root  6196 Mar  6 05:17 ifdown-ib
.
.
...etc</pre>

We can ignore the lo interface since that refers to the loopback interface, which is a software only implementation of an interface.

If we take a look at this config file, we see the following:

<pre>$ cat /etc/sysconfig/network-scripts/ifcfg-enp0s3
HWADDR="08:00:27:82:83:FC"    # this is the interfaces mac address
TYPE="Ethernet"         # this means it is wired connection, as opposed to wifi 
BOOTPROTO="dhcp"           # this means we are using dhcp
DEFROUTE="yes"              # ignore this 
PEERDNS="yes"               # ignore this 
PEERROUTES="yes"            # ignore this 
IPV4_FAILURE_FATAL="no"     # ignore this 
IPV6INIT="yes"              # ignore this 
IPV6_AUTOCONF="yes"         # ignore this 
IPV6_DEFROUTE="yes"         # ignore this 
IPV6_PEERDNS="yes"          # ignore this 
IPV6_PEERROUTES="yes"       # ignore this 
IPV6_FAILURE_FATAL="no"     # ignore this 
NAME="enp0s3"
UUID="c107488a-e080-4f46-a6a6-241d2e5e8846"
ONBOOT="yes"                      # this means that the network must be up at bootup. 
</pre>

As you can see that this file does not give any of the following information:




<ul>
	<li>IPADDR0 – You can also have IPADDR1, IPADDR2,…etc</li>
	<li>GATEWAY0 – You can also have GATEWAY1, GATEWAY2,…etc</li>
	<li>PREFIX0 – This is the subnet mask, aka netmask, e.g. can have value 24. You can also have PREFIX1, PREFIX2,…etc</li>
</ul>

That is because BOOTPROTO setting is set to ”dhcp” rather than “none”, which means that the dhcp handles all this info. 


You can edit this file directly, (if so then you will need to restart the interface using the ifdown and ifup command, which is covered further down), or you can use the gnome gui interface:

<a href="http://codingbee.net/wp-content/uploads/2015/04/olDLBse.png"><img src="http://codingbee.net/wp-content/uploads/2015/04/olDLBse.png" alt="" width="364" height="281" class="alignnone size-full wp-image-4075" /></a>


<a href="http://codingbee.net/wp-content/uploads/2015/04/NkgeSQ0.png"><img src="http://codingbee.net/wp-content/uploads/2015/04/NkgeSQ0.png" alt="" width="764" height="401" class="alignnone size-full wp-image-4076" /></a>

<a href="http://codingbee.net/wp-content/uploads/2015/04/sRNvIoF.png"><img src="http://codingbee.net/wp-content/uploads/2015/04/sRNvIoF.png" alt="" width="743" height="550" class="alignnone size-full wp-image-4078" /></a>

<a href="http://codingbee.net/wp-content/uploads/2015/04/JeQZ16v.png"><img src="http://codingbee.net/wp-content/uploads/2015/04/JeQZ16v.png" alt="" width="737" height="814" class="alignnone size-full wp-image-4079" /></a>








For example, while dhcp is enabled we have:


<pre>
$ cat ifcfg-enp0s3
HWADDR="08:00:27:82:83:FC"
TYPE="Ethernet"
BOOTPROTO="dhcp"
DEFROUTE="yes"
IPV4_FAILURE_FATAL="no"
IPV6INIT="yes"
IPV6_AUTOCONF="yes"
IPV6_DEFROUTE="yes"
IPV6_FAILURE_FATAL="no"
NAME="enp0s3"
UUID="c107488a-e080-4f46-a6a6-241d2e5e8846"
ONBOOT="yes"
PEERDNS=yes
PEERROUTES=yes
IPV6_PEERDNS=yes
IPV6_PEERROUTES=yes
</pre>

Now if we use the gnome gui to disable dhcp and apply the following settings:

<a href="http://codingbee.net/wp-content/uploads/2015/04/DZGqUDA.png"><img src="http://codingbee.net/wp-content/uploads/2015/04/DZGqUDA.png" alt="" width="740" height="918" class="alignnone size-full wp-image-4080" /></a>


Then the config file changes to:


<pre>
$ cat ifcfg-enp0s3
HWADDR="08:00:27:82:83:FC"
TYPE="Ethernet"
BOOTPROTO=none              # DHCP is disabled   
DEFROUTE="yes"
IPV4_FAILURE_FATAL="no"
IPV6INIT="yes"
IPV6_AUTOCONF="yes"
IPV6_DEFROUTE="yes"
IPV6_FAILURE_FATAL="no"
NAME="enp0s3"
UUID="c107488a-e080-4f46-a6a6-241d2e5e8846"
ONBOOT="yes"
IPADDR=192.168.1.10                # ip address is now static
PREFIX=28                          # prefix is now static
GATEWAY=192.168.1.35               # gateway ip is now now static                        
DNS1=8.8.8.8                       # dns is now static. 
IPV6_PEERDNS=yes
IPV6_PEERROUTES=yes

</pre>



<h2>The resolv config file</h2>

This file is automatically generated (and used internally) by network manager. It is derived from the content in the network config file, e.g. ifcfg-enp0s3.

<pre>
$ cat /etc/resolv.conf
# Generated by NetworkManager
search codingbee.dyndns.org
nameserver 192.168.1.1
</pre>

Never make any changes to the resolv.config file.


<pre>
$ less /usr/share/doc/initscripts-*/sysconfig.txt             
</pre>

This is a help file that describes the role of various networking script files.


<h2>Stopping and starting an interface</h2>

If you make any changes to a network file, then remember that you need to restart the network interface card for the changes to take effect. this is done using the ifdown and ifup commands. Here it is in action:

<pre>
$ nmcli con show
NAME           UUID                                  TYPE            DEVICE
System enp0s3  ad2de184-098f-4eb8-92af-3e6c086f1186  802-3-ethernet  enp0s3
$ ifdown enp0s3
Device 'enp0s3' successfully disconnected.
$ nmcli con show
NAME           UUID                                  TYPE            DEVICE
System enp0s3  ad2de184-098f-4eb8-92af-3e6c086f1186  802-3-ethernet  --
$ ifup enp0s3
Connection successfully activated (D-Bus active path: /org/freedesktop/NetworkManager/ActiveConnection/4)
$ nmcli con show
NAME           UUID                                  TYPE            DEVICE
System enp0s3  ad2de184-098f-4eb8-92af-3e6c086f1186  802-3-ethernet  enp0s3
</pre>

Starting/stopping the interface will be reflected a tiny cross appearing on the network icon on the top right of the gnome interface. Also this will disconnect any ssh/putty sessions. 

Here we used nmcli to view the status. we could have also used nmcli instead of ifup/ifdown commands:


<pre>

$ nmcli con show
NAME           UUID                                  TYPE            DEVICE
System enp0s3  ad2de184-098f-4eb8-92af-3e6c086f1186  802-3-ethernet  enp0s3
$ nmcli con down "System enp0s3"
Connection 'System enp0s3' successfully deactivated (D-Bus active path: /org/freedesktop/NetworkManager/ActiveConnection/5)
$ nmcli con up "System enp0s3"
Connection successfully activated (D-Bus active path: /org/freedesktop/NetworkManager/ActiveConnection/6)
$ nmcli con show
NAME           UUID                                  TYPE            DEVICE
System enp0s3  ad2de184-098f-4eb8-92af-3e6c086f1186  802-3-ethernet  enp0s3
</pre>


Note, starting/stopping a network device using nmcli or ifup/ifdown won't survive a reboot.  In order to survive a reboot you need to edit the device's configuration settings file, which is located in <code>/etc/sysconfig/network-scripts/</code>. 

Alternatively you can make this persistant by using the nmcli command like this:


<pre>
$ nmcli connection modify System\ enp0s3 connection.autoconnect no
$ cat ifcfg-enp0s3 | grep ONBOOT                            ONBOOT=no
$ nmcli connection modify System\ enp0s3 connection.autoconnect yes
$ cat ifcfg-enp0s3 | grep ONBOOT                            ONBOOT=yes
</pre> 

the nmcli command is long wielded, but luckily you can use the tab+tab autocomplete to make things easier. ]]></Content>
		<Date><![CDATA[2015-04-18]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Foreman - hooks]]></Title>
		<Content><![CDATA[

<pre>
cat /var/log/foreman/production.log | grep hook
</pre>

https://github.com/theforeman/foreman_hooks

http://m0dlx.com/blog/Extending_Foreman_quickly_with_hook_scripts.html

https://www.google.co.uk/search?q=ruby+vsphere&ie=utf-8&oe=utf-8&gws_rd=cr&ei=Des5VYeqB8i5OJKJgfAK


about mktemp: 

http://www.cyberciti.biz/tips/shell-scripting-bash-how-to-create-empty-temporary-file-quickly.html


http://stackoverflow.com/questions/10956234/problems-with-mktemp

https://github.com/theforeman/foreman_custom_parameters

https://gist.github.com/rubiojr/1195252






ruby vmware api:

https://github.com/rlane/rbvmomi

rbvmomi
https://rubygems.org/gems/rbvmomi


https://gist.github.com/rubiojr/1195252

https://github.com/rlane/rbvmomi/blob/master/examples/annotate.rb



trollop
http://manageiq.github.io/trollop/

https://rubygems.org/gems/trollop

]]></Content>
		<Date><![CDATA[2015-04-29]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ENC|External Node Classifier|Foreman|Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Foreman]]></Categories>
	</post>
	<post>
		<Title><![CDATA[PowerShell - nslookup]]></Title>
		<Content><![CDATA[If you want to check if machines fqdn is mapped to a ip address in dns, then on a windows machine you do:


start->enter "nslookup"

then type domain name, and hit enter, e.g.:


You should get an output like this:

<pre>
> google.com
Non-authoritative answer:
Name:    google.com
Addresses:  2a00:1450:4009:800::200e
          216.58.210.46
>
</pre>]]></Content>
		<Date><![CDATA[2015-04-29]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[PowerShell]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>PowerShell]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Network Manager (including nmcli and nmtui)]]></Title>
		<Content><![CDATA[Need to watch LA video again. 
There's basically 2 ways to ifcfg files. First you way is to do it manually via vim, as we saw earlier. The second way is to do it via Network Manager's collection of gui/cli/tui tools. 


<ul>
	<li>gui gnome interface - changes you make here gets passed to the network service, which in turn applies the changes to the various config files</li>
	<li>directly editing the config files using vim - note you will need to user the ifdown and ifup commands to load in the changes. Therefore this approach cannot be done through a putty/ssh connection because your session could terminate as soon as you use the ifdown command, and preventing you from using the ifup command.</li>
	<li>nmcli command - </li>
	<li>nmtui -</li>
</ul>



Alternatively If you want to reset your network settings back to dhcp, then you can do it via the gui gnome interface or use the following command:


<pre>
$ dhclient
dhclient(9142) is already running - exiting.

This version of ISC DHCP is based on the release available
on ftp.isc.org.  Features have been added and other changes
have been made to the base software release in order to make
it work better with this distribution.

Please report for this software via the CentOS Bugs Database:
    http://bugs.centos.org/

exiting.


</pre>











Network Manager is a service and a set of gui/tui/cli utilites are designed to make it easier to configure your network configurations and then loading them in. The NetworkManager service acts as a wrapper around the network server. 

See also:

https://www.centos.org/forums/viewtopic.php?f=50&t=47598



Tip: nmcli commands can get quite long and difficult to remember, but luckily you can use the tab+tab autocomplete feature to make it much easier to use. 




nmcli lets you manage your connections from the command line. 




<pre>
$ nmcli device status
DEVICE  TYPE      STATE      CONNECTION
enp0s3  ethernet  connected  System enp0s3
enp0s8  ethernet  unmanaged  --
lo      loopback  unmanaged  --
</pre>

Note: here enp0s8 is not managed by NM, that's because:

<pre>
$ cat /etc/sysconfig/network-scripts/ifcfg-enp0s8 | grep NM_CONTROLLED
NM_CONTROLLED=no
</pre>


Here are the connection (config files) that NM is actively using

<pre>
$ nmcli connection show
NAME           UUID                                  TYPE            DEVICE
enp0s8         7607287c-31fd-4cb3-b663-a4e4120ef265  802-3-ethernet  --
System enp0s3  ad2de184-098f-4eb8-92af-3e6c086f1186  802-3-ethernet  enp0s3
</pre>

Here, the enp0s3 device is using the connection called "System enp0s3", which corresponds to:

<pre>
$ cat /etc/sysconfig/network-scripts/ifcfg-enp0s3 | grep NAME
NAME="System enp0s3"
</pre>

To create a new connection, we do:


<pre>

$ nmcli connection add con-name "CustCon" ifname enp0s3 type ethernet autoconnect no
Connection 'CustCon' (4088f445-74b7-4f75-a8c1-513d13392ff8) successfully added.
</pre>

Running this command ended up creating the following new connection:

<pre>
$ ls -l /etc/sysconfig/network-scripts/ifcfg-CustCon              
-rw-r--r--. 1 root root 278 Oct  3 18:58 /etc/sysconfig/network-scripts/ifcfg-CustCon
</pre>

The contents of this connedction file is:

<pre>
$ cat ifcfg-CustCon
TYPE=Ethernet
BOOTPROTO=dhcp
DEFROUTE=yes
PEERDNS=yes
PEERROUTES=yes
IPV4_FAILURE_FATAL=no
IPV6INIT=yes
IPV6_AUTOCONF=yes
IPV6_DEFROUTE=yes
IPV6_PEERDNS=yes
IPV6_PEERROUTES=yes
IPV6_FAILURE_FATAL=no
NAME=CustCon
UUID=991d8f70-7f92-481e-9202-c7afe5a434c0
DEVICE=enp0s3
ONBOOT=no

</pre>

As you can see, you can have more than one connection per interface and switch between them. 

Next we can see the available connections:

<pre>
$ nmcli connection show
NAME           UUID                                  TYPE            DEVICE
CustCon        4088f445-74b7-4f75-a8c1-513d13392ff8  802-3-ethernet  --
enp0s8         7607287c-31fd-4cb3-b663-a4e4120ef265  802-3-ethernet  --
System enp0s3  ad2de184-098f-4eb8-92af-3e6c086f1186  802-3-ethernet  enp0s3
</pre>

As you can see, our new connection is not attached to a device, i.e it is not currently being used. 

To switch between connections, you do:

<pre>
$ nmcli connection down System\ enp0s3
Connection 'System enp0s3' successfully deactivated (D-Bus active path: /org/freedesktop/NetworkManager/ActiveConnection/5)
</pre>

This brings down the network session:

<pre>
$ ip addr show enp0s3
2: enp0s3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP qlen 1000
    link/ether 08:00:27:d4:f9:99 brd ff:ff:ff:ff:ff:ff
$ nmcli device status
DEVICE  TYPE      STATE         CONNECTION
enp0s3  ethernet  disconnected  --
enp0s8  ethernet  unmanaged     --
lo      loopback  unmanaged     --

</pre>

Next we bring up the network, but this time with the new connection:
<pre>
$ nmcli connection up CustCon
Connection successfully activated (D-Bus active path: /org/freedesktop/NetworkManager/ActiveConnection/4)

</pre>

Now we can check that our new connection is now active:

<pre>
$ nmcli connection show
NAME           UUID                                  TYPE            DEVICE
enp0s8         7607287c-31fd-4cb3-b663-a4e4120ef265  802-3-ethernet  --
System enp0s3  ad2de184-098f-4eb8-92af-3e6c086f1186  802-3-ethernet  --
CustCon        991d8f70-7f92-481e-9202-c7afe5a434c0  802-3-ethernet  enp0s3
enp0s3-static  0d936343-5c8e-4545-9174-f295b6c9ae86  802-3-ethernet  --

</pre>

The main purpose of this approach is that it lets you try/use/experiment various connection settings without losing the original.  

Here's the nmcli help info:


<pre>
$ nmcli -h
Usage: nmcli [OPTIONS] OBJECT { COMMAND | help }

OPTIONS
  -t[erse]                                   terse output
  -p[retty]                                  pretty output
  -m[ode] tabular|multiline                  output mode
  -f[ields] <field1,field2,...>|all|common   specify fields to output
  -e[scape] yes|no                           escape columns separators in values
  -n[ocheck]                                 don't check nmcli and NetworkManager versions
  -a[sk]                                     ask for missing parameters
  -w[ait] <seconds>                          set timeout waiting for finishing operations
  -v[ersion]                                 show program version
  -h[elp]                                    print this help

OBJECT
  g[eneral]       NetworkManager's general status and operations
  n[etworking]    overall networking control
  r[adio]         NetworkManager radio switches
  c[onnection]    NetworkManager's connections
  d[evice]        devices managed by NetworkManager
  a[gent]         NetworkManager secret agent or polkit agent
</pre>

As you can see, nmcli is actually a high-level command that is used to access a bunch of subcommands (aka objects).


<h2>
Network Manager Terminal User Interface nmtui
</h2>
This tool is another way of configuring your network settings. This tool is similar to what the Network Manager gnome gui does. ]]></Content>
		<Date><![CDATA[2015-04-29]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - The concept of default routes and DNS]]></Title>
		<Content><![CDATA[Let's say we have 2 networks:

<a href="http://codingbee.net/wp-content/uploads/2015/04/IMG_20150501_1954472.jpg"><img class="alignnone size-full wp-image-4135" src="http://codingbee.net/wp-content/uploads/2015/04/IMG_20150501_1954472.jpg" alt="IMG_20150501_195447~2" width="1944" height="1931" /></a>Let's say we have two networks,

Now the ip ranges of each network is:

Network 1 - 192.168.0.1/28
Network 2 - 10.0.0.1/28

One of the devices in network 1 (i.e. 192.168.0.1/82.2.140.10) is connected directly to the internet. This device has 2 interfaces, hence it has 2 ip address, one facing off to the internet (192.168.0.1), and the other to network 1 (82.2.140.10).

Therefore if any other machines in network-1 wants to send out data to the internet, then it has to do so via 192.168.0.1. Hence all other network-1 devices needs to be configured so that it's default gateway setting is set to 192.168.0.1. The default gateway device then forwards the data on. 

However for network-2, there are no devices that are connected directly to the internet. However one device, 192.168.0.150/10.0.0.1, is a device that is a member of both networks. One interface is facing off to network1 and the other to network2.

Therefore it is possible of a network-2 device to indirectly send data to the internet via 10.0.0.1, which in turn forwards it to 192.168.0.1.   

In order for this to work, the default gateway for 10.0.0.25 should be set to 10.0.0.1. 

Note, there is a rule, which is that the default gateway ip address must be in the same ip range as all the other devices in the network. 

Now what if 192.168.0.05 wants to forward data to 1.0.0.25. In this case, it has be done 10.0.0.1. Hence for this to be possible, we need to set up a second "route" whereby if the target ip address is in the network2 range, then it will forward the data to 192.168.0.150 rather than the default gateway (192.168.0.1). 

You can temporarily add this route using the "ip" command. First we check the current routing info 192.168.0.5:

<pre>
$ ip route show
default via 192.168.1.1 dev enp0s3  proto static  metric 100
192.168.0.1/24 dev enp0s3  proto kernel  scope link  src 192.168.0.5  metric 100

</pre>


Now we add the new route:

<pre>
$ ip route add 10.0.0.0/28 via 192.168.1.150
</pre> 

Note, the above doesn't give any outputs. 

Now to check this has worked we do:


<pre>
$ ip route show
default via 192.168.1.1 dev enp0s3  proto static  metric 100
<strong>10.0.0.0/28 via 192.168.1.150 dev enp0s3</strong>
192.168.1.0/24 dev enp0s3  proto kernel  scope link  src 192.168.1.124  metric 100
</pre>


However for the rhcsa exam, you are only required to know how to change the default gateway. Which you can simply do by editing the relevant network-scripts. 


<h2>Domain Name Servers (DNS)</h2>

These are servers that just contains a list of friendly machine fqdn/hostnames and what ip-number they are mapped to. Once again you can specify what your dns server is in your network-scripts. 


A domain server can reside anywhere, i.e. inside a local network or outside on the internet.


 

<strong>Also see:
</strong>



A machine can be part of more than one network. e.g. eth0 interface can be linked to LAN A, whereas eth1 can be connected to “LAN B”. In these scenarios, the machine needs to know which network outgoing traffic needs to be sent to. By definition, LAN A and B will have different IP ranges, and as a result, you can specify which interface to use for a given range of IP addresses.

<a href="http://codingbee.net/wp-content/uploads/2014/05/Network-coomands.png"><img class="alignnone size-full wp-image-84" src="http://codingbee.net/wp-content/uploads/2014/05/Network-coomands.png" alt="Network-coomands" width="717" height="193" /></a>

This routing table contains several columns, column 1 (destinations) indicates the IP range, and column 3 (Genmask) helps to define that range more accurately.

&nbsp;

If column 1, contains the entry “default” (which also would mean that the “G” flag is enabled), then it means that it is the default gateway.

&nbsp;

If the IP falls outside the LAN A and B range, then you could specify the routing for the default gateway, and all out-of-range outbound traffic will be sent to that destination. This tends to be the router’s ip address, this is how it is done:


In the routing table, wherever there is an asterisk, it means that data is being redirected within the LAN.


<strong>Must survive reboot:</strong>

ifdown {interface name} boot # switches off an interface and ensures that it doesn’t come back on during reboot.

ifup {interface name}   boot # brings interface back up again (i.e. connects interface to internet). And ensures it

# automatically connects again after a reboot.


<strong>Book ref:</strong>

page 38, chapter 3 – good technique on how to use ping effectively.

page 40, chapter 3 – this list the various ifconfig options.

<strong>Study guide ref:</strong>

Inserttexthere

<strong>Need to learn more about:</strong>

arp command – bottom of page 40, chapter 3. I think this associates an IP address to a mac address.

<a href="http://www.cyberciti.biz/faq/linux-setup-default-gateway-with-route-command/">http://www.cyberciti.biz/faq/linux-setup-default-gateway-with-route-command/</a>

<a href="http://answers.yahoo.com/question/index?qid=20081006214718AAlmOAZ">http://answers.yahoo.com/question/index?qid=20081006214718AAlmOAZ</a> – this explain what the “RX” and “TX” values means in the ifconfig data.

<a href="http://www.cyberciti.biz/faq/centos-add-route-command/">http://www.cyberciti.biz/faq/centos-add-route-command/</a>



]]></Content>
		<Date><![CDATA[2015-04-30]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|RedHat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Intro to Rundeck]]></Title>
		<Content><![CDATA[Rundeck can be use to run the same bash command, shell script, serverspec scripts,....etc....on multiple VMs at the same time. 

Can also be used to orchestrate puppet runs. 

There is a bit of an overlap on which tool to use, e.g. do a "passwd {username}"  using Rundeck to change someones password, or use puppet.  

https://github.com/rundeck-plugins/rundeck-logstash-plugin


Also checkout:

http://logstash.net/
https://www.elastic.co/products/kibana
]]></Content>
		<Date><![CDATA[2015-05-01]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[rundeck]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Handy command line utilities for network troubleshooting]]></Title>
		<Content><![CDATA[Here are some useful commands to help you locate and resolve network related issues:


<ul>
	<li><strong>ping</strong> - tests connectivity</li>
	<li><strong>traceroute</strong> - gives routing info. I.e. it gives you a list of all devices a data packet went throught to reach it's destination. However this command isn't that useful anymore because a lot of routers are locked down and doesn't provide any info. I.e. there is filtering on the routers. When this occurs, each row is displayed as "* * *"</li>
	<li><strong>tracepath </strong>- alternative to traceroute</li>
	<li><strong>dig </strong>- gets DNS information</li>
	<li><strong>nmap </strong>- This is useful for troubleshooting network related issues by showing you what ports are opening and the corresponding services listening in on them. Be careful you have permission of the target machine owner before running this command against it.</li>
	<li><strong>ss -atn</strong> - this is a better version of the netstat command "netstat". </li>

</ul>







<h2>Troubleshooting approach</h2>

Here's a good approach to review your network settings in order to find any faults:

<ol>
	<li>find out your machines ip address - do this using <code>ip addr show</code></li>
	<li>Check that default route is set, and default gateway ip is in the same range as your machine's ip address. You can do this using <code>ip route show</code></li>
	<li>Check what is the dns ip address that is currently being used. Best way to do this is <code>cat /etc/resolv.conf | grep "nameserver"</code></li>
	<li>now use ping to see if you can ping to the default gateway, i.e. default router</li>
	<li>now ping the dns</li>
	<li>now use the host command, i.e. <code>host {fqdn}</code>. This will return the fqdn's ip address. This will test that dns's host name resolution is working as expected. However if /etc/hosts already has a matching entry, then the host command will still return a positive result, even if the dns server is switched off. Note the windows equivalent to this command is nslookup.</li>
	<li>using dig command, i.e. <code>dig {fqdn}</code>, this gives more detailed info and it will specifically test the dns server, and ignore the local /etc/hosts file. If everything is ok, then this command will  output "NOERROR" somewhere within the output. If however if gives "NXDOMAIN", then that means that the domain doesn't exist.</li>



Here's another troubleshooting approach:


Here is a good network troubleshooting technique using ping. Do the following in sequence to test connectivity in small steps that gradually gets bigger:
<ol>
	<li>         ping localhost    # This checks machine’s internal networking software is working correctly. “localhost” is an alias in /etc/hosts file. If sucsseful then do:</li>
	<li>         ping eth0                                  # check connection to the eth0 (networking card hardware) interface. If successful, then do:</li>
	<li>         ping {default gateway ip}           # pings to the local router</li>
	<li>         ping {another device on same network}               #</li>
	<li>         ping {external ip address}         # If it fails then it means that there is a breakdown in the connection somewhere between our router and destination server.</li>
</ol>



 




]]></Content>
		<Date><![CDATA[2015-05-01]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Shell Jobs]]></Title>
		<Content><![CDATA[

<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What keyboard shortcut can you use to send a running command to the background?"]
ctrl+z
[/toggle]
[toggle title="What commnd lets you view all the commands that are paused/running in the background?"]
$ jobs
[/toggle]
[toggle title="What command can resume a job number 5, but with it still running in the background?"]
$ bg %5
[/toggle]
[toggle title="How can you run a command in the background as soon as it is triggered?"]
At the end of your command type "&" and then press enter. 
[/toggle]
[toggle title="What is the command to bring the job with id '3', to the forefront?"]
$ fg 3
[/toggle]
[toggle title="What is the command to forcefully kill the background job that has the id '2'?"]
kill -9 %2
[/toggle]
[/accordion]

<hr/>

<h2>Intro</h2>
In Linux, there are a lot of services that starts when your machine boots up. These services in turn launches lots of processes that the service requires. To view a full list of all these services you do:


<pre>
$  ps -ef | wc -l
158
$  ps aux | wc -l       # alternative command. Not used that much anymore. 
158
</pre>

In my case, I piped the output to wc command to do a line count instead. Hence in my case I have 158 processes that are currently running.

However some of the processes in the above list are not started by a service, instead they are triggered by you, when you run various commands in your current bash/putty terminal. These are a special type of processes, and they are referred to as "jobs". These jobs get terminated as soon as you close your bash/putty terminal. To view a list of your jobs, do:


<pre>
$ ps -u
USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root      1367  0.0  2.6 193316 26832 tty1     Ss+  May01   0:11 /usr/bin/Xorg :0 -background none -verbose -auth /run/gdm/auth-for-
root      8886  0.0  0.2 116752  2860 pts/0    Ss   07:01   0:01 -bash
root      9820  0.0  0.2 116248  2336 pts/1    Ss+  07:55   0:00 /bin/bash
root     10578  1.0  0.4 153128  4676 pts/0    T    08:38   0:00 vim
root     10579  0.0  0.1 123372  1380 pts/0    R+   08:38   0:00 ps -u
</pre> 


Sometime a job is keeping your shell busy and stopping you from doing anything else, e.g. when you run the "find" command to search the entire filesystem for a file. In those situations, you can do <code>ctrl+z</code> keyboard shortcut to first pause the process:


<pre>
$ find / -name hello.txt
^Z
[2]+  Stopped                 find / -name hello.txt
$
</pre> 


Then use the bg command to start the process again, but this time running in the background:


<pre>
$ bg
[2]+ find / -name hello.txt &
</pre> 

You can then monitor the job's progress using the jobs command:


<pre>
$ jobs
[2]-  Done                    find / -name hello.txt
</pre>


There's a shorthand way to make a command run in the background as soon as it is triggered, and that is by ending the command with an ampersand:



<pre>
$ jobs
$ sleep 200 &
[1] 3837
$ jobs
[1]+  Running                 sleep 200 &
$
</pre>

This saves the need for hitting ctrl+z followed by "bg"


Here's another example:


<pre>
$ sleep 600
^Z
[1]+  Stopped                 sleep 600
$ jobs
[1]+  Stopped                 sleep 600
$ bg
[1]+ sleep 600 &
$ jobs
[1]+  Running                 sleep 600 &
</pre>


Note: bg by default always gets applied to the last shell job.


If you want to bring a background job back to the foreground, the you use the fg command followed by the job id:


<pre>
$ sleep 200          # create a process
^Z                                           # send the job the background. 
[1]+  Stopped                 sleep 200
$ sleep 600
^Z
[2]+  Stopped                 sleep 600
$ sleep 300
^Z
[3]+  Stopped                 sleep 300
$ jobs
[1]   Stopped                 sleep 200
[2]-  Stopped                 sleep 600
[3]+  Stopped                 sleep 300
$ bg
[3]+ sleep 300 &
$ jobs
[1]-  Stopped                 sleep 200
[2]+  Stopped                 sleep 600
[3]   Running                 sleep 300 &
$ jobs 2                 # this displays a particular job by it's id. 
[2]+  Stopped                 sleep 600
$ jobs
[1]-  Stopped                 sleep 200
[2]+  Stopped                 sleep 600
[3]   Running                 sleep 300 &
$ bg 2
[2]+ sleep 600 &
$ jobs
[1]+  Stopped                 sleep 200
[2]   Running                 sleep 600 &
[3]-  Running                 sleep 300 &
$ bg 1
[1]+ sleep 200 &
$ jobs
[1]   Running                 sleep 200 &
[2]-  Running                 sleep 600 &
[3]+  Running                 sleep 300 &
$ fg 2                                     # This brings job with id "2" to the foreground
sleep 600
^C                                         # Here we did ctrl+c to terminate the job
[root@localhost ~]# jobs
[1]-  Running                 sleep 200 &
[2]+  Running                 sleep 300 &


</pre>

You can also kill a background job like this:

<pre>
$ kill -9 %{job id}
</pre>

Here we used the "%" to indicate that this is a job id rather than a process id. 

Tip: while using vim, You can use ctrl+z to put vim in the background so that you can look up something. Then use fg to return back to your vim session. 


 

]]></Content>
		<Date><![CDATA[2015-05-02]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Processes]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What is the command to list all processes running on the machine?"]
$ ps -ef
# this list's (e)very process in (f)ull format
[/toggle]
[toggle title="What command can visually show process dependencies, along with pid values and command line options?"]
$ pstree -ap
[/toggle]
[toggle title="What is the command to display all the pids for processes called 'httpd' that are running under the user 'apache'?"]
$ pgrep -lu apache httpd
[/toggle]
[/accordion]

<hr/>


In linux there are lots of processes running, to get a full list, we use the ps command:


The following command gives a complete list of all the processes that are currently running:

<pre>
$ ps aux
USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1  0.0  0.4  59744  4848 ?        Ss   May01   0:05 /usr/lib/systemd/systemd --switched-root --system --deserialize 24
root         2  0.0  0.0      0     0 ?        S    May01   0:00 [kthreadd]
root         3  0.0  0.0      0     0 ?        S    May01   0:00 [ksoftirqd/0]
root         5  0.0  0.0      0     0 ?        S<   May01   0:00 [kworker/0:0H]
.
.
...etc
</pre>


This lists (a)ll processes for every (u)ser and it lifts the “must have tty” restri(x)ion so to also allowing to display processes owned by system accounts (e.g. processes owned by the sshd service).


If you see the man page for ps, you'll find that the "aux" is bsd syntax, which isn't recommended. Instead we should be using the "standard syntax" alternative for specifying flags:


<pre>
$ ps -ef
UID        PID  PPID  C STIME TTY          TIME CMD
root         1     0  0 May01 ?        00:00:05 /usr/lib/systemd/systemd --switched-root --system --deserialize 24
root         2     0  0 May01 ?        00:00:00 [kthreadd]
.
.
.
</pre>


This lists the same processes, but gives different column info when compared to the aux, you can fix this by manually specifying what column you want:

<pre>
$ ps -eo user,pid,pcpu,pmem,vsize,rss,tname,stat,start_time,bsdtime,command | head
USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1  0.0  0.4  59744  4848 ?        Ss   May01   0:05 /usr/lib/systemd/systemd --switched-root --system --deserialize 24
root         2  0.0  0.0      0     0 ?        S    May01   0:00 [kthreadd]
root         3  0.0  0.0      0     0 ?        S    May01   0:00 [ksoftirqd/0]
root         5  0.0  0.0      0     0 ?        S<   May01   0:00 [kworker/0:0H]
root         6  0.0  0.0      0     0 ?        S    May01   0:00 [kworker/u2:0]
root         7  0.0  0.0      0     0 ?        S    May01   0:00 [migration/0]
root         8  0.0  0.0      0     0 ?        S    May01   0:00 [rcu_bh]
root         9  0.0  0.0      0     0 ?        S    May01   0:00 [rcuob/0]
root        10  0.0  0.0      0     0 ?        R    May01   0:04 [rcu_sched]
</pre>

You can find info about all the columns in the ps command's man page. Here's some one liner descriptions for some of the more less obvious columns:

<strong>VSZ</strong> - This is the amount of ram space that this process has reserved. 
<strong>RSS</strong> - This is the amount of ram that this process is actually using. 
<strong>TTY</strong> - A "?" indicates that this is process is not a shell job, i.e. it is a background process that has been triggered by a service, or parent processes
<strong>STAT</strong> - this means status, where "S" means it is asleep, and "R" means it is running. 


<h2>Parent and Child process</h2>
Processes can trigger other child processes. You can find the parent process id for a given process by displaying the parent process id "ppid" column:

<pre>
[root@localhost /]# ps -eo user,pid,ppid,pcpu,pmem,vsize,rss,tname,stat,start_time,bsdtime,command | head
USER       PID  PPID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1     0  0.0  0.4  59744  4848 ?        Ss   May01   0:05 /usr/lib/systemd/systemd --switched-root --system --deserialize 24
root         2     0  0.0  0.0      0     0 ?        S    May01   0:00 [kthreadd]
root         3     2  0.0  0.0      0     0 ?        S    May01   0:00 [ksoftirqd/0]
root         5     2  0.0  0.0      0     0 ?        S<   May01   0:00 [kworker/0:0H]
root         6     2  0.0  0.0      0     0 ?        S    May01   0:00 [kworker/u2:0]
root         7     2  0.0  0.0      0     0 ?        S    May01   0:00 [migration/0]
root         8     2  0.0  0.0      0     0 ?        S    May01   0:00 [rcu_bh]
root         9     2  0.0  0.0      0     0 ?        S    May01   0:00 [rcuob/0]
root        10     2  0.0  0.0      0     0 ?        R    May01   0:04 [rcu_sched]
</pre>


However this isn't that easy to following. A better command to use is the pstree, which shows the relationships more visually:



<pre>
$ pstree -ap
systemd,1 --switched-root --system --deserialize 24
  ├─ModemManager,626
  │   ├─{ModemManager},635
  │   └─{ModemManager},691
  ├─NetworkManager,2112 --no-daemon
  │   ├─dhclient,11814 -d -q -sf /usr/libexec/nm-dhcp-helper -pf /var/run/dhclient-enp0s3.pid -
  ├─at-spi-bus-laun,3794
  │   ├─dbus-daemon,3798 --config-file=/etc/at-spi2/accessibility.conf --nofork --print-address 3
  │   │   └─{dbus-daemon},3800
  │   ├─{at-spi-bus-laun},3795
  │   ├─{at-spi-bus-laun},3797
  │   └─{at-spi-bus-laun},3799
  ├─at-spi2-registr,3803 --use-gnome-session
  │   └─{at-spi2-registr},3806
  ├─atd,1332 -f
  ├─gdm,1334
  │   ├─gdm-simple-slav,1355 --display-id /org/gnome/DisplayManager/Displays/_0
  │   │   ├─Xorg,1367 :0 -background none -verbose -auth /run/gdm/auth-for-gdm-AS4ueL/database -seat seat0 -nolisten tcp vt1
  │   │   ├─gdm-session-wor,3593
  │   │   │   ├─gnome-session,3626 --session gnome-classic
  │   │   │   │   ├─abrt-applet,4015
  │   │   │   │   │   └─{abrt-applet},4035
  │   │   │   │   ├─gnome-settings-,3816
  │   │   │   │   │   ├─{gnome-settings-},3824
  │   │   │   │   │   ├─{gnome-settings-},3829
.
.
...etc
</pre>

Here is the help info for pstree:

<pre>
$ pstree --help
pstree: unrecognized option '--help'
Usage: pstree [ -a ] [ -c ] [ -h | -H PID ] [ -l ] [ -n ] [ -p ] [ -g ] [ -u ]
              [ -A | -G | -U ] [ PID | USER ]
       pstree -V
Display a tree of processes.

  -a, --arguments     show command line arguments
  -A, --ascii         use ASCII line drawing characters
  -c, --compact       don't compact identical subtrees
  -h, --highlight-all highlight current process and its ancestors
  -H PID,
  --highlight-pid=PID highlight this process and its ancestors
  -g, --show-pgids    show process group ids; implies -c
  -G, --vt100         use VT100 line drawing characters
  -l, --long          don't truncate long lines
  -n, --numeric-sort  sort output by PID
  -N type,
  --ns-sort=type      sort by namespace type (ipc, mnt, net, pid, user, uts)
  -p, --show-pids     show PIDs; implies -c
  -s, --show-parents  show parents of the selected process
  -S, --ns-changes    show namespace transitions
  -u, --uid-changes   show uid transitions
  -U, --unicode       use UTF-8 (Unicode) line drawing characters
  -V, --version       display version information
  -Z,
  --security-context   show SELinux security contexts
  PID    start at this PID; default is 1 (init)
  USER   show only trees rooted at processes of this user
</pre>


Note if a child process is note responding for any reason, then a quick way to fix this is by killing the parent process.   



One of the things you will often find yourself doing is locating a particular process. This can be done by piping the output of "ps -ef" to a grep command. However you can also use the pgrep command. For example to list all processes called something like "http" and are running under the user "apache", we do: 

<pre>
$ pgrep -lu apache http
20428 httpd
20429 httpd
20430 httpd
20431 httpd
20432 httpd

</pre>]]></Content>
		<Date><![CDATA[2015-05-02]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Memory usage]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What is the command too see how much ram is free/used?"]
$ free -m
[/toggle]
<hr/>



If you want to see how much ram your machine has installed and how much of it is being used, then you need to use the "free" command:


<pre>
$ free -m
              total        used        free      shared  buff/cache   available
Mem:            993         266         440           6         286         573
Swap:          2047           0        2047

</pre> 


We used the -m option to output the info in megabytes. 


In this case we have about 1gb of ram installed, of which 266MB is being used and 440MB is free. 
]]></Content>
		<Date><![CDATA[2015-05-02]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Monitoring system load using the Top command]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What is the command to view real time info for the processes with the pids '20428' and '1'?"]
top -p 20428,1
[/toggle]

[/accordion]

<hr/>





If you want to see how much load your machine is under, then we use the top command:


<pre>
$ top
</pre>

This gives real-time-updating output of:



<pre>
top - 22:07:24 up  2:54,  2 users,  <strong>load average: 0.00, 0.01, 0.05</strong>
Tasks: 128 total,   3 running, 125 sleeping,   0 stopped,   0 zombie
%Cpu(s):  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
KiB Mem :  1017480 total,   392620 free,   297988 used,   326872 buff/cache
KiB Swap:  2097148 total,  2097148 free,        0 used.   560664 avail Mem

  PID USER      PR  NI    VIRT    RES    SHR S %CPU %MEM     TIME+ COMMAND
    1 root      20   0   59592   7040   3928 S  0.0  0.7   0:03.67 systemd
    2 root      20   0       0      0      0 S  0.0  0.0   0:00.00 kthreadd
    3 root      20   0       0      0      0 S  0.0  0.0   0:00.17 ksoftirqd/0
    5 root       0 -20       0      0      0 S  0.0  0.0   0:00.00 kworker/0:0H
.
.
.
</pre>

On the first line we have a "load average" entry, followed by 3 values. 

These values shows the average number of processes in the queue (waiting to be allocated a cpu) in the last 1, 5, and 15 minutes respectively. If these values are successfully increasing then it means that the machine's load is declining. However if it is successively increasing then it means that the machine load is increasing. 


By default, the top command only lists the top 30 or so processes, order by cpu usage. If you want to use top to monitor certain processes, then you can do:
<pre>
$ top –p {pid1},{pid2},{pid3},...etc  
</pre>

This will only list the process with the given pid values, which makes it easier to work with, when you are interested in some particular processes. .

Top is an interactive utility and While in "top" mode, if you then select “h”, you will find various options available:

<pre>
<span style='font-size:12px'>Help for Interactive Commands - procps-ng version 3.3.10
Window 1:Def: Cumulative mode Off.  System: Delay 3.0 secs; Secure mode Off.

  Z,B,E,e   Global: 'Z' colors; 'B' bold; 'E'/'e' summary/task memory scale
  l,t,m     Toggle Summary: 'l' load avg; 't' task/cpu stats; 'm' memory info
  0,1,2,3,I Toggle: '0' zeros; '1/2/3' cpus or numa node views; 'I' Irix mode
  f,F,X     Fields: 'f'/'F' add/remove/order/sort; 'X' increase fixed-width

  L,&,<,> . Locate: 'L'/'&' find/again; Move sort column: '<'/'>' left/right
  R,H,V,J . Toggle: 'R' Sort; 'H' Threads; 'V' Forest view; 'J' Num justify
  c,i,S,j . Toggle: 'c' Cmd name/line; 'i' Idle; 'S' Time; 'j' Str justify
  x,y     . Toggle highlights: 'x' sort field; 'y' running tasks
  z,b     . Toggle: 'z' color/mono; 'b' bold/reverse (only if 'x' or 'y')
  u,U,o,O . Filter by: 'u'/'U' effective/any user; 'o'/'O' other criteria
  n,#,^O  . Set: 'n'/'#' max tasks displayed; Show: Ctrl+'O' other filter(s)
  C,...   . Toggle scroll coordinates msg for: up,down,left,right,home,end

  k,r       Manipulate tasks: 'k' kill; 'r' renice 
  d or s    Set update interval
  W,Y       Write configuration file 'W'; Inspect other output 'Y'
  q         Quit
          ( commands shown with '.' require a visible task display window )
Press 'h' or '?' for help with Windows,
Type 'q' or <Esc> to continue</span>
</pre>


There is also a tui version of the top command called htop. Which you need to install:

<pre>
$ yum install htop
</pre>

Then do:

<pre>
$ htop
</pre>

See also:
http://www.thegeekstuff.com/2010/01/15-practical-unix-linux-top-command-examples/
http://www.tecmint.com/12-top-command-examples-in-linux/



]]></Content>
		<Date><![CDATA[2015-05-02]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Signals and stopping processes]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What command will list a full list of available kill signals?"]
$ kill -l
[/toggle]
[toggle title="Where can you find info about each of these signals?"]
$ man 7 signal
[/toggle]
[toggle title="How do you send the -9 kill signal to a process with id '25'?"]
$ kill -s 9 25
[/toggle]
[toggle title="What is the command to kill all processes called 'httpd'?"]
$ killall httpd

[/toggle]
[toggle title="What is the command?"]


[/toggle]
[toggle title="What is the command?"]


[/toggle]
[/accordion]

<hr/>







<h2>Intro</h2>
One thing you may want to do is to stop a process. This is done by sending a process a "signal". A signal is an instruction that can be sent to a process. Processes aren't allowed to ignore an incoming signal.


There's a standard set of signals you can send to a process. The kill command let's you view the full list of available signals:


<pre>
$ kill -l
 1) SIGHUP       2) SIGINT       3) SIGQUIT      4) SIGILL       5) SIGTRAP
 6) SIGABRT      7) SIGBUS       8) SIGFPE       9) SIGKILL     10) SIGUSR1
11) SIGSEGV     12) SIGUSR2     13) SIGPIPE     14) SIGALRM     15) SIGTERM
16) SIGSTKFLT   17) SIGCHLD     18) SIGCONT     19) SIGSTOP     20) SIGTSTP
21) SIGTTIN     22) SIGTTOU     23) SIGURG      24) SIGXCPU     25) SIGXFSZ
26) SIGVTALRM   27) SIGPROF     28) SIGWINCH    29) SIGIO       30) SIGPWR
31) SIGSYS      34) SIGRTMIN    35) SIGRTMIN+1  36) SIGRTMIN+2  37) SIGRTMIN+3
38) SIGRTMIN+4  39) SIGRTMIN+5  40) SIGRTMIN+6  41) SIGRTMIN+7  42) SIGRTMIN+8
43) SIGRTMIN+9  44) SIGRTMIN+10 45) SIGRTMIN+11 46) SIGRTMIN+12 47) SIGRTMIN+13
48) SIGRTMIN+14 49) SIGRTMIN+15 50) SIGRTMAX-14 51) SIGRTMAX-13 52) SIGRTMAX-12
53) SIGRTMAX-11 54) SIGRTMAX-10 55) SIGRTMAX-9  56) SIGRTMAX-8  57) SIGRTMAX-7
58) SIGRTMAX-6  59) SIGRTMAX-5  60) SIGRTMAX-4  61) SIGRTMAX-3  62) SIGRTMAX-2
63) SIGRTMAX-1  64) SIGRTMAX

</pre>





These signals are documented in section 7 of the  signal's man page:

<pre>
$ man 7 signal
</pre>

Here's an extract of the relevant part:


<pre>
   Standard signals
       Linux supports the standard signals listed below.  Several signal numbers are architecture-dependent, as indicated in the
       "Value" column.  (Where three values are given, the first one is usually valid for alpha and sparc, the  middle  one  for
       x86,  arm, and most other architectures, and the last one for mips.  (Values for parisc are not shown; see the Linux ker‐
       nel source for signal numbering on that architecture.)  A - denotes that a signal is absent on the  corresponding  archi‐
       tecture.)

       First the signals described in the original POSIX.1-1990 standard.


       Signal     Value     Action   Comment
       ──────────────────────────────────────────────────────────────────────
       SIGHUP        1       Term    Hangup detected on controlling terminal
                                     or death of controlling process
       SIGINT        2       Term    Interrupt from keyboard
       SIGQUIT       3       Core    Quit from keyboard
       SIGILL        4       Core    Illegal Instruction
       SIGABRT       6       Core    Abort signal from abort(3)
       SIGFPE        8       Core    Floating point exception
       <strong>SIGKILL       9       Term    Kill signal</strong>
       SIGSEGV      11       Core    Invalid memory reference
       SIGPIPE      13       Term    Broken pipe: write to pipe with no
                                     readers
       SIGALRM      14       Term    Timer signal from alarm(2)
       <strong>SIGTERM      15       Term    Termination signal</strong>
       SIGUSR1   30,10,16    Term    User-defined signal 1
       SIGUSR2   31,12,17    Term    User-defined signal 2
       SIGCHLD   20,17,18    Ign     Child stopped or terminated
       SIGCONT   19,18,25    Cont    Continue if stopped
       SIGSTOP   17,19,23    Stop    Stop process
       SIGTSTP   18,20,24    Stop    Stop typed at terminal
       SIGTTIN   21,21,26    Stop    Terminal input for background process
       SIGTTOU   22,22,27    Stop    Terminal output for background process

       The signals SIGKILL and SIGSTOP cannot be caught, blocked, or ignored.



</pre>

The most commonly used singals are 9 and 15. Signal 15 is the signal to send to a process, if you want that process to be gracefully terminated. Signal 9 is the signal you send if you want to forcefully terminate a process. These two signals are by far the main signals that are used. Signal 1 is also used occasionally and what this does is that it reloads any related config files. 

It is best practice to always try signal 15, and issue signal 9 as last resort, due to signal 9's forceful nature.  

Aside from that we hardly use any of the other signals. Now to send any of these signals to a process, you can can do it from withing the top command, simply type "k" after you have started up top. Alternatively you can use the "kill" command:

<pre>
$ kill {pid}
</pre> 

In nearly all cases the signal we want to send to a process is a kill signal. That's why the command for sending a signal to a process is called "kill", which can be a bit misleading. 

The kill command by default will always send signal 15 (i.e. graceful termination). If you want to send signal 9 (immediate forceful termination) instead, the you do it like this:


<pre>
$ kill -s 9 {pid}
</pre>



If you want kill a all processes of a certain name, then you can do this using the killall command:


<pre>
$ killall {process-name}

</pre>

Here's an example:


<pre>
$ vim &
[1] 4747
$ vim &
[2] 4748

[1]+  Stopped                 vim
$ vim &
[3] 4749

[2]+  Stopped                 vim
$ vim &
[4] 4750

[3]+  Stopped                 vim
$ jobs
[1]   Stopped                 vim
[2]   Stopped                 vim
[3]-  Stopped                 vim
[4]+  Stopped                 vim
$ killall -9 vim
[1]   Killed                  vim
[2]   Killed                  vim
[3]-  Killed                  vim
[4]+  Killed                  vim
$ jobs
$


</pre>

Notice that with the killall command, you don't specify the pid, instead you specify the process's name. 


There is another command that is similar to the killall command and that is the pkill command:

<pre>
$ pkill {search-term}     
</pre>


This works in a similar way to the killall command. The main difference is that you can kill without knowing the full and proper name of the process.

Be careful when using thepkill command  because you might end up killing a process that you need. Hence it is best to first run the  <a href="http://codingbee.net/tutorials/rhcsa/rhcsa-processes/">pgrep -l {search-term}</a> command to confirm that you are going to kill the processes that you to kill.

See also:

http://unixhelp.ed.ac.uk/CGI/man-cgi?signal+7    (for some reason thedie.net website doesn’t list all the signals.)

http://www.howtogeek.com/howto/linux/kill-linux-processes-easier-with-pkill/

http://www.thegeekstuff.com/2009/12/4-ways-to-kill-a-process-kill-killall-pkill-xkill/]]></Content>
		<Date><![CDATA[2015-05-02]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Setting process priorities by setting nice values]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What is the command to view help info about highest/lowest priority settings?"]
$ nice <span style='letter-spacing:0.1px'>-</span>-help
[/toggle]
[toggle title="What is the command to view all running process's priorities?"]
$ ps -eo pid,nice,command
[/toggle]
[toggle title="What is the command to start vim with priority of -5?"]
$ nice -n -5 vim
[/toggle]
[toggle title="What is the command to change the priority to -10 for the process with the id 2351?"]
$ renice -n -10 2351
[/toggle]
[/accordion]

<hr/>


<h2>Intro</h2>
Changing a process’s priority is a good way to make your machine run more efficiently. Changing process priorities is to do with increasing/decreasing how much CPU time a process can have while the machine is running.

For example if you are setting up a machine to primarily run as a web server, then it is a good idea to elevate the web-server related processes so that they get more access to the CPU.

modifying the priority is to do with changing a process’s “nice” settings. The nice setting takes a value from -20 to +19, where -20 is the highest priority…and…+19 is the lowest priority. Hence the higher the nice value, the more "nice" a process is in allowing other processes to spend more time with the cpu.

You can view the nice values using to the <code>top</code> command, where the nice values are given under the "NI" column:
<pre>$ top - 16:17:48 up 13 min,  2 users,  load average: 0.00, 0.02, 0.05
Tasks: 128 total,   2 running, 126 sleeping,   0 stopped,   0 zombie
%Cpu(s):  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
KiB Mem :  1017480 total,   427188 free,   269520 used,   320772 buff/cache
KiB Swap:  2097148 total,  2097148 free,        0 used.   589188 avail Mem

  PID USER      PR  <mark>NI</mark>    VIRT    RES    SHR S %CPU %MEM     TIME+ COMMAND
 1759 root      20   0  141620   5448   4212 S  0.3  0.5   0:00.41 sshd
    1 root      20   0   59592   7032   3928 S  0.0  0.7   0:01.82 systemd
    2 root      20   0       0      0      0 S  0.0  0.0   0:00.00 kthreadd
    3 root      20   0       0      0      0 S  0.0  0.0   0:00.03 ksoftirqd/0
    5 root       0 -20       0      0      0 S  0.0  0.0   0:00.00 kworker/0:0H
    6 root      20   0       0      0      0 S  0.0  0.0   0:00.00 kworker/u2:0
    7 root      rt   0       0      0      0 S  0.0  0.0   0:00.00 migration/0
.
.
.
...etc
</pre>
You can also use the ps command to view nice values:
<pre>$ ps -eo user,pid,nice,pcpu,pmem,vsize,rss,tname,stat,start_time,bsdtime,command | head
USER       PID  NI %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1   0  0.0  0.6  59592  7036 ?        Ss   16:04   0:01 /usr/lib/systemd/systemd --switched-root --system --deserialize 24
root         2   0  0.0  0.0      0     0 ?        S    16:04   0:00 [kthreadd]
root         3   0  0.0  0.0      0     0 ?        S    16:04   0:00 [ksoftirqd/0]
root         5 -20  0.0  0.0      0     0 ?        S&lt;   16:04   0:00 [kworker/0:0H]
root         6   0  0.0  0.0      0     0 ?        S    16:04   0:00 [kworker/u2:0]
root         7   -  0.0  0.0      0     0 ?        S    16:04   0:00 [migration/0]
root         8   0  0.0  0.0      0     0 ?        S    16:04   0:00 [rcu_bh]
root         9   0  0.0  0.0      0     0 ?        S    16:04   0:00 [rcuob/0]
root        10   0  0.0  0.0      0     0 ?        R    16:04   0:00 [rcu_sched]
</pre>
A process's nice value is usually set to "0", (i.e. average priority) as indicated by the NI column above. The PR column is short for "priority", and it is equal to "20+(nice value). This nice range is specified in the <code>nice</code> command's help info:
<pre>$  nice --help
Usage: nice [OPTION] [COMMAND [ARG]...]
Run COMMAND with an adjusted niceness, which affects process scheduling.
With no COMMAND, print the current niceness.  Niceness values range from
-20 (most favorable to the process) to 19 (least favorable to the process).

Mandatory arguments to long options are mandatory for short options too.
  -n, --adjustment=N   add integer N to the niceness (default 10)
      --help     display this help and exit
      --version  output version information and exit

NOTE: your shell may have its own version of nice, which usually supersedes
the version described here.  Please refer to your shell's documentation
for details about the options it supports.

GNU coreutils online help: &lt;http://www.gnu.org/software/coreutils/&gt;
For complete documentation, run: info coreutils 'nice invocation'

</pre>
By default, all processes starts with a nice value of “0”, in other words average priority.

You can specify a nice value at the time of starting a new process:
<pre>$ nice –n -10 {command}   
</pre>
For example:
<pre>$ nice –n -10 service vsftpd start    # this starts the ftp server with a higher priority.
</pre>
However if you want to change the nice value of a process that is already running, then you need to use the <code>renice</code> command instead. Here's the renice command's help info:
<pre>$ renice --help

Usage:
 renice [-n]  [-p|--pid] ...
 renice [-n]   -g|--pgrp ...
 renice [-n]   -u|--user ...

Options:
 -g, --pgrp         interpret argument as process group ID
 -n, --priority    specify the nice increment value
 -p, --pid          interpret argument as process ID (default)
 -u, --user &lt;name|id&gt;   interpret argument as username or user ID
 -h, --help             display help text and exit
 -V, --version          display version information and exit

For more information see renice(1).

</pre>
However if you want to change the priority of a command/process that is already running, then you do this like this:
<pre>$ renice –n 5 {process’s-pid-value}
</pre>
Therefore you first need to use ps, to identify the pid of the process you want to reprioritize. Then to check if the renice command has worked, you can use the ps command to see the new nice value:
<pre>$ ps -p  {process’s-pid-value} -o user,pid,nice,tname,start_time,command
</pre>
You can also change the nice value interactively using the top command:
<pre>$ top –p {pid1},{pid2},{pid3},...etc  
</pre>
This will only list the process with the given pid values, which makes it easier to work with, when you are interested in some particular processes. .

While in "top" mode, if you then select “h”, you will find that you need to use the “r” option to renice a process:
<pre>Help for Interactive Commands - procps-ng version 3.3.10
Window 1:Def: Cumulative mode Off.  System: Delay 3.0 secs; Secure mode Off.

  Z,B,E,e   Global: 'Z' colors; 'B' bold; 'E'/'e' summary/task memory scale
  l,t,m     Toggle Summary: 'l' load avg; 't' task/cpu stats; 'm' memory info
  0,1,2,3,I Toggle: '0' zeros; '1/2/3' cpus or numa node views; 'I' Irix mode
  f,F,X     Fields: 'f'/'F' add/remove/order/sort; 'X' increase fixed-width

  L,&amp;,&lt;,&gt; . Locate: 'L'/'&amp;' find/again; Move sort column: '&lt;'/'&gt;' left/right
  R,H,V,J . Toggle: 'R' Sort; 'H' Threads; 'V' Forest view; 'J' Num justify
  c,i,S,j . Toggle: 'c' Cmd name/line; 'i' Idle; 'S' Time; 'j' Str justify
  x,y     . Toggle highlights: 'x' sort field; 'y' running tasks
  z,b     . Toggle: 'z' color/mono; 'b' bold/reverse (only if 'x' or 'y')
  u,U,o,O . Filter by: 'u'/'U' effective/any user; 'o'/'O' other criteria
  n,#,^O  . Set: 'n'/'#' max tasks displayed; Show: Ctrl+'O' other filter(s)
  C,...   . Toggle scroll coordinates msg for: up,down,left,right,home,end

<strong>  k,r       Manipulate tasks: 'k' kill; 'r' renice</strong>  
  d or s    Set update interval
  W,Y       Write configuration file 'W'; Inspect other output 'Y'
  q         Quit
          ( commands shown with '.' require a visible task display window )
Press 'h' or '?' for help with Windows,
Type 'q' or  to continue


</pre>
After that, "renice it", and the nice column should reflect the new nice value straight away.]]></Content>
		<Date><![CDATA[2015-05-02]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Using RPM]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What is the command to install tree.rpm that is currently residing in the local directory?"]
$ rpm –ivh tree.rpm         
[/toggle]
[toggle title="What is the command to install tree.rpm, which resides at http://codingbee.net/tree.rpm?"]
$ rpm -ivh http://codingbee.net/tree.rpm 
[/toggle]
[toggle title="What is the command to list all packages that contains the word 'tree'?"]
$ rpm –qa | grep tree 
[/toggle]
[toggle title="What is the command to uninstall a package called 'tree'?"]
$ rpm –e tree 
[/toggle]
[toggle title="What is the command to upgrade package called tree?"]
$ rpm –Uvh tree.rpm 
[/toggle]
[toggle title="What is the command do view info data for the package 'tree'?"]
$ rpm -qi tree
[/toggle]
[toggle title="What is the command view full list of all files that have been added to our machine as part of installing the package 'tree'?"]
$ rpm -ql tree
[/toggle]
[toggle title="What is the command to locate all installed documentation that came from installing the package 'tree'?"]
$ rpm -qd tree
[/toggle]
[toggle title="What is the command to locate all config files that came from installing the package 'tree'?"]
$ rpm -qc tree
[/toggle]
[toggle title="What is the command to view the list of files that are contained within tree.rpm before installing it, assuming tree.rpm is in your current working directory?"]
$ rpm -qpl tree.rpm
[/toggle]
[/accordion]

<hr/>

Redhat Package Manager (aka rpm) is the main utility used for installings software packages. yum is actually a wrapper for RPM which has been created to make rpm easier to use. Here is how you install an RPM package that you have already downloaded:


<pre>
$ rpm –ivh {package-name}.rpm         
</pre>
Note: this isn't best practice, because it wont handle rpm dependencies. Better to do it with yum command instead. 

Here we (i)nstall the packaged in (v)erbose mode, and using a (h)ashed progress bar to show the progress. 

Note, the rpm file could exist on the internet, e.g.: 

<pre>
$ rpm -ivh http://yum.puppetlabs.com/puppetlabs-release-el-6.noarch.rpm 
</pre>


Unfortunately the above command doesn’t give any “installation successful” messages. So, after installing it, you can then check if it has been installed by running the following command:

<pre>
$ rpm –qa | grep tree 
</pre>


This rpm command (q)ueries and lists (a)ll installed packages only.  


To uninstall a package, you use the (e)rase option:


<pre>
$ rpm –e tree 
</pre>


Note, you don’t have to state the full package name to uninstall it.

After uninstalling it, you can do “rpm –qa” again to confirm that package has definitely been uninstalled.


<h2>Upgrading packages</h2>
Upgrading a package, is done like this:

<pre>
$ rpm –Uvh {package-name}.rpm 
</pre>

Here we use the (U)pgrade option to upgrade the package (if it has already been installed). If the package hasn't been installed in the first place, then the U option automatically installs the latest version. 


There is also a yum equivalent that you can try:

<pre>
$ yum update {package-name} 
</pre>


<h2>Querying installed packages</h2>

rpm lets you find various information about your installed packages. This is main reason why the rpm command is still commonly used. Rpm maintains it's own internal db that holds information about the various installed packages, and the "-q" option let's us query this db. 

Also note that the rpm command does not know anything about yum repos. Yum repos are specific to the yum utility. Hence you can't use rpm to view a list of packages that are available for installation (but not installed yet).


Here's how you can find high level (i)nformation about a package:
<pre>
$ rpm -qi tree
Name        : tree
Version     : 1.6.0
Release     : 10.el7
Architecture: x86_64
Install Date: Sat 11 Apr 2015 00:25:27 BST
Group       : Applications/File
Size        : 89505
License     : GPLv2+
Signature   : RSA/SHA256, Fri 04 Jul 2014 06:36:46 BST, Key ID 24c6a8a7f4a80eb5
Source RPM  : tree-1.6.0-10.el7.src.rpm
Build Date  : Mon 09 Jun 2014 20:28:53 BST
Build Host  : worker1.bsys.centos.org
Relocations : (not relocatable)
Packager    : CentOS BuildSystem <http://bugs.centos.org>
Vendor      : CentOS
URL         : http://mama.indstate.edu/users/ice/tree/
Summary     : File system tree viewer
Description :
The tree utility recursively displays the contents of directories in a
tree-like format.  Tree is basically a UNIX port of the DOS tree
utility.
</pre>








rpm lets you see what (d)ocumentations was installed along with the package's installation:

<pre>
$ rpm -qd tree
/usr/share/doc/tree-1.6.0/LICENSE
/usr/share/doc/tree-1.6.0/README
/usr/share/man/man1/tree.1.gz

</pre> 


Here's a command that lists all the files that have been installed from a package:


<pre>
$ rpm -ql tree
/usr/bin/tree
/usr/share/doc/tree-1.6.0
/usr/share/doc/tree-1.6.0/LICENSE
/usr/share/doc/tree-1.6.0/README
/usr/share/man/man1/tree.1.gz
</pre>
 


You can take the same approach for querying the location of all the config files that have been deployed as part of a package installation: 


<pre>
$ rpm -qc openssh-server
/etc/pam.d/sshd
/etc/ssh/sshd_config
/etc/sysconfig/sshd
</pre>




<h2>Querying downloaded RPM files</h2>

Let's say you download an rpm package from the internet:

<pre>
$  wget http://yum.puppetlabs.com/puppetlabs-release-el-7.noarch.rpm
--2015-05-04 08:52:32--  http://yum.puppetlabs.com/puppetlabs-release-el-7.noarch.rpm
Resolving yum.puppetlabs.com (yum.puppetlabs.com)... 192.155.89.90, 2600:3c03::f03c:91ff:fedb:6b1d
Connecting to yum.puppetlabs.com (yum.puppetlabs.com)|192.155.89.90|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 10160 (9.9K) [application/x-redhat-package-manager]
Saving to: ‘puppetlabs-release-el-7.noarch.rpm’

100%[==========================================================================================>] 10,160      --.-K/s   in 0s

2015-05-04 08:52:32 (115 MB/s) - ‘puppetlabs-release-el-7.noarch.rpm’ saved [10160/10160]

$ ls -l puppetlabs-release-el-7.noarch.rpm
-rw-r--r--. 1 root root 10160 Aug 26  2014 puppetlabs-release-el-7.noarch.rpm

</pre>

Now a common thing you will want to do is to see what what files this rpm will deploy should we install it. We can view this using the (p)ackage option:


<pre>
$ rpm -qpl puppetlabs-release-el-7.noarch.rpm
warning: puppetlabs-release-el-7.noarch.rpm: Header V4 RSA/SHA1 Signature, key ID 4bd6ec30: NOKEY
/etc/pki/rpm-gpg/RPM-GPG-KEY-nightly-puppetlabs
/etc/pki/rpm-gpg/RPM-GPG-KEY-puppetlabs
/etc/yum.repos.d/puppetlabs.repo
</pre>

If we omit the "p" option then rpm will only end up querying it's own internal db, rather than the local rpm file. 

Another cool thing is that you don't even need to download the rpm, you can simply do the following instead:


<pre>
$ rpm -qpl http://yum.puppetlabs.com/puppetlabs-release-el-7.noarch.rpm
warning: http://yum.puppetlabs.com/puppetlabs-release-el-7.noarch.rpm: Header V4 RSA/SHA1 Signature, key ID 4bd6ec30: NOKEY
/etc/pki/rpm-gpg/RPM-GPG-KEY-nightly-puppetlabs
/etc/pki/rpm-gpg/RPM-GPG-KEY-puppetlabs
/etc/yum.repos.d/puppetlabs.repo
</pre>

You can then use options (c)onfig files, (d)ocumentations,...etc, in the same way to query rpm files, as long as you use it in conjuction with the local (p)ackage option, as given in the above example. 



See also:

http://www.tecmint.com/20-practical-examples-of-rpm-commands-in-linux/]]></Content>
		<Date><![CDATA[2015-05-03]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Yum Repositories]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What is the command to list all configured yum repos?"]
$ yum repolist
[/toggle]
[toggle title="In which directory are all the repos housed in?"]
/etc/yum.repos.d/
[/toggle]
[toggle title="What is the naming convention of a yum repo file?"]
It can be called anything you like, but the file name must contain the ".repo" suffix

[/toggle]
[toggle title="What are the entries of a .repo file?"]<span style="font-size:13px">[repo-name]                              # This stanza's name. This must be unique. 
name=Local Network Yum Repo        # The repo's name, as displayed by "yum repolist"
baseurl=ftp://192.168.75.132/pub/  # The directory that contains the repo's repodata file
enabled=1                          # Optional: tells yum whether to use this repo, 1=yes, 0=no
gpgcheck=0                         # whether GPG check is enabled. 1=yes, 0=no
gpgkey=file:<mark>///</mark>etc/pki/rpm-gpg-RPM-GPG-KEY-redhat-release    # Required only if gpgcheck=1</span>
[/toggle]
[toggle title="What are the http, ftp and local forms of the baseurl entry?"]
baseurl = http://servername/my/repo     
baseurl = ftp://servername/my/repo      
baseurl = file:<mark>///</mark>srv/my/repo/       
[/toggle]
[toggle title="What is the command to list all repos including the disabled ones?"]
$ yum repolist all

[/toggle]
[toggle title="What is the command to create a new repo from the command line?"]<span style='font-size:14px'>$ yum-config-manager <span style="letter-spacing:0.1px">-</span>-add-repos=http://download.fedoraproject.org/pub/epel/7/$basearch</span>
[/toggle]
<hr/>


In windows if you want to install a software, e.g. firefox, then you open up an internet browser and download it from somewhere on the internet. 

However for RHEL, we take a different approach. We have to configure our machine to use remote yum repositories.

Yum uses pre-configured online repositories (that are in the form of websites and ftp sites, or even local folders containing rpm files) to search for and install software packages. Yum can also be used to install software that have been downloaded….and it will still be able automatically install dependencies.


These yum repos can hosts thousands of open source software packages that are in the form of .rpm files. Once we have configured to use an rpm repo, we then have access to all their rpm packages, and can then install them using the "yum" command. 

First off, let's see what yum repos we currently have access to, which we do using yum's repolist option:


<pre>
$ yum repolist
Loaded plugins: fastestmirror, langpacks
Loading mirror speeds from cached hostfile
 * base: centos.hyve.com
 * epel: mirror.bytemark.co.uk
 * extras: mirror.synergyworks.co.uk
 * updates: mirror.as29550.net
repo id                  repo name                                           status
!base/7/x86_64           CentOS-7 - Base                                     8,652
!extras/7/x86_64         CentOS-7 - Extras                                      84
!updates/7/x86_64        CentOS-7 - Updates                                    362
repolist: 16,848
</pre>
 

The config files that makes yum aware and access all these yum repos, are stored in <code>/etc/yum.repos.d</code> directory:


<pre>
$ ls -l /etc/yum.repos.d/
total 52
-rw-r--r--. 1 root root 1664 Mar 31  2015 CentOS-Base.repo
-rw-r--r--. 1 root root 1309 Mar 31  2015 CentOS-CR.repo
-rw-r--r--. 1 root root  649 Mar 31  2015 CentOS-Debuginfo.repo
-rw-r--r--. 1 root root  290 Mar 31  2015 CentOS-fasttrack.repo
-rw-r--r--. 1 root root 1331 Mar 31  2015 CentOS-Sources.repo
-rw-r--r--. 1 root root 1002 Mar 31  2015 CentOS-Vault.repo
-rw-r--r--. 1 root root 1023 Sep 16 19:45 epel.repo
-rw-r--r--. 1 root root 1056 Nov 25  2014 epel-testing.repo
-rw-r--r--. 1 root root  364 Sep 11 10:59 foreman-plugins.repo
-rw-r--r--. 1 root root  334 Sep 11 10:59 foreman.repo
-rw-r--r--. 1 root root 1250 Aug 25  2014 puppetlabs.repo
-rw-r--r--. 1 root root  158 Sep 11  2014 rhscl-ruby193-epel-7-x86_64.repo
-rw-r--r--. 1 root root  159 Jun 18 11:34 rhscl-v8314-epel-7-x86_64.repo
</pre>

If you want to access a new yum repo, then you need to create a ".repo" file in this directory. Yum will then scan this directory and load in all the .repo files. 

You can name the .repo file to your liking, .e.g whatever.repo. But it's best practice to choosing something sensible. 

The .repo file has to contain at least 4 lines:


<pre>
<span style="font-size:12px">
$ cat localnet.repo
[localnet]                   # This stanza's name. You can choose any name, but it must be unique. 
name=Local Network Yum Repo        # The repo's name, as displayed by "yum repolist"
baseurl=ftp://192.168.75.132/pub/  # The directory that contains the repo's repodata
enabled=1                          # Optional: tells yum whether to use this repo, 1=yes, 0=no
gpgcheck=0                         # whether GNU Privacy Guard check is enabled. 1=yes, 0=no
gpgkey=file:<mark>///</mark>etc/pki/rpm-gpg-RPM-GPG-KEY-redhat-release    # Required only if gpgcheck=1
</span>
</pre>

This repodata file contains info about location and groupings of packages that yum uses to find and install. 

The most important setting in the .repo file is the baseurl. This can take the form of:

<pre>
baseurl = http://servername/my/repo  
baseurl = ftp://servername/my/repo   
baseurl = file:///srv/my/repo/       
</pre>

Notice the 3 forward slashes, when declaring a "file" baseurl. The first 2 slashes are part of the standard syntax, e.g. "http://" and "ftp://", whereas the third slash represent's your local machine's root directory. 

Note, there are some rpm that you can install, whose sole purpose is to place is to setup+enable a yum repo, i.e. place the .repo file and gpg keys. <a href="https://docs.puppetlabs.com/guides/install_puppet/install_el.html">Installing puppet software</a> is an example of this. After that you can then use yum to do the actual puppet insstallation.  


Once you have created the .repo file, you can then use yum repolist to confirm that it exists. Here's an example:


<pre>
$ touch /etc/yum.repos.d/test.repo
[root@localhost yum.repos.d]# vi /etc/yum.repos.d/test.repo
[root@localhost yum.repos.d]# cat /etc/yum.repos.d/test.repo
[mytestrepo]
name=CodingBee repo
baseurl=https://yum.puppetlabs.com/el/7/products/x86_64/
enabled=1
gpgcheck=0
[root@localhost yum.repos.d]# yum repolist
Loaded plugins: fastestmirror, langpacks
repo id                     repo name                     status
base/7/x86_64               CentOS-7 - Base                8,652
epel/x86_64                 Extra Packages for OEL         7,760
extras/7/x86_64             CentOS-7 - Extras                 84
<strong>mytestrepo                  CodingBee repo                   162</strong>
updates/7/x86_64            CentOS-7 - Updates               362
repolist: 17,020

</pre>

Note: you can do <code>yum repolist all</code> to view all repos, including those that are currently disabled. 


<h2>Using the yum-config-manger</h2>
Manually creating a .repo file can be tedious. Luckily there is a way to auto generate the .repo using <strong>yum-config-manger</strong>. All this command needs is the base url. Here's an example: 

<pre>
$ yum-config-manager --add-repos=http://download.fedoraproject.org/pub/epel/7/$basearch
</pre>

All this command will do is create the .repo file. So for this example, it created:

<pre>
$ cat /etc/yum.repos.d/download.fedoraproject.org_pub_epel_7_.repo

[download.fedoraproject.org_pub_epel_7_]
name=added from: http://download.fedoraproject.org/pub/epel/7/
baseurl=http://download.fedoraproject.org/pub/epel/7/
enabled=1
</pre>


You can also user yum-config-manager to enable/disable repos. Or you can disable/enable the repos by manually editing the .repo file. 










Also see:

[bash]
man yum.conf     #this describes the yum.conf file and the various option available.
[/bash]



<strong>Need to learn more about:</strong>

http://www.tecmint.com/20-practical-examples-of-rpm-commands-in-linux/

<a href="http://yum.baseurl.org/wiki/RepoCreate">http://yum.baseurl.org/wiki/RepoCreate</a>

<a href="http://www.cyberciti.biz/tips/redhat-centos-fedora-linux-setup-repo.html">http://www.cyberciti.biz/tips/redhat-centos-fedora-linux-setup-repo.html</a>

<a href="http://superuser.com/questions/451298/how-do-i-create-yum-repo-file">http://superuser.com/questions/451298/how-do-i-create-yum-repo-file</a>]]></Content>
		<Date><![CDATA[2015-05-03]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Using Yum]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="How can you get yum to prompt you what available sub commands are?"]
The double tab technique
[/toggle]
[toggle title="True or False, uninstalling a package using yum will also uninstall the dependencies?"]
True  - However if you have installed the package a long time ago, and since then you have installed other packages that have some of the same dependencies, then these dependencies will remain.  
[/toggle]
[toggle title="What is the command to remove the httpd package?"]
$ yum remove httpd 
[/toggle]
[toggle title="What is the command to find all available packages called 'nmap'?"]
$ yum search nmap

[/toggle]
[toggle title="What is the command to view info about the package 'nmap'?"]
$ yum info nmap

[/toggle]
[toggle title="What is the command to install the package 'nmap'?"]
$ yum install nmap

[/toggle]
[toggle title="What are the 3 commands to list all packages, installed packages, and available packages?"]
$ yum list all 
$ yum list installed 
$ yum list available
[/toggle]
[toggle title="What is the command to install all the latest packages including security updates?"]
$ yum update
[/toggle]
[toggle title="What is the command to install the 'tree' package from a repo recalled 'epel' repo rather than any other repo?"]
$ yum --enablerepo=epel install tree
[/toggle]
[toggle title="What is the command to list all package groups?"]
$ yum grouplist

[/toggle]
[toggle title="What is the command to install a the tree.rpm file which is in the current directory?"]
$ yum localinstall tree.rpm

[/toggle]
[/accordion]

<hr/>

To get an idea on what you can do with yum and how to use it, first check out it's help info:


<pre>
$ yum --help
</pre>


<h2>Install a package</h2>
The most common thing you will want to do is install a package. But first you need to see if that package is available for installation, we can check this by using yum's "search" subcommand, e.g.:

<pre>
$ yum search nmap
Loaded plugins: fastestmirror, langpacks
Loading mirror speeds from cached hostfile
 * base: centos.hyve.com
 * epel: mirror.euserv.net
 * extras: mirror.synergyworks.co.uk
 * updates: mirror.as29550.net
======================================================== N/S matched: nmap =========================================================
nmap-frontend.noarch : The GTK+ front end for nmap
nmap-ncat.x86_64 : Nmap's Netcat replacement
nmap.x86_64 : Network exploration tool and security scanner

  Name and summary matches only, use "search all" for everything.

</pre>


The above shows us that the package is available, but it doesn't say whether or not this package is already installed. To find this out out, you need to use the "info" subcommand:


<pre>
$ yum info nmap
Loaded plugins: fastestmirror, langpacks
Loading mirror speeds from cached hostfile
 * base: centos.hyve.com
 * epel: mirror.bytemark.co.uk
 * extras: mirror.synergyworks.co.uk
 * updates: mirror.as29550.net
Available Packages
Name        : nmap
Arch        : x86_64
Epoch       : 2
Version     : 6.40
Release     : 4.el7
Size        : 3.9 M
<strong>Repo        : base/7/x86_64</strong>
Summary     : Network exploration tool and security scanner
URL         : http://nmap.org/
Licence     : GPLv2 and LGPLv2+ and GPLv2+ and BSD
Description : Nmap is a utility for network exploration or security auditing.  It supports
            : ping scanning (determine which hosts are up), many port scanning techniques
            : (determine what services the hosts are offering), and TCP/IP fingerprinting
            : (remote host operating system identification). Nmap also offers flexible target
            : and port specification, decoy scanning, determination of TCP sequence
            : predictability characteristics, reverse-identd scanning, and more. In addition
            : to the classic command-line nmap executable, the Nmap suite includes a flexible
            : data transfer, redirection, and debugging tool (netcat utility ncat), a utility
            : for comparing scan results (ndiff), and a packet generation and response analysis
            : tool (nping).


</pre>


Here the Repo entry is the indicator of whether or not this package is installed. If it is installed , it will say "installed", otherwise it will give the source repo's name. 

Let's now go ahead and installe the package:


<pre>
$ yum install nmap
</pre>

Then confirm that it has been installed:

<pre>
$ yum info nmap | grep "^Repo"
Repo        : installed
</pre>


If you want to remove the package, then use the "remove" subcommand: 

<pre>yum remove nmap </pre>


The following Lists all the different groups of packages that are available. 


<pre>
$ yum grouplist 
Available environment groups:
   Minimal Install
   Compute Node
   Infrastructure Server
   File and Print Server
   MATE Desktop
Available Groups:
   CIFS file server
   Compatibility libraries
   Console internet tools
   Desktop
   Desktop Platform
   Desktop Platform Development
   Development Tools
   Eclipse
</pre>

“package groups” are a collection of packages that are used for a specific task/setup. E.g. if you want to set up a web server, then you should install the package group called “Web Server”


The following installs a group-package. I think that you only use double-quotes when the group package name includes spaces.

<pre>
$ yum groupinstall “{group-package-name}” 

$ yum groupremove “{group-package-name}”    # This un-installs package groups.</pre>
</pre>



<h2>listing all/installed/available packages</h2>


<pre>
$ yum list all | wc -l
16708
$ yum list installed | wc -l
1306
$ yum list available | wc -l
15345
</pre>
Note: the numbers don't add up, not sure why. 


Another way to list all installed packages, is using rpm like this:

<pre>
rpm -qa
</pre>

Note that that "yum list installed" shows slightly more lines compared to "rpm -qa", this could be because yum also lists package groups. 


Note: you could also do "yum list installed", but I think it gives slightly different info. 

Note: If the same package is available from 2 different repos, and you installed it from one repo, then the other package will still show up in the above list. Here's an example for a package called "libssh2":


<pre>[root@puppet01 ~]# yum list  | grep libssh2  # here 5 packages exists altogether
Trying other mirror.
libssh2.x86_64                         1.4.2-1.el6                    @anaconda-OracleLinuxServer-2013           11252058.x86_64/6.5
libssh2.i686                           1.4.2-1.el6                    oel_public
libssh2-devel.i686                     1.4.2-1.el6                    oel_public
libssh2-devel.x86_64                   1.4.2-1.el6                    oel_public
libssh2-docs.x86_64                    1.4.2-1.el6                    oel_public
[root@puppet01 ~]# yum list installed | grep libssh2  # ....of which one of them is installed. 
libssh2.x86_64                     1.4.2-1.el6                       @anaconda-OracleLinuxServer-201311252058.x86_64/6.5
[root@puppet01 ~]# yum list available | grep libssh2  # ....and 4 are available for install.
libssh2.i686                           1.4.2-1.el6                    oel_public
libssh2-devel.i686                     1.4.2-1.el6                    oel_public
libssh2-devel.x86_64                   1.4.2-1.el6                    oel_public
libssh2-docs.x86_64                    1.4.2-1.el6                    oel_public
[root@puppet01 ~]#</pre>






So far we have seen how to install from online repos. However if you have the actual rpm file on your machine, then you can install it locally using:



<pre>
yum localinstall packagename.arch.rpm 
</pre>
Note, this command will also install any dependencies that this package has. 

Another cool option is:

<pre>
yum whatprovides */{command name}
</pre>

This is useful if you know the command name, but not sure the name of the rpm package this command comes in. 



Sometime's you'll want to clean out all your yum repo cache, you can do this by running:

<pre>
$ yum clean all
</pre>


Here's how to install a package from a particular repo:

<pre>
$ yum --enablerepo=epel install tree
</pre>


See also:

http://www.tecmint.com/20-linux-yum-yellowdog-updater-modified-commands-for-package-mangement/]]></Content>
		<Date><![CDATA[2015-05-03]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Query packages before installing them]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What is the command to download a package called 'tree' without installing it?"]
$ yum install tree    # followed by the 'd' option when prompted.
[/toggle]
[toggle title="What is the command to query the list of files stored inside the file 'tree.rpm'?"]
$ rpm -qpl tree.rpm

[/toggle]
<hr/>


In the last lesson we saw that the rpm command can be used to query installed packages, or individuel rpm files.

However the rpm command doesn't know anything about yum repos, so you can't use rpm to look up info about a package that is available in a yum repo (but hasn't been installed yet). 


The second approach is the better approach. That's becuase you can use yum to just download a .rpm file from a repo, then using all the power of rpm (with the "p" option enabled) to query that file. 


To download an rpm, all you need to do is use yum's install command, and then select the (d)ownload option when prompted (instead of the yes or no options):



<pre>
$ find /var/cache/ -name tree*              # quick check that .rpm doesn't exist yet. 
$ yum install tree
Loaded plugins: fastestmirror, langpacks
Loading mirror speeds from cached hostfile
 * base: centos.hyve.com
 * epel: mirror.euserv.net
 * extras: mirror.synergyworks.co.uk
 * updates: mirror.as29550.net
Resolving Dependencies
--> Running transaction check
---> Package tree.x86_64 0:1.6.0-10.el7 will be installed
--> Finished Dependency Resolution

Dependencies Resolved

======================================================================================================
 Package             Arch         Version           Repository          Size
======================================================================================================
Installing:
 tree                x86_64       1.6.0-10.el7      base                46 k

Transaction Summary
======================================================================================================
Install  1 Package

Total download size: 46 k
Installed size: 87 k
<strong color='red'>Is this ok [y/d/N]: d</strong>
Background downloading packages, then exiting:
tree-1.6.0-10.el7.x86_64.rpm                                             |  46 kB  00:00:00
<strong>exiting because "Download Only" specified</strong>
$ find /var/cache/ -name tree*
/var/cache/yum/x86_64/7/base/packages/tree-1.6.0-10.el7.x86_64.rpm


</pre>


As you can see, when you select yum's download option, it downloads the file somwhere inside the <code>/var/cache</code> directory. 

Now we cna query the file using rpm (along with the "p" option)
<pre>
$ rpm -qpl /var/cache/yum/x86_64/7/base/packages/tree-1.6.0-10.el7.x86_64.rpm
/usr/bin/tree
/usr/share/doc/tree-1.6.0
/usr/share/doc/tree-1.6.0/LICENSE
/usr/share/doc/tree-1.6.0/README
/usr/share/man/man1/tree.1.gz
$ rpm -qpd /var/cache/yum/x86_64/7/base/packages/tree-1.6.0-10.el7.x86_64.rpm
/usr/share/doc/tree-1.6.0/LICENSE
/usr/share/doc/tree-1.6.0/README
/usr/share/man/man1/tree.1.gz
</pre>

]]></Content>
		<Date><![CDATA[2015-05-04]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Managing KVM based Virtual Machines]]></Title>
		<Content><![CDATA[<h2>KVM overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What is the command to install all the vm related packages?"]
$ yum install \
virt-manager \
libvirt \
libvirt-client \
libvirt-python \
python-virtinst
qemu-kvm \
qemu-img \
[/toggle]
[toggle title="What is the command to enable the kernel's virtualisation feature, and how do check for it?"]
$ modprobe kvm
$ lsmod | grep kvm
kvm                   461126  0
[/toggle]
[toggle title="What is the command to start+enable the virtualization related service?"]
$  systemctl start libvirtd
$  systemctl enable libvirtd
[/toggle]
[toggle title="What are the three main utilities that interacts with the above sevice?"]
- virt-manager   # open gui tool
- virsh          # cli tool for managing tools
- virt-install   # cli alternative to gui tool
[/toggle]
[toggle title="What is the command?"]


[/toggle]
[toggle title="What is the command?"]


[/toggle]
[toggle title="What is the command?"]


[/toggle]
[toggle title="What is the command?"]


[/toggle]
[toggle title="What is the command?"]


[/toggle]
[toggle title="What is the command?"]


[/toggle]
[/accordion]

<hr/>




<h2>Hardware requirements</h2>

kvm is what reserves cpu resources and use the reserved cpu resources to hosts guest vms on the host machine. KVM allows virtualisation to happen. 

If your RHEL machine is a vm, e.g. your machine is running inside a virtualbox, then you can't run KVM inside it. Hence the first requirement is that you have to have a physical machine that's running RHEL. 

Another requirement is that your machine cpu must support the following features: 

<pre>
$ egrep 'vmx|svm' /proc/cpuinfo
</pre>

Either vmx or svm flag needs to exist. 

Most most modern cpu's support this. If your's don't then it's likely that you're using a machine that has a rather old cpu. 

<h2>Required packages</h2>

You need to use yum to install the following packages:

<ol>
	<li>virt-manager - this gives us the gnome based user interface for managing VMs. You can access this by going to <code>Applications -> System tools -> Virtual Machine Manager</code>. This utility interacts with with the libvirtd daemon. This get's installed as a dependency when installing libvirt</li>
	<li>libvirt - this is a middleman that allows virt-manage/virsh to interact with qemu+kvm</li>
	<li>qemu-kvm</li>
	<li>qemu-img</li>
	<li>libvirt-python</li>
	<li>python-virtinst</li>
	<li>libvirt-client - this provides a commandline alternative (called virsh) to virt-manager.</li>

</ol>




<pre>
$ yum install virt-manager libvirt qemu-kvm qemu-img libvirt-python python-virtinst libvirt-client
</pre>




<h2>kernel requirements</h2>

You need to enable the following kernal module:


<pre>
$ lsmod | grep kvm   
$ modprobe kvm
$ lsmod | grep kvm
kvm                   461126  0
</pre>




<h2>The libvirtd service</h2>
The libvirtd is a service that manages virtualization in general, and is not just limited to kvm virtualization. 

To manage vms, this service needs to be running+enabled:

<pre>
$  systemctl status libvirtd -l
libvirtd.service - Virtualization daemon
   Loaded: loaded (/usr/lib/systemd/system/libvirtd.service; enabled)
   Active: active (running) since Mon 2015-05-04 14:06:49 BST; 18s ago
     Docs: man:libvirtd(8)
           http://libvirt.org
 Main PID: 2289 (libvirtd)
   CGroup: /system.slice/libvirtd.service
           └─2289 /usr/sbin/libvirtd

May 04 14:06:49 localhost.localdomain libvirtd[2289]: libvirt version: 1.2.8, package: 16.el7_1.2 (CentOS BuildSystem <http://bugs.centos.org>, 2015-03-26-23:17:42, worker1.bsys.centos.org)
May 04 14:06:49 localhost.localdomain libvirtd[2289]: Module /usr/lib64/libvirt/connection-driver/libvirt_driver_lxc.so not accessible
May 04 14:06:49 localhost.localdomain systemd[1]: Started Virtualization daemon.

</pre>

The libvirtd service can be managed by the following tools:


<ul>
	<li><strong>virt-manager</strong> - This is a GUI that allows you to easily manage vms</li>
	<li><strong>virsh</strong> - short for virtualization shell. It let's you manage vms using scripts. Really useful when managing multiple VMs.</li>
	<li><strong>virt-install</strong> -lets you create vms. It's a cli alternative to virt-manager</li>
</ul>]]></Content>
		<Date><![CDATA[2015-05-04]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - virsh and virt-manager]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What is the command to enter the vm managing's interactive shell?"]
$ virsh
virsh #
[/toggle]
[toggle title="What is the command to list all VMs?"]
$ virsh
virsh # list --all
[/toggle]
[toggle title="What is the command to list only running VMs?"]
$ virsh
virsh # list
[/toggle]
[toggle title="How can you make it easier to using this interactive shell?"]
tab+tab technique
[/toggle]
[toggle title="What is the command to start a vm called 'puppetmaster'?"]
$ virsh
virsh # start puppetmaster
[/toggle]
[toggle title="What is the command to shutdown the vm called 'puppetmaster'?"]
$ virsh
virsh # destroy puppetmaster
[/toggle]
[toggle title="Which directory houses each vm's xml config file?"]
/etc/libvirt/qemu
[/toggle]
[toggle title="What is the command to start the gui for managing VMs?"]
$ virt-manager
[/toggle]
[toggle title="What is the command to edit an existing vm called 'puppetmaster'?"]
virsh # edit puppetmaster

[/toggle]
[toggle title="What is the command?"]


[/toggle]
[/accordion]

<hr/>


virsh is it's own virtual interactive shell which is triggered by simply running the virsh command:


<pre>
$ virsh
Welcome to virsh, the virtualisation interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh # quit

$
</pre>

Tip: When inside a virsh session, use tab+tab technique to autocomplete as much as you can to make life easier.  

Here's how to list all your vm's (both running and switched off):


<pre>
virsh # list --all
 Id    Name                           State
----------------------------------------------------


</pre>


If you just want to view a list of vms that are currently running, then simply omit the "-all" flag and do:


<pre>
virsh # list
 Id    Name                           State
----------------------------------------------------


</pre>


To start a vm, do:


<pre>
virsh # start {machine-name}
</pre>


To stop a machine do:


<pre>
virsh # destroy {machine-name}
</pre>

This doesn't actually delete the vm, it only powers it down, and hence can be started up again later. 

The entire configuration of a vm is stored as an xml file, which is named after the vm's name, and these xml config files are all stored in the following directory:


<pre>
/etc/libvirt/qemu
</pre>

If you want to make changes to one of these xml files, the you should do it via the virsh command like this:


<pre>
virsh # edit {machine-name}
</pre>

That is so that libvirt doesn't edit the xml config file at the same time as you are editing it. 


<h2>virt-manager</h2>

virt-manager is a gui tool that let's you create/manage/destroy vms. It's similar to the way virtualbox is used. 

You can start virt-manager GUI like this:


<pre>
$ virt-manager
</pre>]]></Content>
		<Date><![CDATA[2015-05-04]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Scheduling jobs with cron]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What is the command to check the status of the cron's service?"]
$ systemctl status crond
[/toggle]
[toggle title="What are the 3 main ways to create cron jobs?"]
- Add an entry to the <code>/etc/crontab</code> file or drop files in <code>/etc/cron.d</code>.
- Using the <code>crontab -e</code> command.
- By dropping shell scripts into the <code>/etc/cron.hourly</code>, <code>/etc/cron.daily</code>, <code>/etc/cron.weekly</code>, or <code>/etc/cron.monthly</code> directories. This is via "anacron".
[/toggle]
[toggle title="What is the /etc/crontab entry to run the 'ls' command at 5.01am (min & hour), of 3rd (day of the month) of (every) April, of any day (i.e. doesn't mater whether it is mon-sun), every year.  It doesn’t matter if the day is Monday, Tuesday,… etc. This job is run under the username of 'david'. The output is also piped to /tmp/cron.log?"]
1 5 3 4 * david ls >> /tmp/cron.log  
[/toggle]
[toggle title="Same as the previous answer, but this time runs ls, at 5.01am, 6.01am, 7.01am…..10.01am. Hence runs the command several times, during a 5-hour window, on that particular day every year. Also the command is run by under the context of a user called 'jenny'?"]
1 5-10 3 4 * ls >> jenny /tmp/cron.log
[/toggle]
[toggle title="Same as the first /etc/crontab entry's answer, but this time runs 3 times within the hour, 5.00am, 5.17am, and 5.48am. Also run this as the root user?"]
0,17,48 5 3 4 * ls >> /tmp/cron.log 
[/toggle]
[toggle title="Same as the first /etc/crontab entry's answer, but runs 6 times within the hour, 5.00am, 5.10am, 5.20am, 5.30am, 5.40am, and 5.50am?"]
0,10,20,30,40,50 5 3 4 * david ls  >> /tmp/cron.log 
[/toggle]
[toggle title="Same as the previous example but written in shorthand form?"]
*/10 5 3 4 * david ls  >> /tmp/cron.log 
[/toggle]
[toggle title="Same as previous example, but this time the cron job is run every 3 days. This means that it will run on the following days?"]
*/10 5 */3 4 * david ls >> /tmp/cron.log
[/toggle]
[toggle title="Same as the first /etc/crontab, but this time run the job at 5 mins past midnight every Sunday?"]
5 0 * * Sunday david ls >> /tmp/cron.log  
[/toggle]
[toggle title="In which directory can you drop in custom crontab files so that they automatically get picked up by the crond daemon?"]
/etc/cron.d
[/toggle]
[toggle title="What can the custom cron files be called?"]
There is no naming convention, you can call them anything you want. 
[/toggle]
[toggle title="What is the command to create a cron job straight from the command line, in a vim-like editor (also the cron has to be run as the user 'david')?"]
$ su - david
$ crontab –e  
[/toggle]
[toggle title="The above comamnd creates a file storing the cron job. Where is this file located?"]
/var/spool/cron
[/toggle]
[toggle title="What will this file be called?"]
It will be named after the user who ran the crontab command. 
[/toggle]
[/accordion]

<hr/>




There will be times when you will want to schedule some task to automatically occur in the future. There are 2 solutions that are available that let's you do this "at", and "cron". The "at" utility is designed for scheduling an adhoc job, e.g. perform a patch installation at 9pm on Sunday. Whereas the "cron" utility is designed for scheduling a job that needs to be repeated on a regular basis, e.g. take backups at 5am every morning.


<h2>The crond service</h2>
If you want to run a task at regular intervals, then you need to use cron solution. The cron solution relies on the crond service in order for it to work:


<pre>
$ systemctl status crond
crond.service - Command Scheduler
   Loaded: loaded (/usr/lib/systemd/system/crond.service; enabled)
   Active: active (running) since Wed 2015-05-06 20:32:44 BST; 56min ago
 Main PID: 1343 (crond)
   CGroup: /system.slice/crond.service
           └─1343 /usr/sbin/crond -n

May 06 20:32:44 localhost.localdomain systemd[1]: Started Command Scheduler.
May 06 20:32:44 localhost.localdomain crond[1343]: (CRON) INFO (RANDOM_DELAY will be scaled with factor 31% if used.)
May 06 20:32:46 localhost.localdomain crond[1343]: (CRON) INFO (running with inotify support)


</pre>

The crond service is running by default and is actually used by other services too. 



There are 3 ways to create cron jobs:

<ol>
	<li>Add an entry to the <code>/etc/crontab</code> file.</li>
	<li>Using the <code>crontab -e</code> command. Note, in case you're wondering, then is no such command called "cron".</li>
	<li>By dropping shell scripts into the <code>/etc/cron.hourly</code>, <code>/etc/cron.daily</code>, <code>/etc/cron.weekly</code>, or <code>/etc/cron.monthly</code> directories. This is via <a href="http://codingbee.net/tutorials/rhcsa/rhcsa-scheduling-jobs-with-anacron/">anacron</a>.</li>
</ol>

Let's take a look at each approach in turn:

<h2>Adding a cron job via the /etc/crontab file</h2>

You can add your cron job by adding a line to the following:


<pre>
$ cat /etc/crontab
SHELL=/bin/bash
PATH=/sbin:/bin:/usr/sbin:/usr/bin
MAILTO=root

# For details see man 4 crontabs

# Example of job definition:
# .---------------- minute (0 - 59)
# |  .------------- hour (0 - 23)
# |  |  .---------- day of month (1 - 31)
# |  |  |  .------- month (1 - 12) OR jan,feb,mar,apr ...
# |  |  |  |  .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat
# |  |  |  |  |
# *  *  *  *  * user-name  command to be executed

</pre>



While you can add you cron jobs here, in practice this file is mainly used for defining various environment variables to assist with the running of the cron jobs, using the other ways. The first few lines of this file defines a number of env variables, which will help automated tasks to run properly, they are:

<ul>
	<li><strong>SHELL</strong> — shell environment used for running jobs (in the example, the Bash shell)</li>
	<li><strong>PATH</strong> — paths to executable programs</li>
	<li><strong>MAILTO</strong> — username of the user who receives the output of the cron jobs by email (reporting on jobs that passed/failed). If the MAILTO variable is not defined (MAILTO=), then email is not sent.</li>
</ul>

Note, to help you understand, you can use this online <a href="https://cronwtf.github.io/">cron job checker</a> which describes cron jobs in a more human readable format.

Here are some cron entry examples:

<strong>Example 1</strong>
<pre>
1 5 3 4 * david ls >> /tmp/cron.log  
</pre>            

This runs the ls command (and pipes the output to /tmp/cron.log) at 5.01am (min=1 & hour=5), of 3rd (day of the month) of April (month=4), of any day (i.e. doesn't mater whether it is mon-sun, as indicated by the "*"), every year (this is always assumed for cron jobs).. This job is run under the username of "david". 


<strong>Example 2</strong>


<pre>1 5-10 3 4 * ls >> jenny /tmp/cron.log</pre>               

This is the same as the 1st example, but runs ls, at 5.01am, 6.01am, 7.01am…..10.01am. Hence runs the command several times, during a 5-hour window, on that particular day every year. Also this time the job is run as the user "jenny".

Note: you can think of the asterisk to mean "every". Hence if the hour setting is represented by "*" then it is actually equivalent to "0-24". Similarly an asterisk for the minute setting represents "0-60"  

<strong>Example 3</strong>


<pre>
0,17,48 5 3 4 * ls >> /tmp/cron.log 
</pre> 


This is the same as the 1st example, but runs 3 times within the hour, 5.00am, 5.17am, and 5.48am. Also note that we have omitted the username, by default this job will be run the context of the root user. 



<strong>Example 4</strong>

<pre>
0,10,20,30,40,50 5 3 4 * david ls  >> /tmp/cron.log 
</pre> 

This is the same as the 1st example, but runs 6 times within the hour, 5.00am, 5.10am, 5.20am, 5.30am, 5.40am, and 5.50am.



<strong>Example 5</strong>

<pre>
*/10 5 3 4 * david ls >> /tmp/cron.log
</pre>

Shorthand way of writing the above command (basically any number divisible by 10, without leaving a remainder).



<strong>Example 6</strong>

<pre>
*/10 5 */3 4 * david ls >> /tmp/cron.log
</pre>

Same as previous example, but this time the cron job is run every 3 days. This means that it will run on the following days. 

1,4,7,10,13,16,19,22,25,28,31  (assuming there are 31 days in the given month)

In this particular scenario, the job won't always run every 3 days. That's because due to the way cron works internally, cron always start job runs from the first day of the month (when there is an asterisk in the day setting). So on a 31 day month, it will run on day 31, and then the very next day, since that is the first day of the new month again. However it will work as expected when going from a 30 day month to a 31 day month.   

You will get a similar situation with the minute setting too, if you choose an "every" number that is not divisible by 60. In the previous example we used "*/10", so it was ok, but if we did something like "*/11" (which is every 11 minutes), then we'll get this issue from rolling from one hour to the next. 

The same applies for the hour and month settings too. 


<strong>Example 7</strong>
<pre>
5 0 * * Sunday david ls >> /tmp/cron.log  
</pre>            

This runs the job at 5 mins past midnight every Sunday.

It also has "every day", but "Sunday" over-rides this.    



<h2>Adding cron jobs to /etc/cron.d</h2>

Defining cron jobs in /etc/crontab is no longer the recommended approach. That's because over time you could end up adding lots of cron jobs to a single file, which would make it unwieldy. Also if one of cron jobs has a syntax error then it could cause all the other cron jobs from running.  


The better apprach is to use /etc/crontab file as a template to create your own crontab file. You then place your custom crontab file in the <code>/etc/cron.d</code> so that it get's picked up by the crond daemon. 


<pre>
$ ls -l /etc/cron.d
total 16
-rw-r--r--. 1 root root 128 Jul 30  2014 0hourly
-rw-r--r--. 1 root root 108 Mar  6 02:12 raid-check
-rw-------. 1 root root 235 Mar  6 05:45 sysstat
-rw-r--r--. 1 root root 187 Jan 27  2014 unbound-anchor

$ cat /etc/cron.d/sysstat
# Run system activity accounting tool every 10 minutes
*/10 * * * * root /usr/lib64/sa/sa1 1 1
# 0 * * * * root /usr/lib64/sa/sa1 600 6 &
# Generate a daily summary of process accounting at 23:53
53 23 * * * root /usr/lib64/sa/sa2 -A
</pre>

The files you place in this directory can be called anything you want. 

This is possible way for an application (e.g. firefox) to check for updates. If there is new version of Firefox then Firefox would then prompt the user to do an upgrade to the latest version. To make this possible, the firefox custom cron file would get dropped into /etc/cron.d during the installation of firefox. 




<h2>Using the "crontab" command</h2>

The crontab command (short for cron tables) is the recommended way to add/edit/delete cron jobs, that are to be run under the uid of a particular user:

<pre>
$ crontab -h
crontab: invalid option -- 'h'
crontab: usage error: unrecognized option
Usage:
 crontab [options] file
 crontab [options]
 crontab -n [hostname]

Options:
 -u <user>  define user
 -e         edit user's crontab
 -l         list user's crontab
 -r         delete user's crontab
 -i         prompt before deleting
 -n <host>  set host in cluster to run users' crontabs
 -c         get host in cluster to run users' crontabs
 -s         selinux context
 -x <mask>  enable debugging

Default operation is replace, per 1003.2


</pre>


The crond service is the service that reads these tables and performs the scheduled tasks. You can create/edit/delete your cron jobs by running the following command:

<pre>
$ crontab –e  
</pre>

Running this command will result in a vim like editor opening up where you write/edit/delete your cron jobs. These cron jobs will run under the id of the user who ran the <code>crontab -e</code> command. Hence if you want to run a cron job as a certain user, then you need to first su into that user before running the above command.     

After saving your changes and exiting the vim-like editor, behind the scenes a new file (named after the user) gets generated/updated in the following directory:

<pre>
/var/spool/cron
</pre>  


When you create a new cron job, you are effectively creating/updating a file that stores cron jobs (for a given user). These "cron files" are stored in the following directory:


These cron files are named after the user that the cron job will run under. For example, cron jobs created while logged in as the root user are stored in the following file:

<pre>
$ ls -l /var/spool/cron/root
-rw-------. 1 root root 59 May  6 22:48 /var/spool/cron/root
[root@localhost ~]# cat /var/spool/cron/root
*  *  *  *  * echo "`date` - hello world" >> /tmp/cron.log
</pre>

Note: The files in the /var/spool/cron folder are not supposed to be created/edited directly, they are only supposed to be created/edited/deleted via the crontab command.  


Once you have created your cron jobs, you can view them by viewing the files in /var/spool/cron directly, or using the crontab's "l" option:


<pre>
$ crontab -l
*  *  *  *  * echo "`date` - hello world" >> /tmp/cron.log
*/10 5 3 4 * ls   >> /tmp/cron.log
</pre>


If you are the root user, and you want to edit another user’s cron-tables, then instead of switching users (via su command) you can use the crontab's "u" option:

<pre>
$ id
uid=0(root) gid=0(root) groups=0(root) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023
$ grep sher /etc/passwd
sher:x:1002:1002::/home/sher:/bin/bash
$ ls -l /var/spool/cron | grep sher
$ crontab -eu sher                        # opens vim mode to write in the cron job
no crontab for sher - using an empty one
crontab: installing new crontab
[root@localhost ~]# crontab -lu sher
*  *  *  *  * echo "`date` - hello world" >> /tmp/cron.log
[root@localhost ~]# ls -l /var/spool/cron | grep sher
-rw-------. 1 root root 59 May  9 21:39 sher
$ cat /var/spool/cron/sher
*  *  *  *  * echo "`date` - hello world" >> /tmp/cron.log

</pre>    



 




<strong>See also:</strong>

<a href="http://www.centos.org/docs/2/rhl-cg-en-7.2/autotasks.html">http://www.centos.org/docs/2/rhl-cg-en-7.2/autotasks.html</a>

<a href="https://access.redhat.com/knowledge/docs/en-US/Red_Hat_Enterprise_Linux/3/html/System_Administration_Guide/ch-autotasks.html">https://access.redhat.com/knowledge/docs/en-US/Red_Hat_Enterprise_Linux/3/html/System_Administration_Guide/ch-autotasks.html</a>

<a href="https://access.redhat.com/knowledge/docs/en-US/Red_Hat_Enterprise_Linux/6/html/Deployment_Guide/ch-Automating_System_Tasks.html">https://access.redhat.com/knowledge/docs/en-US/Red_Hat_Enterprise_Linux/6/html/Deployment_Guide/ch-Automating_System_Tasks.html</a>

<a href="http://www.thegeekstuff.com/2009/06/15-practical-crontab-examples/">http://www.thegeekstuff.com/2009/06/15-practical-crontab-examples/</a>

<a href="http://www.thegeekstuff.com/2011/07/cron-every-5-minutes/">http://www.thegeekstuff.com/2011/07/cron-every-5-minutes/</a>

<a href="http://www.thegeekstuff.com/2011/12/crontab-command/">http://www.thegeekstuff.com/2011/12/crontab-command/</a>

<a href="http://www.cyberciti.biz/faq/linux-when-does-cron-daily-weekly-monthly-run/">http://www.cyberciti.biz/faq/linux-when-does-cron-daily-weekly-monthly-run/</a>

<a href="http://www.howtogeek.com/101288/how-to-schedule-tasks-on-linux-an-introduction-to-crontab-files/">http://www.howtogeek.com/101288/how-to-schedule-tasks-on-linux-an-introduction-to-crontab-files/</a>

<a href="http://helpdeskgeek.com/linux-tips/crontab-howto-tutorial-syntax/">http://helpdeskgeek.com/linux-tips/crontab-howto-tutorial-syntax/</a>

<a href="http://www.unixgeeks.org/security/newbie/unix/cron-1.html">http://www.unixgeeks.org/security/newbie/unix/cron-1.html</a>

<a href="http://www.cyberciti.biz/faq/linux-show-what-cron-jobs-are-setup/">http://www.cyberciti.biz/faq/linux-show-what-cron-jobs-are-setup/</a>]]></Content>
		<Date><![CDATA[2015-05-04]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Scheduling jobs with the "at" utility]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What is the command to check the 'at' utility's service status?"]
$ systemctl status atd
[/toggle]
[toggle title="What is the command to schedule an 'at' task 5 minutes into the future?"]
$ at now+5min
[/toggle]
[toggle title="How do you exit the 'at' utility's interactive mode?"]
ctrl+d
[/toggle]
[toggle title="What is the command to run 'ls -l', and store output to '/tmp/at.log' at 5 mins from now?"]
$ at now+5min
at> ls -l > /tmp/at.log
at> <EOT>                  # ctrl+d
job 1 at Sun May 10 00:22:00 2015
[/toggle]
[toggle title="After the above 'at' task has been created, which directory is the corresponding encrypted file created in?"]
/var/spool/at/
[/toggle]
[toggle title="What is the command to view all the scheduled 'at' tasks that are in the queue?"]
$ atq
# or 
$ at -l
[/toggle]
[toggle title="What is the command to delete 'at' task number '4'?"]
$ atrm 4
[/toggle]
[toggle title="What is the command to schedule an 'at' job at 11:05am on 25th Jan ?"]
$ at 11:05am jan25

[/toggle]
[toggle title="What is the command to schedule the script, '/tmp/test-script.sh' to run 2 minutes from now?"]
$ at -f /tmp/test-script.sh now+2min 
[/toggle]
[/accordion]

<hr/>


If you want to run a one-off job at some point in the future, then you need to use "at" command. 


“at” is feature that is similar to cron. The difference is that “at” is used for automating a one-off scheduled tasks, whereas cron is used for repeating the same task periodically.

For example, if you have a new kernel update that you need to install, then you might want to run that in the early hours of Sunday morning. In which case, you would use “at”.




The "at" solution relies on the atd service for it to function:


<pre>
$ systemctl status atd
atd.service - Job spooling tools
   Loaded: loaded (/usr/lib/systemd/system/atd.service; enabled)
   Active: active (running) since Wed 2015-05-06 20:32:44 BST; 1h 3min ago
 Main PID: 1344 (atd)
   CGroup: /system.slice/atd.service
           └─1344 /usr/sbin/atd -f

May 06 20:32:44 localhost.localdomain systemd[1]: Started Job spooling tools.

</pre>

To create a new "at job", you use the "at" command:


<pre>
$  whatis at
at (1)               - queue, examine or delete jobs for later execution
</pre>


The “at” command actually starts it’s own interactive shell, here is an example:


<pre>
$ at now+5min
at> ls -l > /tmp/at.log
at> <EOT>
job 1 at Sun May 10 00:22:00 2015
</pre>

You have to do <code>ctrl+d</code> to exit the shell again, after you have finished writing your commands. 

The above "at" session created a file behind then scenes, for each "at" session. You can find them here:


<pre>
$ ls -l /var/spool/at
total 4
-rwx------. 1 root   root   2810 May 10 00:20 a00002016bf9fd
drwx------. 2 daemon daemon    6 Oct  7  2014 spool
</pre>

Each file has an encrypted like name. This file is a script and it will disappear as soon as the the scheduled task has occured. Here's what this file looks like:


<pre>
$ cat /var/spool/at/a00002016bf9fd
#!/bin/sh
# atrun uid=0 gid=0
# mail root 0
umask 22
XDG_SESSION_ID=425; export XDG_SESSION_ID
HOSTNAME=localhost.localdomain; export HOSTNAME
SELINUX_ROLE_REQUESTED=; export SELINUX_ROLE_REQUESTED
SHELL=/bin/bash; export SHELL
HISTSIZE=1000; export HISTSIZE
SSH_CLIENT=192.168.1.244\ 38439\ 22; export SSH_CLIENT
SELINUX_USE_CURRENT_RANGE=; export SELINUX_USE_CURRENT_RANGE
SSH_TTY=/dev/pts/0; export SSH_TTY
USER=root; export USER
LS_COLORS=rs=0:di=01\;34:ln=01\;36:mh=00:pi=40\;33:so=01\;35:do=01\;35:bd=40\;33\;01:cd=40\;33\;01:or=40\;31\;01:mi=01\;05\;37\;41:su=37\;41:sg=30\;43:ca=30\;41:tw=30\;42:ow=34\;42:st=37\;44:ex=01\;32:\*.tar=01\;31:\*.tgz=01\;31:\*.arc=01\;31:\*.arj=01\;31:\*.taz=01\;31:\*.lha=01\;31:\*.lz4=01\;31:\*.lzh=01\;31:\*.lzma=01\;31:\*.tlz=01\;31:\*.txz=01\;31:\*.tzo=01\;31:\*.t7z=01\;31:\*.zip=01\;31:\*.z=01\;31:\*.Z=01\;31:\*.dz=01\;31:\*.gz=01\;31:\*.lrz=01\;31:\*.lz=01\;31:\*.lzo=01\;31:\*.xz=01\;31:\*.bz2=01\;31:\*.bz=01\;31:\*.tbz=01\;31:\*.tbz2=01\;31:\*.tz=01\;31:\*.deb=01\;31:\*.rpm=01\;31:\*.jar=01\;31:\*.war=01\;31:\*.ear=01\;31:\*.sar=01\;31:\*.rar=01\;31:\*.alz=01\;31:\*.ace=01\;31:\*.zoo=01\;31:\*.cpio=01\;31:\*.7z=01\;31:\*.rz=01\;31:\*.cab=01\;31:\*.jpg=01\;35:\*.jpeg=01\;35:\*.gif=01\;35:\*.bmp=01\;35:\*.pbm=01\;35:\*.pgm=01\;35:\*.ppm=01\;35:\*.tga=01\;35:\*.xbm=01\;35:\*.xpm=01\;35:\*.tif=01\;35:\*.tiff=01\;35:\*.png=01\;35:\*.svg=01\;35:\*.svgz=01\;35:\*.mng=01\;35:\*.pcx=01\;35:\*.mov=01\;35:\*.mpg=01\;35:\*.mpeg=01\;35:\*.m2v=01\;35:\*.mkv=01\;35:\*.webm=01\;35:\*.ogm=01\;35:\*.mp4=01\;35:\*.m4v=01\;35:\*.mp4v=01\;35:\*.vob=01\;35:\*.qt=01\;35:\*.nuv=01\;35:\*.wmv=01\;35:\*.asf=01\;35:\*.rm=01\;35:\*.rmvb=01\;35:\*.flc=01\;35:\*.avi=01\;35:\*.fli=01\;35:\*.flv=01\;35:\*.gl=01\;35:\*.dl=01\;35:\*.xcf=01\;35:\*.xwd=01\;35:\*.yuv=01\;35:\*.cgm=01\;35:\*.emf=01\;35:\*.axv=01\;35:\*.anx=01\;35:\*.ogv=01\;35:\*.ogx=01\;35:\*.aac=01\;36:\*.au=01\;36:\*.flac=01\;36:\*.mid=01\;36:\*.midi=01\;36:\*.mka=01\;36:\*.mp3=01\;36:\*.mpc=01\;36:\*.ogg=01\;36:\*.ra=01\;36:\*.wav=01\;36:\*.axa=01\;36:\*.oga=01\;36:\*.spx=01\;36:\*.xspf=01\;36:; export LS_COLORS
MAIL=/var/spool/mail/root; export MAIL
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin; export PATH
PWD=/root; export PWD
LANG=en_GB.UTF-8; export LANG
SELINUX_LEVEL_REQUESTED=; export SELINUX_LEVEL_REQUESTED
HISTCONTROL=ignoredups; export HISTCONTROL
SHLVL=1; export SHLVL
HOME=/root; export HOME
LOGNAME=root; export LOGNAME
SSH_CONNECTION=192.168.1.244\ 38439\ 192.168.1.124\ 22; export SSH_CONNECTION
LESSOPEN=\|\|/usr/bin/lesspipe.sh\ %s; export LESSOPEN
XDG_RUNTIME_DIR=/run/user/0; export XDG_RUNTIME_DIR
cd /root || {
         echo 'Execution directory inaccessible' >&2
         exit 1
}
${SHELL:-/bin/sh} << 'marcinDELIMITER19cb82f9'
<strong>ls -l > /tmp/at.log                                    # The "at" job's main task. </strong>

marcinDELIMITER19cb82f9
</pre>

As you can see this is a shell script, where the "at" job's main task is close to the bottom. 

To view a list of all jobs in the (q)ueue, you use the atq command:

<pre>
$ atq
3       Sun May 10 00:51:00 2015 a root
4       Sun May 10 01:22:00 2015 a root
5       Sun May 10 01:23:00 2015 a root
</pre>

Or alternatively you can usingh the "at" command's -l option:

<pre>
$ at -l
3       Sun May 10 00:51:00 2015 a root
4       Sun May 10 01:22:00 2015 a root
5       Sun May 10 01:23:00 2015 a root
</pre>
 


The first column's display's the job's id. Which is useful if you want to kill a job, which you do using the atrm command:


<pre>
$ atrm 4
$ atq
3       Sun May 10 00:51:00 2015 a root
5       Sun May 10 01:23:00 2015 a root
</pre>




<h2>Picking a schedule time</h2>

Earlier we picked a time that was 5 minutes from now:


<pre>
$ at now+5min
....
</pre>


Note: the shortest unit you can do is minutes, hence you can’t do seconds.


However any of the following would have also worked:

<pre>
at now+5hour
at now+5day
at now+5week
at now+5month
at now+5year
</pre>


You can also run your command at a more specific time like this:



<pre>$ at 11:05am jan25     </pre>

Or if it's on the current day, you cand do:


<pre>$ at 11:05am  </pre>


Or you can even user more human friendly wordings, e.g.:


<pre>
$ at 14:15 tomorrow
</pre>

<h2>scheduling a shell script</h2>

You can also instruct “at” to run a shell script file, rather than entering the commands using the interactive terminal. This is done using the "f" option, here's is an example of this:


<pre>
$ at -f /home/root/test-script.sh now+2min  
</pre>]]></Content>
		<Date><![CDATA[2015-05-04]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Scheduling jobs with and "at"]]></Title>
		<Content><![CDATA[There will be times when you will want to schedule some task to automatically occur in the future. There are 2 solutions that are available that let's you do this "at", and "cron". The "at" utility is designed for scheduling an adhoc job, e.g. perform a patch installation at 9pm on Sunday. Whereas the "cron" utility is designed for scheduling a job that needs to be repeated on a regular basis, e.g. take backups at 5am every morning.

The next couple of lessons will cover both "cron" and "at". 



<h2>Using "at"</h2>
If you want to run a one-off job at some point in the future, then you need to use "at" command. 

"at" is comprised of the atd service:


<pre>
$ systemctl status atd
atd.service - Job spooling tools
   Loaded: loaded (/usr/lib/systemd/system/atd.service; enabled)
   Active: active (running) since Wed 2015-05-06 20:32:44 BST; 1h 3min ago
 Main PID: 1344 (atd)
   CGroup: /system.slice/atd.service
           └─1344 /usr/sbin/atd -f

May 06 20:32:44 localhost.localdomain systemd[1]: Started Job spooling tools.

</pre>

To create a new "at job", you use the "at" command:


<pre>
$  whatis at
at (1)               - queue, examine or delete jobs for later execution
</pre>





]]></Content>
		<Date><![CDATA[2015-05-04]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[gpg - pgp keys]]></Title>
		<Content><![CDATA[/etc/pki/rpm-gpg/rpm-gpg


https://www.google.co.uk/search?q=/etc/pki/rpm-gpg/rpm-gpg&ie=utf-8&oe=utf-8&gws_rd=cr&ei=QdlIVcq_JtfdaoGfgKgE  


]]></Content>
		<Date><![CDATA[2015-05-05]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[gpg|pgp]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Foreman - notes]]></Title>
		<Content><![CDATA[output a json formated content into easy to read:

cat /tmp/cher.test | python -m json.tool | less


see this for more info about pretty-print json:

http://stackoverflow.com/questions/352098/how-can-i-pretty-print-json 





the following still generates a foreman run report without doing any changes:

puppet agent --test --noop



to restart foreman we do:


service httpd restart    # be careful with this command. 





reporting related commands:



The following generates a report:

vim /usr/lib/ruby/site_ruby/1.8/puppet/reports/cher.rb

this script requires the following config file:

vim /etc/puppet/foreman-cher.yaml

You can instruct puppetmaster to start using this script in the /etc/puppet/puppet.conf file. Under the "master" stanza there is a reports setting:

https://docs.puppetlabs.com/references/latest/configuration.html#report

for this setting, 








The main foreman log is located at:


/var/log/foreman/production.log







During the foreman installation, a postgres db is installed 

This db is called foreman, and a "foreman" user is also created which has full rights to this db. 

to access this db as foreman user, do:

sudo -u foreman psql

then do the following to list all tables in foreman db:

\dt

Seeteh following for more info:

http://blog.jasonmeridth.com/posts/postgresql-command-line-cheat-sheet/









vim /etc/foreman-proxy/settings.yml

vim /etc/foreman-proxy/settings.d/dns.yml




ls /var/lib/tftpboot/pxelinux.cfg/



ls /var/lib/tftpboot/


less /tmp/is_foreman_proxy_running.log



crontab -l



service foreman-proxy status



You can store iso images (4.5gb in size) in the following directory:


$ cd /var/www/html/mirror


This can be 

]]></Content>
		<Date><![CDATA[2015-05-08]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Foreman]]></Tags>
		<Status><![CDATA[private]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Git - Create a pull request]]></Title>
		<Content><![CDATA[

do the following to add new branch to stash:

git clone profiles repo:            git clone ssh://github.com/in/projectname.git
git checkout new-base-profiles

git branch temporary-branch-name         # creates new branch
git checkout temporary-branch-name
# make changes to files.
git add
git commit
git push origin temporary-branch-name
then on stash -> create pull request


source = temporary-branch-name
destination = new-base-profile]]></Content>
		<Date><![CDATA[2015-05-08]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[git]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>git]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Rsyslogd and Journald]]></Title>
		<Content><![CDATA[In RHEL7 there are 3 main systems for capturing logs:

<ul>
	<li><strong>rsyslogd</strong> - Running processes/services usually send log messages to rsyslogd, which in turn usually writes them into log files in /var/log/... </li>
	<li><strong>journald</strong> - This system is relatively new and came as part of systemd. It logs all information that it receives from systemd. For example if systemd fails to startup a process, then this failure will get logged via journald. It actually logs all the same messages that rsyslogd logs.</li>

	<li>Write directly to a file. For example the apache software writes directly to log files, although it can be configured to pass log messages to rsyslogd instead. It's best to use rsyslog where possible for better consistency.</li>

</ul>

]]></Content>
		<Date><![CDATA[2015-05-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Using Journald]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What is the command to check the journald's service?"]
$ systemctl status systemd-journald
[/toggle]
[toggle title="Which 2 commands can be used to view journald log entries?"]
- journalctl
- systemctl
[/toggle]
[toggle title="What is the command to view just a the last few lines (without tailing it)?"]
$ journalctl -xn          # provide extra e(x)planation and last 10 li(n)es
[/toggle]
[toggle title="What is the command to just view all the 'info' labelled journald log entries?"]
$ journalctl -xp info     # filter for (p)riority 
[/toggle]
[toggle title="What is the command to tail journald logs?"]
$ journalctl -xf          # (f)ollow the log
[/toggle]
[toggle title="What is the command to just view journald logs relating to the sshd service?"]
$ journalctl -xu sshd     # filter for (u)nit
[/toggle]
[toggle title="What is the command to just tail in realtie, journald logs relating to the httpd service?"]
$ journalctl -fxu httpd.service   
[/toggle]
[toggle title="What is the command to just view journald logs relating to the httpd service, and only dislplay log entries that are labelled 'debug'?"]
$ journalctl -fxu sshd.service -p debug
[/toggle]

<hr/>







<h2>Intro</h2>
Journald comes as a part of systemd and runs in the form of a service:

<pre>
$ systemctl status systemd-journald
systemd-journald.service - Journal Service
   Loaded: loaded (/usr/lib/systemd/system/systemd-journald.service; static)
   Active: active (running) since Sat 2015-09-26 16:35:29 BST; 42min ago
     Docs: man:systemd-journald.service(8)
           man:journald.conf(5)
 Main PID: 468 (systemd-journal)
   Status: "Processing requests..."
   CGroup: /system.slice/systemd-journald.service
           └─468 /usr/lib/systemd/systemd-journald

Sep 26 16:35:29 puppetmaster.local systemd-journal[468]: Runtime journal is using 6.2M (max 49.6M,...).
Sep 26 16:35:29 puppetmaster.local systemd-journal[468]: Runtime journal is using 6.2M (max 49.6M,...).
Sep 26 16:35:29 puppetmaster.local systemd-journal[468]: Journal started
Sep 26 16:35:38 puppetmaster.local systemd-journal[468]: Runtime journal is using 6.2M (max 49.6M,...).
Hint: Some lines were ellipsized, use -l to show in full.

</pre>


It logs everything that it receives from systemd. journald stores all it's log entries into a binary file. Hence you can't view this file directly. Instead you need to use commands to to read the binary. systemctl can read this binary, as you can from the bottom half of the output.


However to get more detailed log entries from journald, you need to use the <code>journalctl</code> command. Running this command on it's own gives absolutely everything:


<pre>
$ journalctl
-- Logs begin at Sun 2015-05-10 09:40:20 BST, end at Sun 2015-05-10 11:09:18 BST. --
May 10 09:40:20 localhost.localdomain systemd-journal[90]: Runtime journal is using 6.2M (max 49.6M, leaving 74.5M of free 490.5M, c
May 10 09:40:20 localhost.localdomain systemd-journal[90]: Runtime journal is using 6.2M (max 49.6M, leaving 74.5M of free 490.5M, c
.
.
...etc

</pre>
 
Journald even includes all the log entries that are captured by rsyslog. that's because rsyslog  runs as a service, and therefore systemd is fully aware of everything that rsyslog logs and passes that onto journald. Therefore journald essentially aggregates all the logs in stores them in a single log. 

You can view the last few lines like this:


<pre>
$ journalctl -xn
</pre>

Here we have chosen also to view additional e(x)planation for some log entries. The 'n' means display the last few (n)umber of lines, which defaults to 10. 

Tip: It is best practice to always use the "x" flag when using journalctl. So for the rest of this arcticle we will always use the "x" option with all of our journalctl command. You can view log entries that are labelled as "info", using the priority flag:

<pre>
journalctl -xp info
</pre>

You can view all the available priority labels in the journalctl man pages. You can also follow (tail) journald logs like this:

<pre>
$ journalctl -fx
</pre>

Another thing you will want to do is view the logs entries for a given (u)nit, e.g. the sshd.service:


<pre>
$ journalctl -xu sshd
-- Logs begin at Sun 2015-05-10 09:40:20 BST, end at Sun 2015-05-10 11:19:01 BST. --
May 10 09:40:44 localhost.localdomain systemd[1]: Starting OpenSSH server daemon...
May 10 09:40:44 localhost.localdomain systemd[1]: Started OpenSSH server daemon.
May 10 09:40:44 localhost.localdomain sshd[1333]: Server listening on 0.0.0.0 port 22.
May 10 09:40:44 localhost.localdomain sshd[1333]: Server listening on :: port 22.
May 10 09:41:39 localhost.localdomain sshd[1777]: Accepted password for root from 192.168.1.244 port 7768 ssh2
</pre>

This only gives all log entries relating to the sshd.service. 


Therefore a really useful troubleshooting technique would be something like:

<pre>
$ journalctl -fxu httpd.service
</pre>

This will tail journald for any log entries relating to the httpd service, and provide any explanations where available (which are indicated by the "--" prefix). Here's a sample output of this:

<pre>
$ journalctl -fxu httpd.service
-- Logs begin at Sat 2015-09-26 16:35:25 BST. --
Sep 26 16:35:38 puppetmaster.local systemd[1]: Starting The Apache HTTP Server...
-- Subject: Unit httpd.service has begun with start-up
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
--
-- Unit httpd.service has begun starting up.
Sep 26 16:35:41 puppetmaster.local systemd[1]: Started The Apache HTTP Server.
-- Subject: Unit httpd.service has finished start-up
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
--
-- Unit httpd.service has finished starting up.
--
-- The start-up result is done.
Sep 26 16:35:52 puppetmaster.local puppet-master[2983]: Starting Puppet master version 3.8.2
Sep 26 16:36:19 puppetmaster.local puppet-master[3298]: Compiled catalog for puppetmaster.local in...ds
Sep 26 17:05:51 puppetmaster.local puppet-master[3298]: Compiled catalog for puppetmaster.local in...ds
Sep 26 17:35:50 puppetmaster.local puppet-master[3298]: Compiled catalog for puppetmaster.local in...ds
Sep 26 18:05:50 puppetmaster.local puppet-master[3298]: Compiled catalog for puppetmaster.local in...ds

</pre>


You can filter this further using the priority, the best one for troubleshooting purposes is "debug":



<pre>$ journalctl -fxu sshd.service -p debug</pre>]]></Content>
		<Date><![CDATA[2015-05-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Rsyslog logging]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What is the command, to check the logging service's status which is used by  httpd, yum, ssh,...etc?"]
$ systemctl status rsyslog
[/toggle]
[toggle title="What are the 8 log priority labels?"]
- debug
- info
- notice
- warn (or warning)
- err
- crit
- alert
- emerg
[/toggle]
[toggle title="What is the main config for rsyslog?"]
/etc/rsyslog.conf
[/toggle]
[toggle title="What is the command to view the main log file in realtime?"]
$ tail -F /var/log/messages
# or 
$ tailf /var/log/messages
[/toggle]
[toggle title="What is the command to add 'hello world' to to /var/log/messages?"]
$ logger "hello world"
[/toggle]

[/accordion]

<hr/>




<h2>Intro</h2>

Nearly all services (e.g. httpd, vsftpd, and yum) records messages in their various log files. However they don’t write these logs entries directly to the associated log files (i.e. they don't do “echo “xxx” >>  log-file.txt”). Instead, these services send their log entries to an intermediary service called rsyslog (short for “remote system logger”). rsyslog then writes the log entries to the appropriate log file.

rsyslog actually runs as a service:


<pre>
$ systemctl status rsyslog
rsyslog.service - System Logging Service
   Loaded: loaded (/usr/lib/systemd/system/rsyslog.service; enabled)
   Active: active (running) since Sun 2015-05-10 09:40:34 BST; 2h 6min ago
 Main PID: 638 (rsyslogd)
   CGroup: /system.slice/rsyslog.service
           └─638 /usr/sbin/rsyslogd -n

May 10 09:40:34 localhost.localdomain systemd[1]: Started System Logging Service.


</pre>

For rsyslog to work, each message is sent to rsyslog with a priority label. Here are the <a href="http://linux.die.net/man/5/rsyslog.conf">priority levels</a>:
<ol>
	<li>debug</li>
	<li>info</li>
	<li>notice</li>
	<li>warn (or warning)</li>
	<li>err</li>
	<li>crit</li>
	<li>alert</li>
	<li>emerg</li>
</ol>

This comes in useful when we come to look at the rsyslog's main config file, which is:


<pre>
$ cat /etc/rsyslog.conf
# rsyslog configuration file

# For more information see /usr/share/doc/rsyslog-*/rsyslog_conf.html
# If you experience problems, see http://www.rsyslog.com/doc/troubleshoot.html

#### MODULES ####

# The imjournal module bellow is now used as a message source instead of imuxsock.
$ModLoad imuxsock # provides support for local system logging (e.g. via logger command)
$ModLoad imjournal # provides access to the systemd journal
#$ModLoad imklog # reads kernel messages (the same are read from journald)
#$ModLoad immark  # provides --MARK-- message capability

# Provides UDP syslog reception
#$ModLoad imudp
#$UDPServerRun 514

# Provides TCP syslog reception
#$ModLoad imtcp
#$InputTCPServerRun 514


#### GLOBAL DIRECTIVES ####

# Where to place auxiliary files
$WorkDirectory /var/lib/rsyslog

# Use default timestamp format
$ActionFileDefaultTemplate RSYSLOG_TraditionalFileFormat

# File syncing capability is disabled by default. This feature is usually not required,
# not useful and an extreme performance hit
#$ActionFileEnableSync on

# Include all config files in /etc/rsyslog.d/
$IncludeConfig /etc/rsyslog.d/*.conf                 # this is important. 

# Turn off message reception via local log socket;
# local messages are retrieved through imjournal now.
$OmitLocalLogging on

# File to store the position in the journal
$IMJournalStateFile imjournal.state


#### RULES ####

# Log all kernel messages to the console.
# Logging much else clutters up the screen.
#kern.*                                                 /dev/console

# Log anything (except mail) of level info or higher.
# Don't log private authentication messages!
*.info;mail.none;authpriv.none;cron.none                /var/log/messages

# The authpriv file has restricted access.
authpriv.*                                              /var/log/secure

# Log all the mail messages in one place.
mail.*                                                  -/var/log/maillog


# Log cron stuff
cron.*                                                  /var/log/cron

# Everybody gets emergency messages
*.emerg                                                 :omusrmsg:*

# Save news errors of level crit and higher in a special file.
uucp,news.crit                                          /var/log/spooler

# Save boot messages also to boot.log
local7.*                                                /var/log/boot.log


# ### begin forwarding rule ###
# The statement between the begin ... end define a SINGLE forwarding
# rule. They belong together, do NOT split them. If you create multiple
# forwarding rules, duplicate the whole block!
# Remote Logging (we use TCP for reliable delivery)
#
# An on-disk queue is created for this action. If the remote host is
# down, messages are spooled to disk and sent when it is up again.
#$ActionQueueFileName fwdRule1 # unique name prefix for spool files
#$ActionQueueMaxDiskSpace 1g   # 1gb space limit (use as much as possible)
#$ActionQueueSaveOnShutdown on # save messages to disk on shutdown
#$ActionQueueType LinkedList   # run asynchronously
#$ActionResumeRetryCount -1    # infinite retries if host is down
# remote host is: name/ip:port, e.g. 192.168.0.1:514, port optional
#*.* @@remote-host:514
# ### end of the forwarding rule ###


</pre>


Let's take a look at parts of this file, first we have:


<pre>
.
.
# Include all config files in /etc/rsyslog.d/
$IncludeConfig /etc/rsyslog.d/*.conf             
.
.
</pre>

This rsyslog.d directory that can hold other rsyslog related config files:


<pre>
$ pwd
/etc/rsyslog.d
$ ls -l
total 8
-rw-r--r--. 1 root root 2564 Sep 19  2014 gluster.conf.example
-rw-r--r--. 1 root root   49 Mar 26 13:03 listen.conf

</pre>


Next we have the following really important section:



<pre>
.
.
# Log anything (except mail) of level info or higher.
# Don't log private authentication messages!
*.info;mail.none;authpriv.none;cron.none                /var/log/messages
.
.
</pre>

In plain English, the above line says: log ALL messages of priority "info" and above (*.info), but filter out all "mail" related messages (mail.none), and all "news" related  messages (news.none), and all authentication messages (authpriv.none), and all "cron" messages (cron.none). As you can see, nearly all messages gets sent to <code>/var/log/messages</code>. 

The messages that have been filtered out, are actually logged elsewhere, as indicated by the following extract:

<pre>
.
.
# The authpriv file has restricted access.
authpriv.*                                              /var/log/secure

# Log all the mail messages in one place.
mail.*                                                  -/var/log/maillog


# Log cron stuff
cron.*                                                  /var/log/cron
.
.
</pre>

The "-" in -/var/log/maillog means that the log entries can temporarily reside in a buffer if necessary.

Finally we have:


<pre>
# Everybody gets emergency messages
*.emerg                                                 :omusrmsg:*
</pre>

Here a special "omusrmsg" plugin is being used so that any emergency message is appended to ALL log files. 
 

One final thing to remember, if you do make any changes to the above config files, then you need to restart the rsyslog service for the changes to take into effect:


<pre>
 systemctl status rsyslog
rsyslog.service - System Logging Service
   Loaded: loaded (/usr/lib/systemd/system/rsyslog.service; enabled)
   Active: active (running) since Sun 2015-05-10 09:40:34 BST; 4h 43min ago
 Main PID: 638 (rsyslogd)
   CGroup: /system.slice/rsyslog.service
           └─638 /usr/sbin/rsyslogd -n

May 10 09:40:34 localhost.localdomain systemd[1]: Started System Logging Service.
</pre>



You can monitor log entries in realtime using the tail command (with the follow flag):


<pre>$ tail -F /var/log/messages</pre>

Or you could do:

<pre>$ tailf /var/log/messages</pre>

Another option you can do is:
<pre>
$ vim /var/log/messages
</pre>
The benefit of this approach is that vim will intelligently use syntax highlighting to highlight error messages. 


If you have multiple log files, then you can view them all in real time like this:



<pre>
$ tail -F /var/log/httpd/*.log
==> /var/log/httpd/default_error.log <==

==> /var/log/httpd/foreman_access.log <==
127.0.0.1 - - [19/Sep/2015:21:05:02 +0100] "HEAD / HTTP/1.1" 301 - "-" "-"
127.0.0.1 - - [19/Sep/2015:21:05:01 +0100] "HEAD / HTTP/1.1" 301 - "-" "-"
127.0.0.1 - - [25/Sep/2015:22:03:40 +0100] "HEAD / HTTP/1.1" 500 - "-" "-"
127.0.0.1 - - [25/Sep/2015:22:07:12 +0100] "HEAD / HTTP/1.1" 301 - "-" "-"
127.0.0.1 - - [26/Sep/2015:08:17:29 +0100] "HEAD / HTTP/1.1" 301 - "-" "-"
127.0.0.1 - - [26/Sep/2015:08:17:30 +0100] "HEAD / HTTP/1.1" 301 - "-" "-"
127.0.0.1 - - [26/Sep/2015:08:22:10 +0100] "HEAD / HTTP/1.1" 301 - "-" "-"
127.0.0.1 - - [26/Sep/2015:08:22:10 +0100] "HEAD / HTTP/1.1" 301 - "-" "-"
127.0.0.1 - - [26/Sep/2015:16:35:43 +0100] "HEAD / HTTP/1.1" 301 - "-" "-"
127.0.0.1 - - [26/Sep/2015:16:35:43 +0100] "HEAD / HTTP/1.1" 301 - "-" "-"

==> /var/log/httpd/foreman_error.log <==

==> /var/log/httpd/foreman-ssl_access_ssl.log <==
127.0.0.1 - - [26/Sep/2015:16:36:18 +0100] "GET /node/puppetmaster.local?format=yml HTTP/1.1" 200 321 "-" "Ruby"
127.0.0.1 - - [26/Sep/2015:16:36:20 +0100] "POST /api/reports HTTP/1.1" 201 511 "-" "Ruby"
127.0.0.1 - - [26/Sep/2015:17:05:49 +0100] "GET /node/puppetmaster.local?format=yml HTTP/1.1" 200 321 "-" "Ruby"
127.0.0.1 - - [26/Sep/2015:17:05:51 +0100] "POST /api/hosts/facts HTTP/1.1" 201 773 "-" "Ruby"
127.0.0.1 - - [26/Sep/2015:17:05:51 +0100] "GET /node/puppetmaster.local?format=yml HTTP/1.1" 200 321 "-" "Ruby"
127.0.0.1 - - [26/Sep/2015:17:05:52 +0100] "POST /api/reports HTTP/1.1" 201 518 "-" "Ruby"
127.0.0.1 - - [26/Sep/2015:17:35:48 +0100] "GET /node/puppetmaster.local?format=yml HTTP/1.1" 200 321 "-" "Ruby"
127.0.0.1 - - [26/Sep/2015:17:35:50 +0100] "POST /api/hosts/facts HTTP/1.1" 201 773 "-" "Ruby"
127.0.0.1 - - [26/Sep/2015:17:35:50 +0100] "GET /node/puppetmaster.local?format=yml HTTP/1.1" 200 321 "-" "Ruby"
127.0.0.1 - - [26/Sep/2015:17:35:51 +0100] "POST /api/reports HTTP/1.1" 201 510 "-" "Ruby"

==> /var/log/httpd/foreman-ssl_error_ssl.log <==

==> /var/log/httpd/puppet_access_ssl.log <==
127.0.0.1 - - [26/Sep/2015:17:05:48 +0100] "GET /production/node/puppetmaster.local?transaction_uuid=ce413f09-b8be-4585-91f1-2395f72fa880&fail_on_404=true HTTP/1.1" 200 5010 "-" "Ruby"
127.0.0.1 - - [26/Sep/2015:17:05:50 +0100] "GET /production/file_metadatas/pluginfacts?links=manage&recurse=true&ignore=.svn&ignore=CVS&ignore=.git&checksum_type=md5 HTTP/1.1" 200 303 "-" "Ruby"
127.0.0.1 - - [26/Sep/2015:17:05:50 +0100] "GET /production/file_metadatas/plugins?links=manage&recurse=true&ignore=.svn&ignore=CVS&ignore=.git&checksum_type=md5 HTTP/1.1" 200 303 "-" "Ruby"
127.0.0.1 - - [26/Sep/2015:17:05:51 +0100] "POST /production/catalog/puppetmaster.local HTTP/1.1" 200 579 "-" "Ruby"
</pre>

Note: tailf can't handle multiple log files. 


Finally you can add your own custom log entries to /var/log/messages using the logger command. For example to add an entry with the message "hello world":

<pre>
$ logger "hello world"
</pre>

Now let's confirm that this has worked:


<pre>
$ tail -3 /var/log/messages
Nov 11 20:10:01 puppetmaster systemd: Starting Session 17 of user root.
Nov 11 20:10:01 puppetmaster systemd: Started Session 17 of user root.
Nov 11 20:13:36 puppetmaster root: hello world
</pre>

]]></Content>
		<Date><![CDATA[2015-05-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - understanding Logrotate]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What is the main logrotate system's config file?"]
/etc/logrotate.conf
[/toggle]
[toggle title="Where can rpm packages drop in custom logrotate.conf config files?"]
/etc/logrotate.d
[/toggle]
[toggle title="True or false, custom logrotate files overides the default settings specified in the main logrotate file?"]
True
[/toggle]
[toggle title="Where can you find help info for logrotate?"]
$ man logrotate
[/toggle]
[toggle title="What is the command to check the logrotate's service status?"]
Trick question, logrotate doesn't actually have a service.
[/toggle]
[toggle title="Where is the logrotate's cron job located?"]
/etc/cron.daily/logrotate
[/toggle]
[/accordion]

<hr/>



Over time, log files such as /var/log/messages can get so big that they exceed linux's max file size limit, or they use up all the available disk space. To prevent the above scenarios from happening, we using a system called Logrotate. 


The way it works is that after a certain period of time, log rotate renames and archives the existing log file, and then create a brand new empty log file to take it's place. Log rotate also retains the x most recent logs, and deletes the rest. 

The advantage of this is that logrotate ensures that the disk space doesn't all get used up. However the compromise is that we lose the full historical log data over time, which is a problem in the very small chance if we want to view them. To overcome this, you can set up a remote "log server", which all rotated logs are sent to, whicl 


To configure logratate, we have the following:


<pre>
$ ls -l /etc | grep logrotate
-rw-r--r--.  1 root root      662 Jul 31  2013 logrotate.conf
drwxr-xr-x.  2 root root     4096 Apr  3 16:39 logrotate.d
</pre>

The logrotate.d directory is used by other rpm packages to drop in it's own custom logrotate config files into. Hence in the logrotate.d directory we have:

<pre>
$ ls -l /etc/logrotate.d
total 56
-rw-r--r--. 1 root root 178 Jun 18  2014 chrony
-rw-r--r--. 1 root root  71 Mar  5 22:19 cups
-rw-r--r--. 1 root root 541 Sep 19  2014 glusterfs
-rw-r--r--. 1 root root 172 Mar  6 03:08 iscsiuiolog
-rw-r--r--. 1 root root 165 Mar 26 23:20 libvirtd
-rw-r--r--. 1 root root 163 Mar 26 23:20 libvirtd.qemu
-rw-r--r--. 1 root root 106 Mar  6 02:18 numad
-rw-r--r--. 1 root root 136 Jun 10  2014 ppp
-rw-r--r--. 1 root root 408 Mar  6 05:05 psacct
-rw-r--r--. 1 root root 115 Mar 13 22:35 samba
-rw-r--r--. 1 root root 219 Mar 26 13:23 sssd
-rw-r--r--. 1 root root 210 Oct 13  2014 syslog
-rw-r--r--. 1 root root 100 Dec  4 13:18 wpa_supplicant
-rw-r--r--. 1 root root 100 Mar  9 20:39 yum
</pre>


Any configurations in the logrotate.d directory will override the default settings which are set in the main <code>logrotate.conf</code> settings. 

Now let's take a look at the logrotate.conf file:


<pre>
cat /etc/logrotate.conf
# see "man logrotate" for details
# rotate log files weekly
weekly

# keep 4 weeks worth of backlogs
rotate 4

# create new (empty) log files after rotating old ones
create

# use date as a suffix of the rotated file
dateext

# uncomment this if you want your log files compressed
#compress

# RPM packages drop log rotation information into this directory
include /etc/logrotate.d

# no packages own wtmp and btmp -- we'll rotate them here
/var/log/wtmp {
    monthly
    create 0664 root utmp
        minsize 1M
    rotate 1
}

/var/log/btmp {
    missingok
    monthly
    create 0600 root utmp
    rotate 1
}

# system-specific logs may be also be configured here.


</pre>

Now let's take a look at package specific logrotate config files:


<pre>
 cat /etc/logrotate.d/yum
/var/log/yum.log {
    missingok
    notifempty
    size 30k
    yearly
    create 0600 root root
}

</pre>

You can find what each of the above means by checking out the logrotate's man pages:

<pre>
$ man logrotate
</pre>

logrotate doesn't function as a service, instead it runs in the form of a daily cron job:

<pre>
$ pwd
/etc/cron.daily
$ ls -l | grep logrotate
-rwx------. 1 root root 180 Jul 31  2013 logrotate
</pre>

Note, becuase of this, you can't rotate logs less than a day, e.g. every hour, which wouldn't be good practice anyway. 


See also:

http://www.thegeekstuff.com/2010/07/logrotate-examples/


<a href="http://www.thegeekstuff.com/2012/01/rsyslog-remote-logging/">http://www.thegeekstuff.com/2012/01/rsyslog-remote-logging/</a> (this actually to do with setting up a log server, which you need know for rhce)]]></Content>
		<Date><![CDATA[2015-05-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Overview of Partitions and Logical Volumes]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What is the command to display all the sata based storage devices attached to your machine?"]
$ ls -l /dev | grep "sd[a-z]$"
[/toggle]
[toggle title="What is the command to display all the ide based storage devices attached to your machine?"]
$ ls -l /dev | grep "hd[a-z]$"
[/toggle]
[toggle title="What is the command to display all the virtual disk based storage devices attached to your machine?"]
$ ls -l /dev | grep "vd[a-z]$"
[/toggle]
[toggle title="Which directories have their own dedicated devices?"]
- /boot
- /
[/toggle]
[/accordion]

<hr/>



To make the most out of your machine's disk storages, it's important that you organize your machine's hdds. There are 2 ways to organize your disk space, they are "partitions", and "Logical Volumes". 

Partitioning is a bit like splitting your hdds in smaller pieces (aka partitions), so that one hdd acts as if it is several smaller hdds. Each partition can then be allocated to a particular part of the linux filesystem.

Partitions and Logical Volumes are similar, but the key difference is that once you create a partition of a certain disk size, you can't easily change it afterwards (without formatting the partition). Whereas with Logical Volumes (aka LVs) you can increase/decrease it's size based on your needs. This makes LVs much more versatile. However LVs have a drawback, which is that a machine can't access a LV's content during the machine's main boot process. Hence in theory if all your machine's hdd are organized into LVs and you're not using any partitions, then your machine would fail to even start up. Hence why partitions are important, since partitions are understood by a machine at a hardware level, whereas LVs are at a software level.   We will cover more about Logical Volumes and LVM later. 

In windows, when you add a new hdd, it appears as a new drive. Hence in a windows machine we have c drive, e drive, f drive,...etc. This means that in windows we have several root directories, one for each hdd. However in Linux, we  just have the one root directory "/" and all the hdds diskpace (whether it is in the form of whole hdds, partitions, or Logical Volumes) can be attached to various parts to this overall hierarchy to add more disk space for certain directories. more about this later. 

Every hardware component of a machine is referred to in Linux by a file, this includes each of the machine's hdd. These files all resides in the <code>/dev</code> directory, aka the system (dev)ices directory. All files that represents a hdd, have the file naming convention of "sda", "sdb", "sdc"....etc. Hence to view list of all hdd's we can do:


<pre>
$ ls -l /dev | grep "sd[a-z]$"
brw-rw----. 1 root disk      8,   0 May 17 11:58 sda
</pre>

Note: I think "sd" stands for "sata disk". There are others to, e.g. hda, which refers to IDE hdds, and vda which refers to virtual disks. 


In the above example, our machine only has one hdd. 


The files in /dev are not designed to be interacted directly. But we often view the content of this directory to understand our machine's makeup.


Similarly, partitions are also represented with the same file name convention, but with a digit suffix:
 

<pre>
$ ls -l /dev | grep "sd[a-z]"
brw-rw----. 1 root disk      8,   0 May 17 11:58 sda
brw-rw----. 1 root disk      8,   1 May 17 11:58 sda1
brw-rw----. 1 root disk      8,   2 May 17 11:58 sda2
</pre>

In this instance, the sda hdd has been organised into 2 partitions. 

If we only had “sda” and no numbers, then it means that the “a” hdd hasn’t been partitioned.

Note: IDE hdds are represented by hda, hdb,…..etc. and partitions being hda1, hda2….etc
Note: virtual disks are represented by vda, vdb,…..etc. and partitions being vda1, vda2….etc.


There are some directories, such as <code>/boot</code> that needs it's own dedicated partition in order for the machine to run effectively. Alternatively we can allocate a whole hdd to <code>/boot</code> instead of a partition, but if that hdd's size is 1TB, and <code>/boot</code> only needs 5GB of disk space at most, then all that extra diskspace would not get used and will go to waste. 


Here's a list of common directories that usually have their own dedicated partitions:


<ul>
	<li><code>/boot</code> - this is used heavily during machine boot.</li>
	<li><code>/</code> - While this is at the top level, it doesn't cover directories that have their own dedicated partition/hdd/logical-volume</li>
</ul>

 
  

  ]]></Content>
		<Date><![CDATA[2015-05-17]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Creating Partitions]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What command do you use to create MBR based partitions for /dev/sdb?"]
$ fdisk /dev/sdb
[/toggle]
[toggle title="What is the command to create GPT based partitions for the device /dev/sdb?"]
$ gdisk /dev/sdb
[/toggle]
[toggle title="What is the command to refresh your partition tables?"]
$ partprobe   # or you can reboot your machine
[/toggle]
[toggle title="How many types of partitions are there and what are they called?"]
3 types: 
- primary partitions
- extended partitions
- logical partitions.
[/toggle]
[toggle title="What is the command to list all storage devices in /dev?"]
$ ls -l /dev | grep "^b"
[/toggle]
[toggle title="The 3 steps to creating a partition and bring it into use?"]
1 - Create the partition (using fdisk or gdisk)
2 - Format the partition (using mkfs)
3 - Mount the partition (using the “mount” tool)
[/toggle]
[toggle title="Which file stores info about devices and partitions?"]
$ cat /proc/partitions  # note, no need to restart/partprobe the machine if this up-to-date.
[/toggle]
[toggle title="What is the command to display all your storage devices along with corresponding partitions in a visual tree structure?"]
$ lsblk
[/toggle]
[/accordion]

<hr/>


There is basically 3 steps to creating a partition and bring it into use:


<ol>
	<li><strong>Create the partition</strong> – this is done using “fdisk” tool</li>
	<li><strong>Format the partition</strong> – This is also known as installing a filesystem. It is done using the “mkfs” command</li>
	<li><strong>Mount the partition</strong> – done using the “mount” tool (and automate it by updating the fstab config file)</li>
</ol>

In this lesson, we will just cover the first step of the journey. We will cover steps 2 and 3 in later lessons. 



Since my Centos machine is running under virtualbox, I shutdown now my vm, and used virtualbox to add new hdd to my machine, then started up my machine again. After that, I can now see the following new device:

<pre>
$ ls -l /dev | grep "sd[a-z]$"
brw-rw----. 1 root disk      8,   0 May 17 15:42 sda
brw-rw----. 1 root disk      8,  16 May 17 15:42 sdb    # this is new
</pre>

Since this is a new hdd, it also hasn't been partitioned:


<pre>
$  ls -l /dev | grep "sd[a-z]"
brw-rw----. 1 root disk      8,   0 May 17 15:42 sda
brw-rw----. 1 root disk      8,   1 May 17 15:42 sda1
brw-rw----. 1 root disk      8,   2 May 17 15:42 sda2
brw-rw----. 1 root disk      8,  16 May 17 15:42 sdb
</pre>


Note: all items that starts with the the letter "b" (aka block devices), are storage devices:

<pre>
$ ls -l /dev/ | grep "^b"
brw-rw----. 1 root    disk    253,   0 Oct  2 21:52 dm-0
brw-rw----. 1 root    disk    253,   1 Oct  2 21:52 dm-1
brw-rw----. 1 root    disk      8,   0 Oct  2 21:52 sda
brw-rw----. 1 root    disk      8,   1 Oct  2 21:52 sda1
brw-rw----. 1 root    disk      8,   2 Oct  2 21:52 sda2
</pre> 


Another way to check what are the currently active storage devices and their respective partitions, is by viewing the following file:


<pre>
cat /proc/partitions
major minor  #blocks  name

  11        0      56748 sr0
   8       16    2155084 sdb
   8        0   20971520 sda
   8        1     512000 sda1
   8        2   20458496 sda2
 253        0    2097152 dm-0
 253        1   18358272 dm-1
</pre>


Now to create/manage/delete partition, we need to use an interactive cli based utility called "fdisk". We have to pass in the device name to this utilty so that it knows which device we are working with:


<pre>
$ fdisk /dev/sdb
Welcome to fdisk (util-linux 2.23.2).

Changes will remain in memory only, until you decide to write them.
Be careful before using the write command.

Device does not contain a recognized partition table
Building a new DOS disklabel with disk identifier 0x2f383010.

Command (m for help):

</pre>

Note, the warning message, this is expected since it is a new hdd that hasnt been partitioned yet.


If we select the "m" option, we get:


<pre>
Command (m for help): m
Command action
   a   toggle a bootable flag
   b   edit bsd disklabel
   c   toggle the dos compatibility flag
   d   delete a partition
   g   create a new empty GPT partition table
   G   create an IRIX (SGI) partition table
   l   list known partition types                      # useful
   m   print this menu
   n   add a new partition                             # useful
   o   create a new empty DOS partition table
   p   print the partition table                       # useful
   q   quit without saving changes
   s   create a new empty Sun disklabel
   t   change a partition's system id
   u   change display/entry units
   v   verify the partition table
   w   write table to disk and exit                    # useful
   x   extra functionality (experts only)

Command (m for help):

</pre>

Now, let's try to (p)rint the partition table for our new device:


<pre>
Command (m for help): p

Disk /dev/sdb: 2206 MB, 2206806016 bytes, 4310168 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk label type: dos
Disk identifier: 0x2f383010

   Device Boot      Start         End      Blocks   Id  System

Command (m for help):

</pre>
As you can see, the table itself is currently empty. That's because the data that the "p" option displays is actually stored in a special internal partition where the MBR resides. This internal partition is very small, i.e. less than 1kb in size, and it holds information about the partition setup for the rest of the hdd. But since this is a blank new device, it's doesn't even have this special internal partition either.

Notice that this device is just over 2gb in size. 


Now let's use option "n" start creating a new partition:


<pre>
Command (m for help): n
Partition type:
   p   primary (0 primary, 0 extended, 4 free)
   e   extended
Select (default p):
</pre>

At this point you'll discover that there are different types of partitions.
 

<h2>Primary, Extended, and Logical partition types</h2>

You can only create up to a maximum of 4 partitions on a given hdd. This limit is caused by the fact that the MBR's partition is only 1-2 kb in size and hence can't hold information for more than 4 partitions. As a way to overcome this restriction, the concept of Primary/Extended/Logical partitions was introduced.  

<ul>
	<li><strong>Primary partition</strong> - These are the convention partitions that would create by default. If you only plan to create 4 partitions or less, then you only need to create primary partition and you don't have to worry about extended and logical partitions.</li>
	<li><strong>Extended partition</strong> - If you want to create more than 4 partitions on a hdd, now or in the future, then you need to create one of the partition as an extended partition. An extended partition is not actually a partition, instead it is essentially an empty container inside which you can create (logical) partitions. You cannot store data inside an extended partition directly, instead you first have to create logical partition inside the extended partition. You can only create one extended partition per hdd</li>
	<li><strong>logical partition</strong> - These are the names of any partitions created inside an extended partition. OS uses logical partitions in the same was as a typical primary partition</li>
</ul>


Best practice tip: You should always create an extended partition in the off chance that you might need to have more than 4 partitions. 

In my case, I'll create my first primary partition  which is going to be 10MB in size.:


<pre>
Command (m for help): n
Partition type:
   p   primary (0 primary, 0 extended, 4 free)
   e   extended
Select (default p):
Using default response p
Partition number (1-4, default 1):
First sector (2048-4310167, default 2048):
Using default value 2048
Last sector, +sectors or +size{K,M,G} (2048-4310167, default 4310167): +10M
Partition 1 of type Linux and of size 10 MiB is set

</pre> 

Notice that that the first sector start at 2048 rather than 0. That's because the 0-2047 is reserved for setting up the MBR's special internal partition. 


Now if we view the partition table again we have:


<pre>
 p

Disk /dev/sdb: 2206 MB, 2206806016 bytes, 4310168 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk label type: dos
Disk identifier: 0x2f383010

   Device Boot      Start         End      Blocks   Id  System
/dev/sdb1            2048       22527       10240   83  Linux
</pre>

Note, this hasn't been set yet, we have to use the "w" option to apply the changes. At this stage it is just showing what would happen if we select the "w" option. 

Let's now repeat the process to create 3 10MB primary partitions in total, after which our partition table now looks like:


<pre>
Command (m for help): p

Disk /dev/sdb: 2206 MB, 2206806016 bytes, 4310168 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk label type: dos
Disk identifier: 0x2f383010

   Device Boot      Start         End      Blocks   Id  System
/dev/sdb1            2048       22527       10240   83  Linux
/dev/sdb2           22528       43007       10240   83  Linux
/dev/sdb3           43008       63487       10240   83  Linux

Command (m for help):

</pre> 

Let's go ahead and create an extended partition now:


<pre>
Command (m for help): n
Partition type:
   p   primary (3 primary, 0 extended, 1 free)
   e   extended
Select (default e):                             # notice that the default is now "e", which is nice. 
Using default response e
Selected partition 4
First sector (63488-4310167, default 63488):
Using default value 63488
Last sector, +sectors or +size{K,M,G} (63488-4310167, default 4310167): +1024M        # tip: leave this blank!
Partition 4 of type Extended and of size 1 GiB is set

Command (m for help):
</pre> 

Since our extended partition is a container for other partitions, I have set this too 1GB in size. 

Tip: best to leave the last sector blank, so that you fully use up your hdd's remaining available space. 

Now if we try to create yet another partition, then fdisk is smart enough to automatically default to creating a logical partition now:




<pre>
Command (m for help): n
All primary partitions are in use
Adding logical partition 5
First sector (65536-2160639, default 65536):
Using default value 65536
Last sector, +sectors or +size{K,M,G} (65536-2160639, default 2160639): +10M
Partition 5 of type Linux and of size 10 MiB is set

Command (m for help):

</pre>


let's do this a couple more times so that we have 3 logical partitions in total, then let's see the partition table again:


<pre>
Command (m for help): p

Disk /dev/sdb: 2206 MB, 2206806016 bytes, 4310168 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk label type: dos
Disk identifier: 0x2f383010

   Device Boot      Start         End      Blocks   Id  System
/dev/sdb1            2048       22527       10240   83  Linux
/dev/sdb2           22528       43007       10240   83  Linux
/dev/sdb3           43008       63487       10240   83  Linux
/dev/sdb4           63488     2160639     1048576    5  Extended
/dev/sdb5           65536       86015       10240   83  Linux
/dev/sdb6           88064      108543       10240   83  Linux
/dev/sdb7          110592      131071       10240   83  Linux

Command (m for help):
</pre>
 
As you can see, the Id/System for all our primary/logical partitions has defaulted to 83/Linux. 

This indicates the partition type. Also since primary and logical partitions are effectively is the same thing, it therefore belongs to the same type, even though they have different names. 


Use option "l" to view the full list of partition types:


<pre>
Command (m for help): l

 0  Empty           24  NEC DOS         81  Minix / old Lin bf  Solaris
 1  FAT12           27  Hidden NTFS Win <mark>82  Linux swap / So</mark> c1  DRDOS/sec (FAT-
 2  XENIX root      39  Plan 9          <mark>83  Linux</mark>           c4  DRDOS/sec (FAT-
 3  XENIX usr       3c  PartitionMagic  84  OS/2 hidden C:  c6  DRDOS/sec (FAT-
 4  FAT16 <32M      40  Venix 80286     85  Linux extended  c7  Syrinx
 <mark>5  Extended</mark>        41  PPC PReP Boot   86  NTFS volume set da  Non-FS data
 6  FAT16           42  SFS             87  NTFS volume set db  CP/M / CTOS / .
 7  HPFS/NTFS/exFAT 4d  QNX4.x          88  Linux plaintext de  Dell Utility
 8  AIX             4e  QNX4.x 2nd part <mark>8e  Linux LVM</mark>       df  BootIt
 9  AIX bootable    4f  QNX4.x 3rd part 93  Amoeba          e1  DOS access
 a  OS/2 Boot Manag 50  OnTrack DM      94  Amoeba BBT      e3  DOS R/O
 b  W95 FAT32       51  OnTrack DM6 Aux 9f  BSD/OS          e4  SpeedStor
 c  W95 FAT32 (LBA) 52  CP/M            a0  IBM Thinkpad hi eb  BeOS fs
 e  W95 FAT16 (LBA) 53  OnTrack DM6 Aux a5  FreeBSD         ee  GPT
 f  W95 Ext'd (LBA) 54  OnTrackDM6      a6  OpenBSD         ef  EFI (FAT-12/16/
10  OPUS            55  EZ-Drive        a7  NeXTSTEP        f0  Linux/PA-RISC b
11  Hidden FAT12    56  Golden Bow      a8  Darwin UFS      f1  SpeedStor
12  Compaq diagnost 5c  Priam Edisk     a9  NetBSD          f4  SpeedStor
14  Hidden FAT16 <3 61  SpeedStor       ab  Darwin boot     f2  DOS secondary
16  Hidden FAT16    63  GNU HURD or Sys af  HFS / HFS+      fb  VMware VMFS
17  Hidden HPFS/NTF 64  Novell Netware  b7  BSDI fs         fc  VMware VMKCORE
18  AST SmartSleep  65  Novell Netware  b8  BSDI swap       fd  Linux raid auto
1b  Hidden W95 FAT3 70  DiskSecure Mult bb  Boot Wizard hid fe  LANstep
1c  Hidden W95 FAT3 75  PC/IX           be  Solaris boot    ff  BBT
1e  Hidden W95 FAT1 80  Old Minix

Command (m for help):


</pre>

Most of these types are not importannt, but the ones that are have been highlighted above. Here is a summary of each of these:


<ul>
	<li><strong>5/extended</strong> - this is an extended partition.</li>
	<li><strong>83/Linux</strong> - this is the default and indicates either a primary or logical partition</li>
	<li><strong>8e/LVM</strong> - used for creating physical volumes</li>
	<li><strong>Swap</strong> - Used for creating pseudo ram, covered later</li>
</ul>

Let's now go ahead, and apply our changes by saving this new partition table (using option "w") to the special internal mbr's partition. 

<pre>
Command (m for help): w
The partition table has been altered!

Calling ioctl() to re-read partition table.
Syncing disks.
[root@localhost dev]#
</pre>

When using option "w" you might get the following error message:


<pre>
Command (m for help): w
The partition table has been altered!

Calling ioctl() to re-read partition table.

WARNING: Re-reading the partition table failed with error 16: Device or resource busy.
The kernel still uses the old table. The new table will be used at
the next reboot or after you run partprobe(8) or kpartx(8)
Syncing disks.
</pre>

<strong>IMPORTANT: When this happens, you may need to then reboot the machine for the changes to actually take affect. </strong>

Instead of rebooting the machine, you may be able to get away with just running the partprobe or partx commands, although doesnt always work. 



<pre>
$ whatis partx
partx (8)            - tell the Linux kernel about the presence and numbering of on-disk partitions
$ whatis partprobe
partprobe (8)        - inform the OS of partition table changes
</pre>


Once you have rebooted your machine, you can then confirm the new partitions have been set up by checking the partitions tables:


<pre>
$ cat /proc/partitions
major minor  #blocks  name

  11        0      56748 sr0
   8       16    2155084 sdb
   8       17      10240 sdb1    # primary partition
   8       18      10240 sdb2    # primary partition
   8       19      10240 sdb3    # primary partition
   8       20          1 sdb4    # extended partition
   8       21      10240 sdb5    # logical partition
   8       22      10240 sdb6    # logical partition
   8       23      10240 sdb7    # logical partition
   8        0   20971520 sda
   8        1     512000 sda1
   8        2   20458496 sda2
 253        0    2097152 dm-0
 253        1   18358272 dm-1
</pre>

Note: you don't need to restart your machine, if this file already shows all the new partitions. 

and also the existance of the new files in /dev:


<pre>
$ ls -l /dev | grep "sdb"
brw-rw----. 1 root disk      8,  16 May 17 19:22 sdb
brw-rw----. 1 root disk      8,  17 May 17 19:22 sdb1
brw-rw----. 1 root disk      8,  18 May 17 19:22 sdb2
brw-rw----. 1 root disk      8,  19 May 17 19:22 sdb3
brw-rw----. 1 root disk      8,  20 May 17 19:22 sdb4
brw-rw----. 1 root disk      8,  21 May 17 19:22 sdb5
brw-rw----. 1 root disk      8,  22 May 17 19:22 sdb6
brw-rw----. 1 root disk      8,  23 May 17 19:22 sdb7
</pre>

Another really useful command, is "lsblk", which shows all your hdds and partitions in a tree structure:

<pre>
$ lsblk
NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda               8:0    0   20G  0 disk
├─sda1            8:1    0  500M  0 part /boot
└─sda2            8:2    0 19.5G  0 part
  ├─centos-swap 253:0    0    2G  0 lvm  [SWAP]   # this is a logical volume
  └─centos-root 253:1    0 17.5G  0 lvm  /        # this is a logical volume
sdb               8:16   0  2.1G  0 disk
├─sdb1            8:17   0   10M  0 part
├─sdb2            8:18   0   10M  0 part
├─sdb3            8:19   0   10M  0 part
├─sdb4            8:20   0    1K  0 part   
├─sdb5            8:21   0   10M  0 part
├─sdb6            8:22   0   10M  0 part
└─sdb7            8:23   0 1001M  0 part
sr0              11:0    1 55.4M  0 rom
</pre>

The lsblk shows a lot more info than just a list hdds/partitions, e.g. it shows which partition is being used as part of lvm, which partitions/HDD are currently mounted,...and etc. We will revisit this command again as we cover all these other topics.  


Note: if you want to create a GPT based partition, then you need to use the "gdisk" command. The gdisk command works in a very similar way to the fdisk command. ]]></Content>
		<Date><![CDATA[2015-05-17]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Installing a Filesystem onto a partition (aka formatting)]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="Where can you find info about what mount options are available for each time of filesystem?"]
$ man mount

[/toggle]
[toggle title="What is the command to view info about what filesystem types are supported in your machine?"]
$ man fs
[/toggle]
[toggle title="What is the command to view help info about the 'xfs' filesystem?"]
$ man xfs
[/toggle]
[toggle title="How do you list all the available formatting commands?"]
$ mkfs  # then type tab+tab
[/toggle]
[toggle title="What is the command to install the xfs filesystem onto '/dev/sdb7', and then label this filesystem with the name 'DummyStorage'?"]
$ mkfs.xfs -L "DummyStorage" /dev/sdb7
[/toggle]
[toggle title="What is the command to view all your device's/partition's filesystem type, UUID, as well as corresponding label (if any)?"]
$ blkid
[/toggle]
[toggle title="What is the above command actually displaying?"]
It displays filesystems rather than devices or partitions. If a device/partition does not have filesystem on it, then it doesn't appear in the list. 
[/toggle]
[toggle title="What is the command assign the label 'storage' to an existing xfs filesystem that is installed on /dev/sdb7?"]
$ xfs_admin -L "storage" /dev/sdb7
[/toggle]
[toggle title="What is the command assign the label 'webdata' to an existing ext4 filesystem that is installed on /dev/sdb1?"]
$ e2label /dev/sdb1 webdata
[/toggle]
[toggle title="What is the command to delete the filesystem that is installed on /dev/sdb1?"]
$ cat /dev/zero > /dev/sdb1
[/toggle]
[/accordion]
<hr/>




To recap, There are 3 steps to creating a partition and bring it into use:


<ol>
	<li><strong>Create the partition</strong> – this is done using “fdisk” tool</li>
	<li><strong>Format the partition</strong> – This is also known as installing a filesystem. It is done using the “mkfs” command</li>
	<li><strong>Mount the partition</strong> – done using the “mount” tool (and automate it by updating the fstab config file)</li>
</ol>


In the last lesson we covered step 1, now we'll cover step 2. 


<h2>File system types</h2>
There are several main filesystem types that are available:


<ul>
	<li><strong>XFS</strong> - This is now the default filesystem in RHEL 7.</li>
	<li><strong>Ext4</strong> - This used to be the default file system up to and including RHEL 6</li>
	<li><strong><a href="http://en.wikipedia.org/wiki/Btrfs">Btrfs</a></strong> - This is going to become the successor to XFS. That's because this is a "Copy-on-Write" (aka CoW) file system, i.e. this filesystem has a fail-safe mechanism, where the filesystem takes a copy of a file and save it elsewhere just before any changes are applied to the original file. Hence with btrfs you can always rollback to previous versions of a particular file. In this respect Btrfs works a bit like a version-control-system, like svn, git, etc.</li>
	<li><strong>vfat</strong> - This is typically used for removable media, e.g. USB pens</li>
	<li><strong>GFS2</strong> - This is typically used if you have multiple nodes that want to write to the same filesystem at the same time</li>
	<li><strong>Gluster</strong> - This is a distributed filesystem, that means that to the end user, this filesystem looks like it resides on on server, but behind the scene it is spread across multiple servers. This filesystem is useful for setting up cloud environments.</li>
</ul>

Note: some features are not available on all fs types, e.g. <a href="http://codingbee.net/tutorials/rhcsa/rhcsa-access-control-list-acl/">ACL</a> is not available on xfs. The mount commnd's man page tells you what options are available for each type. 

To view a complete list of supported filesystems, check out:


<pre>
$ man fs
</pre>

Each file system type has their own man page as well, e.g. to learn more about xfs, you do:


<pre>$ man xfs</pre>




In the last lesson we covered how to created partitions, but didn't format (aka install filesystem on) them. This is a bit like a library that doesn't have any bookshelves, and hence unable to store any books. Installing a filesystem is a bit like filling the library up with book shelves. 


<h2>Install a file system</h2>
To install a filesystem on a partition (or an unpartitioned hdd) you need to use one of the following commands:


<pre>
$ mkfs   # I pressed the tab key twice, write after typing "mkfs"
mkfs         mkfs.cramfs  mkfs.ext3    mkfs.fat     mkfs.msdos   mkfs.xfs
mkfs.btrfs   mkfs.ext2    mkfs.ext4    mkfs.minix   mkfs.vfat
</pre> 


Since xfs is recommended filesystem, we will therefore use mkfs.xfs:


<pre>
$ whatis mkfs.xfs
mkfs.xfs (8)         - construct an XFS filesystem
</pre>


Let's now install the xfs filesystem onto one of our partitions/hdds:

<pre>
$ mkfs.xfs -L "DummyStorage" /dev/sdb7
meta-data=/dev/sdb7              isize=256    agcount=4, agsize=64064 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=0        finobt=0
data     =                       bsize=4096   blocks=256256, imaxpct=25
         =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0 ftype=0
log      =internal log           bsize=4096   blocks=853, version=2
         =                       sectsz=512   sunit=0 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0

</pre>


As you can see, you need to pass in the file representation of the partition as the main parameter. This is the same with all the other mkfs utilities. 

Notice that I used the "L" switch. This is optional, but is recommended because it lets you assign a human friendly name to your partition/hdd. This is important because the current name, (which in this case is /dev/sdb7) was a name that was auto-assigned to do the device, and if you then plug in a usb-pen, then reboot your machine, then there is a chance that the usb-pen will change which devices are auto-assigned which name. Hence labels solve this problem.  

Having a label or knowing the UUID (which we will cover next) is important, since we need this info as part of the 3rd and final step, which is mounting the partition. 


As soon as a hdd/partition is formatted, a unique ID (UUID) is attached to that hdd/partition. You can use the blkid command to view a list of all formatted partitions/hdd along with their UUID value:

<pre>
$ blkid
/dev/sda1: UUID="547c78ce-34b5-4e31-84d2-bf72b196fa57" TYPE="xfs"
/dev/sda2: UUID="JnfbeO-XPAG-AS0O-Mrtw-hnwn-7hqf-Mhk46t" TYPE="LVM2_member"
/dev/sdb1: LABEL="DummyExt4Storage" UUID="eeb45452-3e56-4455-a7c0-0e2e41d8a3cf" TYPE="ext4"
/dev/sdb7: LABEL="DummyStorage" UUID="f252bb11-abaa-4744-8001-e82b0e066b17" TYPE="xfs"
/dev/mapper/centos-swap: UUID="94263d84-5357-4566-9987-fc74ca946a96" TYPE="swap"
/dev/mapper/centos-root: UUID="17f09dae-88d1-4813-9738-8996819cf6ba" TYPE="xfs"
</pre>

Notice that our labels are displayed here too.

There are number of partitions that are listed in /proc/partition (or lsblk), but are not listed by blkid, e.g. /dev/sdb3. That's because, these missing hdd/partitions have not been formatted yet, and consequently they don't have a UUID value yet. The blkid command's primary purpose is to show the hdd/partition UUID value for each device. Another cool thing about the blkid is that it also tells you what file system type has been installed on each partition/hdd.


So what is the difference between lsblk and blkid? blkid command essentially gives information about all the filesystems that exists on your machine along with the device that's housing it, whereas lsblk lists all the devices/partitions/LVs, but doesn't indicate which devices/parttitions/LVs has a filesytem installed on it. 


If one of your xfs partitions doesn't have a label, then you simply add using the <strong>xfs_admin</strong> command:


<pre>
$ blkid | grep sdb7
/dev/sdb7: UUID="b8612f42-96fe-4671-89bb-9ee51691a311" TYPE="xfs"
$ xfs_admin -L "XfsPartition" /dev/sdb7
writing all SBs
new label = "XfsPartition"
$ blkid | grep sdb7
/dev/sdb7: LABEL="XfsPartition" UUID="b8612f42-96fe-4671-89bb-9ee51691a311" TYPE="xfs"
</pre> 


You can also assign a label to an existing ext* filesystem. For example if /dev/sdb1 is an ext4 based filesystem. Then you label it with the e2label command:

<pre>
$ e2label /dev/sdb1 webdata
</pre>

Here we have set the label for /dev/sdb1 to webdata. 


<h2>Using the "file" command to analyse block devices</h2>

If you have a hdd/partition and want to find out which file system type is installed on it, then the best way to do this is by using the "file" command with the (s)pecial file option enabled so that it does additional analysis of it's target. I also enabled the symbolic (L)ink option, so to auto-resolve any sybmolic links. Here are a few examples:

<pre>
$ file -sL /dev/sda1
/dev/sda1: SGI XFS filesystem data (blksz 4096, inosz 256, v2 dirs)
</pre>

Next we have:

<pre>
$ file -sL /dev/sdb2
/dev/sdb2: data
</pre>

Here it means that sdb2 doesn't have a filesystem installed yet, i.e. it hasn't been formatted yet.


Here's an example of a partition with an ext4 filesystem installed on it:
<pre>
$ file -sL /dev/sdb1
/dev/sdb1: Linux rev 1.0 ext4 filesystem data, UUID=eeb45452-3e56-4455-a7c0-0e2e41d8a3cf, volume name "DummyExt4Storage" (extents) (64bit) (huge files)
</pre>



Next we have an example of an LVM's physical volume:

<pre>
$ file -sL /dev/sda2
/dev/sda2: LVM2 PV (Linux Logical Volume Manager), UUID: JnfbeO-XPAG-AS0O-Mrtw-hnwn-7hqf-Mhk46t, size: 20949499904
</pre>

We'll cover LVM later.
 

<h2>Delete a file system</h2>

There could be times when you will want to delete a filesytem from a partition. You might think that deleting a file system could be done by deleting entire partition (and recreate it again) using fdisk. However that doesn't work since deleting a partition doesn't delete any data on the disk, it just deletes the artificial partition boundaries. You can see my attempt at doing this here, for the /dev/sdb1 partition:

<pre>
$ blkid
/dev/sr0: UUID="2015-03-02-14-24-20-00" LABEL="VBOXADDITIONS_4.3.24_98716" TYPE="iso9660"
/dev/sda1: UUID="547c78ce-34b5-4e31-84d2-bf72b196fa57" TYPE="xfs"
/dev/sda2: UUID="JnfbeO-XPAG-AS0O-Mrtw-hnwn-7hqf-Mhk46t" TYPE="LVM2_member"
/dev/mapper/centos-swap: UUID="94263d84-5357-4566-9987-fc74ca946a96" TYPE="swap"
/dev/mapper/centos-root: UUID="17f09dae-88d1-4813-9738-8996819cf6ba" TYPE="xfs"
/dev/sdb1: UUID="9fa9e491-8e21-4322-a1c2-dc269825e824" TYPE="xfs"


$ fdisk /dev/sdb
Welcome to fdisk (util-linux 2.23.2).

Changes will remain in memory only, until you decide to write them.
Be careful before using the write command.


Command (m for help): d
Selected partition 1
Partition 1 is deleted

Command (m for help): w
The partition table has been altered!

Calling ioctl() to re-read partition table.
Syncing disks.


$ blkid
/dev/sr0: UUID="2015-03-02-14-24-20-00" LABEL="VBOXADDITIONS_4.3.24_98716" TYPE="iso9660"
/dev/sda1: UUID="547c78ce-34b5-4e31-84d2-bf72b196fa57" TYPE="xfs"
/dev/sda2: UUID="JnfbeO-XPAG-AS0O-Mrtw-hnwn-7hqf-Mhk46t" TYPE="LVM2_member"
/dev/mapper/centos-swap: UUID="94263d84-5357-4566-9987-fc74ca946a96" TYPE="swap"
/dev/mapper/centos-root: UUID="17f09dae-88d1-4813-9738-8996819cf6ba" TYPE="xfs"
/dev/sdb: PTTYPE="dos"


$ fdisk /dev/sdb
Welcome to fdisk (util-linux 2.23.2).

Changes will remain in memory only, until you decide to write them.
Be careful before using the write command.


Command (m for help): n
Partition type:
   p   primary (0 primary, 0 extended, 4 free)
   e   extended
Select (default p): p
Partition number (1-4, default 1):
First sector (2048-4310167, default 2048):
Using default value 2048
Last sector, +sectors or +size{K,M,G} (2048-4310167, default 4310167): +200M
Partition 1 of type Linux and of size 200 MiB is set

Command (m for help): w
The partition table has been altered!

Calling ioctl() to re-read partition table.
Syncing disks.
$ blkid
/dev/sr0: UUID="2015-03-02-14-24-20-00" LABEL="VBOXADDITIONS_4.3.24_98716" TYPE="iso9660"
/dev/sda1: UUID="547c78ce-34b5-4e31-84d2-bf72b196fa57" TYPE="xfs"
/dev/sda2: UUID="JnfbeO-XPAG-AS0O-Mrtw-hnwn-7hqf-Mhk46t" TYPE="LVM2_member"
/dev/mapper/centos-swap: UUID="94263d84-5357-4566-9987-fc74ca946a96" TYPE="swap"
/dev/mapper/centos-root: UUID="17f09dae-88d1-4813-9738-8996819cf6ba" TYPE="xfs"
/dev/sdb1: UUID="9fa9e491-8e21-4322-a1c2-dc269825e824" TYPE="xfs"


</pre>


Hence deleting a file system is best done with the help of the <a href="http://en.wikipedia.org/?title=/dev/zero">/dev/zero</a> file:


<pre>
$ cat /dev/zero > /dev/sdb1
</pre>

Here's the approach in action:


<pre>
$ lsblk
NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda               8:0    0   20G  0 disk
├─sda1            8:1    0  500M  0 part /boot
└─sda2            8:2    0 19.5G  0 part
  ├─centos-swap 253:0    0    2G  0 lvm  [SWAP]
  └─centos-root 253:1    0 17.5G  0 lvm  /
sdb               8:16   0  2.1G  0 disk
└─sdb1            8:17   0  200M  0 part
sr0              11:0    1 55.4M  0 rom
$ blkid | grep sdb1
/dev/sdb1: UUID="9fa9e491-8e21-4322-a1c2-dc269825e824" TYPE="xfs"
$ cat /dev/zero > /dev/sdb1
cat: write error: No space left on device
$ lsblk
NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda               8:0    0   20G  0 disk
├─sda1            8:1    0  500M  0 part /boot
└─sda2            8:2    0 19.5G  0 part
  ├─centos-swap 253:0    0    2G  0 lvm  [SWAP]
  └─centos-root 253:1    0 17.5G  0 lvm  /
sdb               8:16   0  2.1G  0 disk
└─sdb1            8:17   0  200M  0 part
sr0              11:0    1 55.4M  0 rom
$ blkid | grep sdb1

</pre>

As you can see the filesystem has been deleted but the partition itself stays in intact. 
]]></Content>
		<Date><![CDATA[2015-05-17]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - mounting a partition]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What is the command to find more info about what all the /etc/fstab columns mean?"]
$ man fstab
[/toggle]
[toggle title="What is the command to find more about what the default mount options are?"]
$ man fstab
[/toggle]
[toggle title="What is the command to manually mount '/dev/sdb1' to /mnt?"]
$ mount /dev/sdb1 /mnt
[/toggle]
[toggle title="What are the three command to indicate whether a mounting has been successful?"]
$ lsblk
# or
$ mount
# or
$ df -h
[/toggle]
[toggle title="What are the 2 commands to manually unmount '/dev/sdb1' from /mnt?"]
$ umount /mnt
# or
$ umount /dev/sdb1
[/toggle]
[toggle title="What are the 2 ways to automount a filesystem during machine boot up?"]
- automounting via systemd
- Adding an entry to the /etc/fstab file
[/toggle]
[toggle title="What are each field in the fstab file mean?"]
column 1 - UUID=... or LABEL=... or /dev/mapper/
column 2 - mountpoint
column 3 - filesystem type
column 4 - comma seperated mount options
column 5 - backup support - normally 1, but zero for swaps
column 6 - fsck (file system check) - normally 2, but 0 for swaps
[/toggle]
[toggle title="What is the command to check that the entry you added in your fstab is valid?"]
$ mount -a
[/toggle]
[/accordion]

<hr/>


To recap, There are 3 steps to creating a partition and bring it into use:

<ol>
	<li>Create the partition – this is done using “fdisk” tool</li>
	<li>format the partition (ie. install a filesystem on it) – done using the “mkfs” tool</li>
	<li>mount the partition – done using the “mount” tool (and automate it by updating the fstab config file)</li>
</ol>


In the last 2 lessons we covered how to create a partition, and then format it. So in this lesson we'll cover the last step, which is how to mount a partition/hdd. 


Going back to our library example, we now have a library (analogous to a partition/hdd), that now contains bookshelves (analogous to a file system). The final step is now to unlock the library's front door (analogous to mounting), so that books (analogous to files and folders) can be brought into the library to fill up the book shelves.   

By the way, the phrase "mounting a file system" and "mounting a partition or hdd" both mean the same thing. 

There's basically 2 ways to mount a file system:

<ul>
	<li>manually mount a file system</li>
	<li>automatically mount a file system during machine startup</li>
</ul>

 

<h2>manually mount a file system</h2>

To do the mounting, you first need to have a directory that the filesystem will attach itself to. Directories used for this purposes is called a "mountpoint". For example /dev/sda1 is mounted onto "/boot", as shown by the lsblk command:


<pre>
$ lsblk
NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda               8:0    0   20G  0 disk
<strong>├─sda1            8:1    0  500M  0 part /boot
</strong>└─sda2            8:2    0 19.5G  0 part
  ├─centos-swap 253:0    0    2G  0 lvm  [SWAP]
  └─centos-root 253:1    0 17.5G  0 lvm  /
sdb               8:16   0  2.1G  0 disk
├─sdb1            8:17   0   10M  0 part
├─sdb2            8:18   0   10M  0 part
├─sdb3            8:19   0   10M  0 part
├─sdb4            8:20   0    1K  0 part
├─sdb5            8:21   0   10M  0 part
├─sdb6            8:22   0   10M  0 part
└─sdb7            8:23   0 1001M  0 part
sr0              11:0    1 55.4M  0 rom
</pre>

That means that whenever you create a new file under the /boot directory, then that file gets stored in /dev/sda1 behind the scenes. 

Also notice that devices sdb1 through to sdb7 don't have any mountpoints. That's because they haven't been mounted yet. So let's try mounting sdb1. First of we need a mountpoint, we can either create a new directory, or use an existing directory. 

note: any content that is already in the mount-point directory will be temporarily inaccessible while it has a filesystem mounted on it. but becomes available again after you unmount the filesystem again. That’s why it is good practice to avoid using mountpoint directory for anything other than for mounting a filesystem on it.



In my case, I'll choose use an existing directory, /mnt, which is currently an empty:

<pre>
$ ls -l /mnt/
total 0
</pre>


Now to do the actual mounting, we use the "mount" command like this:


<pre>
$ mount /dev/sdb1 /mnt
</pre>
 
This command doesn't give any output if it is successful. 


<h3>Checking mounting has been successful</h3>
There are 3 main ways to check whether the mounting has been successful.

<strong>method 1 - use the "lsblk" command</strong> 
The first method is by using lsblk:


<pre>
$ lsblk
NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda               8:0    0   20G  0 disk
├─sda1            8:1    0  500M  0 part /boot
└─sda2            8:2    0 19.5G  0 part
  ├─centos-swap 253:0    0    2G  0 lvm  [SWAP]
  └─centos-root 253:1    0 17.5G  0 lvm  /
sdb               8:16   0  2.1G  0 disk
<strong>├─sdb1            8:17   0   10M  0 part /mnt       # Success</strong>
├─sdb2            8:18   0   10M  0 part
├─sdb3            8:19   0   10M  0 part
├─sdb4            8:20   0    1K  0 part
├─sdb5            8:21   0   10M  0 part
├─sdb6            8:22   0   10M  0 part
└─sdb7            8:23   0 1001M  0 part
sr0              11:0    1 55.4M  0 rom

</pre>

<strong>method 2 - use the "mount" command</strong>
The second approach is by using the is by running the "mount" on its own. However this can output a lot of info, hence the need to grep the results:


<pre>
$ mount | grep "^/dev"
/dev/mapper/centos-root on / type xfs (rw,relatime,seclabel,attr2,inode64,noquota)
/dev/sda1 on /boot type xfs (rw,relatime,seclabel,attr2,inode64,noquota)
<strong>/dev/sdb1 on /mnt type ext4 (rw,relatime,seclabel,data=ordered)</strong>
</pre> 

Also another cool thing about the mount command is that it shows what mount features have been enabled on a certain filesystem, e.g. filesystem, relatime,....etc. 


<strong>method 3 - use the df command</strong>

The df command gives an overview of filesystem diskspace usage, and consequently lists all partitions/hdds that are currently mounted:


<pre>
$ df -h
Filesystem               Size  Used Avail Use% Mounted on
/dev/mapper/centos-root   18G  4.3G   14G  25% /
devtmpfs                 488M     0  488M   0% /dev
tmpfs                    497M   80K  497M   1% /dev/shm
tmpfs                    497M   26M  472M   6% /run
tmpfs                    497M     0  497M   0% /sys/fs/cgroup
/dev/sda1                497M  143M  354M  29% /boot
<strong>/dev/sdb1                8.7M  172K  7.9M   3% /mnt</strong>
</pre>



<h3>Unmounting a partition or hdd</h3>

If you want to unmount sdb1, then you use the "umount" command. With this command you simply have to specify either the mountpoint or the device name for the unmount to work, i.e. you can simply run:


<pre>
$ umount /mnt
</pre>

or: 

<pre>
$ umount /dev/sdb1
</pre>


<h2>Labelling a filesystem</h2>

All filesystems have a UUID as an identifier but that doesn't tell us what that filesystem is being used for. That's why it is best practice to Label your filesystems. Labelling filesystem also has the benefit of making it easier to do automounting using fstab. We'll cover more about automomounting in the next section.  

The way you label a filesystem varies depending on whether you are labelling an xfs filesystem or an ext2/ext3/ext4 filesystem.


<h3>Labelling a xfs filesystem</h3> 
Let's say you have an xfs filesystem installed on <em>/dev/sdb1</em>, then to label it you need to use the xfs_admin command:

<pre>
$ xfs_admin -L webdata /dev/sdb1
</pre>

In this case we have labelled it "webdata". you can confirm that this has worked like this:

<pre>
$ xfs_admin -l /dev/sdb1
label = "webdata"
</pre>

Or you can confirm via blkid:

<pre>
$ blkid
/dev/sda1: UUID="fb2a0d0d-1b5e-492a-ad4b-9b68d0cc3b28" TYPE="xfs"
/dev/sda2: UUID="4r5cqp-Vwrb-Wb4T-PRTh-zSoF-cfhe-ZEfaZ7" TYPE="LVM2_member"
/dev/mapper/centos_puppetmaster-root: UUID="c1b8426a-6c8b-48c0-8cb6-b4c8cf780239" TYPE="xfs"
/dev/mapper/centos_puppetmaster-swap: UUID="db6994b0-eaf2-45b2-973a-a24d21130e02" TYPE="swap"
/dev/sdb1: LABEL="webdata" UUID="e133c115-7dc8-49e1-a321-0a387813af75" TYPE="xfs"
</pre>



<h3>Labelling a ext4 filesystem</h3> 

If instead <em>/dev/sdb1</em> is an ext4 based filesystem. Then you label it either with the e2label command. In this case, I'll use e2label:

<pre>
$ e2label /dev/sdb1 webdata
</pre>
Note: you can also label an ext* filesystem using the <code>tune2fs</code> command.  

You can check it has worked like this:

<pre>$ e2label /dev/sdb1
webdata
</pre>

or confirm via blkid again:

<pre>
blkid
/dev/sda1: UUID="fb2a0d0d-1b5e-492a-ad4b-9b68d0cc3b28" TYPE="xfs"
/dev/sda2: UUID="4r5cqp-Vwrb-Wb4T-PRTh-zSoF-cfhe-ZEfaZ7" TYPE="LVM2_member"
/dev/mapper/centos_puppetmaster-root: UUID="c1b8426a-6c8b-48c0-8cb6-b4c8cf780239" TYPE="xfs"
/dev/mapper/centos_puppetmaster-swap: UUID="db6994b0-eaf2-45b2-973a-a24d21130e02" TYPE="swap"
/dev/sdb1: LABEL="webdata" UUID="50063cb0-5c9f-4f45-917a-70c270232126" TYPE="ext4"
</pre>


<h2>Auto mount a file system during boot time</h2>


There is a problem with mounting manually using the "mount" command, and that is that it won't persist, i.e. it won't survive a reboot. So if you reboot your machine, then the manually mounted file systems will become unmounted again after the reboot. 

To make it persistant, you need to automate the mounting while the machine is booting up. There 2 ways to do this: 

<ol>
	<li><a href="http://codingbee.net/tutorials/rhcsa/rhcsa-automounting-using-systemd/">automounting via systemd</a></li>
	<li>Adding an entry to the <code>/etc/fstab</code> file</li>
</ol>


In this lesson we're going to cover the /etc/fstab approach:

There's three ways to refer to a filesystem in column 1 of fstab. By it's device path, by "UUID=" or by "LABEL=". The recommend approach is either "UUID" or "LABEL". For human readability it is best to use "LABEL" To use this approach, you first have to lable your filesystems.


<pre>
$ cat /etc/fstab

#
# /etc/fstab
# Created by anaconda on Sat Mar 14 19:14:11 2015
#
# Accessible filesystems, by reference, are maintained under '/dev/disk'
# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info
#
/dev/mapper/centos-root /                       xfs     defaults        1 1
UUID=547c78ce-34b5-4e31-84d2-bf72b196fa57 /boot                   xfs     defaults        1 2
/dev/mapper/centos-swap swap                    swap    defaults        0 0
</pre>



The fstab (aka filesystem table) file's main purpose is to automounting block devices. 

Each row is made up of column entries:


<strong>Column 1</strong> - This can be the block's UUID=... or LABEL=... value, or a logical volume name, /dev/mapper/... . Notice that we don't specify block devices, e.g. /dev/sda1. That's because occasionally it is possible that a block device actually gets mapped to a different hdd/partition if machine is rebooted. That's why block devices can be ambiguous.  

<strong>Column 2</strong> - This is the mountpoint

<strong>Column 3</strong> - This is the file system type, e.g. ext4, xfs,...etc. 

<strong>Column 4</strong> - This is where you can specify various mount options. It can be set to "defaults", or a comma seperated list. The fstab man page and the mount page lists all the mount options that are independent of any file system types. Whereas file system type mount options are detailed in their respective file system type man pages. In the man pages you'll discover that the "defaults" covers the following:


<pre>
.
.
defaults
              Use default options: rw, suid, dev, exec, auto, nouser, and async.
.
.
</pre>   

These are the defaults for non file system specific default mount options. For file system specific default options, you will need to check out the respective mount options. 

<strong>Column 5</strong> - This is to do with backup support for the <a href="http://en.wikipedia.org/wiki/Dump_(program)">dump backup utility</a>. This is not used that often anymore but if you do want to your filesystem to be backed up then you should enable this by setting it to "1". It is recommended to set this to "1". If however you are dealing with a swap partition, then you should leave this disabled by setting it to "0".

<strong>Column 6</strong> - relates to the ordering that fsck performs all it check through all the devices. the root directory (/) should be set to 1, and all other filesystems should be set to 2. This is so that the root filesystem has a higher priority over all other file systems. You should also disable this check (by setting it to 0) for readonly filesystems, or swap partitions. In all other cases, it is recommended that this is set to "2".


For more info about these 6 column's check out:

<pre>$ man fstab</pre>

This man page is actually really handy. 
  


Once you have added your lines in the fstab, you can then test that your new configuration works by running the following:


<pre>
$ mount -a
</pre>


This will mount all block devices that are specified in the /etfc/fstab, unless they are already mounted. If all goes well then the above command won't output anything. You can check whether your hdd/partition has been mounted by using any of the lsblk/mount/df commands as discussed earlier in this article. 

Note, it is important to test your /etc/fstab settings because, any errors in it will stop your machine from rebooting. ]]></Content>
		<Date><![CDATA[2015-05-19]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Encrypted Filesystems]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What happens when you try to cd into the mountpoint of an encrypted filesystem?"]
You will get prompted to enter a password. 
[/toggle]
[toggle title="What is the command to encrypt /dev/sdb3?"]
$ cryptsetup luksFormat /dev/sdb3
[/toggle]
[toggle title="What is the command to check that an encrypted 'filesytem container' has been installed on /dev/sdb3?"]
$ blkid | grep "^/dev/sdb3"
[/toggle]
[toggle title="What is the command to unlock your /dev/sdb3 give the new uncrypted block device, the name 'decryptedpartition'?"]
$ cryptsetup luksOpen /dev/sdb3 decryptedpartition 
# you will get prompted for a password
[/toggle]
[toggle title="What is the command to check that this filesystem is a child element of /dev/sdb3?"]
$ lsblk
[/toggle]
[toggle title="Where will the new 'decryptedpartition' block device appear?"]
/dev/mapper/decryptedpartition
[/toggle]
[toggle title="What is the command to install the xfs filesystem on 'decryptedpartition'?"]
$ mkfs.xfs /dev/mapper/decryptedpartition
[/toggle]
[toggle title="What is the command to install the 'decryptedpartition' device to the mountpoint '/tmp/secret/?"]
$ mount /dev/mapper/decryptedpartition /tmp/secret
[/toggle]
[toggle title="What is the command to unmount this decrypted partition?"]
$ umount /dev/mapper/decryptedpartition
[/toggle]
[toggle title="What is the command 'log out' of the decrypted partition?"]
$ cryptsetup luksClose /dev/mapper/decryptedpartition
[/toggle]
[/accordion]

<hr/>



In Linux it is possible to encrypt a filesystem so that you get a prompt to enter a password when you try to cd into it's corresponding mountpoint. Hence this is an extra layer of security on top of ugo+rwx and SELinux system. The way it works is that there is an extra step to perform just before installing the actual filesystem. Here's the process to creating an encrypted partition (using /dev/sdb3 as an example):


<ol>
	<li>Create the partition - i.e. <code>fdisk /dev/sdb</code>.</li>
	<li>Run the <code>cryptsetup luksFormat /dev/sdb3</code> command, note the uppercase "F". This will prompt you to choose a new password for your encrypted partition.</li>
	<li>Unlock your encrypted partition using <code>cryptsetup luksOpen /dev/sdb3 decryptedpartition</code>, note the upper case "O". Here we also specify the name we want for the unencrypted partition which in this example is "decryptedpartition". This will prompt you to enter the password that you set earlier. After you enter the correct password, a new block device will appear in /dev/mapper/decryptedpartition</li>
	<li>Format the newly decrypted block device, in the same way as you would do for any other partition, <code>mkfs.xfs /dev/mapper/decryptedpartition</code></li>
	<li>mount your decrypted partition: <code>mount /dev/mapper/decryptedpartition /tmp/secret</code>, make sure you create the mountpoint if it doesn't already exist. You should now be able to start storing files in /tmp/secret and it will gets stored in the encrypted filesystem behind the scense, e.g. <code>touch /tmp/secret/testfile.txt</code></li>
</ol>


Now let's see all this in action:


<strong>1. create a partition using fdisk, or choose an existing partition</strong>
In my case I will use the following existing partition:


<pre>
$ ls -l /dev/sdb3
brw-rw----. 1 root disk 8, 19 May 21 09:14 /dev/sdb3

</pre>





<strong>2. Apply cryptsetup's luksFormat</strong>

Linux Unified Key Setup (LUKS): This is the name for Linux’s partition encryption system, and works by applying a special kind of encrypted file-system container. LUKS is ideal if you want to use it to encrypt contents of usb-pens.

Before we apply the luksFormat, let's first see if sdb3 currently has a file system installed on it:

<pre>
$ blkid | grep "^/dev/sdb3"
</pre>

As expected, it doesn't. 

Now let's install the luksFormat:

<pre>
$ cryptsetup luksFormat /dev/sdb3

WARNING!
========
This will overwrite data on /dev/sdb3 irrevocably.

Are you sure? (Type uppercase yes): YES
Enter passphrase:
Verify passphrase:

</pre>

Now let's check blkid again:

<pre>
$ blkid | grep "^/dev/sdb3"
/dev/sdb3: UUID="5838a034-cfc2-4301-8cbc-c8c26f63fd75" TYPE="crypto_LUKS"
</pre>

Now it does have it. This is a special filesystem that essentially acts as a secure container which is designed to hold an ordinary filesystem, such as ext4, which we'll install further down. 

<strong>3. Unlock the encrypted partition using cryptsetup's luksOpen</strong>
Before we unlock the encrypted, partitions, let's first see our list of block devices:


<pre>
$ lsblk
NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda               8:0    0   20G  0 disk
├─sda1            8:1    0  500M  0 part /boot
└─sda2            8:2    0 19.5G  0 part
  ├─centos-swap 253:0    0    2G  0 lvm  [SWAP]
  └─centos-root 253:1    0 17.5G  0 lvm  /
sdb               8:16   0  2.1G  0 disk
├─sdb1            8:17   0   10M  0 part
├─sdb2            8:18   0   10M  0 part
├─sdb3            8:19   0   10M  0 part
├─sdb4            8:20   0    1K  0 part
├─sdb5            8:21   0   10M  0 part
├─sdb6            8:22   0   10M  0 part
└─sdb7            8:23   0 1001M  0 part
sr0              11:0    1 55.4M  0 rom
</pre>


Let's now unlock our encrypted partition: 


<pre>
$ ls -l /dev/mapper/ | grep decryptedpartition

$ cryptsetup luksOpen /dev/sdb3 decryptedpartition
Enter passphrase for /dev/sdb3:
</pre>


Now let's view lsblk again:


<pre>
$ lsblk
NAME                   MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINT
sda                      8:0    0   20G  0 disk
├─sda1                   8:1    0  500M  0 part  /boot
└─sda2                   8:2    0 19.5G  0 part
  ├─centos-swap        253:0    0    2G  0 lvm   [SWAP]
  └─centos-root        253:1    0 17.5G  0 lvm   /
sdb                      8:16   0  2.1G  0 disk
├─sdb1                   8:17   0   10M  0 part
├─sdb2                   8:18   0   10M  0 part
├─sdb3                   8:19   0   10M  0 part
<strong>│ └─decryptedpartition 253:2    0    8M  0 crypt
</strong>├─sdb4                   8:20   0    1K  0 part
├─sdb5                   8:21   0   10M  0 part
├─sdb6                   8:22   0   10M  0 part
└─sdb7                   8:23   0 1001M  0 part
sr0                     11:0    1 55.4M  0 rom

</pre>



As a result, this has created the following block device:

<pre>
$ ls -l /dev/mapper/ | grep decryptedpartition
lrwxrwxrwx. 1 root root       7 May 21 09:26 decryptedpartition -> ../dm-2

$ ls -l /dev | grep dm-2
brw-rw----. 1 root disk    253,   2 May 21 09:26 dm-2
</pre>

We can now chech the status of our newly decrypted block device, either via the symbolic link:

<pre>
$ cryptsetup status /dev/mapper/decryptedpartition
/dev/mapper/decryptedpartition is active.
  type:    LUKS1
  cipher:  aes-xts-plain64
  keysize: 256 bits
  device:  /dev/sdb3
  offset:  4096 sectors
  size:    16384 sectors
  mode:    read/write

</pre>

or directly:

<pre>
$ cryptsetup status /dev/dm-2
/dev/dm-2 is active.
  type:    LUKS1
  cipher:  aes-xts-plain64
  keysize: 256 bits
  device:  /dev/sdb3
  offset:  4096 sectors
  size:    16384 sectors
  mode:    read/write
</pre>




<strong>4. Format the decrypted block device</strong>

Now let's install a file system onto the partition:

<pre>
$ mkfs.ext4 /dev/mapper/decryptedpartition
mke2fs 1.42.9 (28-Dec-2013)
Filesystem label=
OS type: Linux
Block size=1024 (log=0)
Fragment size=1024 (log=0)
Stride=0 blocks, Stripe width=0 blocks
2048 inodes, 8192 blocks
409 blocks (4.99%) reserved for the super user
First data block=1
Maximum filesystem blocks=8388608
1 block group
8192 blocks per group, 8192 fragments per group
2048 inodes per group

Allocating group tables: done
Writing inode tables: done
Creating journal (1024 blocks): done
Writing superblocks and filesystem accounting information: done

</pre>


<strong>5. Mount your decrypted partition</strong>

Let's first create the mountpoint:


<pre>$ mkdir /tmp/xfsmountpoint/</pre>


Just before the mounting, let's check the block device's status:

<pre>
cryptsetup status /dev/mapper/decryptedpartition
/dev/mapper/decryptedpartition is active.
  type:    LUKS1
  cipher:  aes-xts-plain64
  keysize: 256 bits
  device:  /dev/sdb3
  offset:  4096 sectors
  size:    16384 sectors
  mode:    read/write
</pre>




Then do the mounting:


<pre>
$ mount /dev/mapper/decryptedpartition /tmp/xfsmountpoint/
</pre>


Now let's check the status again:

<pre>
 cryptsetup status /dev/mapper/decryptedpartition
/dev/mapper/decryptedpartition is active <strong>and is in use</strong>.
  type:    LUKS1
  cipher:  aes-xts-plain64
  keysize: 256 bits
  device:  /dev/sdb3
  offset:  4096 sectors
  size:    16384 sectors
  mode:    read/write


</pre>





<h2>Closing an encrypted partition</h2>


After you have finished working on your encrypted partition, you can then close it by taking the following steps:

<ol>
	<li>unmount the encrypted file system, <code>umount /dev/mapper/decryptedpartition</code></li>
	<li>Then close your decrypted partition, <code>cryptsetup luksClose /dev/mapper/decryptedpartition</code>, note the uppercase "C", this results in the deletion of the /dev/mapper/decryptedpartition block device. </li>
</ol>]]></Content>
		<Date><![CDATA[2015-05-21]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Automounting Encrypted Filesystems during boot time]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What is the entry you need to add to the fstab file to automount the decrypted ext4 based '/dev/mapper/decryptedpartition' on to the mount point, '/tmp/secretmount'?"]
/dev/mapper/decryptedpartition   /tmp/ext4secretmount  ext4 defaults 1 2
[/toggle]
[toggle title="Which file do you need to add an entry in order to prompt your machine to request for the encrypted filesystem's password during boot up?"]
/etc/crypttab
[/toggle]
[toggle title="This file is usually empty, so what entry do you need to add this file to autoprompt you for the password on bootup?"]
decryptedpartition    /dev/sdb3      none
[/toggle]
[toggle title="However what entry do you need to add this file so that the password is automatically inputted for you on boot up?"]
decryptedpartition    /dev/sdb3      {password in plaintesxt}
[/toggle]

[/accordion]

<hr/>



In the last lesson we saw how to manually mount a luks encrypted partition. 


However as mentioned earlier, any partitions (including encrypted partitions) that are mounted directly using the mount command will become unmounted again when you reboot the machine. To overcome this we would need to add an entry to the fstab file. Following on from the last lesson's example, we have:


<pre>$ blkid | grep "^/dev/sdb3"
/dev/sdb3: UUID="5838a034-cfc2-4301-8cbc-c8c26f63fd75" TYPE="crypto_LUKS"
$ lsblk
NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda               8:0    0   20G  0 disk
├─sda1            8:1    0  500M  0 part /boot
└─sda2            8:2    0 19.5G  0 part
  ├─centos-swap 253:0    0    2G  0 lvm  [SWAP]
  └─centos-root 253:1    0 17.5G  0 lvm  /
sdb               8:16   0  2.1G  0 disk
├─sdb1            8:17   0   10M  0 part
├─sdb2            8:18   0   10M  0 part
├─sdb3            8:19   0   10M  0 part
├─sdb4            8:20   0    1K  0 part
├─sdb5            8:21   0   10M  0 part
├─sdb6            8:22   0   10M  0 part
└─sdb7            8:23   0 1001M  0 part
sr0              11:0    1 55.4M  0 rom
</pre>

Then we open up access to our decrypted block device by running:

<pre>
$ cryptsetup luksOpen /dev/sdb3 decryptedpartition
Enter passphrase for /dev/sdb3:
Enter passphrase for /dev/sdb3:
</pre>

After that we have now exposed the decrypted block device that we want to mount:

<pre>
$ lsblk
NAME                   MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINT
sda                      8:0    0   20G  0 disk
├─sda1                   8:1    0  500M  0 part  /boot
└─sda2                   8:2    0 19.5G  0 part
  ├─centos-swap        253:0    0    2G  0 lvm   [SWAP]
  └─centos-root        253:1    0 17.5G  0 lvm   /
sdb                      8:16   0  2.1G  0 disk
├─sdb1                   8:17   0   10M  0 part
├─sdb2                   8:18   0   10M  0 part
├─sdb3                   8:19   0   10M  0 part
│ └─decryptedpartition 253:2    0    8M  0 crypt
├─sdb4                   8:20   0    1K  0 part
├─sdb5                   8:21   0   10M  0 part
├─sdb6                   8:22   0   10M  0 part
└─sdb7                   8:23   0 1001M  0 part
sr0                     11:0    1 55.4M  0 rom
</pre>

Now to automount this, we would need to add the following line in our fstab:


<pre>
$ cat /etc/fstab

#
# /etc/fstab
# Created by anaconda on Sat Mar 14 19:14:11 2015
#
# Accessible filesystems, by reference, are maintained under '/dev/disk'
# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info
#
/dev/mapper/centos-root /                       xfs     defaults        1 1
UUID=547c78ce-34b5-4e31-84d2-bf72b196fa57 /boot                   xfs     defaults        1 2
/dev/mapper/centos-swap swap                    swap    defaults        0 0
<strong>/dev/mapper/decryptedpartition   /tmp/ext4secretmount  ext4 defaults 1 2
</strong>

</pre>

Now to confirm that this works, we do a "mount all":


<pre>
$ mount -a
$ lsblk
NAME                   MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINT
sda                      8:0    0   20G  0 disk
├─sda1                   8:1    0  500M  0 part  /boot
└─sda2                   8:2    0 19.5G  0 part
  ├─centos-swap        253:0    0    2G  0 lvm   [SWAP]
  └─centos-root        253:1    0 17.5G  0 lvm   /
sdb                      8:16   0  2.1G  0 disk
├─sdb1                   8:17   0   10M  0 part
├─sdb2                   8:18   0   10M  0 part
├─sdb3                   8:19   0   10M  0 part
<strong>│ └─decryptedpartition 253:2    0    8M  0 crypt /tmp/ext4secretmount</strong>
├─sdb4                   8:20   0    1K  0 part
├─sdb5                   8:21   0   10M  0 part
├─sdb6                   8:22   0   10M  0 part
└─sdb7                   8:23   0 1001M  0 part
sr0                     11:0    1 55.4M  0 rom
</pre>
 
Success!


However there is a problem, we generated the /dev/mapper/decryptedpartition block device by running the "cryptsetup luksOpen" command, however this isn't persistant. So to make this block device available again, you need to run "cryptsetup luksOpen" after every boot. Since the fstab file is referring a block device that doesn't exist yet, automounting will fail. 

In this situation, the solution is to prompt the machine (during boot time) to first unlock encrypted partitions before it reads the /etc/fstab and starts the automounting process. This is achieved by adding an entry to the /etc/crypttab file:


<pre>
$ cat /etc/crypttab
decryptedpartition    /dev/sdb3      none
</pre> 


The first 2 fields are essentially the same parameters that are passed into the <code>cryptsetup luksOpen...</code> command. 

The last field shows "none", which means that we don't want to use an automated approach to passing in a password. 
As a result the machine will now prompt for a password at the very early stages of booting up. 

 ]]></Content>
		<Date><![CDATA[2015-05-21]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Logical Volume Management (LVM) Overview]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What is the command to list all the available lvm commands along with a short description?"]
$ lvm help
[/toggle]
[/accordion]

<hr/>

LVM is all about creating/managing <em>logical volumes</em>. A logical volume (aka LV), can be thought of as a partition on a "virtual hdd". The storage space of that logical volume can span across several block devices (i.e. actual partitions and hdds). LVs gives you a lot of flexibility to help you manage your various disk space more effectively. It also lets you create any size "virtual partition" that you want, e.g. you can have a 10TB virtual partition (in the form of a LV, which, behind the scenes is made up from several hdds/partitions)

LVM's are a better alternative to using partitions for managing your storage space. Partitions are still necessary for some of the core directories, e.g. the <code>/boot</code> directory needs to have it's own primary partition or hdd. But after that it is better to have all other filesystems running on LVs. 

Here are some of the advantages LVM offers over partitions:

<ul>
	<li>logical volumes are more flexible - LVs can be made up of several hdds/partitions, and you can extend/shrink LVs on the fly.</li>
	<li>easily replace hdds that are nearing the end of their lifetime</li>
	<li>Easily take backups in the form of LVM snapshots</li>
	<li>Easy to add new hdd's to existing LVM setup</li>
	<li>Can create nearly unlimited LVs, i.e. not limited in the same way as with partitions, which you can only have up to 15 partitions per hdd.</li>
</ul>



Assuming that you already have a hdd or partition, then the overall process to creating LVs is as follows:

<ol>
	<li>Create new physical volumes (out of hdd’s and partitions)</li>
	<li>Add physical volumes to a volume group. If no VGs currently exist, then create one.</li>
	<li>Create a logical volume from a volume group.</li>
</ol>


You can treat an LV in the same way that you treat hdds/partitions, i.e. you can install a filesystem on it and mount it. However it has a couple of key differences:

&nbsp;
<ul>
	<li>The mbr won’t store info about LVs</li>
	<li>You can’t mount <code>/boot</code> on a LV, since mbr won’t be able to pass over the boot process to an LV.</li>
</ul>

To create an LV volume, you have to take the following steps:

&nbsp;
<ol>
	<li>Create “Physical Volumes” from block devices. (i.e. partitions and hdds) that have partition-id of 8e (for LVM). These block devices shouldn’t have been formatted (and therefore mounted). The act of creating a pv is like transforming a whole block device into a form of lego.</li>
	<li>Next you add the physical volume to a <em>Volume Group</em>. This is a bit like putting your legos into a lego bucket (i.e. VG), so that they are ready to be used for creating logical-volumes. The legos are often referred to as “physical extents”</li>
	<li>Next you create a “Logical Volume” by pulling out the required disk space from the volume-group.</li>
	<li>The LV can then be used like a partition, i.e. you can install a filestystem (format it) and mount it for use.</li>
</ol>
Note: you cannot install a filesystem on a physical volume or volume group.....they are just intermediary steps that leads to creating logical volumes. This means that physical volumes and volume groups, as standalones....are not that useful.

<h2>LVM commands</h2>


Here's a summary of all the available LVM commands:



<pre>
$ lvm help
  Available lvm commands:
  Use 'lvm help <command>' for more information

  devtypes        Display recognised built-in block device types
  dumpconfig      Dump configuration
  formats         List available metadata formats
  help            Display help for commands
  lvchange        Change the attributes of logical volume(s)
  lvconvert       Change logical volume layout
  lvcreate        Create a logical volume
  lvdisplay       Display information about a logical volume
  lvextend        Add space to a logical volume
  lvmchange       With the device mapper, this is obsolete and does nothing.
  lvmdiskscan     List devices that may be used as physical volumes
  lvmsadc         Collect activity data
  lvmsar          Create activity report
  lvreduce        Reduce the size of a logical volume
  lvremove        Remove logical volume(s) from the system
  lvrename        Rename a logical volume
  lvresize        Resize a logical volume
  lvs             Display information about logical volumes
  lvscan          List all logical volumes in all volume groups
  pvchange        Change attributes of physical volume(s)
  pvresize        Resize physical volume(s)
  pvck            Check the consistency of physical volume(s)
  pvcreate        Initialize physical volume(s) for use by LVM
  pvdata          Display the on-disk metadata for physical volume(s)
  pvdisplay       Display various attributes of physical volume(s)
  pvmove          Move extents from one physical volume to another
  pvremove        Remove LVM label(s) from physical volume(s)
  pvs             Display information about physical volumes
  pvscan          List all physical volumes
  segtypes        List available segment types
  tags            List tags defined on this host
  vgcfgbackup     Backup volume group configuration(s)
  vgcfgrestore    Restore volume group configuration
  vgchange        Change volume group attributes
  vgck            Check the consistency of volume group(s)
  vgconvert       Change volume group metadata format
  vgcreate        Create a volume group
  vgdisplay       Display volume group information
  vgexport        Unregister volume group(s) from the system
  vgextend        Add physical volumes to a volume group
  vgimport        Register exported volume group with system
  vgmerge         Merge volume groups
  vgmknodes       Create the special files for volume group devices in /dev
  vgreduce        Remove physical volume(s) from a volume group
  vgremove        Remove volume group(s)
  vgrename        Rename a volume group
  vgs             Display information about volume groups
  vgscan          Search for all volume groups
  vgsplit         Move physical volumes into a new or existing volume group
  version         Display software and driver version information


</pre>




]]></Content>
		<Date><![CDATA[2015-05-21]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Creating a Logical Volume]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What is the command to create a partition from /dev/sdb?"]
$ fdisk /dev/sdb
[/toggle]
[toggle title="What is the command to create a PV from /dev/sdb1?"]
$ pvcreate /dev/sdb1
[/toggle]
[toggle title="What is the command to create a VG with the name <em>vgdata1vg</em> from the PV,  /dev/sdb1?"]
$ vgcreate vgdata1vg /dev/sdb1
[/toggle]
[toggle title="What is the command to create a 300MB LV (called <em>lvmyfirstlv</em>) from the VG, <em>vgdata1vg</em>?"]
$ lvcreate -n lvmyfirstlv -L 300M vgdata1vg
[/toggle]
[toggle title="What are the 2 symbolic links that are created under the /dev directory after running the above command?"]
/dev/mapper/vgdata1vg-lvmyfirstlv   # i.e. /dev/mapper/{vgname}-{lvname}
# and:
/dev/vgdata1vg/lvmyfirstlv   # i.e. /dev/{vg-name}/{lv-name}
[/toggle]
[toggle title="What is the command to install the ext4 filesystem on LV, lvmyfirstlv, which was created from the VG, vgdata1vg?"]
$ mkfs.ext4 /dev/vgdata1vg/lvmyfirstlv
[/toggle]
[toggle title="What is the command for creating the mountpoint, /tmp/lvdata1?"]
$ mkdir /tmp/lvdata1
[/toggle]
[toggle title="What is the command for manually mounting the LV, /dev/vgdata1vg/lvmyfirstlv onto the mountpoint /tmp/lvdata1?"]
$ mount /dev/vgdata1vg/lvmyfirstlv /tmp/lvdata1
[/toggle]
[toggle title="What is the entry you need to add to the /etc/fstab file to automount this logical volumen at boot time?"]
/dev/vgdata1vg/lvmyfirstlv   /tmp/lvdata1 ext4 defaults 0 0
[/toggle]
[toggle title="What is the command to check that the above entry is correct?"]
$ mount -a
[/toggle]
[toggle title="What are the 3 commands to check that filesystem has been mounted?"]
$ df -h    # this lists all active filesystems
# or
$ lsblk
# or 
$ mount
[/toggle]

[/accordion]

<hr/>







To understand how to create a Logical Volume, it's best to walk through a scenario:

<strong>Scenario</strong>
In this scenario we have attached a new 2GB hdd to our machine (/dev/sdb), and we now want to do the following:

<ol>
	<li>We want to create 3 partitions on this hdd, of which the second and third partitions are going to be used as part of LVM. The 1st partition will be 1GB in size, whereas the 2nd and 3rd partition will be 500MB each. </li>
	<li>Create Physical Volumes out of partitions 2 and 3. 
 	<li>We will create a new volume group, which will comprise of the 2nd and 3rd partitions. </li>
	<li>We will then create 2 logical volumes from this volume group.</li>
	<li>The first lv will have the xfs filesystem installed on it. And the second will have the ext4 filesystem installed on it. </li>
	<li>These logical volume then needs to be mounted and be available for use</li>
</ol>

<h2>Create partitions to be used for LVM</h2>
Right now we just have the 2GB hdd with no partitions:

<pre>
$ lsblk
NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda               8:0    0   20G  0 disk
├─sda1            8:1    0  500M  0 part /boot
└─sda2            8:2    0 19.5G  0 part
  ├─centos-swap 253:0    0    2G  0 lvm  [SWAP]
  └─centos-root 253:1    0 17.5G  0 lvm  /
<mark>sdb               8:16   0  2.1G  0 disk</mark>
sr0              11:0    1 55.4M  0 rom
</pre>
   
Now let's use fdisk to create the 3 partitions:

<pre>
$ fdisk /dev/sdb
Welcome to fdisk (util-linux 2.23.2).

Changes will remain in memory only, until you decide to write them.
Be careful before using the write command.


Command (m for help): n
Partition type:
   p   primary (0 primary, 0 extended, 4 free)
   e   extended
Select (default p):
Using default response p
Partition number (1-4, default 1):
First sector (2048-4310167, default 2048):
Using default value 2048
Last sector, +sectors or +size{K,M,G} (2048-4310167, default 4310167): +1100M
Partition 1 of type Linux and of size 1.1 GiB is set

Command (m for help): n
Partition type:
   p   primary (1 primary, 0 extended, 3 free)
   e   extended
Select (default p): p
Partition number (2-4, default 2):
First sector (2254848-4310167, default 2254848):
Using default value 2254848
Last sector, +sectors or +size{K,M,G} (2254848-4310167, default 4310167): +500M
Partition 2 of type Linux and of size 500 MiB is set

Command (m for help): n
Partition type:
   p   primary (2 primary, 0 extended, 2 free)
   e   extended
Select (default p): p
Partition number (3,4, default 3):
First sector (3278848-4310167, default 3278848):
Using default value 3278848
Last sector, +sectors or +size{K,M,G} (3278848-4310167, default 4310167):
Using default value 4310167
Partition 3 of type Linux and of size 503.6 MiB is set

Command (m for help): p

Disk /dev/sdb: 2206 MB, 2206806016 bytes, 4310168 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk label type: dos
Disk identifier: 0xf1258b4b

   Device Boot      Start         End      Blocks   Id  System
/dev/sdb1            2048     2254847     1126400   83  Linux
/dev/sdb2         2254848     3278847      <mark>512000   83  Linux</mark>
/dev/sdb3         3278848     4310167      <mark>515660   83  Linux</mark>

Command (m for help): w
The partition table has been altered!

Calling ioctl() to re-read partition table.
Syncing disks.
$ lsblk
NAME            MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
sda               8:0    0    20G  0 disk
├─sda1            8:1    0   500M  0 part /boot
└─sda2            8:2    0  19.5G  0 part
  ├─centos-swap 253:0    0     2G  0 lvm  [SWAP]
  └─centos-root 253:1    0  17.5G  0 lvm  /
sdb               8:16   0   2.1G  0 disk
├─sdb1            8:17   0   1.1G  0 part
├─sdb2            8:18   0   500M  0 part
└─sdb3            8:19   0 503.6M  0 part
sr0              11:0    1  55.4M  0 rom

</pre>
 
As you can see we have now created 3 partitions. However partitions 2 and 3 are currently listed as standard Linux partitions (type 83) as highlighted above. Therefore we need to switch this to the LVM partition type:


<pre>
$ fdisk /dev/sdb
Welcome to fdisk (util-linux 2.23.2).

Changes will remain in memory only, until you decide to write them.
Be careful before using the write command.


Command (m for help): m
Command action
   a   toggle a bootable flag
   b   edit bsd disklabel
   c   toggle the dos compatibility flag
   d   delete a partition
   g   create a new empty GPT partition table
   G   create an IRIX (SGI) partition table
   l   list known partition types
   m   print this menu
   n   add a new partition
   o   create a new empty DOS partition table
   p   print the partition table
   q   quit without saving changes
   s   create a new empty Sun disklabel
   t   change a partition's system id
   u   change display/entry units
   v   verify the partition table
   w   write table to disk and exit
   x   extra functionality (experts only)

Command (m for help): t
Partition number (1-3, default 3): 2
Hex code (type L to list all codes): L

 0  Empty           24  NEC DOS         81  Minix / old Lin bf  Solaris
 1  FAT12           27  Hidden NTFS Win 82  Linux swap / So c1  DRDOS/sec (FAT-
 2  XENIX root      39  Plan 9          83  Linux           c4  DRDOS/sec (FAT-
 3  XENIX usr       3c  PartitionMagic  84  OS/2 hidden C:  c6  DRDOS/sec (FAT-
 4  FAT16 <32M      40  Venix 80286     85  Linux extended  c7  Syrinx
 5  Extended        41  PPC PReP Boot   86  NTFS volume set da  Non-FS data
 6  FAT16           42  SFS             87  NTFS volume set db  CP/M / CTOS / .
 7  HPFS/NTFS/exFAT 4d  QNX4.x          88  Linux plaintext de  Dell Utility
 8  AIX             4e  QNX4.x 2nd part <mark>8e  Linux LVM</mark>       df  BootIt
 9  AIX bootable    4f  QNX4.x 3rd part 93  Amoeba          e1  DOS access
 a  OS/2 Boot Manag 50  OnTrack DM      94  Amoeba BBT      e3  DOS R/O
 b  W95 FAT32       51  OnTrack DM6 Aux 9f  BSD/OS          e4  SpeedStor
 c  W95 FAT32 (LBA) 52  CP/M            a0  IBM Thinkpad hi eb  BeOS fs
 e  W95 FAT16 (LBA) 53  OnTrack DM6 Aux a5  FreeBSD         ee  GPT
 f  W95 Ext'd (LBA) 54  OnTrackDM6      a6  OpenBSD         ef  EFI (FAT-12/16/
10  OPUS            55  EZ-Drive        a7  NeXTSTEP        f0  Linux/PA-RISC b
11  Hidden FAT12    56  Golden Bow      a8  Darwin UFS      f1  SpeedStor
12  Compaq diagnost 5c  Priam Edisk     a9  NetBSD          f4  SpeedStor
14  Hidden FAT16 <3 61  SpeedStor       ab  Darwin boot     f2  DOS secondary
16  Hidden FAT16    63  GNU HURD or Sys af  HFS / HFS+      fb  VMware VMFS
17  Hidden HPFS/NTF 64  Novell Netware  b7  BSDI fs         fc  VMware VMKCORE
18  AST SmartSleep  65  Novell Netware  b8  BSDI swap       fd  Linux raid auto
1b  Hidden W95 FAT3 70  DiskSecure Mult bb  Boot Wizard hid fe  LANstep
1c  Hidden W95 FAT3 75  PC/IX           be  Solaris boot    ff  BBT
1e  Hidden W95 FAT1 80  Old Minix
Hex code (type L to list all codes): 8e
Changed type of partition 'Linux' to 'Linux LVM'

Command (m for help): t
Partition number (1-3, default 3):
Hex code (type L to list all codes): 8e
Changed type of partition 'Linux' to 'Linux LVM'

Command (m for help): p

Disk /dev/sdb: 2206 MB, 2206806016 bytes, 4310168 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk label type: dos
Disk identifier: 0xf1258b4b

   Device Boot      Start         End      Blocks   Id  System
/dev/sdb1            2048     2254847     1126400   83  Linux
/dev/sdb2         2254848     3278847      512000   <strong>8e  Linux LVM</strong>
/dev/sdb3         3278848     4310167      515660   <strong>8e  Linux LVM</strong>

Command (m for help): w
The partition table has been altered!

Calling ioctl() to re-read partition table.
Syncing disks.

</pre>  


We have now created the 3 partitions and allocated partions 2 and 3 for LVM usage. This now means that we can create physical volumes out of partitions 2 and 3. 


<h2>Create physical volumes</h2>

There are a few commands available for doing pv related tasks:


<pre>
$ pv
pvchange   pvck       pvcreate   pvdisplay  pvmove     pvremove   pvresize   pvs        pvscan
</pre>

Let's list all the pvs that currently exists:

<pre>
$ pvs
  PV         VG     Fmt  Attr PSize  PFree
  /dev/sda2  centos lvm2 a--  19.51g    0
</pre>

Note: The "pvdislay" command also shows pretty much the same info.


As you can see, /dev/sdb2 and /dev/sdb3 are not listed here. To create PVs, we use the pvcreate command:


<pre>
$ pvcreate --help
  pvcreate: Initialize physical volume(s) for use by LVM

pvcreate
        [--norestorefile]
        [--restorefile file]
        [--commandprofile ProfileName]
        [-d|--debug]
        [-f[f]|--force [--force]]
        [-h|-?|--help]
        [--labelsector sector]
        [-M|--metadatatype 1|2]
        [--pvmetadatacopies #copies]
        [--bootloaderareasize BootLoaderAreaSize[bBsSkKmMgGtTpPeE]]
        [--metadatasize MetadataSize[bBsSkKmMgGtTpPeE]]
        [--dataalignment Alignment[bBsSkKmMgGtTpPeE]]
        [--dataalignmentoffset AlignmentOffset[bBsSkKmMgGtTpPeE]]
        [--setphysicalvolumesize PhysicalVolumeSize[bBsSkKmMgGtTpPeE]
        [-t|--test]
        [-u|--uuid uuid]
        [-v|--verbose]
        [-y|--yes]
        [-Z|--zero {y|n}]
        [--version]
        PhysicalVolume [PhysicalVolume...]
</pre>


Let's now initialize the 2nd and 3rd partitions into physical volumes now:


<pre>
$ pvcreate /dev/sdb2
  Physical volume "/dev/sdb2" successfully created
$ pvcreate /dev/sdb3
  Physical volume "/dev/sdb3" successfully created
$ pvs
  PV         VG     Fmt  Attr PSize   PFree
  /dev/sda2  centos lvm2 a--   19.51g      0
  /dev/sdb2         lvm2 ---  500.00m 500.00m
  /dev/sdb3         lvm2 ---  503.57m 503.57m
</pre>

Now we have 2 physical volumes, but they haven't been added to a VG yet, hence the empty fields under the VG column. 



<h2>Create volume group</h2>

There are a few commands available for doing vg related tasks:

<pre>
$ vg
vgcfgbackup    vgck           vgdisplay      vgimport       vgmknodes      vgrename       vgsplit
vgcfgrestore   vgconvert      vgexport       vgimportclone  vgreduce       vgs
vgchange       vgcreate       vgextend       vgmerge        vgremove       vgscan
</pre>

Let's see what vg currently exists using the vgs command:


<pre>
$ vgs
  VG     #PV #LV #SN Attr   VSize  VFree
  centos   1   2   0 wz--n- 19.51g    0
</pre>


Now to create a new vg, we use the vgcreate command:


<pre>
 vgcreate --help
  vgcreate: Create a volume group

vgcreate
        [-A|--autobackup {y|n}]
        [--addtag Tag]
        [--alloc AllocationPolicy]
        [-c|--clustered {y|n}]
        [--commandprofile ProfileName]
        [-d|--debug]
        [-h|--help]
        [-l|--maxlogicalvolumes MaxLogicalVolumes]
        [--metadataprofile ProfileName]
        [-M|--metadatatype 1|2]
        [--[vg]metadatacopies #copies]
        [-p|--maxphysicalvolumes MaxPhysicalVolumes]
        [-s|--physicalextentsize PhysicalExtentSize[bBsSkKmMgGtTpPeE]]
        [-t|--test]
        [-v|--verbose]
        [--version]
        [-y|--yes]
        [ PHYSICAL DEVICE OPTIONS ]
        VolumeGroupName PhysicalDevicePath [PhysicalDevicePath...]

</pre>


Let's now create our new vg, "vgdata1vg", but for demonstration purposes, we'll only add one physical volume initially:


<pre>
$ vgcreate vgdata1vg /dev/sdb2
  Volume group "vgdata1vg" successfully created
</pre>

Let's now confirm that our vg now exists:

<pre>
$ vgs
  VG        #PV #LV #SN Attr   VSize   VFree
  centos      1   2   0 wz--n-  19.51g      0
  vgdata1vg   1   0   0 wz--n- 496.00m 496.00m
</pre>

Notice here that it gives info that our vg is only made up of 1 PV so far, and that no LV have been created from vgdata1vg. The vgs command is really useful because it gives you a nice overall LVM status. 

Now that our vg exists, to add the other pv to our vg, we need to use the vgextend command:


<pre>
$  vgextend --help
  vgextend: Add physical volumes to a volume group

vgextend
        [-A|--autobackup y|n]
        [--restoremissing]
        [--commandprofile ProfileName]
        [-d|--debug]
        [-f|--force]
        [-h|--help]
        [-t|--test]
        [-v|--verbose]
        [--version]
        [-y|--yes]
        [ PHYSICAL DEVICE OPTIONS ]
        VolumeGroupName PhysicalDevicePath [PhysicalDevicePath...]
</pre>
 

Therfore let's add sdb3 to vgdata1vg right now:

<pre>
$ vgextend vgdata1vg /dev/sdb3
  Volume group "vgdata1vg" successfully extended
$ vgs
  VG        #PV #LV #SN Attr   VSize   VFree
  centos      1   2   0 wz--n-  19.51g      0
  vgdata1vg   2   0   0 wz--n- 996.00m 996.00m
</pre>

This has now worked. Notice that total disk space is about 4MB less than the 1GB that was expected. That's becuase that 4MB is used by LVM internally for metadata purposes. 


If we now run the pvs command now, we'll see that both PVs are now attached to our new VG:


<pre>
$ pvs
  PV         VG        Fmt  Attr PSize   PFree
  /dev/sda2  centos    lvm2 a--   19.51g      0
  /dev/sdb2  vgdata1vg lvm2 a--  496.00m 496.00m
  /dev/sdb3  vgdata1vg lvm2 a--  500.00m 500.00m
</pre>



<h2>Create Logical Volumes</h2>


There are a few commands available for doing lv related tasks:

<pre>
$ lv
lvchange     lvdisplay    lvmchange    lvmdump      lvmsar       lvrename     lvscan
lvconvert    lvextend     lvmconf      lvmetad      lvreduce     lvresize
lvcreate     lvm          lvmdiskscan  lvmsadc      lvremove     lvs
</pre>




Let's see what LVs currently exists using the lvs command:


<pre>
$ lvs
  LV   VG     Attr       LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  root centos -wi-ao---- 17.51g
  swap centos -wi-ao----  2.00g

</pre>

These LVs are also represented in the form of files, under /dev/mapper:


<pre>
$ ls -l /dev/mapper/
total 0
lrwxrwxrwx. 1 root root       7 May 22 18:39 centos-root -> ../dm-1
lrwxrwxrwx. 1 root root       7 May 22 18:39 centos-swap -> ../dm-0
crw-------. 1 root root 10, 236 May 22 18:39 control

</pre>

As you can see the naming convention of these files are "vgname-lvname"


Now to create LVs, we need to use the lvcreate command:


<pre>
$ lvcreate --help
  lvcreate: Create a logical volume

lvcreate
        [-A|--autobackup {y|n}]
        [-a|--activate [a|e|l]{y|n}]
        [--addtag Tag]
        [--alloc AllocationPolicy]
        [-H|--cache
          [--cachemode {writeback|writethrough}]
        [--cachepool CachePoolLogicalVolume{Name|Path}]
        [-c|--chunksize ChunkSize]
        [-C|--contiguous {y|n}]
        [--commandprofile ProfileName]
        [-d|--debug]
        [-h|-?|--help]
        [--errorwhenfull {y|n}]
        [--ignoremonitoring]
        [--monitor {y|n}]
        [-i|--stripes Stripes [-I|--stripesize StripeSize]]
        [-k|--setactivationskip {y|n}]
        [-K|--ignoreactivationskip]
        {-l|--extents LogicalExtentsNumber[%{VG|PVS|FREE}] |
         <strong>-L|--size LogicalVolumeSize[bBsSkKmMgGtTpPeE]}</strong>
        [-M|--persistent {y|n}] [-j|--major major] [--minor minor]
        [--metadataprofile ProfileName]
        [-m|--mirrors Mirrors [--nosync]
          [{--mirrorlog {disk|core|mirrored}|--corelog}]]
        <strong>[-n|--name LogicalVolumeName]</strong>
        [--noudevsync]
        [-p|--permission {r|rw}]
        [--poolmetadatasize MetadataSize[bBsSkKmMgG]]
        [--poolmetadataspare {y|n}]]
        [--[raid]minrecoveryrate Rate]
        [--[raid]maxrecoveryrate Rate]
        [-r|--readahead {ReadAheadSectors|auto|none}]
        [-R|--regionsize MirrorLogRegionSize]
        [-T|--thin
          [--discards {ignore|nopassdown|passdown}]
        [--thinpool ThinPoolLogicalVolume{Name|Path}]
        [-t|--test]
        [--type VolumeType]
        [-v|--verbose]
        [-W|--wipesignatures {y|n}]
        [-Z|--zero {y|n}]
        [--version]
        <strong>VolumeGroupName [PhysicalVolumePath...]</strong>
.
.
.
</pre>


The important options are highlighted above. Based on the help info, let's now create our new LVs:

<pre>
$ lvcreate -n lvmyfirstlv -L 300M vgdata1vg
  Logical volume "lvmyfirstlv" created.
$ lvcreate -n lvmysecondlv -L 400M vgdata1vg
  Logical volume "lvmysecondlv" created.

</pre>


Let's now confirm that this has worked:


<pre>
$ lvs
  LV           VG        Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  root         centos    -wi-ao----  17.51g
  swap         centos    -wi-ao----   2.00g
  lvmyfirstlv  vgdata1vg -wi-a----- 300.00m
  lvmysecondlv vgdata1vg -wi-a----- 400.00m
</pre>

And also:

<pre>
$ vgs
  VG        #PV #LV #SN Attr   VSize   VFree
  centos      1   2   0 wz--n-  19.51g      0
  vgdata1vg   2   2   0 wz--n- 996.00m 296.00m
</pre>


Also the following files have been created:


<pre>
$ ls -l /dev/mapper/
total 0
lrwxrwxrwx. 1 root root       7 May 22 18:39 centos-root -> ../dm-1
lrwxrwxrwx. 1 root root       7 May 22 18:39 centos-swap -> ../dm-0
crw-------. 1 root root 10, 236 May 22 18:39 control
<strong>lrwxrwxrwx. 1 root root       7 May 22 22:30 vgdata1vg-lvmyfirstlv -> ../dm-2
lrwxrwxrwx. 1 root root       7 May 22 22:31 vgdata1vg-lvmysecondlv -> ../dm-3
</strong>$ ls -l /dev/ | egrep "dm-2|dm-3"
<strong>brw-rw----. 1 root disk     253,   2 May 22 22:30 dm-2
brw-rw----. 1 root disk     253,   3 May 22 22:31 dm-3
</strong>

</pre>

We also have the following symbolic links that points to the same block devices as well:


<pre>

$ ls -l /dev | grep vgdata1vg
drwxr-xr-x. 2 root root           80 May 22 22:31 vgdata1vg
$ ls -l /dev/vgdata1vg
total 0
lrwxrwxrwx. 1 root root 7 May 22 22:30 lvmyfirstlv -> ../dm-2
lrwxrwxrwx. 1 root root 7 May 22 22:31 lvmysecondlv -> ../dm-3

</pre>


As you can see we have 2 sets of symbolic links pointing to the same block files. The naming convention this time is:

<pre>/dev/{vg-name}/{lv-name}</pre>

These symbolic links are designed to make our lives easier when we want to refer to a particular LV. These symbolic links becomes useful when we come to installing filesystems and mounting our LVs. 

It doesn't matter which symbolic links you use, as long as you are comfortable with them. 


<h2>Install file system onto logical volumes</h2>

Just before we start installing file system's let's take a look at the current status:



<pre>
lsblk
NAME                       MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
sda                          8:0    0    20G  0 disk
├─sda1                       8:1    0   500M  0 part /boot
└─sda2                       8:2    0  19.5G  0 part
  ├─centos-swap            253:0    0     2G  0 lvm  [SWAP]
  └─centos-root            253:1    0  17.5G  0 lvm  /
sdb                          8:16   0   2.1G  0 disk
├─sdb1                       8:17   0   1.1G  0 part
├─sdb2                       8:18   0   500M  0 part
│ └─vgdata1vg-lvmyfirstlv  253:2    0   300M  0 lvm
└─sdb3                       8:19   0 503.6M  0 part
  └─vgdata1vg-lvmysecondlv 253:3    0   400M  0 lvm
sr0                         11:0    1  55.4M  0 rom
[root@localhost /]# blkid
/dev/sr0: UUID="2015-03-02-14-24-20-00" LABEL="VBOXADDITIONS_4.3.24_98716" TYPE="iso9660"
/dev/sda1: UUID="547c78ce-34b5-4e31-84d2-bf72b196fa57" TYPE="xfs"
/dev/sda2: UUID="JnfbeO-XPAG-AS0O-Mrtw-hnwn-7hqf-Mhk46t" TYPE="LVM2_member"
/dev/mapper/centos-swap: UUID="94263d84-5357-4566-9987-fc74ca946a96" TYPE="swap"
/dev/mapper/centos-root: UUID="17f09dae-88d1-4813-9738-8996819cf6ba" TYPE="xfs"
/dev/sdb2: UUID="rWS5kI-EvQy-C1yO-S1C2-QJeP-i52B-XhNRZ0" TYPE="LVM2_member"
/dev/sdb3: UUID="iwFLr7-fDHl-wdnT-mwKU-FSKq-xdEH-GcrnGD" TYPE="LVM2_member"
</pre>

Now let's install the ext4 filesystem on the first LV:


<pre>
$  mkfs.ext4 /dev/vgdata1vg/lvmyfirstlv
mke2fs 1.42.9 (28-Dec-2013)
Filesystem label=
OS type: Linux
Block size=1024 (log=0)
Fragment size=1024 (log=0)
Stride=0 blocks, Stripe width=0 blocks
76912 inodes, 307200 blocks
15360 blocks (5.00%) reserved for the super user
First data block=1
Maximum filesystem blocks=33947648
38 block groups
8192 blocks per group, 8192 fragments per group
2024 inodes per group
Superblock backups stored on blocks:
        8193, 24577, 40961, 57345, 73729, 204801, 221185

Allocating group tables: done
Writing inode tables: done
Creating journal (8192 blocks): done
Writing superblocks and filesystem accounting information: done

</pre>

Now lets install the xfs filesystem on the second LV:


<pre>
$ mkfs.xfs /dev/mapper/vgdata1vg-lvmysecondlv
meta-data=/dev/mapper/vgdata1vg-lvmysecondlv isize=256    agcount=4, agsize=25600 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=0        finobt=0
data     =                       bsize=4096   blocks=102400, imaxpct=25
         =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0 ftype=0
log      =internal log           bsize=4096   blocks=853, version=2
         =                       sectsz=512   sunit=0 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0

</pre>


Now let's recheck the status:


<pre>
$ lsblk
NAME                       MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
sda                          8:0    0    20G  0 disk
├─sda1                       8:1    0   500M  0 part /boot
└─sda2                       8:2    0  19.5G  0 part
  ├─centos-swap            253:0    0     2G  0 lvm  [SWAP]
  └─centos-root            253:1    0  17.5G  0 lvm  /
sdb                          8:16   0   2.1G  0 disk
├─sdb1                       8:17   0   1.1G  0 part
├─sdb2                       8:18   0   500M  0 part
│ └─vgdata1vg-lvmyfirstlv  253:2    0   300M  0 lvm
└─sdb3                       8:19   0 503.6M  0 part
  └─vgdata1vg-lvmysecondlv 253:3    0   400M  0 lvm
sr0                         11:0    1  55.4M  0 rom
$ blkid
/dev/sr0: UUID="2015-03-02-14-24-20-00" LABEL="VBOXADDITIONS_4.3.24_98716" TYPE="iso9660"
/dev/sda1: UUID="547c78ce-34b5-4e31-84d2-bf72b196fa57" TYPE="xfs"
/dev/sda2: UUID="JnfbeO-XPAG-AS0O-Mrtw-hnwn-7hqf-Mhk46t" TYPE="LVM2_member"
/dev/mapper/centos-swap: UUID="94263d84-5357-4566-9987-fc74ca946a96" TYPE="swap"
/dev/mapper/centos-root: UUID="17f09dae-88d1-4813-9738-8996819cf6ba" TYPE="xfs"
/dev/sdb2: UUID="rWS5kI-EvQy-C1yO-S1C2-QJeP-i52B-XhNRZ0" TYPE="LVM2_member"
/dev/sdb3: UUID="iwFLr7-fDHl-wdnT-mwKU-FSKq-xdEH-GcrnGD" TYPE="LVM2_member"
<strong>/dev/mapper/vgdata1vg-lvmyfirstlv: UUID="c35e024a-fd04-45de-afc8-566c634593cd" TYPE="ext4"
/dev/mapper/vgdata1vg-lvmysecondlv: UUID="02e140d7-0323-40d2-b7ee-5728f06305e3" TYPE="xfs"
</strong>
</pre>

As you can see, our new block devices (which are the file representation of our logical volumes) are now listed because they have filesystems installed on them. 



<h2>Mount logical volumes</h2>
Now that our logical volumes are ready to be mounted. First let's create out mountpoints:

<pre>
$ mkdir /tmp/lvdata1 /tmp/lvdata2
</pre>

Now we can either mount them manually, or automount them by adding the necessary entry in the /etc/fstab file. 
In this demo we'll manually mount the first LV and automount the second LV. 

Let's first do the manual mount:



<pre>
$ lsblk
NAME                       MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
sda                          8:0    0    20G  0 disk
├─sda1                       8:1    0   500M  0 part /boot
└─sda2                       8:2    0  19.5G  0 part
  ├─centos-swap            253:0    0     2G  0 lvm  [SWAP]
  └─centos-root            253:1    0  17.5G  0 lvm  /
sdb                          8:16   0   2.1G  0 disk
├─sdb1                       8:17   0   1.1G  0 part
├─sdb2                       8:18   0   500M  0 part
│ └─vgdata1vg-lvmyfirstlv  253:2    0   300M  0 lvm
└─sdb3                       8:19   0 503.6M  0 part
  └─vgdata1vg-lvmysecondlv 253:3    0   400M  0 lvm
sr0                         11:0    1  55.4M  0 rom

$ mount /dev/vgdata1vg/lvmyfirstlv  /tmp/lvdata1

$ lsblk
NAME                       MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
sda                          8:0    0    20G  0 disk
├─sda1                       8:1    0   500M  0 part /boot
└─sda2                       8:2    0  19.5G  0 part
  ├─centos-swap            253:0    0     2G  0 lvm  [SWAP]
  └─centos-root            253:1    0  17.5G  0 lvm  /
sdb                          8:16   0   2.1G  0 disk
├─sdb1                       8:17   0   1.1G  0 part
├─sdb2                       8:18   0   500M  0 part
<strong>│ └─vgdata1vg-lvmyfirstlv  253:2    0   300M  0 lvm  /tmp/lvdata1</strong>      # success
└─sdb3                       8:19   0 503.6M  0 part
  └─vgdata1vg-lvmysecondlv 253:3    0   400M  0 lvm
sr0                         11:0    1  55.4M  0 rom


</pre>

Another check we can do is:


<pre>
df -h
Filesystem                         Size  Used Avail Use% Mounted on
/dev/mapper/centos-root             18G  4.3G   14G  25% /
devtmpfs                           488M     0  488M   0% /dev
tmpfs                              497M   80K  497M   1% /dev/shm
tmpfs                              497M  7.0M  490M   2% /run
tmpfs                              497M     0  497M   0% /sys/fs/cgroup
/dev/sda1                          497M  143M  354M  29% /boot
<strong>/dev/mapper/vgdata1vg-lvmyfirstlv  283M  2.1M  262M   1% /tmp/lvdata1</strong>
</pre>

Now lets automate the mounting of the second LV by adding the entry in the /etc/fstab file:


<pre>
$ cat /etc/fstab

#
# /etc/fstab
# Created by anaconda on Sat Mar 14 19:14:11 2015
#
# Accessible filesystems, by reference, are maintained under '/dev/disk'
# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info
#
/dev/mapper/centos-root /                       xfs     defaults        1 1
UUID=547c78ce-34b5-4e31-84d2-bf72b196fa57 /boot                   xfs     defaults        1 2
/dev/mapper/centos-swap swap                    swap    defaults        0 0
<strong>/dev/mapper/vgdata1vg-lvmysecondlv   /tmp/lvdata2             xfs     defaults 0 0</strong>

</pre>

Now let's test our new entry by trying the automount:


<pre>
$ lsblk
NAME                       MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
sda                          8:0    0    20G  0 disk
├─sda1                       8:1    0   500M  0 part /boot
└─sda2                       8:2    0  19.5G  0 part
  ├─centos-swap            253:0    0     2G  0 lvm  [SWAP]
  └─centos-root            253:1    0  17.5G  0 lvm  /
sdb                          8:16   0   2.1G  0 disk
├─sdb1                       8:17   0   1.1G  0 part
├─sdb2                       8:18   0   500M  0 part
│ └─vgdata1vg-lvmyfirstlv  253:2    0   300M  0 lvm  /tmp/lvdata1
└─sdb3                       8:19   0 503.6M  0 part
  └─vgdata1vg-lvmysecondlv 253:3    0   400M  0 lvm
sr0                         11:0    1  55.4M  0 rom

$ mount -a

$ lsblk
NAME                       MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
sda                          8:0    0    20G  0 disk
├─sda1                       8:1    0   500M  0 part /boot
└─sda2                       8:2    0  19.5G  0 part
  ├─centos-swap            253:0    0     2G  0 lvm  [SWAP]
  └─centos-root            253:1    0  17.5G  0 lvm  /
sdb                          8:16   0   2.1G  0 disk
├─sdb1                       8:17   0   1.1G  0 part
├─sdb2                       8:18   0   500M  0 part
│ └─vgdata1vg-lvmyfirstlv  253:2    0   300M  0 lvm  /tmp/lvdata1
└─sdb3                       8:19   0 503.6M  0 part
  └─vgdata1vg-lvmysecondlv 253:3    0   400M  0 lvm  /tmp/lvdata2
sr0                         11:0    1  55.4M  0 rom
</pre>

We can confirm that this has worked using the mount, df, and the blkid commands:

<pre>
$ df -h
Filesystem                          Size  Used Avail Use% Mounted on
/dev/mapper/centos-root              18G  4.3G   14G  25% /
devtmpfs                            488M     0  488M   0% /dev
tmpfs                               497M   80K  497M   1% /dev/shm
tmpfs                               497M  7.0M  490M   2% /run
tmpfs                               497M     0  497M   0% /sys/fs/cgroup
/dev/sda1                           497M  143M  354M  29% /boot
/dev/mapper/vgdata1vg-lvmyfirstlv   283M  2.1M  262M   1% /tmp/lvdata1
<strong>/dev/mapper/vgdata1vg-lvmysecondlv  397M   21M  377M   6% /tmp/lvdata2
</strong>


$ blkid | grep "vgdata1vg"
/dev/mapper/vgdata1vg-lvmyfirstlv: UUID="c35e024a-fd04-45de-afc8-566c634593cd" TYPE="ext4"
/dev/mapper/vgdata1vg-lvmysecondlv: UUID="02e140d7-0323-40d2-b7ee-5728f06305e3" TYPE="xfs"



$ mount | grep "^/dev"
/dev/mapper/centos-root on / type xfs (rw,relatime,seclabel,attr2,inode64,noquota)
/dev/sda1 on /boot type xfs (rw,relatime,seclabel,attr2,inode64,noquota)
<strong>/dev/mapper/vgdata1vg-lvmyfirstlv on /tmp/lvdata1 type ext4 (rw,relatime,seclabel,data=ordered)
/dev/mapper/vgdata1vg-lvmysecondlv on /tmp/lvdata2 type xfs (rw,relatime,seclabel,attr2,inode64,noquota)
</strong>

</pre>


Now if we did go ahead and rebooted our machine then only our 2nd LV would get automounted since there is an entry for it in the /etc/fstab file. 
]]></Content>
		<Date><![CDATA[2015-05-22]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - LVM names and the Device Mapper]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What is the command to list all device mapper devices?"]
$ ls -l /dev | grep "dm-"
[/toggle]
[toggle title="What is the command to list more symbolic links with more meaningful names for the generic dm-* block files?"]
$ ls -l /dev/mapper/

[/toggle]
[toggle title="What is the command to view the alternative symbolic names for lv devices that are created from the vg called 'vgdata1vg'?"]
$ ls -l /dev/vgdata1vg/

[/toggle]
[/accordion]

<hr/>


When it comes to storage devices, The core linux kernal is self contained and doesn't have a direct understanding of LVMs. This means that the kernel can understand things like sda1 and sda2, but not logical volumes. That's why a software layer called "Device Mapper" has been introduced which bridges the this gap between the kernal and LVs. The device mapper allows the kernel to interact with a new type of devices with the naming convention /dev/dm-0, /dev/dm-1, /dev/dm-2...etc. These are "device mapper" devices, and in this case are block files that are used to represent LVs:


<pre>
$ ls -l /dev | grep "dm-"
brw-rw----. 1 root disk    253,   0 May 23 10:59 dm-0
brw-rw----. 1 root disk    253,   1 May 23 10:59 dm-1
brw-rw----. 1 root disk    253,   2 May 23 10:59 dm-2
brw-rw----. 1 root disk    253,   3 May 23 10:59 dm-3
</pre>  

These dm-x block files are created while the system is booting up. That means that these block devices can represent different LVs the next time the machine boots, which is a bit like the same issue we have when dealing with partitions, sda1, sda2,..etc. Furthermore, the names dm-1, dm-2,...etc are quite generic names and don't actually tell which device is which. That's why the Device Mapper offers a more user friendly approach for referring to these block devices, and that is through the use of symbolic links which have more meaningful names. You can find these symbolic links under <code>/dev/mapper</code>, at the moment we have:


<pre>
$ ls -l /dev/mapper/
total 0
lrwxrwxrwx. 1 root root       7 May 23 10:59 centos-root -> ../dm-1
lrwxrwxrwx. 1 root root       7 May 23 10:59 centos-swap -> ../dm-0
crw-------. 1 root root 10, 236 May 23 10:59 control
<strong>lrwxrwxrwx. 1 root root       7 May 23 10:59 vgdata1vg-lvmyfirstlv -> ../dm-2
lrwxrwxrwx. 1 root root       7 May 23 10:59 vgdata1vg-lvmysecondlv -> ../dm-3
</strong>

</pre>

As you can see, the naming convention of a symbolic link that is pointing to the corresponding block file is: {vg-name}-{lv-name}. This approach is makes it much more meaningful and easier for us to work with. 

That's why, it is best practice to prefixes volume group names with "vg", and logical volume names with "lv", so that it makes it even easier to keep track of things. 

These symbolic links are also created at boot time, and the device mapper automatically resolves the symbolic links to the correct dm-x device so that we don't have to worry about them. 


LVM also creates it's own set of symbolic links, in case you don't want to use the Device Manager generated symbolic links. These symbolic links have the structure of <code>/dev/{vg-name}/{lv-name}</code>, so in our example the following folder exists that house the following symbolic links:


<pre>
$ ls -l /dev/vgdata1vg/
total 0
lrwxrwxrwx. 1 root root 7 May 23 10:59 lvmyfirstlv -> ../dm-2
lrwxrwxrwx. 1 root root 7 May 23 10:59 lvmysecondlv -> ../dm-3
</pre>


It really comes to personal preference on which set of symbolic links you want to use when refering to your Logical Volumes. 

One final thing to add is that the device mapper doesn't just handle LVM related stuff, it also handles a lot more, e.g. LUKS encrypted partitions, RAID setups,...etc. Hence the /dev/mapper directly can end up containing a lot of symbolic links. 
 


]]></Content>
		<Date><![CDATA[2015-05-23]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Extending and shrinking a Logical Volume]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

In the following set of questions let's assume we have a logical volume called 'lvmyfirstlv' that has been created from the volume group called 'vgdata1vg'.

[accordion]
[toggle title="What is the command to check what volume group the logical volume 'lvmyfirstlv', has been created from?"]
$ lvs
[/toggle]
[toggle title="What is the command to check how much available disk space each of your volume group have?"]
$ vgs
[/toggle]
[toggle title="What is the command to make lvmyfirstlv 100MB bigger using available space from the volugme group, 'vgdata1vg'?"]
$ lvextend /dev/vgdata1vg/lvmyfirstlv -L +100M
[/toggle]
[toggle title="What is the command to add all the available space on vgdata1vg to lvmyfirstlv?"]
$ lvextend -l +100%FREE /dev/vgdata1vg/lvmyfirstlv
[/toggle]
[toggle title="What 2 commands can you run to check that the lvextend command has been successful?"]
$ lvs
# and
$ vgs
[/toggle]
[toggle title="What 2 command can you run to check what filesystem is installed on the logical volume, lvmyfirstlv?"]
$ mount | grep lvmyfirstlv
# or
$ blkid | grep lvmyfirstlv
[/toggle]
[toggle title="What is the command to make the ext4 based logical volume, lvmyfirstlv, aware of the extra space (that has been allocated from the volume group, vgdata1vg) and make use of it?"]
$ resize2fs /dev/vgdata1vg/lvmyfirstlv
[/toggle]
[toggle title="If however lvmyfirstlv has an xfs filesystem. then what command do you need to run instead of the above one?"]
$ xfs_growfs {lvmyfirstlv-mountpoint}
[/toggle]
[toggle title="How can you avoid using resize2fs, grow_fs,....etc?"]
Use the "-r" option when using the lvextend command:
$ lvextend -l +100%FREE -r /dev/vgdata1vg/lvmyfirstlv
[/toggle]
[toggle title="What command can you run to check if the filesystem resizing has been successful?"]
$ df -h
[/toggle]
[toggle title="What type of filesystem isn't possible to shrink?"]
The xfs filesystem. The workaround is to create a smaller lv with xfs on it, copy across the data, from the soon-to-be-decommisioned filesystem. Then delete the old file system. 
[/toggle]
[toggle title="What is the command that lets you check by how much you can shrink a logical volume, based on unused space??"]
$ df -h
[/toggle]
[toggle title="What is the command to shrink the filesystem that's on the logical volume, lvmyfirstlv, to 350MB (assuming this logical volume was created from the volume group called 'vgdata1vg')?"]
$ resize2fs /dev/vgdata1vg/lvmyfirstlv 350M
[/toggle]
[toggle title="What do you need to take note of after running the above command?"]
The number of blocks that new filesystem is made up of. 
[/toggle]
[toggle title="Assuming that the new filesystem is 358400 blocks in size, then what command do you run to reduce the lvmyfirstlv logical volume to this size?"]
$ lvreduce -L 358400K /dev/vgdata1vg/lvmyfirstlv
[/toggle]
[toggle title="What command can you run to avoid using the resize2fs command, and also shrink lvmyfirstlv in a single command?"]
$ lvreduce -rL 350M /dev/vgdata1vg/lvmyfirstlv
[/toggle]
[/accordion]

<hr/>






From time to time you will often want to resize your logical volume. For example you may want to increase an logical volume's capacity because it's running out of disk space. Or you might want to shrink a 10TB LV to 1GB because you have discovered that your logical volume will never need more than 800MB during it's lifetime. 


When re-sizing your LV, you have to always be mindful of the filesystem that exists on it already. During a filesystem install on a logical volume, that whole logical volume gets scanned and indexed. Consequently the filesystem will set up a boundary that occupies the entire logical volume's diskspace. Based on this, we have to do things differently depending on whether we want to extend or shrink a logical volume. 


<h2>Extending a logical volume</h2>

Before you can extend the logical volume, you need to first check whether there is enough free capacity in the volume group. If so then we can go ahead and extend it. 


If not, then we need to free-up/extend the volume group first, either by deleting/shrinking other logical volumes that this volume group provides, or by extending the volume group by allocating more physical volumes to it. 



Let's say we want to increase the size of the logical volume, lvmyfirstlv, by 100MB. 

Let's first see how big it is currently:

<pre>
$ df -h
Filesystem                          Size  Used Avail Use% Mounted on
/dev/mapper/centos-root              18G  4.3G   14G  25% /
devtmpfs                            488M     0  488M   0% /dev
tmpfs                               497M   80K  497M   1% /dev/shm
tmpfs                               497M   14M  484M   3% /run
tmpfs                               497M     0  497M   0% /sys/fs/cgroup
/dev/sda1                           497M  143M  354M  29% /boot
<strong>/dev/mapper/vgdata1vg-lvmyfirstlv   283M  2.1M  262M   1% /tmp/lvdata1</strong>
/dev/mapper/vgdata1vg-lvmysecondlv  397M   21M  377M   6% /tmp/lvdata2
</pre>

   
At the moment it is 283MB. Now let's see which volume group that lvmyfirstlv has been created from:


<pre>
$ lvs
  LV           VG        Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  root         centos    -wi-ao----  17.51g
  swap         centos    -wi-ao----   2.00g
  lvmyfirstlv  <strong>vgdata1vg</strong> -wi-ao---- 300.00m
  lvmysecondlv vgdata1vg -wi-ao---- 400.00m
</pre>

Note, the 283MB and 300MB discrepency is caused by LVM using some of the disk space for it's internal workings. 

Now let's see if vgdata1vg has 100MB or more spare capacity:

<pre>
$ vgs
  VG        #PV #LV #SN Attr   VSize   VFree
  centos      1   2   0 wz--n-  19.51g      0
  vgdata1vg   2   2   0 wz--n- 996.00m <strong>296.00m</strong>
</pre>

Here we can see that vgdata1vg has 296MB available which is more than enough. This means that we don't need to find any more free space for vgdata1vg, we can just use the existing free space. 

We can extend lvmyfirstlv using the lvextend command:


<pre>
$ lvextend --help
  lvextend: Add space to a logical volume

lvextend
        [-A|--autobackup y|n]
        [--alloc AllocationPolicy]
        [--commandprofile ProfileName]
        [-d|--debug]
        [-f|--force]
        [-h|--help]
        [-i|--stripes Stripes [-I|--stripesize StripeSize]]
        {-l|--extents [+]LogicalExtentsNumber[%{VG|LV|PVS|FREE|ORIGIN}] |
<strong>         -L|--size [+]LogicalVolumeSize[bBsSkKmMgGtTpPeE]}
</strong>         --poolmetadatasize [+]MetadataVolumeSize[bBsSkKmMgG]}
        [-m|--mirrors Mirrors]
        [--nosync]
        [--use-policies]
        [-n|--nofsck]
        [--noudevsync]
        [-r|--resizefs]
        [-t|--test]
        [--type VolumeType]
        [-v|--verbose]
        [--version]
<strong>        LogicalVolume[Path] [ PhysicalVolumePath... ]
</strong></pre> 

Based on the help, we do:


<pre>
$ lvextend /dev/vgdata1vg/lvmyfirstlv -L +100M
  Size of logical volume vgdata1vg/lvmyfirstlv changed from 300.00 MiB (75 extents) to 400.00 MiB (100 extents).
  Logical volume lvmyfirstlv successfully resized
</pre>


Now let's check if this has been successful:

<pre>
$ lvs
  LV           VG        Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  root         centos    -wi-ao----  17.51g
  swap         centos    -wi-ao----   2.00g
  <strong>lvmyfirstlv  vgdata1vg -wi-ao---- 400.00m</strong>
  lvmysecondlv vgdata1vg -wi-ao---- 400.00m
</pre>

As you can see, it was 300MB before, but now it is 400MB. Also the available space on vgdata1vg has gone down by 100MB:

<pre>
$ vgs
  VG        #PV #LV #SN Attr   VSize   VFree
  centos      1   2   0 wz--n-  19.51g      0
  vgdata1vg   2   2   0 wz--n- 996.00m 196.00m
</pre>

However if we now take a look at df (disk free) command:


<pre>
$ df -h
Filesystem                          Size  Used Avail Use% Mounted on
/dev/mapper/centos-root              18G  4.3G   14G  25% /
devtmpfs                            488M     0  488M   0% /dev
tmpfs                               497M   80K  497M   1% /dev/shm
tmpfs                               497M   14M  484M   3% /run
tmpfs                               497M     0  497M   0% /sys/fs/cgroup
/dev/sda1                           497M  143M  354M  29% /boot
/dev/mapper/vgdata1vg-lvmyfirstlv   <strong>283M</strong>  2.1M  262M   1% /tmp/lvdata1
/dev/mapper/vgdata1vg-lvmysecondlv  397M   21M  377M   6% /tmp/lvdata2


</pre>

You'll see that this hasn't change, it is still showing the same disk size as before. 

That's because the filesystem installed on this logical volume hasn't been made aware of the new disk space. So to make it aware of it, we need to make the filesystem reindex the newly added space, which we do by running the "resize2fs" command:


<pre>
$ whatis resize2fs
resize2fs (8)        - ext2/ext3/ext4 file system resizer
[root@localhost ~]# resize2fs -help
resize2fs 1.42.9 (28-Dec-2013)
$ [-d debug_flags] [-f] [-F] [-M] [-P] [-p] device [new_size]
</pre>


Let's use this command now:


<pre>
$ resize2fs /dev/vgdata1vg/lvmyfirstlv
resize2fs 1.42.9 (28-Dec-2013)
Filesystem at /dev/vgdata1vg/lvmyfirstlv is mounted on /tmp/lvdata1; on-line resizing required
old_desc_blocks = 3, new_desc_blocks = 4
The filesystem on /dev/vgdata1vg/lvmyfirstlv is now 409600 blocks long.

</pre>

Note: if you're dealing with an xfs file system then you would replace "resize2fs" with <strong>xfs_growfs</strong>


Now let's check that this has worked:


<pre>
$ df -h
Filesystem                          Size  Used Avail Use% Mounted on
/dev/mapper/centos-root              18G  4.3G   14G  25% /
devtmpfs                            488M     0  488M   0% /dev
tmpfs                               497M   80K  497M   1% /dev/shm
tmpfs                               497M   14M  484M   3% /run
tmpfs                               497M     0  497M   0% /sys/fs/cgroup
/dev/sda1                           497M  143M  354M  29% /boot
<strong>/dev/mapper/vgdata1vg-lvmyfirstlv   380M  2.3M  355M   1% /tmp/lvdata1</strong>
/dev/mapper/vgdata1vg-lvmysecondlv  397M   21M  377M   6% /tmp/lvdata2

</pre>

As you can see the disk size is now about 100MB bigger. 


<h3>Handy lvextend options</h3> 

There are a few lvextend options that can come in very handy. First off, if you want to assign all the available free space in a volume group to a logical volume, then you can do this with the the "-l" option, like this:


<pre>
$ lvextend  -l +100%FREE /dev/vgdata1vg/lvmyfirstlv
</pre>

Also you can do the filesystem reindex as part of the lvextend command, using the -r option:

<pre>
$ lvextend -l +100%FREE -r /dev/vgdata1vg/lvmyfirstlv 
</pre>

This is handy because you don't have to worry about memorise all the various filesystem rescan commands such as resize2fs for ext2/ext3/ext4 and grow_fs for xfs,....and etc. 



<h2>Shrinking a logical volume</h2>

If you have a 400MB LV with a file system installed on it. Then that file system has also indexed the whole 400MB disk space even though not all of that disk space is being used. Let's say that disk currently holds only 150MB of data, and it's anticipated that 180MB max of diskspace will be used on this LV. In that case we will want to free up the unused disk space by shrinking the LV's disk space down to 180MB.

To do this we need to shrink the file system to 180MB first. That's because the 150MB of data that is currently stored in the lv, is actually spread across the whole 400MB LV. Hence we need to first reduce the file system so that the 150MB of data gets restacked into 180MB, so that the other 220MB is emptied out and no longer occupied by the filesystem.     

Note: It's not possible to reduce an xfs file system. 

That's why before you try to reduce a filesystem, you should always first check the type of filesystem you are trying to reduced. In my case I want to reduce the filesystem on the lv, lvmyfirstlv, by 50MB. 

First we need to check that there is at least 50MB of free space available on this file system, if there isn't then it wont be possible to shrink this filesystem by 50MB:

<pre>
$ df -h
Filesystem                          Size  Used Avail Use% Mounted on
/dev/mapper/centos-root              18G  4.3G   14G  25% /
devtmpfs                            488M     0  488M   0% /dev
tmpfs                               497M   80K  497M   1% /dev/shm
tmpfs                               497M   14M  484M   3% /run
tmpfs                               497M     0  497M   0% /sys/fs/cgroup
/dev/sda1                           497M  143M  354M  29% /boot
/dev/mapper/vgdata1vg-lvmyfirstlv   380M  2.3M  355M   1% /tmp/lvdata1
/dev/mapper/vgdata1vg-lvmysecondlv  461M   21M  441M   5% /tmp/lvdata2

</pre>

Here we have about 441MB free, so shrinking the lv by 50MB should be fine. 


Next we need to check what type of filesystem it is in order to determine if it is a type of file system that supports shrinking:


<pre>
$ mount | grep "^/dev/mapper"
/dev/mapper/centos-root on / type xfs (rw,relatime,seclabel,attr2,inode64,noquota)
/dev/mapper/vgdata1vg-lvmyfirstlv on /tmp/lvdata1 type ext4 (rw,relatime,seclabel,data=ordered)
/dev/mapper/vgdata1vg-lvmysecondlv on /tmp/lvdata2 type xfs (rw,relatime,seclabel,attr2,inode64,noquota)
</pre>

Here we can see it is an ext4 filesystem so we should be able to shrink this. If this however was an xfs file system, then we would have been stuck and would not have been able to shrink the filesystem. In that situation, an alternative approach would be have been to create a new (smaller) LV with xfs installed on it. copy across all the data from the original LV. Then destroy the old LV, since you can't shrink it's filesystem. 

    
Next we need to unmount the LV, since it's not possible to shrink the filesystem while it is use:


<pre>
$ umount /dev/mapper/vgdata1vg-lvmyfirstlv
</pre>


Let's confirm that it has been unmounted:


<pre>
$ lsblk
NAME                       MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
sda                          8:0    0    20G  0 disk
├─sda1                       8:1    0   500M  0 part /boot
└─sda2                       8:2    0  19.5G  0 part
  ├─centos-swap            253:0    0     2G  0 lvm  [SWAP]
  └─centos-root            253:1    0  17.5G  0 lvm  /
sdb                          8:16   0   2.1G  0 disk
├─sdb1                       8:17   0   1.1G  0 part
├─sdb2                       8:18   0   500M  0 part
<strong>│ └─vgdata1vg-lvmyfirstlv  253:2    0   400M  0 lvm</strong>
└─sdb3                       8:19   0 503.6M  0 part
  └─vgdata1vg-lvmysecondlv 253:3    0   464M  0 lvm  /tmp/lvdata2
sr0                         11:0    1  55.4M  0 rom
</pre>


Now if reduce the LV's size by 50MB, then the new size will be 350MB. We now use the resize2fs command, lets first look at it's usage info:


<pre>
$ resize2fs -help
resize2fs 1.42.9 (28-Dec-2013)
Usage: resize2fs [-d debug_flags] [-f] [-F] [-M] [-P] [-p] device [new_size]
</pre>

Based on the help info, we do:

<pre>
$ resize2fs /dev/vgdata1vg/lvmyfirstlv 350M
resize2fs 1.42.9 (28-Dec-2013)
Please run 'e2fsck -f /dev/vgdata1vg/lvmyfirstlv' first.
</pre>
 
Here we are prompted to check the filesystem's integrity first before we do the resize, so let's do as prompted:


<pre>
$ e2fsck -f /dev/vgdata1vg/lvmyfirstlv
e2fsck 1.42.9 (28-Dec-2013)
Pass 1: Checking inodes, blocks, and sizes
Pass 2: Checking directory structure
Pass 3: Checking directory connectivity
Pass 4: Checking reference counts
Pass 5: Checking group summary information
/dev/vgdata1vg/lvmyfirstlv: 11/101200 files (0.0% non-contiguous), 23297/409600 blocks
</pre>

Now let's try the resize2fs command again:


<pre>
$ resize2fs /dev/vgdata1vg/lvmyfirstlv 350M
resize2fs 1.42.9 (28-Dec-2013)
Resizing the filesystem on /dev/vgdata1vg/lvmyfirstlv to 358400 (1k) blocks.
The filesystem on /dev/vgdata1vg/lvmyfirstlv is now 358400 blocks long.

</pre>
  
Note here that it states the new size in terms of kilobyte blocks. We use this when we shrink the logical volume. 

To shrink the lv, we use the lvreduce command:


<pre>
$ lvreduce --help
  lvreduce: Reduce the size of a logical volume

lvreduce
        [-A|--autobackup y|n]
        [--commandprofile ProfileName]
        [-d|--debug]
        [-f|--force]
        [-h|--help]
        {-l|--extents [-]LogicalExtentsNumber[%{VG|LV|FREE|ORIGIN}] |
         -L|--size [-]LogicalVolumeSize[bBsSkKmMgGtTpPeE]}
        [-n|--nofsck]
        [--noudevsync]
        [-r|--resizefs]
        [-t|--test]
        [-v|--verbose]
        [--version]
        [-y|--yes]
        LogicalVolume[Path]


</pre> 


Based on the help info, we run the following command:


<pre>
$ lvreduce -L 358400K /dev/vgdata1vg/lvmyfirstlv
  Rounding size to boundary between physical extents: 352.00 MiB
  WARNING: Reducing active logical volume to 352.00 MiB
  THIS MAY DESTROY YOUR DATA (filesystem etc.)
Do you really want to reduce lvmyfirstlv? [y/n]: y
  Size of logical volume vgdata1vg/lvmyfirstlv changed from 400.00 MiB (100 extents) to 352.00 MiB (88 extents).
  Logical volume lvmyfirstlv successfully resized
</pre>


Now let's confirm that this has worked:



<pre>
$  lsblk
NAME                       MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
sda                          8:0    0    20G  0 disk
├─sda1                       8:1    0   500M  0 part /boot
└─sda2                       8:2    0  19.5G  0 part
  ├─centos-swap            253:0    0     2G  0 lvm  [SWAP]
  └─centos-root            253:1    0  17.5G  0 lvm  /
sdb                          8:16   0   2.1G  0 disk
├─sdb1                       8:17   0   1.1G  0 part
├─sdb2                       8:18   0   500M  0 part
<strong>│ └─vgdata1vg-lvmyfirstlv  253:2    0   352M  0 lvm
</strong>└─sdb3                       8:19   0 503.6M  0 part
  └─vgdata1vg-lvmysecondlv 253:3    0   464M  0 lvm  /tmp/lvdata2
sr0                         11:0    1  55.4M  0 rom
</pre>

Now let's mount the lv again:


<pre>
$  mount /dev/vgdata1vg/lvmyfirstlv /tmp/lvdata1
</pre>

Now let's check if this has worked:


<pre>
$ df -h
Filesystem                          Size  Used Avail Use% Mounted on
/dev/mapper/centos-root              18G  4.3G   14G  25% /
devtmpfs                            488M     0  488M   0% /dev
tmpfs                               497M   80K  497M   1% /dev/shm
tmpfs                               497M   14M  484M   3% /run
tmpfs                               497M     0  497M   0% /sys/fs/cgroup
/dev/sda1                           497M  143M  354M  29% /boot
/dev/mapper/vgdata1vg-lvmysecondlv  461M   21M  441M   5% /tmp/lvdata2
<strong>/dev/mapper/vgdata1vg-lvmyfirstlv   332M  2.1M  309M   1% /tmp/lvdata1</strong>

</pre>

Success. 


<h3>Handy lvreduce options</h3> 

In a similar vain to lvextend, lvreduce also has a "-r" option for autoresizing the filesystem for us, which means that we don't need to bother with using the resize2fs command. But we still have to unmount the LV before we use the lvreduce command. So in this approach we therefore would run the following command: 


<pre>
$ lvreduce -rL 350M /dev/vgdata1vg/lvmyfirstlv
</pre>]]></Content>
		<Date><![CDATA[2015-05-23]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Linux Kernal Overview]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What is the command to display the version of the kernel running?"]
$ uname -r
[/toggle]
[toggle title="Where are the kernel modules stored?"]
/lib/modules
[/toggle]
[toggle title="What is the command to list all the available kernel files?"]
$ ls -lh /boot | grep vmlinuz
[/toggle]
[toggle title="Which top level folder does the kernel creates during bootup?"]
- /proc  (houses running process's data)
- /sys   (houses system device drivers)
- /dev   (houses file representation of all your devices)
[/toggle]
[toggle title="What is the command to view help info about your high level folders?"]
$ man hier
[/toggle]
[/accordion]

<hr/>


The Linux kernel is the core of your OS. You can find out what is your kernel's version using the uname command:


<pre>
$ uname -r
3.10.0-229.14.1.el7.x86_64
</pre>


As a safety measure, never update an existing kernel, just install a new one. Yum is smart enough to do this for you.

The linux kernel is a<a href="http://www.thegeekstuff.com/2012/01/linux-unix-kernel/">monolithic kernel</a>.

The Linux kernel loads device drivers and filesystem drivers from <em>kernel modules</em>. These modules are loaded dynamically, as and when needed. These kernel modules can be found in the <pre>/lib/modules</pre> directory:

<pre>
$ ls -l /lib/modules
total 8
drwxr-xr-x. 8 root root 4096 Sep 19 20:58 3.10.0-229.14.1.el7.x86_64
drwxr-xr-x. 7 root root 4096 Sep 16 19:31 3.10.0-229.el7.x86_64
</pre>

The kernel itself can be found in the <strong>/boot</strong> folder, here is an example of a kernel file:

<pre>
$ ls -lh /boot | grep vmlinuz
-rwxr-xr-x. 1 root root 4.8M Sep 16 19:31 vmlinuz-0-rescue-bb2262658ee64941afea091071b78f45
-rwxr-xr-x. 1 root root 4.8M Sep 15 16:14 vmlinuz-3.10.0-229.14.1.el7.x86_64
-rwxr-xr-x. 1 root root 4.8M Mar  6  2015 vmlinuz-3.10.0-229.el7.x86_64
</pre>


“vmlinuz” refers to the fact that the Linux kernel is stored in a compressed image file. This gets decompressed during bootup.

During boot-up, the kernel creates the following folders:

<ul>
	<li><code>/proc</code> - This contains running processes and any kernel parameters that can modify
</li>
	<li><code>/sys</code> - This contains the detected system devices and details about their drivers. However system devices are not accessible via this folder, they are accessible via the /dev folder.</li>
	<li><code>/dev</code> - This stores the access points for your devices</li>
</ul>

For help info about these high level directories as well as others, checkout:


<pre>
$ man hier
</pre>
]]></Content>
		<Date><![CDATA[2015-05-23]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Kernel Modules]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="True or False, the kernel can directly interact with all hardware?"]
False - it can only directly interact with the core hardware only. With remaining hardware, it interacts via kernel modules. 
[/toggle]
[toggle title="What is the command to list all modules that are currently being used?"]
$ lsmod
[/toggle]
[toggle title="What is the command list all kernel modules, and not just the active ones?"]
$ find /lib/modules/$(uname -r)/kernel -type f -iname '*.o' -or -iname '*.ko'
[/toggle]
[toggle title="What is the command to load in the 'bluetooth'?"]
$ modprobe -v bluetooth
[/toggle]
[toggle title="What is the unload the bluetooth module?"]
$ modprobe -rv bluetooth
[/toggle]
[toggle title="What is the command to view more info about the 'cdrom', kernel module, along with it's parameter/boolean settings?"]
$ modinfo cdrom
[/toggle]
[toggle title="What is the command to load in the 'cdrom' kernel module but this time with the 'autoclose' boolean setting enabled?"]
$ modprobe -v cdrom autoclose=1
[/toggle]
[toggle title="The above isn't persistant so what do you need to do make it persistant, (and also set the 'lockdoor' boolean setting to disabled)?"]
go to the follwing directory:
/lib/modprobe.d
Create a file with a meaningful name and add the following content:
$ cat /lib/modprobe.d/cdrom.conf
options cdrom autoclose=1 lockdoor=0
# then reboot the machine
[/toggle]
[toggle title="Where can you find more info about the modprobe.d folder?"]
$ man modprobe.d
[/toggle]
[toggle title="What is the command to check the 'autoclose' setting for the cdrom kernel module?"]
sysctl -a | grep cdrom | grep autoclose
[/toggle]
[/accordion]

<hr/>




The Linux kernel is the heart of the RHEL Operating System. You can use the uname command to find the kernel's version:


<pre>
$ uname -r
3.10.0-229.1.2.el7.x86_64
</pre>





In order for the kernel to perform it's duties, it has to interact with the machines hardware. The kernel has the ability to directly interact with most of the core hardware. However with other hardware, e.g. usb-pen, the kernel interacts with them via "kernel modules". That's so to keep the kernel as light weight as possible.  

We also have a service called udevd:


<pre>
$ systemctl status systemd-udevd -l
systemd-udevd.service - udev Kernel Device Manager
   Loaded: loaded (/usr/lib/systemd/system/systemd-udevd.service; static)
   Active: active (running) since Sun 2015-05-24 14:11:44 BST; 3h 57min ago
     Docs: man:systemd-udevd.service(8)
           man:udev(7)
 Main PID: 552 (systemd-udevd)
   CGroup: /system.slice/systemd-udevd.service
           └─552 /usr/lib/systemd/systemd-udevd

May 24 14:11:44 localhost.localdomain systemd[1]: Started udev Kernel Device Manager.
</pre> 

This service constantly monitors for any new hardware that is added to the machine (e.g. external usb hard drive), and soon as it detects the new hardware, it then notifies the kernel with the details of the new device, and the kernel then loads in the necessary kernel modules so that it can start interacting and working with the new hardware. 


You can use the lsmod to view a list of kernel modules that are currently in use:


<pre>
$ lsmod
Module                  Size  Used by
xt_CHECKSUM            12549  1
ipt_MASQUERADE         12678  3
nf_nat_masquerade_ipv4    13412  1 ipt_MASQUERADE
tun                    27183  1
ip6t_rpfilter          12546  1
ip6t_REJECT            12939  2
ipt_REJECT             12541  4
xt_conntrack           12760  8
ebtable_nat            12807  0
ebtable_broute         12731  0
bridge                115385  1 ebtable_broute
stp                    12976  1 bridge
llc                    14552  2 stp,bridge
ebtable_filter         12827  0
ebtables               30913  3 ebtable_broute,ebtable_nat,ebtable_filter
ip6table_nat           12864  1
nf_conntrack_ipv6      18738  5
nf_defrag_ipv6         34651  1 nf_conntrack_ipv6
nf_nat_ipv6            14131  1 ip6table_nat
ip6table_mangle        12700  1
ip6table_security      12710  1
ip6table_raw           12683  1
ip6table_filter        12815  1
ip6_tables             27025  5 ip6table_filter,ip6table_mangle,ip6table_security,ip6table_nat,ip6table_raw
iptable_nat            12875  1
nf_conntrack_ipv4      14862  5
nf_defrag_ipv4         12729  1 nf_conntrack_ipv4
nf_nat_ipv4            14115  1 iptable_nat
nf_nat                 26146  3 nf_nat_ipv4,nf_nat_ipv6,nf_nat_masquerade_ipv4
nf_conntrack          105702  7 nf_nat,nf_nat_ipv4,nf_nat_ipv6,xt_conntrack,nf_nat_masquerade_ipv4,nf_conntrack_ipv4,nf_conntrack_ipv6
iptable_mangle         12695  1
iptable_security       12705  1
iptable_raw            12678  1
iptable_filter         12810  1
ip_tables              27239  5 iptable_security,iptable_filter,iptable_mangle,iptable_nat,iptable_raw
ppdev                  17671  0
snd_intel8x0           38274  3
snd_ac97_codec        130476  1 snd_intel8x0
ac97_bus               12730  1 snd_ac97_codec
snd_seq                63074  0
snd_seq_device         14497  1 snd_seq
snd_pcm               103996  2 snd_ac97_codec,snd_intel8x0
serio_raw              13462  0
pcspkr                 12718  0
snd_timer              29562  2 snd_pcm,snd_seq
snd                    75127  12 snd_ac97_codec,snd_intel8x0,snd_timer,snd_pcm,snd_seq,snd_seq_device
parport_pc             28165  0
parport                42348  2 ppdev,parport_pc
video                  19263  0
i2c_piix4              22106  0
i2c_core               40325  1 i2c_piix4
soundcore              15047  1 snd
nfsd                  290215  1
auth_rpcgss            59343  1 nfsd
nfs_acl                12837  1 nfsd
lockd                  93977  1 nfsd
uinput                 17625  0
sunrpc                295293  7 nfsd,auth_rpcgss,lockd,nfs_acl
xfs                   915019  3
libcrc32c              12644  1 xfs
sr_mod                 22416  0
cdrom                  42556  1 sr_mod
sd_mod                 45499  6
crc_t10dif             12714  1 sd_mod
crct10dif_common       12595  1 crc_t10dif
ata_generic            12910  0
pata_acpi              13038  0
ahci                   29870  4
libahci                32009  1 ahci
ata_piix               35038  0
e1000                 149270  0
libata                218854  5 ahci,pata_acpi,libahci,ata_generic,ata_piix
dm_mirror              22135  0
dm_region_hash         20862  1 dm_mirror
dm_log                 18411  2 dm_region_hash,dm_mirror
dm_mod                104038  13 dm_log,dm_mirror
</pre>


Unfortunately there isn't a command that lets you view a list of all modules (loaded and unloaded), therefore we resort to using the "find" command instead: 

<pre>
$ find /lib/modules/$(uname -r)/kernel -type f -iname '*.o' -or -iname '*.ko' | wc -l
2182
</pre>



You can also manually load kernel modules using the modprobe command, for example, let's say you want to load the blueotooth module, first let's check that the module is available for loading:


<pre>
$ find /lib/modules/$(uname -r)/kernel -type f -iname '*.o' -or -iname '*.ko' | grep "bluetooth.ko"
/lib/modules/3.10.0-229.1.2.el7.x86_64/kernel/drivers/platform/x86/toshiba_bluetooth.ko
/lib/modules/3.10.0-229.1.2.el7.x86_64/kernel/net/bluetooth/bluetooth.ko
</pre>

Now let's confirm that it isn't currently loaded:


<pre>
$ lsmod | grep "bluetooth"
</pre>


Now let's load the module, which is done using the modprobe command:


<pre>
$ modprobe -v bluetooth
insmod /lib/modules/3.10.0-229.1.2.el7.x86_64/kernel/net/rfkill/rfkill.ko
insmod /lib/modules/3.10.0-229.1.2.el7.x86_64/kernel/net/bluetooth/bluetooth.ko
$ lsmod | grep bluetooth
bluetooth             372662  0
rfkill                 26536  1 bluetooth
</pre>

Note, I used the (v)erbose to see what's happening. 

Here we can see that that no other modules are dependant on the bluetooth module (indicate by 0) but the bluetooth module is dependant on the rfkill module, which also got loaded behind the scenes. Only one module is dependent on the rfkill (indicated by the 1). 


To unload a module, you use the modprobe again but this time using the (r)emove switch:


<pre>
$ modprobe -rv bluetooth
rmmod bluetooth
rmmod rfkill
$ lsmod | grep bluetooth
</pre>

As you can see this also unloaded the rfkill module since it got loaded up as part of the bluetootch, and also because only the bluetooth module depended on the rfkill module. 


If you try to unload a module that other modules are dependent, then you'll get an error message like this:


<pre>
$  modprobe -rv cdrom
modprobe: FATAL: Module cdrom is in use.
</pre>

Now let's check it's dependencies, we see that:


<pre>
$ lsmod | grep cdrom
cdrom                  42556  1 sr_mod
</pre>

Here we see that the sr_mod modules depends on the cdrom module, if we take a look at the we see:


<pre>
$ lsmod | grep sr_mod
sr_mod                 22416  0
cdrom                  42556  1 sr_mod

</pre>

As you can see, no other modules depends on sr_mod, so we can try unloading the sr_mod module first:

<pre>
$ modprobe -rv sr_mod
rmmod sr_mod
rmmod cdrom
$ lsmod | grep sr_mod
$ lsmod | grep cdrom
</pre>
 
Since the cdrom was only being used via sr_mod, it also got unloaded behind the scenes as well.  

In reality it is very rare that you will ever need to manually load/unload kernel modules because the udevd service does a really good job doing this for you. 

<h2>Monitor the udev service's realtime logs</h2>

The main process underneath the udevd service is the udec

You can monitor in real time which modules are being loaded/unloaded using the udevadm command along with the "monitor" option. To do this, I opened up a separate terminal session to run the monitor the output while I ran "modprobe -rv sr_mod" command in my first terminal:

<pre>
$ udevadm monitor
monitor will print the received events for:
UDEV - the event which udev sends out after rule processing
KERNEL - the kernel uevent

KERNEL[70482.463906] remove   /devices/virtual/bdi/11:0 (bdi)
KERNEL[70482.468373] remove   /devices/pci0000:00/0000:00:01.1/ata2/host1/target1:0:0/1:0:0:0/block/sr0 (block)
KERNEL[70482.471845] remove   /bus/scsi/drivers/sr (drivers)
KERNEL[70482.471969] remove   /module/sr_mod (module)
UDEV  [70482.483339] remove   /devices/virtual/bdi/11:0 (bdi)
UDEV  [70482.486603] remove   /bus/scsi/drivers/sr (drivers)
UDEV  [70482.486662] remove   /module/sr_mod (module)
KERNEL[70482.496314] remove   /module/cdrom (module)
UDEV  [70482.503934] remove   /devices/pci0000:00/0000:00:01.1/ata2/host1/target1:0:0/1:0:0:0/block/sr0 (block)
UDEV  [70482.504962] remove   /module/cdrom (module)
</pre>

The above is the output from the udevd service. If I then attached a new device, e.g. usb-pen, then you can use the "udevadm monitor" to see what modules are being loaded in behind the scenes.  
 



<h2>Finding kernel module details and adjusting kernel module parameter settings</h2>
You can use the modinfo command to get more details about any kernel module, irrespective of whether they are currently loaded or not, for example:

<pre>
$ modinfo cdrom
filename:       /lib/modules/3.10.0-229.1.2.el7.x86_64/kernel/drivers/cdrom/cdrom.ko
license:        GPL
rhelversion:    7.1
srcversion:     EB46A7E87598E0DD56A115E
depends:
intree:         Y
vermagic:       3.10.0-229.1.2.el7.x86_64 SMP mod_unload modversions
signer:         CentOS Linux kernel signing key
sig_key:        34:B5:BC:A2:B7:06:D8:2E:72:A5:BE:3E:E4:09:BE:C7:19:5E:A5:08
sig_hashalgo:   sha256
parm:           debug:bool
parm:           autoclose:bool
parm:           autoeject:bool
parm:           lockdoor:bool
parm:           check_media_type:bool
parm:           mrw_format_restart:bool

</pre>


You can pass various parameters while loading kernel modules. The modinfo command lists a kernel module's available parameters which are listed as "parm" entries. For example for the cdrom kernel module, the last 6 lines shows the list of  parameters that are available. 

Note for boolean parameters, 0=not-enabled and 1=enabled. 


Hence we can enable the cdrom's kernel module's autoclose feature like this:


<pre>
$ modprobe -v cdrom autoclosex=1
insmod /lib/modules/3.10.0-229.1.2.el7.x86_64/kernel/drivers/cdrom/cdrom.ko autoclosex=1
modprobe: ERROR: could not insert 'cdrom': Unknown symbol in module, or unknown parameter (see dmesg)
</pre>

Here it failed because I had a small typo, I used "autoclosex" instead of "autoclose". Now without the typo: 

<pre>
$ modprobe -v cdrom autoclose=1
insmod /lib/modules/3.10.0-229.1.2.el7.x86_64/kernel/drivers/cdrom/cdrom.ko autoclose=1

</pre>

However if the module is already loaded in then you need to unload it first before loading with the parameter settings. 


This is how you set parameters by manually loading the kernel module. However more importantly you will want to automate this during boot time (i.e. make it persistent). To do this you need to create/edit a *.conf file in:


<pre>
$ ls -l /lib/modprobe.d
total 20
-rw-r--r--. 1 root root  79 Mar  6 05:17 cxgb3.conf
-rw-r--r--. 1 root root  79 Mar  6 05:17 cxgb4.conf
-rw-r--r--. 1 root root 382 Mar  5 19:35 dist-alsa.conf
-rw-r--r--. 1 root root 884 Mar  6 03:02 dist-blacklist.conf
-rw-r--r--. 1 root root 496 Mar  6 05:17 libmlx4.conf

</pre>
 
You can find more info about what this directory is used for in the man pages:

<pre>
$ man 5 modprobe.d
</pre>


So for the above example, I created the following file:


<pre>
$ cat /lib/modprobe.d/cdrom.conf
options cdrom autoclose=1 lockdoor=0
</pre>

Note: I also added another option, to give you an idea on how to specify more than one options. 

I also named this file "cdrom.conf", but in actual fact you give it any name you like. Although it's a good idea to name it after the kernel module. 

Also if you include a typo here then the kernel module will fail to load. 


Unfortunately there is no definitive way to check what the parameter settings are for a given kernel module that is loaded. One possible way is to check the contents of <code>/sys/module</code> folder. This folder has a folder that's named after each kernel module that is currently loaded. So if you reload a kernel module then the corresponding folder is deleted and recreated, e.g.:

<pre>
$ ls -l /sys/module/ | grep cdrom
drwxr-xr-x. 5 root root 0 May 25 15:29 cdrom
$ modprobe -r cdrom
$ ls -l /sys/module/ | grep cdrom
$ modprobe cdrom
$ ls -l /sys/module/ | grep cdrom
drwxr-xr-x. 5 root root 0 May 25 16:01 cdrom

</pre>


Each of these modules folder may have contain a "parameters" folder


<pre>
$ find /sys/module/ -name parameters -type d
/sys/module/vt/parameters
/sys/module/hid/parameters
/sys/module/snd/parameters
/sys/module/tpm/parameters
.
.
...etc
</pre>

The parameters folder contains info about which what parameters are currently set to. However this info is not provided for all kernel modules, since it is something that the kernel module's developer chooses whether or not to implement. 

In the case of the cdrom kernel module, the parameter folder is unfortunately not something that's provided. Failing that you could try checking the /var/log/message file. If that also fails then you can try using sysctl command:


<pre>
$ sysctl -a | grep autoclose
dev.cdrom.autoclose = 1
</pre>

We will cover more about sysctl in the next lesson. ]]></Content>
		<Date><![CDATA[2015-05-24]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Kernel Parameters (aka system settings)]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What is the command to view all the current kernel level settings (as well as kernel module settings)?"]
$ sysctl –a
[/toggle]
[toggle title="What is the command to just list only the core kernel settings?"]
$ sysctl –-system
[/toggle]
[toggle title="What is the command to view the current setting for the 'net.ipv6.conf.all.disable_ipv6' switch?"]
$ sysctl -a | grep net.ipv6.conf.all.disable_ipv6
net.ipv6.conf.all.disable_ipv6 = 0   # i.e. currently disabled
[/toggle]
[toggle title="Which file can you add an entry to persistantly enable the 'net.ipv6.conf.all.disable_ipv6' switch?"]
# Append the following:
"net.ipv6.conf.all.disable_ipv6 = 0"
# to the file:
/etc/sysctl.conf
[/toggle]
[toggle title="How do you check that the core kernel setting is now persistant?"]
# reboot the machine then run
$ sysctl -a | grep {setting}
[/toggle]
[/accordion]

<hr/>

In the last article we saw that each kernel module can have a bunch of settings that you can tweak by setting parameters, and also make these changes persistant.  

The kernel itself also comes with its own settings that can be tweaked, via customizing kernel parameters. There are a wide range of kernel level parameters that gets set a the time when the machine is booting up. 


<pre>


$ ls -l /proc/sys
total 0
dr-xr-xr-x. 1 root root 0 May 25 16:31 abi
dr-xr-xr-x. 1 root root 0 May 24 14:11 crypto
dr-xr-xr-x. 1 root root 0 May 25 16:31 debug
dr-xr-xr-x. 1 root root 0 May 25 16:31 dev
dr-xr-xr-x. 1 root root 0 May 24 14:11 fs
dr-xr-xr-x. 1 root root 0 May 24 14:11 kernel
dr-xr-xr-x. 1 root root 0 May 24 14:11 net
dr-xr-xr-x. 1 root root 0 May 25 16:31 sunrpc
dr-xr-xr-x. 1 root root 0 May 24 14:11 vm
</pre>

This folder contains a list of other folders, one for each interface that the kernel offers. 

Each file contained within these folders contain parameter that you can edit to affect the currently running OS. However you lose any of changes when you do a reboot, becuase the entire /proc folder get deleted and recreated during reboot. Note the parameters stored in this directory are often referred to as "system parameters"

Rather than going through each file, to view the parameter settings, you can instead use the sysctl command:

<pre>
$ sysctl –a 
abi.vsyscall32 = 1
crypto.fips_enabled = 0
debug.exception-trace = 1
debug.kprobes-optimization = 1
dev.cdrom.autoclose = 1
dev.cdrom.autoeject = 0
.
.
.
</pre>

Here, the periods are used as delimiters, which you can think of forward slashes to locate the file.  

sysctl can also be used to apply changes to the kernel parameter settings:

<pre>
$ sysctl -a | grep "ip_forward "
net.ipv4.ip_forward = 1
$ sysctl net.ipv4.ip_forward=0
net.ipv4.ip_forward = 0
$ sysctl -a | grep "ip_forward "
net.ipv4.ip_forward = 0
</pre>

However this change only applies to the current OS run time and won't survive a reboot. 

To make it persistant then you need to add the setting to the following file:

<pre>
$ cat /etc/sysctl.conf
# System default settings live in /usr/lib/sysctl.d/00-system.conf.
# To override those settings, enter new settings here, or in an /etc/sysctl.d/<name>.conf file
#
# For more information, see sysctl.conf(5) and sysctl.d(5).

</pre>


<strong>Note: In this example I couldn't get it working with the "net.ipv4.ip_forward" setting. This could be a bug with rhel/centos. However I got it working with the setting "net.ipv6.conf.all.disable_ipv6" which I will use as example for the remainder of this tutorial. Update: I did get this working eventually by doing a kernel and systemd update.</strong>


This file prompts you to see the default settings file, 00-system.conf. This file is useful because it tells you what the "0" and "1" binary values means:


<pre>
$  cat /usr/lib/sysctl.d/00-system.conf
# Kernel sysctl configuration file
#
# For binary values, 0 is disabled, 1 is enabled.  See sysctl(8) and
# sysctl.conf(5) for more details.

# Disable netfilter on bridges.
net.bridge.bridge-nf-call-ip6tables = 0
net.bridge.bridge-nf-call-iptables = 0
net.bridge.bridge-nf-call-arptables = 0

# Controls the maximum shared segment size, in bytes
kernel.shmmax = 4294967295

# Controls the maximum number of shared memory segments, in pages


</pre>



first off, the default setting is:


<pre>
sysctl -a | grep "net.ipv6.conf.all.disable_ipv6"
net.ipv6.conf.all.disable_ipv6 = 0
</pre>

We want to enable this feature (i.e. set this to "1") and make it survive a reboot. To do this we can append the line "net.ipv6.conf.all.disable_ipv6 = 0" to "/etc/sysctl.conf", However we can end up adding a lot of lines to this file and cluttering it up. Instead it is better practice to add your entry in somewhere in the following directory:


<pre>
$ ls -l /etc/sysctl.d/
total 4
lrwxrwxrwx. 1 root root 14 Apr  3 16:19 99-sysctl.conf -> ../sysctl.conf
</pre> 

you can either add the entry to an existing *.conf file or create new *.conf file. In my case I created my own file:

<pre>
$ ls -l /etc/sysctl.d/
total 4
<strong>-rw-r--r--. 1 root root 87 May 28 05:47 10-custom-setting.conf</strong>
lrwxrwxrwx. 1 root root 14 Apr  3 16:19 99-sysctl.conf -> ../sysctl.conf
$ cat /etc/sysctl.d/10-custom-setting.conf
# net.ipv6.conf.all.disable_ipv6 default value is 0
net.ipv6.conf.all.disable_ipv6 = 1


</pre>
 
Note, you need to prefix the .conf file's name with 2 digits followed by a hyphen. This is to do with executing the .conf files in a sequence. However the actaul file name itself can be set to what you prefer. 


Now let's test our new config file like this:



<pre>
$  sysctl -a | grep "net.ipv6.conf.all.disable_ipv6"
net.ipv6.conf.all.disable_ipv6 = 0

$ sysctl --system
* Applying /usr/lib/sysctl.d/00-system.conf ...
net.bridge.bridge-nf-call-ip6tables = 0
net.bridge.bridge-nf-call-iptables = 0
net.bridge.bridge-nf-call-arptables = 0
kernel.shmmax = 4294967295
kernel.shmall = 268435456
* Applying /etc/sysctl.d/10-custom-setting.conf ...
net.ipv6.conf.all.disable_ipv6 = 1
* Applying /usr/lib/sysctl.d/50-default.conf ...
kernel.sysrq = 16
kernel.core_uses_pid = 1
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.default.accept_source_route = 0
fs.protected_hardlinks = 1
fs.protected_symlinks = 1
* Applying /etc/sysctl.d/99-sysctl.conf ...
* Applying /usr/lib/sysctl.d/libvirtd.conf ...
fs.aio-max-nr = 1048576
* Applying /etc/sysctl.conf ...

$ sysctl -a | grep "net.ipv6.conf.all.disable_ipv6"
net.ipv6.conf.all.disable_ipv6 = 1


</pre>
 

After that I reboot the machine and check the setting again:


<pre>
$ sysctl -a | grep "net.ipv6.conf.all.disable_ipv6"
net.ipv6.conf.all.disable_ipv6 = 1
</pre>


Success! 
]]></Content>
		<Date><![CDATA[2015-05-25]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Update the kernel]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="True or false, the older kernel gets uninstalled when you update to the newer kernel?"]
False
[/toggle]
[toggle title="What is the command to list all available kernel versions?"]
$ yum list kernel --showduplicates
[/toggle]
[toggle title="What is the command to update your kernel?"]
$ yum update kernel
[/toggle]
[/accordion]

<hr />

You can update an installed package to it's latest stable version using the <code>yum update {package-name}</code> command. When you use yum to update a package, behind the scenes the old package gets uninstalled and then the newer version gets installed. However that is not the case if you want to update the kernel itself. Since the kernel is the core of the OS, it is too risky to get rid of the existing version of the kernel in case you want to go back to using it again in the future. That's why when you do a kernel update, what ends up happening is that another version of the kernel is also made available on your machine. You can then choose which version of the kernel you want to use during the machine boot time.

When your machine is booting up, the following window is briefly displayed (if you have more than one version of the kernel available on your machine):

<a href="http://codingbee.net/wp-content/uploads/2015/05/available-kernel-versions.png"><img class="alignnone size-full wp-image-4493" src="http://codingbee.net/wp-content/uploads/2015/05/available-kernel-versions.png" alt="" width="720" height="399" /></a>

If you wait a few seconds then the latest kernel version is automatically selected.

Now lets go ahead and install the latest stable version of the kernel, which in my case is:
<pre>
$ yum list kernel --showduplicates
Loaded plugins: fastestmirror, langpacks
Loading mirror speeds from cached hostfile
 * base: mirror.ox.ac.uk
 * epel: download.fedoraproject.org
 * extras: mirror.as29550.net
 * updates: mirror.ox.ac.uk
Installed Packages
kernel.x86_64            3.10.0-229.14.1.el7        @updates
Available Packages
kernel.x86_64            3.10.0-229.el7             base
kernel.x86_64            3.10.0-229.1.2.el7         updates
kernel.x86_64            3.10.0-229.4.2.el7         updates
kernel.x86_64            3.10.0-229.7.2.el7         updates
kernel.x86_64            3.10.0-229.11.1.el7        updates
kernel.x86_64            3.10.0-229.14.1.el7        updates
</pre>
Here we can see one of the kernel is already installed and a newer version is available.

To update the kernel we use yum's update command:
<pre>
$ yum update kernel
</pre>
This should end up creating a new "vmlinuz" file:
<pre>$ rpm -ql kernel | grep "/boot/vmlinuz"
/boot/vmlinuz-3.10.0-229.el7.x86_64
/boot/vmlinuz-3.10.0-229.14.1.el7.x86_64
</pre>
However if for whatever reason your particular vmlinuz file isn't there, then you can generate one by simply running the <strong>dracut</strong> command:
<pre>$ dracut
$
</pre>
You can just run this command on it's own with no extra options. However it is very unlikely you will ever need to use this command.

Now when I reboot the machine, the new kernel is now available and is also the new default since it is the latest version, and is the first kernel in the list:

<a href="http://codingbee.net/wp-content/uploads/2015/05/4V6kULf.png"><img class="alignnone size-full wp-image-4497" src="http://codingbee.net/wp-content/uploads/2015/05/4V6kULf.png" alt="" width="720" height="400" /></a>

After the machine has booted up, if we then go in and see what kernels are installed, we see:
<pre>
$ yum list kernel
Loaded plugins: fastestmirror, langpacks
Loading mirror speeds from cached hostfile
 * base: anorien.csc.warwick.ac.uk
 * extras: mirror.sov.uk.goscomb.net
 * updates: mirror.ox.ac.uk
Installed Packages
kernel.x86_64          3.10.0-123.el7                @anaconda
kernel.x86_64          3.10.0-229.4.2.el7            @updates
</pre>
This time yum shows that both kernels are now installed. However only one of them is currently running and you can can find out which one using the <code>uname -r</code> command.

You can now switch between these 2 kernels by rebooting your machine and then picking your kernel when prompted during the boot up process.]]></Content>
		<Date><![CDATA[2015-05-28]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Intro to SSL]]></Title>
		<Content><![CDATA[Really good guide:

https://jamielinux.com/docs/openssl-certificate-authority/index-full.html

Even better guide:

http://www.zytrax.com/tech/survival/ssl.html#single-cert

https://www.liberiangeek.net/2014/09/install-self-signed-ssl-certificate-nginx-cenos-7/

&nbsp;

https://www.digitalocean.com/community/tutorials/openssl-essentials-working-with-ssl-certificates-private-keys-and-csrs

The following will create a file called myblog.key
<pre>$ openssl genrsa -des3 -out /etc/nginx/ssl/myblog.key 2048</pre>
&nbsp;

This file is required to generate a csr file. Next we create the csr file:

$ openssl req -new -key /etc/nginx/ssl/myblog.key -out myblog.csr

&nbsp;

http://operational.io/openssl-commonly-used-commands/

&nbsp;

http://en.wikipedia.org/wiki/Alice_and_Bob

https://www.google.co.uk/search?q=alice+and+bob+ssl&amp;ie=utf-8&amp;oe=utf-8&amp;gws_rd=cr&amp;ei=i6xyVeKwB-aa7gaXs4PAAg

If you have a website, e.g. your own wordpress blog and you want to set up ssl on it so that your url starts with http<span style="color: #ff0000;"><strong>s</strong></span>://... then you need to understand what is ssl is how it works.

&nbsp;

There are 3 main types of files when it comes to learning about ssl:
<ul>
 	<li>.csr file</li>
 	<li>.pem file</li>
 	<li>.crt</li>
</ul>
You can use the openssl command to create a ".csr" file. when creating the csr file you will get prompted to provide your name, address, email address....etc, but most importantly your website's url. The openssl command will then generate the ".csr" file.  "This file is essentially a request for a certificate" file.

You can then send this .csr file to a list of trusted private corprate companies. These group of comanies are often referred to as "Certificate Authorities", aka "CA".   You can view a list of private companies them from inside firefox:

<a href="http://codingbee.net/wp-content/uploads/2015/06/CA-list.png"><img class="alignnone size-full wp-image-4531" src="http://codingbee.net/wp-content/uploads/2015/06/CA-list.png" alt="CA-list" width="1076" height="895" /></a>

These companies have a file called a ".pem" file. this file is a highly secret and is tightly guarded. No one outside the company is allowed to have access of it, otherwise, it is a major security breach that will make it on national news.

Now we submit our csr file to a CA (any ca listed above will do)  and make a payment to them which can range from a few pounds to millions of pounds, depending on the CA and the level of protection you want. The CA will then use your csr along with it's internal pem file to generate a crt file. This "crt" file is then given back to you, and you place it on your website's server. You can refer to these as "server side crt files"

Your browser comes with included a list of crt files, from the above CA's. These crt files reside internally in the browser. You can think of these crt files as "ca certs".

&nbsp;

Now when your browser connects to your website using a "https" links, then the server will initially forward the server-side-crt file to the browser (laptop). The laptop will then feed that file along with the corresponding ca-cert file into a special highly complex algorithigm. If the result is a green light then the connection is established.

&nbsp;

On some websites, the server-side-crt file is not created by a CA that is listed in your browsers trusted CA list. In that case you will get the "confirm security exception" prompt.

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

https://www.digitalocean.com/community/tutorials/how-to-create-a-ssl-certificate-on-nginx-for-centos-6

http://www.cyberciti.biz/faq/linux-unix-nginx-redirect-all-http-to-https/
https://www.digitalocean.com/community/questions/how-do-i-rewrite-url-s-in-nginx-server-block (this one is better, I think.)

http://www.cyberciti.biz/faq/nginx-self-signed-certificate-tutorial-on-centos-redhat-linux/ (this might also be useful but never tried it)

https://www.digitalocean.com/community/tutorials/openssl-essentials-working-with-ssl-certificates-private-keys-and-csrs (this looks really useful)
here are the 2 files you need to edit
<pre>$ ls -l /etc/nginx/conf.d/ | grep -E 'default.conf$|ssl.conf$'
-rw-r--r-- 1 root root 1727 Jun  5 10:48 default.conf
-rw-r--r-- 1 root root  635 Jun  5 10:33 ssl.conf
</pre>
&nbsp;

Useful links:

<a href="http://prefetch.net/articles/checkcertificate.html">http://prefetch.net/articles/checkcertificate.html</a>  (this script is really useful to check if cert has expired)]]></Content>
		<Date><![CDATA[2015-06-03]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ssl]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - The Boot Process]]></Title>
		<Content><![CDATA[http://www.dedoimedo.com/computers/grub.html

http://www.pluralsight.com/courses/red-hat-enterprise-linux-6-booting-runlevels



Here's what happens when you switch on your machine:

<ol>
	<li>BIOS (Basic Input Output System) - this performs POST (Power On Self Test) - This basically checks all the main hardware is responding, e.g. cpu, ram, motherboard,...etc. If everything passes, it then moves on to the next process. BIOS (Basic Input Output System), The BIOS's code actually resides on the motherboard itself, i.e. it is the motherboard's firmware. The BIOS does various things like setting the system clock. However a key it does is that it determines which device to boot from, e.g. network boot, cdrom, hdd,...etc. It is usually the hdd</li>
	<li>BIOS identifies the boot devicece - this is a storage device, where the mbr contains the first part of GRUB 2 along with the partition table. BIOS then hands over the boot process to GRUB 2</li>
	<li>GRUB2 is executed - The purpose of GRUB2 is to load the kernel and initrd. At this point the GRUB menu is temporarily displayed.
<a href="http://codingbee.net/wp-content/uploads/2015/06/tcWzWBw.png"><img src="http://codingbee.net/wp-content/uploads/2015/06/tcWzWBw.png" alt="tcWzWBw" width="635" height="355" class="alignnone size-full wp-image-4520" /></a></li>
	<li>Root filesystem is mounted from the kernel</li>
	<li>Sytemd is started</li>
	<li>Everything else is started up - e.g. starting logging, mounting partitions, starting default services </li>

</ol>






01. The Boot Process

<b>Commands:</b>

Inserttexthere

<b>Config files:</b>

/etc/inittab            # this file is where you can set the default run-level. This file used to be a place where you entered all kinds of settings, but now is only used for setting the runlevel. The other settings are

# now set elsewhere.

<b>Notes:</b>

The following steps (boot process) takes place during a system start-up:
<ol>
	<li>      <b>BIOS</b><b>initializes – </b>The BIOS does the following:</li>
	<li>      BIOS is short for Basic Input Output System</li>
	<li>      BIOS is installed on the motherboard itself.</li>
	<li>      The BIOS initializes basic peripherals on the system</li>
	<li>      The BIOS sets the system clock</li>
	<li>      The BIOS searches for the boot devices (I think by scanning the MBR in each hdd)</li>
	<li>        Once the BIOS finds the boot device, it then runs the bootloader software (in this case GRUB) that exists on it.</li>
	<li>      After the BIOS has initialized, the <b>BIOS then hands over the boot process to the</b><b>Bootloader</b> (kown as GRUB) <b>– </b>GRUB does the following:</li>
	<li>      The default bootloader for RHEL is called GRUB. GRUB actually exists in the form of 2 software component (aka stages)</li>
	<li>      Stage 1 GRUB – This resides within the MBR, and it’s only purpose to point to the location of the grub.conf file.</li>
	<li>      Stage 2 GRUB – This resides in the /boot partition of the hdd. IT is what generates the GRUB menu.</li>
	<li>      After (stage 1) GRUB starts, it searches it’s config file to identify what the default kernel image is, and then loads it</li>
	<li>      Stage 1 GRUB then hands over control to Stage 2 GRUB which then generates the grub menu for the user.</li>
	<li>      User then selects the OS (or kernel version) to boot-up with (if not the default), and then GRUB loads the given kernel into the RAM as well as the initrd image (initramfs-{kernel-version}.img).</li>
	<li>      GRUB then hands over control of the boot process to the newly loaded linux kernel.</li>
	<li>      After GRUB <b>loads the first Kernel Image into memory, it then hands over control to the kernel – </b>the kernel does the following:</li>
	<li>      The kernel detects what hardware is on the system</li>
	<li>      Once it has detected everything, it then loads the necessary drivers from it’s initial RAM-file-system (initrd image)</li>
	<li>      This ram-file-system contains device drivers, important scripts, and binary utilities…..that aid with the system boot process.</li>
	<li>      initrd is used by kernel as temporary root file system until kernel is booted and the real root file system is mounted.</li>
	<li>      It then mounts the root file system in read only mode.</li>
	<li>        It then starts the “init” process (which should have pid of 1, since it is the very first process that gets started)</li>
	<li>      <b>After the Kernel has started the “init” process, it then hands over control to the init process – </b>init does the following:</li>
	<li>      After init has started it then runs the necessary script which are located in /etc/init</li>
	<li>      Init then reads the /etc/inittab file to determine the default run level.</li>
	<li>      Start the run-level scripts for the default run-level. This will start the various services (you should see various messages coming up...e.g…. “starting sendmail …. OK”)</li>
	<li>      The system services finishes starting up and a <b>login screen is presented</b>.</li>
</ol>
<b>Must survive reboot:</b>

Inserttexthere

<b>Software to install:</b>

Inserttexthere

<b>GUI tool:</b>

Inserttexthere

<b>Book ref:</b>

Inserttexthere

<b>Study guide ref:</b>

Inserttexthere

<b>Need to learn more about:</b>

<a href="http://www.thegeekstuff.com/2011/02/linux-boot-process/">http://www.thegeekstuff.com/2011/02/linux-boot-process/</a>]]></Content>
		<Date><![CDATA[2015-06-05]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|RedHat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Create a Self Signed Certificate (SSL)]]></Title>
		<Content><![CDATA[You can create a self signed certificate using the ]]></Content>
		<Date><![CDATA[2015-06-05]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ssl]]></Tags>
		<Status><![CDATA[draft]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Understanding GRUB2]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What is that brief menu called that's displayed briefly during machine bootup, prompting you to select a kernel version?"]
The grub menu
[/toggle]
[toggle title="What is config file you can edit to customize this menu's behaviour, e.g. the timeout period?"]
/etc/default/grub
[/toggle]
[toggle title="What is the directory's content that you can also edit to customize this menu's behaviour, e.g. the timeout period?"]
/etc/grub.d
[/toggle]
[toggle title="What is the command to preview your potential new grub configuration?"]
$ grub2-mkconfig | less
[/toggle]
[toggle title="What is the command to persistantly apply your changes?"]
$ grub2-mkconfig > /boot/grub2/grub.cfg 
[/toggle]
[/accordion]

<hr/>


<h2>The GRUB menu</h2>
The <strong>grub menu</strong> is something that get's displayed briefly when your machine is booting up, and it looks like this:


<a href="http://codingbee.net/wp-content/uploads/2015/06/vdsRWtz.png"><img src="http://codingbee.net/wp-content/uploads/2015/06/vdsRWtz.png" alt="vdsRWtz" width="739" height="415" class="alignnone size-full wp-image-4589" /></a>




A important task you may need to do is access the grub menu to <a href="http://codingbee.net/tutorials/rhcsa/rhcsa-managing-services/#Switching_targets_via_the_grub_menu">edit the GRUB parameters to enter the rescue or emergency target</a>. 



This is useful if you need to access your machine in rescue mdoe or emergency mode (scroll down to line start with "linux16", go to the end of line, type systemd.unit={target-name}.unit).  


<h2>Configuring GRUB</h2>
GRUB is short for the "Grand Unified Bootloader" and it is the very first linux related component that is activated when powering up a machine. 

The main config file for grub is:


<pre>
/boot/grub2/grub.cfg
</pre>

However you're not supposed to edit this file directly. Instead you edit this file indirectly by first editing the following:

<ul>
	<li>/etc/default/grub   # this is a file</li>
	<li>/etc/grub.d</li>
</ul>

Any change here can then be fed into the <code>grub2-mkconfig</code> which will generate output that can be piped into <code>/boot/grub2/grub.cfg</code>. 


First we preview the new configuratons:

$ grub2-mkconfig | less



You can control the GRUB's behaviour via the GRUB2's main config file:

<pre>
$ cat /etc/default/grub
GRUB_TIMEOUT=5
GRUB_DISTRIBUTOR="$(sed 's, release .*$,,g' /etc/system-release)"
GRUB_DEFAULT=saved
GRUB_DISABLE_SUBMENU=true
GRUB_TERMINAL_OUTPUT="console"
GRUB_CMDLINE_LINUX="rd.lvm.lv=centos/swap crashkernel=auto  rd.lvm.lv=centos/root vconsole.font=latarcyrheb-sun16 vconsole.keymap=uk rhgb quiet"
GRUB_DISABLE_RECOVERY="true"
</pre> 

Here the 2 main settings are, GRUB_TIMEOUT - which sets how long the grub menu is displayed before disappearing, and, GRUB_CMDLINE_LINUX, this lists all the arguements that are passed through the kernel while the machine is booting. Here you can remove the "rhgb quiet" in order to get more verbose output when the machine is loading up. 



Note, you can also create grub config files in the following directory too:

<pre>
$ ls -l /etc/grub.d/
total 64
-rwxr-xr-x. 1 root root  8698 Jun 30  2014 00_header
-rwxr-xr-x. 1 root root  9517 Jun 30  2014 10_linux
-rwxr-xr-x. 1 root root 10275 Jun 30  2014 20_linux_xen
-rwxr-xr-x. 1 root root  2559 Jun 30  2014 20_ppc_terminfo
-rwxr-xr-x. 1 root root 11110 Jun 30  2014 30_os-prober
-rwxr-xr-x. 1 root root   214 Jun 30  2014 40_custom
-rwxr-xr-x. 1 root root   216 Jun 30  2014 41_custom
-rw-r--r--. 1 root root   483 Jun 30  2014 README


</pre>

However it is very unlikely that you will need to do anything in this directory. 

Once you have made any changes to grub configurations, you need to run the following command:


<pre>
$ grub
grub2-bios-setup       grub2-macbless         grub2-mkpasswd-pbkdf2  grub2-render-label
grub2-editenv          grub2-menulst2cfg      grub2-mkrelpath        grub2-script-check
grub2-file             <mark>grub2-mkconfig</mark>         grub2-mkrescue         grub2-set-default
grub2-fstest           grub2-mkfont           grub2-mkstandalone     grub2-sparc64-setup
grub2-glue-efi         grub2-mkimage          grub2-ofpathname       grub2-syslinux2cfg
grub2-install          grub2-mklayout         grub2-probe            grubby
grub2-kbdcomp          grub2-mknetdir         grub2-reboot


</pre>


This command essentially updates the MBR with the new grub configurations, e.g. new timeout value. 


Hence if you made any changes to the main grub config file, then you need to run the grub2-mkconfig command, to get it loaded into the mbr: 

<pre>
$ grub2-mkconfig > /boot/grub2/grub.cfg
Generating grub configuration file ...
Found linux image: /boot/vmlinuz-3.10.0-229.4.2.el7.x86_64
Found initrd image: /boot/initramfs-3.10.0-229.4.2.el7.x86_64.img
Found linux image: /boot/vmlinuz-3.10.0-123.el7.x86_64
Found initrd image: /boot/initramfs-3.10.0-123.el7.x86_64.img
Found linux image: /boot/vmlinuz-0-rescue-3cf91f1bd95c4bbfbccf8ee2b5424021
Found initrd image: /boot/initramfs-0-rescue-3cf91f1bd95c4bbfbccf8ee2b5424021.img
done
</pre>

This will update the main grub.cfg file. This is a file that you're not supposed to update directly. Instead you use grub2-mkconfig to generate an updated file /boot/grub2/grub.cfg for you. 

       

https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/System_Administrators_Guide/ch-Working_with_the_GRUB_2_Boot_Loader.html 



]]></Content>
		<Date><![CDATA[2015-06-05]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Understanding systemd]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What is the command for managing 'units', and how do you view it's help info?"]
$ systemctl <span style='letter-spacing=0.1px'>-</span>-help

[/toggle]
[toggle title="What is the command to view a full list of the types of units there are?"]
$ systemctl <span style='letter-spacing=0.1px'>-</span>-type=help
[/toggle]
[toggle title="What is the command to list every single units, of all types and irrespective whether they are enabled or not?"]
$ systemctl list-unit-files
[/toggle]
[toggle title="What is the command that is the same as above but this time only display service units?"]
$ systemctl list-unit-files <span style='letter-spacing=0.1px'>-</span>-type service
[/toggle]
[toggle title="What is the command that is the same as above but this time only display running units?"]
$ systemctl list-units <span style='letter-spacing=0.1px'>-</span>-type=service
[/toggle]
[toggle title="What is the command to check the status of the sshd.service unit?"]
$ systemctl status sshd.service
[/toggle]
[toggle title="Which directory houses all your unit files?"]
/usr/lib/systemd/system
[/toggle]
[/accordion]

<hr/>



During the boot process, systemd is started right after the kernel has been loaded.  

Systemd is a big part of the RHEL OS. The main command that's used to query/manage systemd is <code>systemctl</code>:

<pre>
$ systemctl --help
systemctl [OPTIONS...] {COMMAND} ...

Query or send control commands to the systemd manager.

  -h --help           Show this help
     --version        Show package version
  -t --type=TYPE      List only units of a particular type
     --state=STATE    List only units with particular LOAD or SUB or ACTIVE state
  -p --property=NAME  Show only properties by this name
  -a --all            Show all loaded units/properties, including dead/empty
                      ones. To list all units installed on the system, use
                      the 'list-unit-files' command instead.
     --reverse        Show reverse dependencies with 'list-dependencies'
  -l --full           Don't ellipsize unit names on output
     --fail           When queueing a new job, fail if conflicting jobs are
                      pending
     --irreversible   When queueing a new job, make sure it cannot be implicitly
                      cancelled
     --ignore-dependencies
                      When queueing a new job, ignore all its dependencies
     --show-types     When showing sockets, explicitly show their type
  -i --ignore-inhibitors
                      When shutting down or sleeping, ignore inhibitors
     --kill-who=WHO   Who to send signal to
  -s --signal=SIGNAL  Which signal to send
  -H --host=[USER@]HOST
                      Show information for remote host
  -P --privileged     Acquire privileges before execution
  -q --quiet          Suppress output
     --no-block       Do not wait until operation finished
     --no-wall        Don't send wall message before halt/power-off/reboot
     --no-reload      When enabling/disabling unit files, don't reload daemon
                      configuration
     --no-legend      Do not print a legend (column headers and hints)
     --no-pager       Do not pipe output into a pager
     --no-ask-password
                      Do not ask for system passwords
     --system         Connect to system manager
     --user           Connect to user service manager
     --global         Enable/disable unit files globally
     --runtime        Enable unit files only temporarily until next reboot
  -f --force          When enabling unit files, override existing symlinks
                      When shutting down, execute action immediately
     --root=PATH      Enable unit files in the specified root directory
  -n --lines=INTEGER  Number of journal entries to show
  -o --output=STRING  Change journal output mode (short, short-monotonic,
                      verbose, export, json, json-pretty, json-sse, cat)
     --plain          Print unit dependencies as a list instead of a tree

Unit Commands:
  list-units                      List loaded units
  list-sockets                    List loaded sockets ordered by address
  start [NAME...]                 Start (activate) one or more units
  stop [NAME...]                  Stop (deactivate) one or more units
  reload [NAME...]                Reload one or more units
  restart [NAME...]               Start or restart one or more units
  try-restart [NAME...]           Restart one or more units if active
  reload-or-restart [NAME...]     Reload one or more units if possible,
                                  otherwise start or restart
  reload-or-try-restart [NAME...] Reload one or more units if possible,
                                  otherwise restart if active
  isolate [NAME]                  Start one unit and stop all others
  kill [NAME...]                  Send signal to processes of a unit
  is-active [NAME...]             Check whether units are active
  is-failed [NAME...]             Check whether units are failed
  status [NAME...|PID...]         Show runtime status of one or more units
  show [NAME...|JOB...]           Show properties of one or more
                                  units/jobs or the manager
  set-property [NAME] [ASSIGNMENT...]
                                  Sets one or more properties of a unit
  help [NAME...|PID...]           Show manual for one or more units
  reset-failed [NAME...]          Reset failed state for all, one, or more
                                  units
  list-dependencies [NAME]        Recursively show units which are required
                                  or wanted by this unit or by which this
                                  unit is required or wanted

Unit File Commands:
  list-unit-files                 List installed unit files
  enable [NAME...]                Enable one or more unit files
  disable [NAME...]               Disable one or more unit files
  reenable [NAME...]              Reenable one or more unit files
  preset [NAME...]                Enable/disable one or more unit files
                                  based on preset configuration
  is-enabled [NAME...]            Check whether unit files are enabled

  mask [NAME...]                  Mask one or more units
  unmask [NAME...]                Unmask one or more units
  link [PATH...]                  Link one or more units files into
                                  the search path
                                  the search path
  get-default                     Get the name of the default target
  set-default NAME                Set the default target

Job Commands:
  list-jobs                       List jobs
  cancel [JOB...]                 Cancel all, one, or more jobs

Snapshot Commands:
  snapshot [NAME]                 Create a snapshot
  delete [NAME...]                Remove one or more snapshots

Environment Commands:
  show-environment                Dump environment
  set-environment [NAME=VALUE...] Set one or more environment variables
  unset-environment [NAME...]     Unset one or more environment variables

Manager Lifecycle Commands:
  daemon-reload                   Reload systemd manager configuration
  daemon-reexec                   Reexecute systemd manager

System Commands:
  default                         Enter system default mode
  rescue                          Enter system rescue mode
  emergency                       Enter system emergency mode
  halt                            Shut down and halt the system
  poweroff                        Shut down and power-off the system
  reboot                          Shut down and reboot the system
  kexec                           Shut down and reboot the system with kexec
  exit                            Request user instance exit
  switch-root [ROOT] [INIT]       Change to a different root file system
  suspend                         Suspend the system
  hibernate                       Hibernate the system
  hybrid-sleep                    Hibernate and suspend the system


</pre>



The main purpose of Systemd is that it essentially controls what "components" to activate/start when you boot up your rhel machine.   

These components are referred to as <em>units</em>, and there are a dozen different types of units: 

<pre>
$ systemctl --type=help
Available unit types:
<mark>service</mark>
socket
<mark>target</mark>
device
mount
automount
snapshot
timer
swap
path
slice
scope
</pre>

The highlighted units are the ones that the RHCSA objectives particularly focuses on.    

If you want to view a list of all units across all unit types, you do:


<pre>
$ systemctl list-unit-files
UNIT FILE                                   STATE
brandbot.path                               disabled
cups.path                                   enabled
systemd-ask-password-console.path           static
systemd-ask-password-plymouth.path          static
abrt-ccpp.service                           enabled
abrt-oops.service                           enabled
abrt-pstoreoops.service                     disabled
abrt-vmcore.service                         enabled
abrt-xorg.service                           enabled
.
....etc
</pre>

Tip: Use tab+tab autocomplete to write systemctl more quickly.

You can then view units by type, e.g. the service units only:


<pre>
$ systemctl list-unit-files --type service
UNIT FILE                                   STATE
abrt-ccpp.service                           enabled
abrt-oops.service                           enabled
abrt-pstoreoops.service                     disabled
abrt-vmcore.service                         enabled
abrt-xorg.service                           enabled
.
...etc
</pre>


Within this list you can filter further to view just the enabled services, like this: 

<pre>
$ systemctl list-units --type service
UNIT                               LOAD   ACTIVE SUB     DESCRIPTION
abrt-ccpp.service                  loaded active exited  Install ABRT coredump hook
abrt-oops.service                  loaded active running ABRT kernel log watcher
abrt-xorg.service                  loaded active running ABRT Xorg log watcher
abrtd.service                      loaded active running ABRT Automated Bug Reporting Tool
accounts-daemon.service            loaded active running Accounts Service

</pre>


You can view the status of particular unit like this:



<pre>
$ systemctl status sshd
sshd.service - OpenSSH server daemon
   Loaded: loaded (/usr/lib/systemd/system/sshd.service; enabled)
   Active: active (running) since Sun 2015-09-20 08:15:32 BST; 11h ago
 Main PID: 1012 (sshd)
   CGroup: /system.slice/sshd.service
           └─1012 /usr/sbin/sshd -D

Sep 20 08:15:32 puppetmaster.local systemd[1]: Started OpenSSH server daemon.
Sep 20 08:15:32 puppetmaster.local sshd[1012]: Server listening on 0.0.0.0 port 22.
Sep 20 08:15:32 puppetmaster.local sshd[1012]: Server listening on :: port 22.
Sep 20 08:15:37 puppetmaster.local sshd[1256]: Accepted publickey for vagrant from 10.0.2.2 port ...:13
Sep 20 08:17:59 puppetmaster.local sshd[4878]: Accepted password for root from 192.168.50.1 port ...sh2
Hint: Some lines were ellipsized, use -l to show in full.

</pre>

Each unit comes with it's own config file, which is referred to as "unit files". All the unit files are house in the <code>/usr/lib/systemd/system</code> directory (The above output also provides the full path to corresponding unit file). This directory contains a lot of unit files:





<pre>
$ ls -l
total 1300
-rw-r--r--. 1 root root   275 Jun 10 17:11 abrt-ccpp.service
-rw-r--r--. 1 root root   380 Jun 10 17:11 abrtd.service
-rw-r--r--. 1 root root   361 Jun 10 17:11 abrt-oops.service
-rw-r--r--. 1 root root   266 Jun 10 17:11 abrt-pstoreoops.service
-rw-r--r--. 1 root root   262 Jun 10 17:11 abrt-vmcore.service
-rw-r--r--. 1 root root   311 Jun 10 17:11 abrt-xorg.service
-rw-r--r--. 1 root root   421 Jun 10  2014 accounts-daemon.service
-rw-r--r--. 1 root root   501 Mar  5  2015 alsa-restore.service
-rw-r--r--. 1 root root   558 Mar  5  2015 alsa-state.service
-rw-r--r--. 1 root root   412 Mar  5  2015 alsa-store.service
-rw-r--r--. 1 root root   645 Mar 26 10:43 anaconda-direct.service
-rw-r--r--. 1 root root   185 Mar 26 10:43 anaconda-nm-config.service
-rw-r--r--. 1 root root   660 Mar 26 10:43 anaconda-noshell.service
-rw-r--r--. 1 root root   387 Mar 26 10:43 anaconda.service
-rw-r--r--. 1 root root   684 Mar 26 10:43 anaconda-shell@.service
-rw-r--r--. 1 root root   322 Mar 26 10:43 anaconda-sshd.service
-rw-r--r--. 1 root root   312 Mar 26 10:43 anaconda.target
drwxr-xr-x. 2 root root  4096 Sep 19 21:01 anaconda.target.wants
-rw-r--r--. 1 root root   498 Mar 26 10:43 anaconda-tmux@.service
-rw-r--r--. 1 root root   275 Jun 10  2014 arp-ethers.service
-rw-r--r--. 1 root root   205 Oct  7  2014 atd.service
-rw-r-----. 1 root root   669 Mar  5  2015 auditd.service
-rw-r--r--. 1 root root   663 Mar  6  2015 auth-rpcgss-module.service
-rw-r--r--. 1 root root   346 Mar  5  2015 autofs.service
lrwxrwxrwx. 1 root root    14 Sep 16 19:35 autovt@.service -> getty@.service
-rw-r--r--. 1 root root  1044 Mar  5  2015 avahi-daemon.service
-rw-r--r--. 1 root root   874 Mar  5  2015 avahi-daemon.socket
-rw-r--r--. 1 root root   546 Sep 15 14:21 basic.target
.
.
</pre>

Each unit file's name has a suffix to indicate what type of unit it is. 


Next we'll take a look at what's inside a unit file. 

]]></Content>
		<Date><![CDATA[2015-06-05]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Managing Services with systemctl]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What is the (rpm) command to list what services are installed when installing the 'httpd' package?"]
$ rpm -ql httpd | grep ".service$"
[/toggle]
[toggle title="What is the command to view all the available options and subcommands for systemctl?"]
$ systemctl <span style="letter-spacing:0.1px">-</span>-help
[/toggle]
[toggle title="What is the command to check the status of the httpd service?"]
$ systemctl status httpd.service
[/toggle]
[toggle title="What is the command to start the httpd service?"]
$ systemctl start httpd.service
[/toggle]
[toggle title="What is the command to enable the httpd service?"]
$ systemctl enable httpd.service
[/toggle]
[/accordion]

<hr/>


<h2>What is a service</h2>
A service is one of the dozen types of unit that systemd manages. 

In Linux, a service is basically an application that is continuously running in the background waiting to be used. A common example of a service, is a web server, for example the "httpd" (aka Apache HTTP web server):

A service is represented by a *.service file, so if you want to check what services that this package has installed, you can do:

<pre>

$ rpm -ql httpd | grep ".service$"
/usr/lib/systemd/system/htcacheclean.service
/usr/lib/systemd/system/httpd.service
</pre>

As you can see, this application comes with 2 services, of which httpd.service is the main service. We'll cover more about *.service files later. 

To manage a service, e.g. start/stop a service, we use the "systemctl" command: 

<pre>
$ systemctl --help
systemctl [OPTIONS...] {COMMAND} ...

Query or send control commands to the systemd manager.

  -h --help           Show this help
     --version        Show package version
  -t --type=TYPE      List only units of a particular type
     --state=STATE    List only units with particular LOAD or SUB or ACTIVE state
  -p --property=NAME  Show only properties by this name
  -a --all            Show all loaded units/properties, including dead/empty
                      ones. To list all units installed on the system, use
                      the 'list-unit-files' command instead.
     --reverse        Show reverse dependencies with 'list-dependencies'
  -l --full           Don't ellipsize unit names on output
     --fail           When queueing a new job, fail if conflicting jobs are
                      pending
     --irreversible   When queueing a new job, make sure it cannot be implicitly
                      cancelled
     --ignore-dependencies
                      When queueing a new job, ignore all its dependencies
     --show-types     When showing sockets, explicitly show their type
  -i --ignore-inhibitors
                      When shutting down or sleeping, ignore inhibitors
     --kill-who=WHO   Who to send signal to
  -s --signal=SIGNAL  Which signal to send
  -H --host=[USER@]HOST
                      Show information for remote host
  -P --privileged     Acquire privileges before execution
  -q --quiet          Suppress output
     --no-block       Do not wait until operation finished
     --no-wall        Don't send wall message before halt/power-off/reboot
     --no-reload      When enabling/disabling unit files, don't reload daemon
                      configuration
     --no-legend      Do not print a legend (column headers and hints)
     --no-pager       Do not pipe output into a pager
     --no-ask-password
                      Do not ask for system passwords
     --system         Connect to system manager
     --user           Connect to user service manager
     --global         Enable/disable unit files globally
     --runtime        Enable unit files only temporarily until next reboot
  -f --force          When enabling unit files, override existing symlinks
                      When shutting down, execute action immediately
     --root=PATH      Enable unit files in the specified root directory
  -n --lines=INTEGER  Number of journal entries to show
  -o --output=STRING  Change journal output mode (short, short-monotonic,
                      verbose, export, json, json-pretty, json-sse, cat)
     --plain          Print unit dependencies as a list instead of a tree

Unit Commands:
  list-units                      List loaded units
  list-sockets                    List loaded sockets ordered by address
  <mark>start</mark> [NAME...]                 Start (activate) one or more units
  <mark>stop</mark> [NAME...]                  Stop (deactivate) one or more units
  reload [NAME...]                Reload one or more units
  <mark>restart</mark> [NAME...]               Start or restart one or more units
  try-restart [NAME...]           Restart one or more units if active
  reload-or-restart [NAME...]     Reload one or more units if possible,
                                  otherwise start or restart
  reload-or-try-restart [NAME...] Reload one or more units if possible,
                                  otherwise restart if active
  isolate [NAME]                  Start one unit and stop all others
  kill [NAME...]                  Send signal to processes of a unit
  is-active [NAME...]             Check whether units are active
  is-failed [NAME...]             Check whether units are failed
  <mark>status</mark> [NAME...|PID...]         Show runtime status of one or more units
  show [NAME...|JOB...]           Show properties of one or more
                                  units/jobs or the manager
  set-property [NAME] [ASSIGNMENT...]
                                  Sets one or more properties of a unit
  help [NAME...|PID...]           Show manual for one or more units
  reset-failed [NAME...]          Reset failed state for all, one, or more
                                  units
  list-dependencies [NAME]        Recursively show units which are required
                                  or wanted by this unit or by which this
                                  unit is required or wanted

Unit File Commands:
  list-unit-files                 List installed unit files
  <mark>enable</mark> [NAME...]                Enable one or more unit files
  <mark>disable</mark> [NAME...]               Disable one or more unit files
  reenable [NAME...]              Reenable one or more unit files
  preset [NAME...]                Enable/disable one or more unit files
                                  based on preset configuration
  is-enabled [NAME...]            Check whether unit files are enabled

  mask [NAME...]                  Mask one or more units
  unmask [NAME...]                Unmask one or more units
  link [PATH...]                  Link one or more units files into
                                  the search path
  get-default                     Get the name of the default target
  set-default NAME                Set the default target

Job Commands:
  list-jobs                       List jobs
  cancel [JOB...]                 Cancel all, one, or more jobs

Snapshot Commands:
  snapshot [NAME]                 Create a snapshot
  delete [NAME...]                Remove one or more snapshots

Environment Commands:
  show-environment                Dump environment
  set-environment [NAME=VALUE...] Set one or more environment variables
  unset-environment [NAME...]     Unset one or more environment variables

Manager Lifecycle Commands:
  daemon-reload                   Reload systemd manager configuration
  daemon-reexec                   Reexecute systemd manager

System Commands:
  default                         Enter system default mode
  rescue                          Enter system rescue mode
  emergency                       Enter system emergency mode
  halt                            Shut down and halt the system
  poweroff                        Shut down and power-off the system
  reboot                          Shut down and reboot the system
  kexec                           Shut down and reboot the system with kexec
  exit                            Request user instance exit
  switch-root [ROOT] [INIT]       Change to a different root file system
  suspend                         Suspend the system
  hibernate                       Hibernate the system
  hybrid-sleep                    Hibernate and suspend the system
</pre>

I have highlighted some of systemctl's most commonly used sub-commands above. 

systemctl doesn't just manage service. It can manage all kinds of other things that can have a state (e.g). These things are referred to as "units".

Here's a list of the different types of units that systemctl can manage:


<pre>
$ systemctl --type=help
Available unit types:
service
socket
target
device
mount
automount
snapshot
timer
swap
path
slice
scope

</pre>

Each unit is represented in the form of a file. All unit files are stored in <code>/usr/lib/systemd/system</code>. 

<h2>Starting/Stopping/Enabling Services</h2>

To check the status of a service we do:

<pre>
$ systemctl status httpd
httpd.service - The Apache HTTP Server
   Loaded: loaded (/usr/lib/systemd/system/httpd.service; <mark>disabled</mark>)
   Active: <mark>inactive (dead)</mark>

Jun 06 10:03:18 centos7.codingbee.dyndns.org systemd[1]: Starting The Apache HTTP Server...
Jun 06 10:03:18 centos7.codingbee.dyndns.org systemd[1]: Started The Apache HTTP Server.
Jun 06 10:44:49 centos7.codingbee.dyndns.org systemd[1]: Stopping The Apache HTTP Server...
Jun 06 10:44:51 centos7.codingbee.dyndns.org systemd[1]: Stopped The Apache HTTP Server.
Jun 06 10:44:57 centos7.codingbee.dyndns.org systemd[1]: Starting The Apache HTTP Server...
Jun 06 10:44:57 centos7.codingbee.dyndns.org systemd[1]: Started The Apache HTTP Server.
Jun 06 10:44:59 centos7.codingbee.dyndns.org systemd[1]: Stopping The Apache HTTP Server...
Jun 06 10:45:00 centos7.codingbee.dyndns.org systemd[1]: Stopped The Apache HTTP Server.

</pre> 


This shows that the httpd service is not currently running (i.e. inactive) and it wont start up by default when you're booting up the machine (i.e. it is disabled). This command also provides a short log history. 



So to start a service, we do:

<pre>
$ systemctl start httpd
$
</pre>

This doesn't give a success output, so let's check the status again to see if this has been successful:

<pre>
$ systemctl status httpd
httpd.service - The Apache HTTP Server
   Loaded: loaded (/usr/lib/systemd/system/httpd.service; <mark>disabled</mark>)
   Active: <mark>active (running)</mark> since Sat 2015-06-06 11:08:52 BST; 47s ago
 Main PID: 4336 (httpd)
   Status: "Total requests: 0; Current requests/sec: 0; Current traffic:   0 B/sec"
   CGroup: /system.slice/httpd.service
           ├─4336 /usr/sbin/httpd -DFOREGROUND
           ├─4337 /usr/sbin/httpd -DFOREGROUND
           ├─4338 /usr/sbin/httpd -DFOREGROUND
           ├─4339 /usr/sbin/httpd -DFOREGROUND
           ├─4340 /usr/sbin/httpd -DFOREGROUND
           └─4341 /usr/sbin/httpd -DFOREGROUND

Jun 06 11:08:52 centos7.codingbee.dyndns.org systemd[1]: Starting The Apache HTTP Server...
Jun 06 11:08:52 centos7.codingbee.dyndns.org systemd[1]: Started The Apache HTTP Server.

</pre>


Here we can see that the service is now running now. However the service is still disabled. This means that this service won't be automatically started when you reboot the machine. You can therefore enable this service:


<pre>
$ systemctl enable httpd
ln -s '/usr/lib/systemd/system/httpd.service' '/etc/systemd/system/multi-user.target.wants/httpd.service'
</pre>

Note: This essentially creates a soft link in the "multi-user.target.wants" folder. We'll cover more about this later.  



Let's now check the status again:

<pre>
$ systemctl status httpd
httpd.service - The Apache HTTP Server
   Loaded: loaded (/usr/lib/systemd/system/httpd.service; <mark>enabled</mark>)
   Active: active (running) since Sat 2015-06-06 11:08:52 BST; 5min ago
 Main PID: 4336 (httpd)
   Status: "Total requests: 0; Current requests/sec: 0; Current traffic:   0 B/sec"
   CGroup: /system.slice/httpd.service
           ├─4336 /usr/sbin/httpd -DFOREGROUND
           ├─4337 /usr/sbin/httpd -DFOREGROUND
           ├─4338 /usr/sbin/httpd -DFOREGROUND
           ├─4339 /usr/sbin/httpd -DFOREGROUND
           ├─4340 /usr/sbin/httpd -DFOREGROUND
           └─4341 /usr/sbin/httpd -DFOREGROUND

Jun 06 11:08:52 centos7.codingbee.dyndns.org systemd[1]: Starting The Apache HTTP Server...
Jun 06 11:08:52 centos7.codingbee.dyndns.org systemd[1]: Started The Apache HTTP Server.
</pre>


Now it is enabled. If we now stop the service and then reboot the machine, by default this service will now automatically start up again during the boot process.


<h2>Viewing services</h2>

To view all currently running services, we do:


<pre>
$ systemctl list-units --type=service
UNIT                                                   LOAD   ACTIVE SUB     DESCRIPTION
abrt-ccpp.service                                      loaded active exited  Install ABRT coredump hook
abrt-oops.service                                      loaded active running ABRT kernel log watcher
abrt-xorg.service                                      loaded active running ABRT Xorg log watcher
abrtd.service                                          loaded active running ABRT Automated Bug Reporting Tool
accounts-daemon.service                                loaded active running Accounts Service
alsa-state.service                                     loaded active running Manage Sound Card State (restore and store)
atd.service                                            loaded active running Job spooling tools
|
|
|..etc

</pre>


To view all services irrespective of whether they are running or not, we do:

<pre>
$ systemctl list-unit-files --type=service
UNIT FILE                                   STATE
abrt-ccpp.service                           enabled
abrt-oops.service                           enabled
abrt-pstoreoops.service                     disabled
abrt-vmcore.service                         enabled
abrt-xorg.service                           enabled
abrtd.service                               enabled
accounts-daemon.service                     enabled
alsa-restore.service                        static
alsa-state.service                          static
alsa-store.service                          static
.
.
...etc
</pre>

This is similar to the old chkconfig command, since it shows which services are currently enabled so that they autostart at boot time. However unlike chkconfig, it only shows the info for the current target (which is a bit like a runlevel). ]]></Content>
		<Date><![CDATA[2015-06-06]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Mount Filesystems during boot time using Systemd]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

Let's say we have the following scenario:
<ul>
	<li>we have the partition: <strong>/dev/sdb1</strong></li>
	<li>we have the mountpoint:  <strong>/tmp/sdb1custom</strong></li>
	<li>the /dev/sdb1 partition has the xfs mountpoint installed on it.</li>
	<li>The /dev/sdb1 needs to be automounted by the <strong>multi-user.target</strong></li>
</ul>


[accordion]
[toggle title="What will be compatible name of the mount unit file that you need to create?"]
$ touch /etc/systemd/system/tmp-sdb1custom.mount
# replace the "/" to create the mount unit's file name.
[/toggle]
[toggle title="What should this file contain?"]
$ cat /etc/systemd/system/tmp-sdb1custom.mount
[Unit]
Description=My new file system

[Mount]
What=/dev/sdb1
Where=/tmp/sdb1custom
Type=xfs
Options=grpquota,ro

[Install]
WantedBy=multi-user.target
[/toggle]
[toggle title="What is the command to activate the mount?"]
$ systemctl start tmp-sdb1custom.mount
[/toggle]
[toggle title="What is the command to activate the automounting?"]
$ systemctl enable tmp-sdb1custom.mount
[/toggle]

[/accordion]

<hr/>



In an earlier lesson we saw how to use the <a href="http://codingbee.net/tutorials/rhcsa/rhcsa-mounting-a-partition/">/etc/fstab</a> file to automatically mount a file system during boot time.  However there's a new way to automount using systemd. This is done by creating a "mount" unit file. You should find some existing ones here:


<pre>
$ ls -l /usr/lib/systemd/system | grep "mount$"
-rw-r--r--. 1 root root  636 Jun 10  2014 dev-hugepages.mount
-rw-r--r--. 1 root root  590 Jun 10  2014 dev-mqueue.mount
-rw-r--r--. 1 root root  115 Jun 10  2014 proc-fs-nfsd.mount
-rw-r--r--. 1 root root  693 Jun 10  2014 proc-sys-fs-binfmt_misc.automount
-rw-r--r--. 1 root root  603 Jun 10  2014 proc-sys-fs-binfmt_misc.mount
-rw-r--r--. 1 root root  681 Jun 10  2014 sys-fs-fuse-connections.mount
-rw-r--r--. 1 root root  685 Jun 10  2014 sys-kernel-config.mount
-rw-r--r--. 1 root root  628 Jun 10  2014 sys-kernel-debug.mount
-rw-r--r--. 1 root root  669 Jun 10  2014 tmp.mount
-rw-r--r--. 1 root root  131 Jun 10  2014 var-lib-nfs-rpc_pipefs.mount
</pre>  
 
To see which of these are active, do:


<pre>
$ systemctl list-units --type=mount
UNIT                         LOAD   ACTIVE SUB     DESCRIPTION
-.mount                      loaded active mounted /
boot.mount                   loaded active mounted /boot
dev-hugepages.mount          loaded active mounted Huge Pages File System
dev-mqueue.mount             loaded active mounted POSIX Message Queue File System
proc-fs-nfsd.mount           loaded active mounted RPC Pipe File System
sys-kernel-config.mount      loaded active mounted Configuration File System
sys-kernel-debug.mount       loaded active mounted Debug File System
var-lib-nfs-rpc_pipefs.mount loaded active mounted RPC Pipe File System

LOAD   = Reflects whether the unit definition was properly loaded.
ACTIVE = The high-level unit activation state, i.e. generalization of SUB.
SUB    = The low-level unit activation state, values depend on unit type.

8 loaded units listed. Pass --all to see loaded but inactive units, too.
To show all installed unit files use 'systemctl list-unit-files'.
</pre>

Now, let's say I created a new partition, sdb1, and I have a filesytem installed on it. 


<pre>
$ lsblk
NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda               8:0    0   20G  0 disk
├─sda1            8:1    0  500M  0 part /boot
└─sda2            8:2    0 19.5G  0 part
  ├─centos-swap 253:0    0    2G  0 lvm  [SWAP]
  └─centos-root 253:1    0 17.5G  0 lvm  /
sdb               8:16   0    2G  0 disk
<strong>└─sdb1            8:17   0    1G  0 part</strong>
sr0              11:0    1 1024M  0 rom
</pre>


I now want to automount sdb1 using systemd. The first thing I want to do is create the mountpoint, and for this example, I'll create:


<pre>$ mkdir /tmp/sdb1custom</pre>



Next I need to create a .mount file. I can't create the .mount file in /usr/lib/systemd/system, because that directory is for systemd's internal use only. Instead I create it in <code>/etc/systemd/system/</code>. The easiest/quickest way to do this is by making a copy from an existing *.mount file, in my case I'll use tmp.mount as my template:

<pre>
$ cp /usr/lib/systemd/system/tmp.mount /etc/systemd/system/tmp-sdb1custom.mount</pre>

Note: the .mount file's name has to mirror the mountpoint's name, just replace the slashes with hyphens.  
 
Now, let's view it's content:

<pre>
$ cat /etc/systemd/system/tmp-sdb1custom.mount
#  This file is part of systemd.
#
#  systemd is free software; you can redistribute it and/or modify it
#  under the terms of the GNU Lesser General Public License as published by
#  the Free Software Foundation; either version 2.1 of the License, or
#  (at your option) any later version.

<strong>[Unit]
Description=Temporary directory
</strong>Documentation=man:hier(7)
Documentation=http://www.freedesktop.org/wiki/Software/systemd/APIFileSystems
DefaultDependencies=no
Conflicts=umount.target
Before=local-fs.target umount.target

<strong>[Mount]
What=tmpfs
Where=/tmp
Type=tmpfs
Options=mode=1777,strictatime
</strong>


# Make 'systemctl enable tmp.mount' work:
<strong>[Install]
WantedBy=local-fs.target
</strong>

</pre>


I have highlighted the mandatory parts in bold above. Using this template as a starting point, I created the following file:

<pre>
$ cat /etc/systemd/system/tmp-sdb1custom.mount
[Unit]
Description=My new file system

[Mount]
What=/dev/sdb1
Where=/tmp/sdb1custom
Type=xfs
Options=grpquota,ro

[Install]
WantedBy=multi-user.target
</pre>

Notice that when compared to a *.service file, a *.mount file has a "[Mount]" section instead of a "[Service]" section. 


Now we can test this by manually mounting using the sytemctl command. But let's confirm it isn't mounted yet:


<pre>
$ mount | grep sdb1
</pre> 

Now let's mount it using the systemctl command:

<pre>
$ systemctl start tmp-sdb1custom.mount
</pre>

Notice that's it's similar to the command you use to start a service. We can confirm that the mount has been successful using systemctl:

<pre>$ systemctl status tmp-sdb1custom.mount
tmp-sdb1custom.mount - My new file system
   Loaded: loaded (/etc/systemd/system/tmp-sdb1custom.mount; disabled)
   Active: <strong>active (mounted)</strong> since Mon 2015-06-08 23:09:21 BST; 6s ago
    Where: /tmp/sdb1custom
     What: /dev/sdb1
  Process: 1745 ExecMount=/bin/mount /dev/sdb1 /tmp/sdb1custom -t xfs -o grpquota,ro (code=exited, status=0/SUCCESS)

Jun 08 23:09:21 localhost.localdomain systemd[1]: Mounting My new file system...
</pre>


We can also check it worked using the mount command:

<pre>
$ mount | grep sdb1
/dev/sdb1 on /tmp/sdb1custom type xfs (ro,relatime,seclabel,attr2,inode64,grpquota)
</pre>


Another way to check is using lsblk:


<pre>$ lsblk
NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda               8:0    0   20G  0 disk
├─sda1            8:1    0  500M  0 part /boot
└─sda2            8:2    0 19.5G  0 part
  ├─centos-swap 253:0    0    2G  0 lvm  [SWAP]
  └─centos-root 253:1    0 17.5G  0 lvm  /
sdb               8:16   0    2G  0 disk
<strong>└─sdb1            8:17   0    1G  0 part /tmp/sdb1custom</strong>
sr0              11:0    1 1024M  0 rom
</pre>

Now to unmount we do:


<pre>$ umount /dev/sdb1</pre>









Now if we want this filesystem to auto-mount during the boot process, we need to enable it (in the same way that we do for services), therefore we do:


<pre>
$ systemctl enable tmp-sdb1custom.mount
ln -s '/etc/systemd/system/tmp-sdb1custom.mount' '/etc/systemd/system/multi-user.target.wants/tmp-sdb1custom.mount'
</pre>

Now let's confirm that it is enabled:


<pre>
$ systemctl status tmp-sdb1custom.mount
tmp-sdb1custom.mount - My new file system
   Loaded: loaded (/etc/systemd/system/tmp-sdb1custom.mount; <mark>enabled</mark>)
   Active: active (mounted) since Mon 2015-06-08 23:09:21 BST; 6min ago
    Where: /tmp/sdb1custom
     What: /dev/sdb1

Jun 08 23:09:21 localhost.localdomain systemd[1]: Mounting My new file system...
</pre>



Now let's go ahead and reboot the machine. After that let's see if sdb1 get's automounted:


<pre>
$ mount | grep sdb1
/dev/sdb1 on /tmp/sdb1custom type xfs (ro,relatime,seclabel,attr2,inode64,grpquota)
</pre>

Success!]]></Content>
		<Date><![CDATA[2015-06-08]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Automounting using systemd and autofs]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

Let's say we have the following scenario:
<ul>
	<li>we have the partition: <strong>/dev/sdb1</strong></li>
	<li>we have the mountpoint:  <strong>/tmp/sdb1custom</strong></li>
	<li>the /dev/sdb1 partition has the xfs mountpoint installed on it.</li>
	<li>The /dev/sdb1 needs to be automounted by the <strong>multi-user.target</strong></li>
</ul>


[accordion]
[toggle title="What will be compatible name of the mount unit file that you need to create?"]
$ touch /etc/systemd/system/tmp-sdb1custom.mount
# replace the "/" to create the mount unit's file name.
[/toggle]
[toggle title="What should this file contain?"]
$ cat /etc/systemd/system/tmp-sdb1custom.mount
[Unit]
Description=My new file system

[Mount]
What=/dev/sdb1
Where=/tmp/sdb1custom
Type=xfs
Options=grpquota,ro

[Install]
WantedBy=multi-user.target
[/toggle]
[toggle title="What is the command to activate the mount?"]
$ systemctl start tmp-sdb1custom.mount
[/toggle]
[toggle title="What is the command to activate the automounting?"]
$ systemctl enable tmp-sdb1custom.mount
[/toggle]
[toggle title="So far we have created a normal systemd mount, what is the next thing you need to to set up autofs style automounting?"]
# stop + disable our new tmp-sdb1custom.mount:
$ systemctl stop tmp-sdb1custom.mount
$ systemctl disable tmp-sdb1custom.mount
[/toggle]
[toggle title="What do you do next?"]
# create the tmp-sdb1custom.mount file
$ cp /etc/systemd/system/tmp-sdb1custom.mount /etc/systemd/system/tmp-sdb1custom.automount
[/toggle]
[toggle title="What do you do next?"]
# modify this file so it looks like this, i.e.  has the '[automount]' stanza


$ cat /etc/systemd/system/tmp-sdb1custom.automount
[Unit]
Description=My new automounted file system

[Automount]

[Install]
WantedBy=multi-user.target

[/toggle]
[toggle title="What do you do next?"]
# start and enable the new automount unit
$ systemctl start tmp-sdb1custom.automount
$ systemctl enable tmp-sdb1custom.automount
[/toggle]

[/accordion]

<hr/>



Earlier when we covered <a href="http://codingbee.net/tutorials/rhcsa/rhcsa-autofs/">autofs</a>, we found how a filesystem is not mounted until you actually cd into it's mount point.  

You can achieve the same thing with systemd as well. To get started, you first need to create a mountpoint and a *.mount file, like we did in the previous lesson, so that it appears that we are going to <a href="http://codingbee.net/tutorials/rhcsa/rhcsa-auto-mount-filesystems-during-boot-time-using-systemd/">auto-mount the file system during machine boot</a>. But this time we don't start the mount "unit" and we ensure that it is disabled.


Let's use the filesystem, sdb1, from the last lesson as an example. So we have our filesystem:


<pre>$ ls -l /dev/sdb1
brw-rw----. 1 root disk 8, 17 Jun  9 06:12 /dev/sdb1
</pre>

...and we have our mountpoint:


<pre>$ ls -l /tmp | grep sdb1
drwxr-xr-x. 2 root root  6 Jun  8 21:01 sdb1custom
</pre>

...and we have our *.mount file:


<pre>
$ cat /etc/systemd/system/tmp-sdb1custom.mount
[Unit]
Description=My new file system

[Mount]
What=/dev/sdb1
Where=/tmp/sdb1custom
Type=xfs
Options=grpquota,ro

[Install]
WantedBy=multi-user.target
</pre>

Next in order to take the automount route, we actually need to stop+disable this unit: 


<pre>
$ systemctl stop tmp-sdb1custom.mount
$ systemctl disable tmp-sdb1custom.mount
rm '/etc/systemd/system/multi-user.target.wants/tmp-sdb1custom.mount'
</pre>



So far so good. Now we come to the steps we need to take to set up the actual automounting feature. To do this we create a new <strong>*.automount</strong> unit file in the /etc/systemd/system directory. This file must have the same name as the corresponding *.mount file, but with a ".automount" suffix. Therefore it's easier to create this file by making it a copy of the .mount file:

<pre>
$ cp /etc/systemd/system/tmp-sdb1custom.mount /etc/systemd/system/tmp-sdb1custom.automount
$ ls -l /etc/systemd/system/tmp-sdb1custom.automount
-rw-r--r--. 1 root root 151 Jun  9 06:30 /etc/systemd/system/tmp-sdb1custom.automount
</pre>

Now you need to edit this file so that it looks like this:

<pre>
$ cat /etc/systemd/system/tmp-sdb1custom.automount
[Unit]
<del datetime="2015-11-07T21:11:09+00:00">Description=My new file system</del>
Description=My new automounted file system

<del datetime="2015-11-07T21:11:09+00:00">
[Mount]
What=/dev/sdb1
Where=/tmp/sdb1custom
Type=xfs
Options=grpquota,ro
</del>

[Automount]
Where=/tmp/sdb1custom

[Install]
WantedBy=multi-user.target
</pre>

Notice, that this unit's main config section is now called "[Automount]". Also notice that I included a "Where" setting in this section. This is actually optional, and if specified, it must match what is in the corresponding .mount file. It's doesn't work like some kind of over-ride mechanism, as you might have initially suspected, you will get an error message if you tried.  

Now let's enable and start this unit:


<pre>
$ systemctl status tmp-sdb1custom.automount
tmp-sdb1custom.automount - My new automounted file system
   Loaded: loaded (/etc/systemd/system/tmp-sdb1custom.automount; disabled)
   Active: inactive (dead)
    Where: /tmp/sdb1custom

$ systemctl start tmp-sdb1custom.automount
$ systemctl enable tmp-sdb1custom.automount
ln -s '/etc/systemd/system/tmp-sdb1custom.automount' '/etc/systemd/system/multi-user.target.wants/tmp-sdb1custom.automount'
$ systemctl status tmp-sdb1custom.automount
tmp-sdb1custom.automount - My new automounted file system
   Loaded: loaded (/etc/systemd/system/tmp-sdb1custom.automount; <mark>enabled</mark>)
   Active: <mark>active (waiting)</mark> since Tue 2015-06-09 06:48:00 BST; 11s ago
    Where: /tmp/sdb1custom

Jun 09 06:48:00 localhost.localdomain systemd[1]: Starting My new automounted file system.
Jun 09 06:48:00 localhost.localdomain systemd[1]: Set up automount My new automounted file system.

</pre>
   
Once it has been started, you will see that the mount command shows that it is now a auto-mounting (aka autofs) mountpoint:

<pre>
$ mount | grep sdb1
systemd-1 on /tmp/sdb1custom type autofs (rw,relatime,fd=45,pgrp=1,timeout=300,minproto=5,maxproto=5,direct)
</pre>



However lsblk shows that it hasn't been mounted yet:


<pre>
$ lsblk
NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda               8:0    0   20G  0 disk
├─sda1            8:1    0  500M  0 part /boot
└─sda2            8:2    0 19.5G  0 part
  ├─centos-swap 253:0    0    2G  0 lvm  [SWAP]
  └─centos-root 253:1    0 17.5G  0 lvm  /
sdb               8:16   0    2G  0 disk
└─sdb1            8:17   0    1G  0 part
sr0              11:0    1 1024M  0 rom
</pre>

Let's now cd into the mountpoint and try again:


<pre>
$ cd /tmp/sdb1custom/
</pre>

Then try lsblk again:


<pre>
$ lsblk
NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda               8:0    0   20G  0 disk
├─sda1            8:1    0  500M  0 part /boot
└─sda2            8:2    0 19.5G  0 part
  ├─centos-swap 253:0    0    2G  0 lvm  [SWAP]
  └─centos-root 253:1    0 17.5G  0 lvm  /
sdb               8:16   0    2G  0 disk
└─sdb1            8:17   0    1G  0 part <mark>/tmp/sdb1custom</mark>
sr0              11:0    1 1024M  0 rom

</pre>


Success!


Also the mount command also shows that it has been mounted:


<pre>
$ mount | grep sdb1
systemd-1 on /tmp/sdb1custom type autofs (rw,relatime,fd=45,pgrp=1,timeout=300,minproto=5,maxproto=5,direct)
<strong>/dev/sdb1 on /tmp/sdb1custom type xfs (ro,relatime,seclabel,attr2,inode64,grpquota)
</strong>
</pre>




]]></Content>
		<Date><![CDATA[2015-06-08]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - r10k and the Puppetfile]]></Title>
		<Content><![CDATA[The "Puppetfile" is list of all puppet modules you want downloaded into your puppet master's modules folder:


The first line of this file should be:


<pre>forge "http://forge.puppetlabs.com"</pre>


After that any puppet forge modules are listed as:


<pre>mod 'puppetlabs/stdlib',  '4.5.1'</pre>


Next any puppet modules hosted on git (e.g. github or any other git server, e.g. stash) can also be added, based on git's commit id:

<pre>
mod 'sudo',
  :git => 'https://github.com/saz/puppet-sudo.git',
  :ref => '231e15fb9311233ee0fe12f4d9bd6ec978d54a2c'
</pre>

or a particular, irrespective of branch:

<pre>
mod 'sudo',
  :git => 'https://github.com/saz/puppet-sudo.git',
  :ref => '231e15fb9311233ee0fe12f4d9bd6ec978d54a2c'
</pre>


or a particular branche's latest commit:

<pre>
mod 'sudo',
  :git => 'https://github.com/saz/puppet-sudo.git',
  :ref => '{branch-name}'
</pre>



Or even a particular git (version) tag:

<pre>
mod 'sudo',
  :git => 'https://github.com/saz/puppet-sudo.git',
  :tag => '1.2.0'
</pre>



Note, ensure you include all module dependencies. Then to check that you all dependencies are included, run:



<pre>$ puppet module list --tree --modulepath /etc/puppetlabs/code/environments/{env-name}/modules</pre>





]]></Content>
		<Date><![CDATA[2015-06-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Resetting the root password (via GRUB2)]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="Assume you don't have the root password and you reboot your machine. What is the first thing you need to do while the machine is booting up?"]
Wait until the grub menu appears and then hit either up/down arrow, in order to disable the timeout. 
[/toggle]
[toggle title="Then what do you do?"]
press "e"
[/toggle]
[toggle title="Then what do you do?"]
Scroll down to the line begining with "linux16"
[/toggle]
[toggle title="Then what do you do?"]
Hit the "end" key
optional: delete the "rhgb quiet" in order to enable the verbose mode.
[/toggle]
[toggle title="Then what do you do?"]
type "rd.break"

[/toggle]
[toggle title="Then what do you do?"]
then press "ctrl+x" 

[/toggle]
[toggle title="You are now logged. Then what command do you run?"]
$ mount -o remount,rw /sysroot

[/toggle]
[toggle title="Then what command do you run?"]
$ chroot /sysroot

[/toggle]
[toggle title="Then what command do you run?"]
$ echo NewPassword | passwd <span style="letter-spacing:1px">-</span>-stdin root 
[/toggle]
[toggle title="Then what command do you run?"]
$ touch /.autorelabel
[/toggle]
[toggle title="Then what command do you run?"]
# run exit twice
$ exit
$ exit
[/toggle]
[toggle title="Then what do you do?"]
Wait for about 2 minutes
[/toggle]
[toggle title="Then what do you do?"]
The machine will have restarted, now you can try logging in as root with the new password. 
[/toggle]

[/accordion]

<hr/>



During machine boot, if you edit the grub parameters just so to enter the rescue/emergency, then you will still get prompted to enter the root password. That's why we take a different approach to reset the root password. When you machine is booting up:

<ol>
	<li>Press down then up arrow keys as soon as the kernel selection menu appears. This will pause the boot process</li>
	<li>press "e" in order to edit your grub parameter settings.</li>
	<li>Scroll down to the <code>linux16</code> line, then press the "end" key to reach the end of that line.</li>
	<li>add <code>rd.break</code> at the end of the "linux16" line.</li>
</ol>


<a href="http://codingbee.net/wp-content/uploads/2015/06/yTg5Po4.png"><img src="http://codingbee.net/wp-content/uploads/2015/06/yTg5Po4.png" alt="rd.break" width="738" height="418" class="alignnone size-full wp-image-4628" /></a>


The rd.break setting instructs the boot process to stop at a specific point during the initramfs process. This is a point where just the main core filesystem has been mounted, but none of the other filesystems have been mounted. 


Tip: also good idea to remove the "rhgb quiet" flags from the grub parameter list so that you can monitor what is happening. 


Then do "ctrl+x", to resume the boot process with the modified grub parameter input. 

After that you'll see:


<a href="http://codingbee.net/wp-content/uploads/2015/06/rtNViYv.png"><img src="http://codingbee.net/wp-content/uploads/2015/06/rtNViYv.png" alt="" width="739" height="420" class="alignnone size-full wp-image-4631" /></a>




Next we have to run a series of specific commands, first we run:


<pre>
switch_root:/# mount -o remount,rw /sysroot
</pre>

This is to make our root filesystem mount writable. Our root filesystem is currently "/sysroot", that's because we are so early in the boot process. Also at this early stage it is set to read-only mode. 


Next we run:

<pre>
switch_root:/# chroot /sysroot
</pre>

This temporarily makes "/sysroot" our root directory instead of "/". This has the affect of changing the command prompt to:

<a href="http://codingbee.net/wp-content/uploads/2015/06/1t1CPgR.png"><img src="http://codingbee.net/wp-content/uploads/2015/06/1t1CPgR.png" alt="" width="735" height="414" class="alignnone size-full wp-image-4633" /></a>

Now we can set the new password by running the following command:


<pre>
sh-4.2# echo NewPassword | passwd --stdin root
</pre>

Here my new root password will become "NewPassword"

Note: rather than running the above command. You can instead use the passwd command in the normal way. 

Finally we run the following command:


<pre>
sh-4.2# touch /.autorelabel
</pre>
 
Now we exit out of chroot:


<pre>
$ exit
</pre>


You should now see something like this:


<a href="http://codingbee.net/wp-content/uploads/2015/06/BYIOeSA.png"><img src="http://codingbee.net/wp-content/uploads/2015/06/BYIOeSA.png" alt="" width="744" height="423" class="alignnone size-full wp-image-4635" /></a>

Then exit again to reboot the system:

<pre>
$ exit
</pre>

<strong>The reboot may take about 2 minutes, which is normal</strong>. That is why you should remove the "rhgb quiet" as suggested so that you can monitor the boot process. 

Now the machine shout reboot and you should then be able to login using your root user's new password. 


See also:

https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/System_Administrators_Guide/sec-Terminal_Menu_Editing_During_Boot.html#proc-Resetting_the_Root_Password_Using_rd.break]]></Content>
		<Date><![CDATA[2015-06-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Ordering your classes]]></Title>
		<Content><![CDATA[this is common, for example, sometimes once class installs dbg software, and then the next class creates the db istself.


<pre>
class mainclass {
  include class1
  include class2
  include class3
  include class4

  Class['class1']
  -> Class['class2']
  -> Class['class3']
  -> Class['class4']
} </pre>]]></Content>
		<Date><![CDATA[2015-06-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Ordering using "Run Stages"]]></Title>
		<Content><![CDATA[So far we have looked at order resources, and ordering classes. 

Run Stages let you do ordering at a higher level. 

https://docs.puppetlabs.com/puppet/latest/reference/lang_run_stages.html]]></Content>
		<Date><![CDATA[2015-06-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Installing and setting up the Apache web server]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:


[accordion]
[toggle title="What is the command to install apache?"]
$ yum install httpd
[/toggle]
[toggle title="What is the main config file of this file?"]
/etc/httpd/conf/httpd.conf
[/toggle]
[toggle title="Which folder do you store all your web content files, html files, css files,...etc?"]
/var/www/html
[/toggle]
[toggle title="What is the command to start the apache service?"]
$ systemctl start httpd
[/toggle]
[toggle title="What ports does this service listen on?"]
port 80  - for httpd
prot 443 - for httpds
[/toggle]
[toggle title="What is the command to add an 'allow' firewall rule so that the apache service can listen on these port?"]
$ firewall-cmd --add-service=http --permanent
$ firewall-cmd --add-service=https --permanent
$ systemctl restart firewalld
[/toggle]
[toggle title="What is the command to view your apaches, selinux allow mapping rules, context assignment rules, and boolean settings?"]
$ sesearch --allow | grep httpd
$ semanage {subcommand} -l | grep httpd
$ getsebool -a | grep httpd
[/toggle]
[toggle title="What is the command to view apache related log messages?"]
$ journalctl -xu httpd.service
[/toggle]
[toggle title="What is the command to view potential apache related SELinux error messages?"]
$ sealert -a /var/log/audit/audit.log
$ sealert -a /var/log/messages
[/toggle]
[toggle title="How do you check your web server is working?"]
Open firefox and go to "http://localhost"
[/toggle]




One of the most common reasons for having a RHEL server is so to set it up as a web server to host websites that other people can access via the internet. In order to set up a website as a web server, you need to install a web server software your machine. The "Apache HTTP Server" is the most commonly web server software used worldwide.  

In the main public yum repos, the apache software goes by the name, "httpd". So the first thing we need to do is install this software:


<pre>
$ yum install httpd
</pre>


httpd also comes with a seperate user manual that you can also install:


<pre>
$ yum install httpd-manual
</pre>

This is actually a web based manual, and to gain access to it, first restart the httpd service:

<pre>
$ systemctl restart httpd.service
</pre>

Then open a web browser and go to:

<pre>
httpd://localhost/manual
</pre>

Now let's see where all the httpd's config files are located:


<pre>
$ rpm -qc httpd
/etc/httpd/conf.d/autoindex.conf
/etc/httpd/conf.d/userdir.conf
/etc/httpd/conf.d/welcome.conf
/etc/httpd/conf.modules.d/00-base.conf
/etc/httpd/conf.modules.d/00-dav.conf
/etc/httpd/conf.modules.d/00-lua.conf
/etc/httpd/conf.modules.d/00-mpm.conf
/etc/httpd/conf.modules.d/00-proxy.conf
/etc/httpd/conf.modules.d/00-systemd.conf
/etc/httpd/conf.modules.d/01-cgi.conf
<strong>/etc/httpd/conf/httpd.conf</strong>
/etc/httpd/conf/magic
/etc/logrotate.d/httpd
/etc/sysconfig/htcacheclean
/etc/sysconfig/httpd
</pre>

httpd's main config file is the httpd.conf file, which I have highlighted in bold above. This file contains a lot of comments to help you configure the file.

A couple of the main setting are 

<pre>
$ grep '^DocumentRoot' /etc/httpd/conf/httpd.conf
DocumentRoot "/var/www/html"
</pre>

<pre>
$ grep '^Listen' /etc/httpd/conf/httpd.conf
Listen 80
</pre>


The DocumentRoot setting shows where you need to store your websites files, e.g. html, css, php, javascript,...and etc. 'Listen' sets what ports to listen on.  


Another handy software that you could use is, "elinks", which is command line based web browser. We can use this to test whether our web server is working from the command line: 


<pre>
$ yum install elinks
</pre>


Here's how you can use elinks:



<pre>
$ elinks --help
ELinks 0.12pre6

Usage: elinks [OPTION]... [URL]...

Options:
  -anonymous [0|1]      Restrict to anonymous mode
  -auto-submit [0|1]    Autosubmit first form
  -base-session <num>   Clone internal session with given ID
  -config-dir <str>     Name of directory with configuration file
  -config-dump          Print default configuration file to stdout
  -config-file <str>    Name of configuration file
  -config-help          Print help for configuration options
  -default-mime-type    MIME type assumed for unknown document types
  -default-keys [0|1]   Ignore user-defined keybindings
  -dump [0|1]           Print formatted versions of given URLs to stdout
  -dump-charset         Codepage to use with -dump
  -dump-color-mode      Color mode used with -dump
  -dump-width           Width of document formatted with -dump
  -eval                 Evaluate configuration file directive
  -force-html           Interpret documents of unknown types as HTML
  -?, -h, -help         Print usage help and exit
  -localhost [0|1]      Only permit local connections
  -long-help            Print detailed usage help and exit
  -lookup               Look up specified host
  -no-connect [0|1]     Run as separate instance
  -no-home [0|1]        Disable use of files in ~/.elinks
  -no-numbering         Disable link numbering in dump output
  -no-references        Disable printing of link references in dump output
  -remote               Control an already running ELinks
  -session-ring <num>   Connect to session ring with given ID
  -source [0|1]         Print the source of given URLs to stdout
  -touch-files [0|1]    Touch files in ~/.elinks when running with -no-connect/-session-ring
  -verbose <num>        Verbose level
  -version              Print version information and exit
</pre>


Now lets test our webserver with elinks, using "localhost" as our website's url:

<pre>
$ elinks http://localhost
</pre>


You might get the following:



<a href="http://codingbee.net/wp-content/uploads/2015/06/UGRNA8K.png"><img src="http://codingbee.net/wp-content/uploads/2015/06/UGRNA8K.png" alt="" width="777" height="618" class="alignnone size-full wp-image-4662" /></a>






This could be caused by the fact that the httpd service might not be running. So do ctrl+c to exit. 


Now let's check the status of the httpd service:



<pre>
$  systemctl status httpd
httpd.service - The Apache HTTP Server
   Loaded: loaded (/usr/lib/systemd/system/httpd.service; disabled)
   Active: inactive (dead)
</pre>

Now let's start this service (enable it so that it start on next boot):


<pre>
$ systemctl enable httpd
ln -s '/usr/lib/systemd/system/httpd.service' '/etc/systemd/system/multi-user.target.wants/httpd.service'
$ systemctl start httpd
</pre>

Now let's recheck the status:


<pre>
$ systemctl status httpd -l
httpd.service - The Apache HTTP Server
   Loaded: loaded (/usr/lib/systemd/system/httpd.service; enabled)
   Active: active (running) since Wed 2015-06-10 21:24:16 BST; 2s ago
  Process: 32080 ExecStop=/bin/kill -WINCH ${MAINPID} (code=exited, status=0/SUCCESS)
 Main PID: 32085 (httpd)
   Status: "Processing requests..."
   CGroup: /system.slice/httpd.service
           ├─32085 /usr/sbin/httpd -DFOREGROUND
           ├─32086 /usr/sbin/httpd -DFOREGROUND
           ├─32087 /usr/sbin/httpd -DFOREGROUND
           ├─32088 /usr/sbin/httpd -DFOREGROUND
           ├─32089 /usr/sbin/httpd -DFOREGROUND
           └─32090 /usr/sbin/httpd -DFOREGROUND

Jun 10 21:24:15 centos7.codingbee.dyndns.org systemd[1]: Starting The Apache HTTP Server...
Jun 10 21:24:16 centos7.codingbee.dyndns.org systemd[1]: Started The Apache HTTP Server.

</pre>

Now let's try elinks again, this time we should get:


<a href="http://codingbee.net/wp-content/uploads/2015/06/OFkuAXY.png"><img src="http://codingbee.net/wp-content/uploads/2015/06/OFkuAXY.png" alt="" width="1009" height="614" class="alignnone size-full wp-image-4663" /></a>


Success!



Note: Also a few other troubleshooting tips is to stop the firewalld service and restart the network service.  

Now let's write our "hello world" index.html file:


<pre>
$ echo "hello world" > /var/www/html/index.html
</pre>


Then if we check our homepage again. we should now see:


<a href="http://codingbee.net/wp-content/uploads/2015/06/PzIhkOq.png"><img src="http://codingbee.net/wp-content/uploads/2015/06/PzIhkOq.png" alt="" width="493" height="612" class="alignnone size-full wp-image-4666" /></a>


Success!


Another thing, earlier we installed the httpd-manual rpm. Let's see what config file came with this package:


<pre>
$ rpm -qc httpd-manual
/etc/httpd/conf.d/manual.conf
</pre> 

Now let's view this file:


<pre>
$ cat /etc/httpd/conf.d/manual.conf
#
# This configuration file allows the manual to be accessed at
# http://localhost/manual/
#
AliasMatch ^/manual(?:/(?:de|en|fr|ja|ko|ru))?(/.*)?$ "/usr/share/httpd/manual$1"

<Directory "/usr/share/httpd/manual">
    Options Indexes
    AllowOverride None
    Require all granted
</Directory>
</pre>

As you can see, "http://localhost/manual/" has a separate folder containing html websites. This means if we do:


<pre>
http://localhost/manual/
</pre>
 

Then web content is served from /usr/share/httpd/manual, rather than the default /var/www/html. Hence we get the following when we open this url in elinks:


<a href="http://codingbee.net/wp-content/uploads/2015/06/zFUpvbj.png"><img src="http://codingbee.net/wp-content/uploads/2015/06/zFUpvbj.png" alt="" width="1002" height="500" class="alignnone size-full wp-image-4668" /></a>]]></Content>
		<Date><![CDATA[2015-06-18]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Handy ruby gems for RSpec testing]]></Title>
		<Content><![CDATA[https://github.com/mcanevet/rspec-puppet-facts  # lets you set a bunch of default facts. ]]></Content>
		<Date><![CDATA[2015-06-18]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>rspec-puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - SELinux Overview]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What is the command to view user guide about selinux?"]
$ man selinux
[/toggle]
[toggle title="What are the 2 main types of security layers, and which type is Selinux?"]
- Discretionary Access Control  
- Mandatory Access Control  (selinux is this type)
[/toggle]
[toggle title="What are the SELinux packages that you should always install before working wiht SElinux?"]
- policycoreutils-python
- policycoreutils-gui
- policycoreutils-devel
- setools-console
- setools-gui
- setroubleshoot
- setroubleshoot-server
[/toggle]
[/accordion]

<hr/>


<h2>What is SELinux?</h2>
SELinux is a system that is primarily used for protecting your machine from potential attacks from the internet.  For example there are a lot of programs, e.g. apache that comes with it's own service-account e.g. "httpd". Hackers may be able to find a way to take control of that service-account and thereby gain access to other, assets, e.g. files, folders, process,..etc. This crucially includes being able to access not apache related assets. For example the apache user has read access to the /etc directory:


 <pre>
$ ls -l / | grep etc
drwxr-xr-x. 143 root    root    8192 Oct 14 17:33 etc
</pre>

Therefore if the httpd service account has been compromised, then the attacker can use this account to look inside this directory (along with lots of other places), in order to malicious activities. 

In this situation the traditional ugo+rwx permission just doesn't offer a way to combat this kind of vulnerability. And that is where SELinux comes to the rescue. 

SELinux essentially does damage control. That is, if the apache service account has been compromised, then the attacker can only access files+folders that the apache service account would normally need access to. Therefore an attacker (while masquarading as the apache user) won't be able to access a folder, such as the "/etc" folder even if the httpd service account has the necessary ugo+rwx privileges for this folder. 

In later article we'll cover later how SELinux manages to achieve this kind of granular access control. In the meantime the following man page gives a short overview of SELinux:

<pre>
$ man selinux
</pre>

The only situation where you might not need SELinux, is if your machine does not have internet access. 


<h2>Layers of Security</h2>
To make a machine secure, you add "layers" of security. SELinux is essentially just another layer of security. 

In Linux, there are different several layers of security, e.g. firewall, ugo+rwx, dmz, password policies, encryption, authentication....etc.  However all these layers doesn't harden security at the OS level. But SELinux does, in fact it's support is built into the kernel. That is why it is referred to as a <strong>Mandatory Access Control</strong>, aka MAC. 

All the other security layers, e.g. ugo+rwx and firewalls, are referred to as <strong>Discretionary Access Control</strong> (aka DAC). All DAC layers are vulnerable to human error. For example the root user who can inadvertently open up vulnerabilities. Whereas SELinux cannot accidentally be made vulnerable by mistake. 

So how do all these DAC and MAC layers work together? When one objects (e.g. a process) attempts to access another object (e.g. config file), then this requests first needs to be approved by all the DAC security layers, and after that it finally needs to get approved by the mac ( SELinux) layer. Access is permitted once the request is permitted by all the security layers..






<h2>Handy SELinux related utilities</h2>

Here are some relevant SELinux utilities that you should install:

<ul>
	<li>policycoreutils-python</li>
	<li>policycoreutils-gui</li>
	<li>setools-console</li>
	<li>setools-gui</li>
	<li>setroubleshoot</li>
	<li>setroubleshoot-server</li>
</ul>



I.e. we do:


<pre>
$ yum install policycoreutils-python policycoreutils-gui setools-console setools-gui setroubleshoot setroubleshoot-server

</pre>

Here's a quick description of each of these:


<pre>
$ yum info policycoreutils-python | grep "Description" -A1
Description : The policycoreutils-python package contains the management tools
            : use to manage an SELinux environment.


$ yum info policycoreutils-gui | grep "^Description" -A1
Description : system-config-selinux is a utility for managing the SELinux
            : environment


$ yum info setools-console | grep "^Description" -A7
Description : SETools is a collection of graphical tools, command-line tools,
            : and libraries designed to facilitate SELinux policy analysis.
            :
            : This package includes the following console tools:
            :
            :   secmds          command line tools: seinfo, sesearch
            :   sediff          semantic policy difference tool


$ yum info setools-gui | grep "^Description" -A7
Description : SETools is a collection of graphical tools, command-line tools,
            : and libraries designed to facilitate SELinux policy analysis.
            :
            : This package includes the following graphical tools:
            :
            :   apol          policy analysis tool
            :   seaudit       audit log analysis tool


$ yum info setroubleshoot | grep "^Description" -A8
Description : setroubleshoot GUI. Application that allows you to view
            : setroubleshoot-server messages.
            : Provides tools to help diagnose SELinux problems. When AVC
            : messages are generated an alert can be generated that will give
            : information about the problem and help track its resolution.
            : Alerts can be configured to user preference. The same tools can be
            : run on existing log files.


$ yum info setroubleshoot-server | grep "^Description" -A8
Description : Provides tools to help diagnose SELinux problems. When AVC
            : messages are generated an alert can be generated that will give
            : information about the problem and help track its resolution.
            : Alerts can be configured to user preference. The same tools can be
            : run on existing log files.

</pre>




https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Security-Enhanced_Linux/chap-Security-Enhanced_Linux-Working_with_SELinux.html


https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/pdf/SELinux_Users_and_Administrators_Guide/Red_Hat_Enterprise_Linux-7-SELinux_Users_and_Administrators_Guide-en-US.pdf]]></Content>
		<Date><![CDATA[2015-06-18]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - SELinux Modes]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What are the 3 main SELinux modes?"]
- Disabled
- Permissive
- Enforcing
[/toggle]
[toggle title="What are the 2 commands that you can use to view what the current SELinux mode is?"]
$ getenforce
# or 
$ sestatus
[/toggle]
[toggle title="What is the command that displays the default SELinux that is set during machine boot up?"]
$ sestatus
[/toggle]
[toggle title="Which file do you need to edit to change the default SELinux mode?"]
/etc/selinux/config
# the reboot the machine
[/toggle]
[toggle title="What is the command to (non-persistantly) switch to enforcing SELinux mode?"]
$ setenforce enforcing
[/toggle]
[toggle title="What is the command to (non-persistantly) switch to permissive SELinux mode?"]
$ setenforce permissive
[/toggle]
[toggle title="What do you need to do to (persistantly) switch to permissive SELinux mode?"]
# edit the /etc/selinux/config
[/toggle]
[toggle title="What is the command to switch to disabled SELinux mode?"]
Trick question, you can't do it on the command line, you have to edit /etc/selinux/config and then reboot the machine.
[/toggle]
[/accordion]

<hr/>

SELinux can be disabled, or running in either "passive" (i.e. permissive) or enforcing (i.e. active) modes. So before you can start understanding/using SELinux, you need to understand these modes when to use them.  


<h2>SELinux Modes</h2>

SELinux can run in three different operational modes:

<ul>
<li><strong>Disabled</strong> - In this mode, SELinux is switched off. In practice this means that none of the linux objects are labelled, i.e. they don't have a security context, and consequently the policy type is not being used for anything. In this situation the only active security layers are the DAC type security layers, e.g. ugo+rwx. </li>

<li><strong>Permissive</strong> - In this mode, all objects are labelled and a Policy type is being used. However it will only monitor and write SELinux breaches to log files rather than deny access of any object interacting with another object. This log file is located at <code>/var/log/audit/audit.log</code>. In other words SELinux is observing but not blocking anything at all. This means that the log files would show what SELinux would have blocked if it was in enforcing mode.</li>
<li><strong>enforcing</strong> - This does all the same thing as permissive mode, and at also actively deny access when there is a breach.</li>
</ul>



To see which SELinux mode is currently in use, we use the getenforce command:

<pre>
$ whatis getenforce
getenforce (8)       - get the current mode of SELinux

$ getenforce
Permissive
</pre>


Alternatively we can use the sestatus command:

<pre>
$ whatis sestatus
sestatus (8)         - SELinux status tool


$ sestatus
SELinux status:                 enabled
SELinuxfs mount:                /sys/fs/selinux
SELinux root directory:         /etc/selinux
Loaded policy name:             targeted
Current mode:                   permissive
Mode from config file:          enforcing             # this is the mode machine will boot into     
Policy MLS status:              enabled
Policy deny_unknown status:     allowed
Max kernel policy version:      28
</pre>


To switch from permissive mode to enforcing, or vice versa, we can use the setenforce command:


<pre>
$ whatis setenforce
setenforce (8)       - modify the mode SELinux is running in

$ getenforce
Permissive

$ setenforce enforcing

$ getenforce
Enforcing
</pre> 



Changing the mode using setenforce will not survive a reboot, as indicated by the sestatus command:


<pre>
$ sestatus
SELinux status:                 enabled
SELinuxfs mount:                /sys/fs/selinux
SELinux root directory:         /etc/selinux
Loaded policy name:             targeted
Current mode:                   enforcing
<strong>Mode from config file:          permissive</strong>
Policy MLS status:              enabled
Policy deny_unknown status:     allowed
Max kernel policy version:      28


</pre>



So to make the change permenant, we edit the following file:


<pre>
$ cat /etc/selinux/config

# This file controls the state of SELinux on the system.
# SELINUX= can take one of these three values:
#     enforcing - SELinux security policy is enforced.
#     permissive - SELinux prints warnings instead of enforcing.
#     disabled - No SELinux policy is loaded.
<strong>SELINUX=enforcing</strong>
# SELINUXTYPE= can take one of these two values:
#     targeted - Targeted processes are protected,
#     minimum - Modification of targeted policy. Only selected processes are protected.
#     mls - Multi Level Security protection.
SELINUXTYPE=targeted
</pre>

Note: we will cover <a href="http://codingbee.net/tutorials/rhcsa/rhcsa-selinux-policy-types-and-selinux-attributes/">SELINUXTYPE</a> later. 



<h2>Disabling SELinux</h2>

You can't use the setenforce command for switching to/away-from the disabled mode. Instead that is done by editing the <code>/etc/selinux/config</code> file, and then rebooting the machine. 

While in disabled mode, any objects that are created won't automatically get labelled. Here's an example of what happends

<pre>
$ cat /etc/selinux/config  | grep "^SELINUX="
SELINUX=disabled

# then reboot the machine, after which we then do:

$ getenforce
Disabled
$ ls -lZ Disabled-testfile.txt
-rw-r--r-- root root <mark>?</mark>                                Disabled-testfile.txt
</pre>

The "?" indicates that this object does not have an SELinux context.

Also when you switch from disabled to permissive/enabled mode, then the reboot can take longer. That's because SELinux scans the whole machine searching for any unlabelled objects (like the one above) and sets a label for them. So that we end up with something like:


<pre>
$ getenforce
Permissive
$ ls -lZ Disabled-testfile.txt
-rw-r--r--. root root <strong>system_u:object_r:admin_home_t:s0</strong> Disabled-testfile.txt
</pre> 





You can also switch SELinux modes by passing in a grub parameter during boot time. you set <code>enforcing=0</code> for permissive, and <code>enforcing=1</code> for enforcing.


Note: at the end of of rhcsa exam you need to always ensure that the machine is left in enforcing mode, unless instructed otherwise.]]></Content>
		<Date><![CDATA[2015-06-18]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Viewing SELinux Contexts]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What is the command to view user account SELinux context for the user account 'david'?"]
$ su - david
$ id 
[/toggle]
[toggle title="What is the command to view the SELinux context of the file /tmp/testfile.txt?"]
$ ls -lZ /tmp/testfile.txt
[/toggle]
[toggle title="What is the command to view all your processes along with their security contexts?"]
$ ps -efZ
[/toggle]
[toggle title="What is the command to view the security contexts for all your ports?"]
$ netstat -Z
[/toggle]
[toggle title="What are each security attribute called in the colon delimited string?"]
user:role:type:level
[/toggle]
[/accordion]
<hr/>


A lot of commands have an option "-Z" that that is specifically for displaying an object's security context. Here's a quick overview of the main ones.


<h2>Viewing a user's SELinux context</h2>
This is done using the id command: 


<pre>
[root@localhost ~]# id
uid=0(root) gid=0(root) groups=0(root) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023
[root@localhost ~]# id -Z
unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023
</pre>




<h2>Viewing a file's/folder's security context</h2>
As shown earlier this is done using the ls command:

<pre>
$ touch testfile.txt
$ ls -lZ testfile.txt
-rw-r--r--. root root unconfined_u:object_r:admin_home_t:s0 testfile.txt
</pre>


<h2>Viewing a process's security context</h2>
A processes SELinux context can be viewed using the ps command:


<pre>
$ ps -Z
LABEL                             PID TTY          TIME CMD
unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 1662 pts/0 00:00:00 bash
unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 2059 pts/0 00:00:00 systemctl
unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 2060 pts/0 00:00:00 less
unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 2352 pts/0 00:00:00 ps

</pre>


as well as the netstat command:


<pre>
$ netstat -Z | head
Active Internet connections (w/o servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name     Security Context               
tcp        0     64 192.168.1.124:ssh       PowershellPC:23838      ESTABLISHED 1658/sshd: root@pts  fined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023
Active UNIX domain sockets (w/o servers)
Proto RefCnt Flags       Type       State         I-Node   PID/Program name     Security Context                                  Path
unix  2      [ ]         DGRAM                    10539    1/systemd            system_u:system_r:init_t:s0                        /run/systemd/shutdownd
unix  5      [ ]         DGRAM                    6267     1/systemd            system_u:system_r:init_t:s0                        /run/systemd/journal/socket
.
.
...etc

</pre>


Notice in all cases, they show all 4 parts, using ":" as a delimiter, and they show all the parts in the order of: <code>user:role:type:level</code>.

You'll notice that the user security contect attribute, always ends with "_u" suffix, and in a similar fashion for role we have (_r), and for type we have (_t). However for the level have a prefix of "s", which indicates (s)ensitivity. 


 
]]></Content>
		<Date><![CDATA[2015-06-18]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - The "user", "role" and "level" security attributes]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What is the command to list all the available 'user' security attribute values?"]
$ seinfo <span style='letter-spacing:1.5px'>-</span>-user
[/toggle]
[toggle title="What is the command to show the logic that SELinux uses to determine which of these values should be assigned for a newly created user?"]
$  semanage login -l
[/toggle]
[toggle title="What is the command to list all the available 'role' security attribute values?"]
$   seinfo <span style='letter-spacing:1.5px'>-</span>-role
[/toggle]
[/accordion]

<hr/>


As mentioned before, the "targeted" policy primarily decides on granting access of one object to another object, based on the "type" security attributes, i.e it is a Type Enforcement based policy.   


That means that that none of the other security attributes values matters that much.   


<h2>The "user" security attribute</h2> 
Eerlier we saw that are 4000+ possible values that a "type" security value can have, since that is the main one. In contrast there are less than 10 possible values that a "user" selinux attribute can have:


<pre>
$ seinfo -u

Users: 8
   sysadm_u
   system_u
   xguest_u
   root
   guest_u
   staff_u
   user_u
   unconfined_u
</pre> 


However the targeted policy still has to assign a user security context value when a user is created. Here are the policy rules in the targeted policy,  to help it determine what user security attribute value is assigned to a newly created user.   


<pre>
$  semanage login -l

Login Name           SELinux User         MLS/MCS Range        Service

__default__          unconfined_u         s0-s0:c0.c1023       *
root                 unconfined_u         s0-s0:c0.c1023       *
system_u             system_u             s0-s0:c0.c1023       *

</pre>


Notice that system_u is for any service accounts that are created during an rpm package install. These service accounts will trigger processes, and these processes will inherit the initiating user's security attribute values, which explains this:


<pre>
<span style='font-size:10px'>$ ps -efZ | grep "httpd"
system_u:system_r:httpd_t:s0    root      1304     1  0 07:20 ?        00:00:00 /usr/sbin/httpd -DFOREGROUND
system_u:system_r:httpd_t:s0    apache    1407  1304  0 07:20 ?        00:00:00 /usr/sbin/httpd -DFOREGROUND
system_u:system_r:httpd_t:s0    apache    1408  1304  0 07:20 ?        00:00:00 /usr/sbin/httpd -DFOREGROUND
system_u:system_r:httpd_t:s0    apache    1409  1304  0 07:20 ?        00:00:00 /usr/sbin/httpd -DFOREGROUND
system_u:system_r:httpd_t:s0    apache    1410  1304  0 07:20 ?        00:00:00 /usr/sbin/httpd -DFOREGROUND
system_u:system_r:httpd_t:s0    apache    1411  1304  0 07:20 ?        00:00:00 /usr/sbin/httpd -DFOREGROUND
</span>
</pre>

Inheriting security attribute values is the default unless the targeted policy specifies otherwise. 


Also notice that the default "user" security attribute value for creating a new user is "unconfined_u". You can modify this table so that when you create a user account with a certain name, then it adopts a different "user" security attribute value. Not sure how to do this yet.  


 
<h2>The "role" security attribute</h2> 
The role security attribute is used for using Roles Based Access Control (RBAC). The targeted policy works by stating with users can access which roles, and in turn, which roles can access which domains.  


<pre>
$ seinfo -r

Roles: 14
   auditadm_r
   dbadm_r
   guest_r
   staff_r
   user_r
   logadm_r
   object_r
   secadm_r
   sysadm_r
<strong>   system_r</strong>
   webadm_r
   xguest_r
   nx_server_r
   unconfined_r
</pre>


Note, "system_r" is usually reserved for system processes. The same goes for system_u, which we saw earlier. They usually both go hand-in-hand. 

roles are hardly used. 



<h2>The "level" security attribute</h2> 
This is something that is exclusive to the MLS policy type is hardly ever used. It is something that you don't need to know about for the red hat exams. ]]></Content>
		<Date><![CDATA[2015-06-18]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - SELinux Booleans]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What is the command to view the man info for SElinux booleans?"]
$ man booleans
[/toggle]
[toggle title="What is the command to view all the current seboolan settings?"]
$ getsebool -a 
[/toggle]
[toggle title="What is the command to view a short description for each boolean setting?"]
$ semanage boolean -l
[/toggle]
[toggle title="What is the command to persistantly+verbosely enable the 'ftp_home_dir' SELinux boolean setting?"]
$ setsebool -PV ftp_home_dir on  
# this takes about 20 seconds
[/toggle]
[/accordion]

<hr/>



You can customise the default behaviour of your (targeted) policy. This is done by configuring various boolean settings that can enable/disable your policy's various policy rules. Going back to our massive textbook analogy, this is a bit like graying-out (or ungraying) various segements of the book. Switching boolean values can be like increasing/decreasing the set of security attributes that a particular security attribute is allowed to have access to.  

All seboolean changes takes effect straight away. Here's a man page that covers more about SELinux booleans:

<pre>
$ man booleans
</pre> 

You can view all your policy's boolean setting using the <code>getboolean</code> command:


<pre>
$ getsebool -a 
abrt_anon_write --> off
abrt_handle_event --> off
abrt_upload_watch_anon_write --> on
antivirus_can_scan_system --> off
antivirus_use_jit --> off
auditadm_exec_content --> on
.
.
...etc.
</pre>

New boolean settings also gets added to this list when in install rpm packages, e.g. httpd. You can find a short description of each seboolean using <code>semanage</code>:



<pre>
$ semanage boolean -l
SELinux boolean        State  Default Description

ftp_home_dir           (off  ,  off)  Determine whether ftpd can read and write files in user home directories.
smartmon_3ware         (off  ,  off)  Determine whether smartmon can support devices on 3ware controllers.
mpd_enable_homedirs    (off  ,  off)  Determine whether mpd can traverse user home directories.
xdm_sysadm_login       (off  ,  off)  Allow the graphical login program to login directly as sysadm_r:sysadm_t
.
.
...etc
</pre>

Notice we have a current "state" column, as well as an on machine reboot "default" column. 


<h2>Enabling and Disabling SELinux Booleans</h2>

To enable/disable SELinux Boolean settings, we need to use the <code>setsebool</code> command:

<pre>
$ whatis setsebool
setsebool (8)        - set SELinux boolean value
</pre> 

E.g. the "ftp_home_dir" is currently disabled, and to enable it we do:


<pre>
$ getsebool ftp_home_dir
ftp_home_dir --> off
$ setsebool -P ftp_home_dir on
$ getsebool ftp_home_dir
ftp_home_dir --> on
</pre>

<strong>Important:</strong> Always use the "-P" to make it persistent. 

The above command for enabling/disabling a boolean setting takes about 20 seconds. That's because the target policy itself is being modified and new binary of the policy is being compiled. The new binary is located in the folder <code>/etc/selinux/targeted/policy</code> 
]]></Content>
		<Date><![CDATA[2015-06-18]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Copying verses moving files in Linux]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="True or False, security contexts stays intact when you move a file from one folder to another?"]
True, but they get reassigned with new SELinux context when you copy files instead of moving.
[/toggle]
[toggle title="What is better, moving or copying files, in order to avoid getting SELinux related errors?"]
Copying files is the better approach. 
[/toggle]
[toggle title="What is the command to check for files with possible incompatible SELinux contects?"]
$ sealert -a /var/log/audit/audit.log
[/toggle]
[toggle title="What is the command to reset the SELinux context (according to the rules defined in the targeted policy) for the file /var/html/hello.html?"]
$ restorecon /var/html/hello.html
[/toggle]
[/accordion]

<hr/>

In this article we are going to take a look at files ending up with the wrong SELinux context, how they ended up with the wrong SELinux context, what are the consequences of having the wrong SELinux context, and what could be done to fix (or avoid) this problem. Files usually end up with the wrong SELinux context when you move a file, instead of copying it. This article will walk you through a typical example. First let's confirm that we are in enforcing mode:

<pre>
$ getenforce
Enforcing
</pre>


Let's say we have set up httpd, we only did a simple install and started the http service:


<pre>
$ yum install httpd 
$ systemctl enable httpd.service
$ systemctl start httpd.service
</pre>


Now let's see if we have a file called hello.html:


<a href="http://codingbee.net/wp-content/uploads/2015/11/918GwO4.png"><img src="http://codingbee.net/wp-content/uploads/2015/11/918GwO4.png" alt="" width="690" height="561" class="alignnone size-full wp-image-5897" /></a>


As you can see this doesn't exist yet. Now let's create this file:

<pre>
$ echo "this is a web page" > /var/www/html/hello.html
</pre>

and then refresh the page:

<a href="http://codingbee.net/wp-content/uploads/2015/11/dZTAbMl.png"><img src="http://codingbee.net/wp-content/uploads/2015/11/dZTAbMl.png" alt="" width="700" height="572" class="alignnone size-full wp-image-5899" /></a>

So far so good. Now we are going  delete this file and recreate it again but this time in a slightly different way, in order to illustrate how SELinux could come into play. So let's first delete this file:

<pre>
$ rm /var/www/html/hello.html
</pre>

Now lets create hello.html again but this time in the root user's home directory:


<pre>
$ pwd
/root
$ echo "this is a web page" > hello.html
$ ls -lZ hello.html
-rw-r--r--. root root unconfined_u:object_r:admin_home_t:s0 hello.html
</pre>

As you can see the hello.html file has inherited it's parent folder's security attribute, in particular the value of the  "type" security attribute: "admin_home_t". 

Now let's move this file into /var/www/html:

<pre>
$ mv /root/hello.html /var/www/html/
$ ls -lZ /var/www/html/hello.html
-rw-r--r--. root root unconfined_u:object_r:admin_home_t:s0 /var/www/html/hello.html

</pre>
Notice that this file has retained the original SELinux context, (which happens to be wrong for files that are in this directory).

Now let's refresh the page:

<a href="http://codingbee.net/wp-content/uploads/2015/11/sQcvbm9.png"><img src="http://codingbee.net/wp-content/uploads/2015/11/sQcvbm9.png" alt="" width="695" height="570" class="alignnone size-full wp-image-5900" /></a>

Now we get a forbidden message. That's because the file has a security context, that the httpd.service isn't allowed to access. 

There's 2 ways to fix this. First we could have used the cp command instead of the mv command. Or we could reset the hello.html file's security to what it should be according to the targeted policy, which we do using the <code>restorecon</code> command. But first let's check the logs to confirm what we are suspecting:


  
<pre>
$ sealert -a /var/log/audit/audit.log
 99% done'list' object has no attribute 'split'
100% done
found 1 alerts in /var/log/audit/audit.log
--------------------------------------------------------------------------------

SELinux is preventing /usr/sbin/httpd from getattr access on the file /var/www/html/hello.html.

*****  Plugin restorecon (99.5 confidence) suggests   ************************

<strong>If you want to fix the label.
/var/www/html/hello.html default label should be httpd_sys_content_t.
Then you can run restorecon.
Do
# /sbin/restorecon -v /var/www/html/hello.html
</strong>
*****  Plugin catchall (1.49 confidence) suggests   **************************

If you believe that httpd should be allowed getattr access on the hello.html file by default.
Then you should report this as a bug.
You can generate a local policy module to allow this access.
Do
allow this access for now by executing:
# grep httpd /var/log/audit/audit.log | audit2allow -M mypol
# semodule -i mypol.pp


Additional Information:
Source Context                system_u:system_r:httpd_t:s0
Target Context                unconfined_u:object_r:admin_home_t:s0
Target Objects                /var/www/html/hello.html [ file ]
Source                        httpd
Source Path                   /usr/sbin/httpd
Port                          <Unknown>
Host                          <Unknown>
Source RPM Packages           httpd-2.4.6-31.el7.centos.1.x86_64
Target RPM Packages
Policy RPM                    selinux-policy-3.13.1-23.el7_1.18.noarch
Selinux Enabled               True
Policy Type                   targeted
Enforcing Mode                Enforcing
Host Name                     puppetagent01.local
Platform                      Linux puppetagent01.local
                              3.10.0-229.14.1.el7.x86_64 #1 SMP Tue Sep 15
                              15:05:51 UTC 2015 x86_64 x86_64
Alert Count                   5
First Seen                    2015-11-09 12:06:51 GMT
Last Seen                     2015-11-09 12:15:19 GMT
Local ID                      5192d41a-a235-4eb3-b31f-e4d4c0d1e8b2

Raw Audit Messages
type=AVC msg=audit(1447071319.198:784): avc:  denied  { getattr } for  pid=4389 comm="httpd" path="/var/www/html/hello.html" dev="dm-0" ino=136577068 scontext=system_u:system_r:httpd_t:s0 tcontext=unconfined_u:object_r:admin_home_t:s0 tclass=file


type=SYSCALL msg=audit(1447071319.198:784): arch=x86_64 syscall=stat success=no exit=EACCES a0=7f65eb34fd18 a1=7ffe1285c500 a2=7ffe1285c500 a3=7f65df9f7792 items=0 ppid=4388 pid=4389 auid=4294967295 uid=48 gid=48 euid=48 suid=48 fsuid=48 egid=48 sgid=48 fsgid=48 tty=(none) ses=4294967295 comm=httpd exe=/usr/sbin/httpd subj=system_u:system_r:httpd_t:s0 key=(null)

Hash: httpd,httpd_t,admin_home_t,file,getattr

</pre>

So now let's run this fix:

<pre>
$ ls -lZ /var/www/html/hello.html
-rw-r--r--. root root unconfined_u:object_r:admin_home_t:s0 /var/www/html/hello.html
$ restorecon /var/www/html/hello.html
$ ls -lZ /var/www/html/hello.html
-rw-r--r--. root root unconfined_u:object_r:httpd_sys_content_t:s0 /var/www/html/hello.html
</pre>
 


<a href="http://codingbee.net/wp-content/uploads/2015/11/dZTAbMl.png"><img src="http://codingbee.net/wp-content/uploads/2015/11/dZTAbMl.png" alt="" width="700" height="572" class="alignnone size-full wp-image-5899" /></a>

Success!

Now we could have avoided this problem to begin with by creating the hello.html file in the correct folder to begin with. Or failing that, we could have used the cp command instead of mv command. i.e.:


<pre>
$ echo "this is a web page" > hello.html
$ ls -lZ /root/hello.html
-rw-r--r--. root root unconfined_u:object_r:admin_home_t:s0 /root/hello.html
$ cp /root/hello.html /var/www/html/hello.html
$ ls -lZ /var/www/html/hello.html
-rw-r--r--. root root unconfined_u:object_r:httpd_sys_content_t:s0 /var/www/html/hello.html
</pre>




]]></Content>
		<Date><![CDATA[2015-06-18]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - SELinux Policy Types and SELinux Attributes]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="Which file lists the 3 main policy types and what they mean?"]
/etc/selinux/config
[/toggle]
[toggle title="What are the 3 policy types which one is most commonly used?"]
- targeted: this policy type is by far the most commonly used worldwide.
- minimum: this is a stripped down version of the targeted policy. It is mainly used for low spec machines that doesn’t have enough power (e.g. cpu speed) to cope with targeted SELinux.
- mls: this is a much more beefed up version of targeted and is sometimes used by governments.
[/toggle]
[toggle title="Which folder stores the policy types in the form of a file?"]
/etc/selinux/
# however by mls and minimum isn't installed by default.
[/toggle]
[toggle title="What is the command to locate the other 2 policy types rpm package, so that we can install them?"]
$ yum list available | grep po-typelicy | grep -E 'mls|minimum'
[/toggle]
[toggle title="Which security attributes does the targetted policy type mainly relies on in order for it to function?"]
the "type" security attribute
[/toggle]
[toggle title="Because of this, what is this type of enforcement also known as?"]
Type Enforcement Policy 
[/toggle]
[toggle title="If a bunch of objects has the same value for it's type security value, then these objects are referred to as belonging to the same....?"]
domain
[/toggle]
[toggle title="What is the syntax for a security label?"]
User_u:Role_r:Type_t:Level_s
[/toggle]
[toggle title="What is the command to list all available 'type' security attribute values (ther are a few thousands of these)?"]
$ seinfo <span style="letter-spacing:1.5px">-</span>-type
[/toggle]
[toggle title="What are the command to list all available 'user', role, and level security attributes, respectively?"]
$ seinfo <span style="letter-spacing:1.5px">-</span>-user
$ seinfo <span style="letter-spacing:1.5px">-</span>-role
$ seinfo <span style="letter-spacing:1.5px">-</span>-sensitivity
[/toggle]
[toggle title="If a policy type doesn't define what 'type' security attribute a certain object should have, then what default type security attribute value would it end up getting?"]
unconfined_t
[/toggle]
[toggle title="What are all user account's type security attribute value equal to?"]
unconfined_t    # this includes the root user, 
                # but excludes service accounts.
[/toggle]
[toggle title="Why is this the case?"]
Because that's how the targeted policy defined them as.
[/toggle]
[toggle title="What does unconfined_t mean in terms of access level?"]
These objects (e.g. users) have unrestricted access, i.e. they are not restricted by the SELinux (if SELinux is using the targeted policy). Instead they are only restricted by DAC layers. 
[/toggle]
[toggle title="What are the type security attribute's value for any processes created by a user?"]
unconfined_t    # that's because the inherit this value from the user. 
[/toggle]
[/accordion]

<hr/>



Earlier we mentioned that an SELinux policy is a really big text book that contains policy rules. In RHEL, there are actually 3 text books that are available to choose from, as indicated here:


<pre>
$ cat /etc/selinux/config

# This file controls the state of SELinux on the system.
# SELINUX= can take one of these three values:
#     enforcing - SELinux security policy is enforced.
#     permissive - SELinux prints warnings instead of enforcing.
#     disabled - No SELinux policy is loaded.
SELINUX=permissive
# SELINUXTYPE= can take one of these two values:
#     targeted - Targeted processes are protected,
#     minimum - Modification of targeted policy. Only selected processes are protected.
#     mls - Multi Level Security protection.
<strong>SELINUXTYPE=targeted</strong>

</pre>

here's a brief overview of each of them:



<ul>
	<li><strong>targeted</strong> - this policy type is by far the most commonly used worldwide.</li>
	<li><strong>minimum</strong> - this is a stripped down version of the targeted policy. It is mainly used for low spec machines that doesn't have enough power (e.g. cpu speed) to cope with targeted SELinux.</li>
	<li><strong>mls</strong> - this is a much more beefed up version of targeted and is sometimes used by governments.</li>
</ul>


All your policy types are contained in the following directory:


<pre>
$ ls -l /etc/selinux/
total 12
-rw-r--r--. 1 root root  548 Jun 19 19:43 config
-rw-r--r--. 1 root root 2321 Jun  9  2014 semanage.conf
<strong>drwxr-xr-x. 6 root root 4096 Jun 19 19:44 targeted</strong>
</pre>

As you can see, we only have the targeted-policy installed at the moment. To install the other ones, you do:


<pre>
$ yum list available | grep policy | grep -E 'mls|minimum'
selinux-policy-minimum.noarch     3.13.1-23.el7_1.7    updates
selinux-policy-mls.noarch         3.13.1-23.el7_1.7    updates

$ yum install selinux-policy-minimum selinux-policy-mls -y
.
.
.

$ ls -l /etc/selinux/
total 20
-rw-r--r--. 1 root root  548 Jun 19 19:43 config
<strong>drwxr-xr-x. 6 root root 4096 Jun 20 07:31 minimum</strong>
<strong>drwxr-xr-x. 7 root root 4096 Jun 20 07:30 mls</strong>
-rw-r--r--. 1 root root 2321 Jun  9  2014 semanage.conf
drwxr-xr-x. 6 root root 4096 Jun 19 19:44 targeted



</pre>


Each of these folders contains modules that are dynamically loaded into the kernel as and when the kernel needs them. As well as dynamically unloading them too.  


The main difference between each policy type is which particular security attribute type it uses as the decider for determining access.  


<h2>The targeted policy type</h2>

The targeted policy is by far the most commonly used type that is in use today. That's why it comes preinstalled by default, whereas the others require manually installing them first before you can use them.

The targeted policy primarily makes use of the "type" security attribute, This attribute is used as the main deciding facter on whether one object (e.g. process) can access another (e.g. file).


This technique of using the "targeted policy" security attribute as the main deciding facter is often referred to as the "<strong>Type Enforcement Policy</strong>", or TE policy for short.       


<h2>Process "domains"</h2>
When we talk about processes, we sometimes refer to the type security attribute as a "domain", this is a way that allows us to refer to a group of processes. 

For example:



<pre>
<span style="font-size:10px">$ ps -efZ | grep crond
system_u:system_r:crond_t:s0-s0:c0.c1023 root 662  1  0 07:20 ?        00:00:00 /usr/sbin/crond -n
system_u:system_r:crond_t:s0-s0:c0.c1023 root 671  1  0 07:20 ?        00:00:00 /usr/sbin/atd -f
unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 root 2672 1663  0 08:01 pts/0 00:00:00 grep --color=auto crond</span>
</pre>


Here we say both crond and atd processes are running under the cron_t domain. 

This means that the crond process can interact with the atd process if it wants' to. But it can't interact with processes of other domains, such as the following:



<pre>
<span style="font-size:10px">$ ps -efZ | grep -E 'lvm_t|httpd_t|sshd_t' | grep -v grep
system_u:system_r:lvm_t:s0      root       502     1  0 07:20 ?        00:00:00 /usr/sbin/lvmetad -f
system_u:system_r:httpd_t:s0    root      1304     1  0 07:20 ?        00:00:00 /usr/sbin/httpd -DFOREGROUND
system_u:system_r:sshd_t:s0-s0:c0.c1023 root 1321  1  0 07:20 ?        00:00:00 /usr/sbin/sshd -D
system_u:system_r:httpd_t:s0    apache    1407  1304  0 07:20 ?        00:00:00 /usr/sbin/httpd -DFOREGROUND
system_u:system_r:httpd_t:s0    apache    1408  1304  0 07:20 ?        00:00:00 /usr/sbin/httpd -DFOREGROUND
system_u:system_r:httpd_t:s0    apache    1409  1304  0 07:20 ?        00:00:00 /usr/sbin/httpd -DFOREGROUND
system_u:system_r:httpd_t:s0    apache    1410  1304  0 07:20 ?        00:00:00 /usr/sbin/httpd -DFOREGROUND
system_u:system_r:httpd_t:s0    apache    1411  1304  0 07:20 ?        00:00:00 /usr/sbin/httpd -DFOREGROUND</span>
</pre>









<h2>SELinux Attributes</h2>

Everything that exists on a machine, has a security context assigned to it. 

As mentioned earlier, a security context is a colon-delimited string of 4 security attributes, which we will cover in more detail later. There are 4 types of SELinux attributes:


<ul>
	<li>User</li>
	<li>Role</li>
	<li>Type</li>
	<li>Level</li>
</ul>

SELinux contexts are always shown in a colon-delimited format and sequence <code>User_u:Role_r:Type_t:Level_s</code>. The suffix makes it easier to make out why which type each of the security attribute is, in case you forget what the ordering is.  


The "type" security attribute, is by far the most important one, out of the 4 security attributes
That's because the most commonly used policy type, the "targeted" policy, is a Type Enforcement based policy, i.e. it decides whether one object (e.g. process) can access another object (e.g. file) based mainly on their "type" security attribute value. 

For example let's take a look at the security context for the apache process, along with apache related files and folders :


<pre>
<span style="font-size:10px">$ ps -efZ | grep httpd
system_u:system_r:httpd_t:s0    root      1304     1  0 07:20 ?        00:00:00 /usr/sbin/httpd -DFOREGROUND
system_u:system_r:httpd_t:s0    apache    1407  1304  0 07:20 ?        00:00:00 /usr/sbin/httpd -DFOREGROUND
system_u:system_r:httpd_t:s0    apache    1408  1304  0 07:20 ?        00:00:00 /usr/sbin/httpd -DFOREGROUND
system_u:system_r:httpd_t:s0    apache    1409  1304  0 07:20 ?        00:00:00 /usr/sbin/httpd -DFOREGROUND
system_u:system_r:httpd_t:s0    apache    1410  1304  0 07:20 ?        00:00:00 /usr/sbin/httpd -DFOREGROUND
system_u:system_r:httpd_t:s0    apache    1411  1304  0 07:20 ?        00:00:00 /usr/sbin/httpd -DFOREGROUND

$ ls -lZ /etc/httpd/
drwxr-xr-x. root root system_u:object_r:httpd_config_t:s0 conf
drwxr-xr-x. root root system_u:object_r:httpd_config_t:s0 conf.d
drwxr-xr-x. root root system_u:object_r:httpd_config_t:s0 conf.modules.d
lrwxrwxrwx. root root system_u:object_r:httpd_log_t:s0 logs -> ../../var/log/httpd
lrwxrwxrwx. root root system_u:object_r:httpd_modules_t:s0 modules -> ../../usr/lib64/httpd/modules
lrwxrwxrwx. root root system_u:object_r:httpd_config_t:s0 run -> /run/httpd</span>
</pre>

Here the apache processes all have the "httpd_t" security attribute. So in the targeted policy there will be a policy rule for the httpd_t selinux attribute and it will state that objects with the "httpd_t" can access any objects that has the type security attribute of any of the following:

<ul>
	<li>httpd_config_t</li>
	<li>httpd_log_t</li>
	<li>httpd_modules_t</li>
	<li>....etc.</li>
</ul>



This approach means that an http related object is prevented from interacting with a unrelated object, e.g. cron related. As a result that means that if httpd/apache does get compromise, then the damage is only limited to itself, so things like ftp, cron, sshd,...etc doesn't also get compromised. 

 


To get an idea of how many "type" security attribute values exists, you can do:

<pre>
$ whatis seinfo
seinfo (1)           - SELinux policy query tool

$ seinfo -t | head

Types: 4620
   bluetooth_conf_t
   cmirrord_exec_t
   colord_exec_t
   foghorn_exec_t
   jacorb_port_t
   pki_ra_exec_t
   pki_ra_lock_t
   sosreport_t
$ seinfo -t | wc -l
4622
</pre>

<h2>The "unconfined_t" type security attribute</h2>


<pre>
<span style="font-size:10px">$ id
uid=0(root) gid=0(root) groups=0(root) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023
$ ps -efZ | grep unconfined_t | head
unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 root 1659 1321  0 07:24 ? 00:00:00 sshd: root@pts/0
unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 root 1663 1659  0 07:24 pts/0 00:00:00 -bash
unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 root 2800 1663  0 08:11 pts/0 00:00:00 ps -efZ
unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 root 2801 1663  0 08:11 pts/0 00:00:00 grep --color=auto unconfined_t
unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 root 2802 1663  0 08:11 pts/0 00:00:00 head</span>
</pre>



Any objects of this are essentially not restricted by SELinux in any way (for the given policy type that's being used). In other words you can think of SELinux being disabled for these objects. The only restrictions that these objects abide by are the DAC (Discretionary Access Control) layer restrictions and not MAC (Mandatroy Access Control).

Hence if a policy (usually the targeted policy) doesn't have a policy rule written for a particular object, then they automatically get assigned the unconfined_t security attribute.


One particular thing to bear in mind, is that the targeted policy also assign the "unconfined_t" to nearly all user accounts. 
 
 

Any object that doesn't have an selinux policy rules that assigns it a type security value, will automatically get assigned to with the type security attribute value of "unconfined_t" type. Here are a few such objects. Objects that belong to the unconfined_t domain, can access each other, but more importantly access any other object on the entire system! 

Objects belonging to the unconfined_t domain sounds like a huge security loophole, but that's not really the case. That's because SELinux original objective is to secure internet facing processes (because they are vulnerable to attacks), and ensuring they are secure rather than securing all the internal going ons. It also secures. SELinux also secures processes that start up at boot, in case one of these processes is compromised and evil.  

Objects of this domain effectively are not restricted by SELinux and are subject to only Discretionary Access Control (DAC), and not MAC (which is provided by SELinux). 
 

Also all user accounts (e.g. user accounted created by using the "useradd" command) including the root user, belong to the unconfined_t domain, that's because that's how it is defined in the "targeted" policy type. Consequently this means that any processes generated by a user automatically inherits all the user's selinux attributes:


<pre>
$ id
uid=0(root) gid=0(root) groups=0(root) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023
$ id -Z
unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023
$ ps -ef | grep grep
root      5969  1662  0 00:43 pts/0    00:00:00 grep --color=auto grep
</pre>

However unconfined_t is just a name to represent a domain that is completely SELinux disabled. There are actually a lot more totally unconfined domains, and you can view a full list of them using the seinfo command:


<pre>
$ seinfo --help
Usage: seinfo [OPTIONS] [EXPRESSION] [POLICY ...]

Print information about the components of a SELinux policy.

EXPRESSIONS:
  -c[NAME], --class[=NAME]         print object classes
  --sensitivity[=NAME]             print sensitivities
  --category[=NAME]                print categories
  -t[NAME], --type[=NAME]          print types (no aliases or attributes)
  -a[NAME], --attribute[=NAME]     print type attributes
  -r[NAME], --role[=NAME]          print roles
  -u[NAME], --user[=NAME]          print users
  -b[NAME], --bool[=NAME]          print conditional booleans
  --constrain                      print constrain statements
  --initialsid[=NAME]              print initial SIDs
  --fs_use[=TYPE]                  print fs_use statements
  --genfscon[=TYPE]                print genfscon statements
  --netifcon[=NAME]                print netif contexts
  --nodecon[=ADDR]                 print node contexts
  --permissive                     print permissive types
  --polcap                         print policy capabilities
  --portcon[=PORT]                 print port contexts
  --protocol=PROTO                 specify a protocol for portcons
  --all                            print all of the above
OPTIONS:
  -x, --expand                     show more info for specified components
  --stats                          print useful policy statistics
  -l, --line-breaks                print line breaks in constrain statements
  -h, --help                       print this help text and exit
  -V, --version                    print version information and exit

For component options, if NAME is provided, then only show info for
NAME.  Specifying a name is most useful when used with the -x option.
If no option is provided, display useful policy statistics (-s).

The default source policy, or if that is unavailable the default binary
policy, will be opened if no policy is provided.

</pre>

So based on this we do:


<pre>
$ seinfo --attribute=unconfined_domain_type -x
   unconfined_domain_type
      sosreport_t
      bootloader_t
      devicekit_power_t
      virt_qemu_ga_unconfined_t
      nova_conductor_t
      dirsrvadmin_unconfined_script_t
      timemaster_t
      nova_objectstore_t
      certmonger_unconfined_t
      unconfined_cronjob_t
      unconfined_sendmail_t
      virtd_lxc_t
      abrt_handle_event_t
      setfiles_mac_t
      initrc_t
      fsadm_t
      lvm_t
      rpm_t
      wine_t
      cinder_scheduler_t
      unconfined_dbusd_t
      unconfined_mount_t
      realmd_consolehelper_t
      rtas_errd_t
      preupgrade_t
      authconfig_t
      snapperd_t
      vmware_host_t
      puppetagent_t
      vmtools_helper_t
      phc2sys_t
      pegasus_openlmi_unconfined_t
      prelink_t
      vmtools_t
      anaconda_t
      cinder_volume_t
      boinc_project_t
      cinder_api_t
      cloud_init_t
      rpm_script_t
      system_cronjob_t
      openshift_initrc_t
      samba_unconfined_net_t
      pki_tomcat_script_t
      kdumpctl_t
      unconfined_service_t
      devicekit_disk_t
      zabbix_script_t
      firstboot_t
      keepalived_unconfined_script_t
      samba_unconfined_script_t
      nagios_eventhandler_plugin_t
      httpd_unconfined_script_t
      openvpn_unconfined_script_t
      depmod_t
      insmod_t
      kernel_t
      livecd_t
      realmd_t
      tomcat_t
      watchdog_unconfined_t
      clvmd_t
      crond_t
      inetd_t
      init_t
      mount_t
      ptp4l_t
      tuned_t
      udev_t
      virtd_t
      nagios_unconfined_plugin_t
      pegasus_openlmi_logicalfile_t
      devicekit_t
      inetd_child_t
      <strong>unconfined_munin_plugin_t</strong>
      semanage_t
      sge_shepherd_t
      xdm_unconfined_t
      unconfined_t
      cluster_t
      openwsman_t
      install_t
      sge_job_t
      xserver_t
      condor_startd_t
      cinder_backup_t


</pre>




See also:

http://wiki.centos.org/HowTos/SELinux



 ]]></Content>
		<Date><![CDATA[2015-06-19]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Active Vector Cache (AVC)]]></Title>
		<Content><![CDATA[The targeted policy is really big, and it can be timeconsuming and resource intensive to look up info each time your OS want's a policy decision. That's why AVC is used. AVC is a cache that stores the most recent policy allow/deny access decisions.   ]]></Content>
		<Date><![CDATA[2015-06-20]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|RedHat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Editing a file's Security Context]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What is the command to give you an insight in how SELinux determines the security context for all your files+folders?"]
$ semanage fcontext -l
[/toggle]
[toggle title="What are the 2 steps to editing a file's SELinux context?"]
1. First update the target policy itself
2. Then you apply the updated policy to your files and folders.
[/toggle]
[toggle title="What is the command to view the semanage command's subcommands?"]
$ semanage <span style='letter-spacing'>-</span>-help
[/toggle]
[toggle title="What is the command, to view each of these subcommand's man pages?"]
$ man semanage-{subcommand}
[/toggle]
[toggle title="Which subcommand is used for editing the SELinux file mapping definition?"]
fcontext
[/toggle]
[toggle title="What is the command to view this command's man page?"]
$ man semanage-fcontext
[/toggle]
[toggle title="What is the command to view the fcontext subcommand's help info?"]
$ semanage fcontext <span style='letter-spacing'>-</span>-help
[/toggle]
[toggle title="Where can you find examples commands for editing a file's targeted policy?"]
$ man semanage-fcontext
[/toggle]
[toggle title="What is the command to apply the updated policy to the file /tmp/testfile.txt?"]
$ restorecon /tmp/testfile.txt
[/toggle]
[toggle title="How do you refresh all files and folders contexts across the whole machine?"]
$ touch /.autorelabel    
# then reboot the machine. 
[/toggle]
[/accordion]

<hr/>


When you create a new file or folder, then SELinux will automatically assign a value to it. The <code>semanage fcontext -l</code> command gives info about how SELinux determines what security context it needs to assign to all your machine's files and folders:

<pre>
<span style="font-size:10px">$ semanage fcontext -l | wc -l
5846
$ semanage fcontext -l
/etc/cron\.(daily|weekly)/sysklogd    regular file       system_u:object_r:logrotate_exec_t:s0
/etc/cron\.d(/.*)?                    all files          system_u:object_r:system_cron_spool_t:s0
/etc/cron\.daily/[sm]locate           regular file       system_u:object_r:locate_exec_t:s0
/etc/cron\.daily/calamaris            regular file       system_u:object_r:calamaris_exec_t:s0
/etc/cron\.daily/certwatch            regular file       system_u:object_r:certwatch_exec_t:s0
/etc/cron\.daily/prelink              regular file       system_u:object_r:prelink_cron_system_exec_t:s0
/etc/cron\.monthly/proftpd            regular file       system_u:object_r:ftpd_exec_t:s0
/etc/cron\.weekly/(c)?fingerd         regular file       system_u:object_r:fingerd_exec_t:s0
/etc/crontab                          regular file       system_u:object_r:system_cron_spool_t:s0
/etc/ctdb/events\.d/.*                regular file       system_u:object_r:bin_t:s0
/etc/cups(/.*)?                       all files          system_u:object_r:cupsd_etc_t:s0
/etc/cups/certs                       directory          system_u:object_r:cupsd_rw_etc_t:s0
/etc/cups/certs/.*                    regular file       system_u:object_r:cupsd_rw_etc_t:s0
/etc/cups/classes\.conf.*             regular file       system_u:object_r:cupsd_rw_etc_t:s0
/etc/cups/client\.conf                regular file       system_u:object_r:etc_t:s0
.
.
...etc.<span style="font-size=10px">
</pre>
Note, the "(/.*)?" means any child elements will inherit the items security context automatically. 

This command essentially gives the definitive info about what SELinux context every single file and folder must have on your machine. The semanage command retrieves info from the particular policy type that's currently active (which is usually the targeted policy type).  


However when you install an rpm package, the rpm in most cases assign security attribute values to all it’s objects, but for those it doesn’t the policy type will take over. In most cases the given objects values are assigned based on inheritance. E.g. when a user starts a new process, then that new process inherits the user’s security attributes, or if a file is created, then that file inherits of the attributes of the folder it resides in.

<h2>Edit a file's security context using Semanage</h2>

Now if you want a file/folder to have a different security context than what's defined by <code>semanage fcontext -l</code>, then the best way to do this is to first modify the targeted policy itself (using the semanage command). This will mean your changes become part of the source information and hence will survive a reboot (or even a relabel, covered later). After that targeted policy is updated, the next part is to apply the new/updated policy rule to the file/folder. To summarise:  

<ol>
	<li>First update the target policy itself</li>
	<li>Then you apply the updated policy to your files and folders.</li>
</ol>



Before we show how you edit one of the targeted policies listed in <code>semanage fcontext -l</code>, let's first explore semanage itself because it is a really powerful and feature rich command.  Here's the help info for the semanage command:


<pre>
semanage -h
usage: semanage [-h]

                {import,export,login,user,port,interface,module,node,fcontext,boolean,permissive,d
                ...

semanage is used to configure certain elements of SELinux policy with-out
requiring modification to or recompilation from policy source.

positional arguments:
  {import,export,login,user,port,interface,module,node,fcontext,boolean,permissive,dontaudit}
    import              Output local customizations
    export              Output local customizations
    login               Manage login mappings between linux users and SELinux
                        confined users
    user                Manage SELinux confined users (Roles and levels for an
                        SELinux user)
    port                Manage network port type definitions
    interface           Manage network interface type definitions
    module              Manage SELinux policy modules
    node                Manage network node type definitions
    fcontext            Manage file context mapping definitions
    boolean             Manage booleans to selectively enable functionality
    permissive          Manage process type enforcement mode
    dontaudit           Disable/Enable dontaudit rules in policy

optional arguments:
  -h, --help            show this help message and exit



</pre>
 
As you can see, semanage has about a dozen sub commands. Each subcommand has it's own man page, which you can access like this:

<pre>
$ man semanage-{subcommand}
</pre>

e.g. 

<pre>
$ man semanage-fcontext
</pre>


In our case we are interested in the fcontext subcommand:

<pre>
$ semanage fcontext -h
usage: semanage fcontext [-h] [-n] [-N] [-S STORE] [ --add ( -t TYPE -f FTYPE -r RANGE -s SEUSER | -e EQUAL ) FILE_SPEC ) | --delete ( -t TYPE -f FTYPE | -e EQUAL ) FILE_SPEC ) | --deleteall  | --extract  | --list -C | --modify ( -t TYPE -f FTYPE -r RANGE -s SEUSER | -e EQUAL ) FILE_SPEC ) ]

positional arguments:
  file_spec             file_spec

optional arguments:
  -h, --help            show this help message and exit
  -C, --locallist       List fcontext local customizations
  -n, --noheading       Do not print heading when listing fcontext object
                        types
  -N, --noreload        Do not reload policy after commit
  -S STORE, --store STORE
                        Select an alternate SELinux Policy Store to manage
<mark>  -a, --add             Add a record of the fcontext object type
  -d, --delete          Delete a record of the fcontext object type
  -m, --modify          Modify a record of the fcontext object type
  -l, --list            List records of the fcontext object type
</mark>  -E, --extract         Extract customizable commands, for use within a
                        transaction
  -D, --deleteall       Remove all fcontext objects local customizations
  -e EQUAL, --equal EQUAL
                        Substitute target path with sourcepath when generating
                        default label. This is used with fcontext. Requires
                        source and target path arguments. The context labeling
                        for the target subtree is made equivalent to that
                        defined for the source.
  -f {a,f,d,c,b,s,l,p}, --ftype {a,f,d,c,b,s,l,p}
                        File Type. This is used with fcontext. Requires a file
                        type as shown in the mode field by ls, e.g. use -d to
                        match only directories or -- to match only regular
                        files. The following file type options can be passed:
                        -- (regular file),-d (directory),-c (character
                        device), -b (block device),-s (socket),-l (symbolic
                        link),-p (named pipe) If you do not specify a file
                        type, the file type will default to "all files".
  -s SEUSER, --seuser SEUSER
                        SELinux user name
  -t TYPE, --type TYPE  SELinux Type for the object
  -r RANGE, --range RANGE
                        MLS/MCS Security Range (MLS/MCS Systems only) SELinux
                        Range for SELinux login mapping defaults to the
                        SELinux user record range.
</pre>  




This indicates that we can add/modify/delete the targeted policies with semanage's fcontext sub command. 

The semanage fcontext's man page actually contains examples on how to do this:


<pre>
$ man semanage-fcontext | grep -A16 "EXAMPLE"
EXAMPLE
       remember to run restorecon after you set the file context
       Add file-context for everything under /web
       # semanage fcontext -a -t httpd_sys_content_t "/web(/.*)?"
       # restorecon -R -v /web

       Substitute /home1 with /home when setting file context
       # semanage fcontext -a -e /home /home1
       # restorecon -R -v /home1

       For home directories under top level directory, for example /disk6/home,
       execute the following commands.
       # semanage fcontext -a -t home_root_t "/disk6"
       # semanage fcontext -a -e /home /disk6/home
       # restorecon -R -v /disk6
</pre>


Now lets try this out, first let's create a file:


<pre>
$ touch /var/www/html/hello.html
$ ls -lZ /var/www/html/hello.html
-rw-r--r--. root root unconfined_u:object_r:httpd_sys_content_t:s0 /var/www/html/hello.html
</pre>

Now let's say we want to change the type security attribute to "admin_home_t", then based on the man page's example, we do:


<pre>
$ semanage fcontext -a -t admin_home_t /var/www/html/hello.html
</pre>

So now we have added a new targets policy to the fcontext catalog:

<pre>
$ semanage fcontext -l | grep hello.html
/var/www/html/hello.html       all files          system_u:object_r:admin_home_t:s0
</pre>

Now to apply this policy rule to the file, we need to use the <code>restorecon</code> command:

<pre>
$ ls -lZ /var/www/html/hello.html
-rw-r--r--. root root unconfined_u:object_r:<strong>httpd_sys_content_t</strong>:s0 /var/www/html/hello.html
$ restorecon /var/www/html/hello.html
$ ls -lZ /var/www/html/hello.html
-rw-r--r--. root root unconfined_u:object_r:<strong>admin_home_t</strong>:s0 /var/www/html/hello.html
</pre>

the <code>restorecon</code> command essentially sets the security context for a file/folder based on the rules specified by <code>semanage fcontext -l</code> command.  

In the above example, we applied the "restorecon" command to only one file. However if you want to run the restorecon command on every single file on your machine, then you can do this by simply creating the following file empty file:

<pre>
$ touch /.autorelabel
$ ls -la / | grep '.autorelabel'
-rw-r--r--.   1 root    root       0 Oct 13 20:02 .autorelabel
</pre>

After this file has been created, you simply reboot the machine for the autorelabel to start. This can be quite time consuming and as result your machine's reboot may take several minutes longer then it usually does. Once the relabelling is complete, the .autorelabel file is automatically deleted. 

Note, creating this /.autorelabel null file is something we need to do when we don't know what the root password is and therefore we perform a password reset. 

An alternative way to doing a complete machine level restorecon is by doing changing the default SELinux mode to disabled and then switching the default back to either permissive of enabled again. This approach will require you do 2 machine reboots instead of one. 


<h2>A common example of why you need to create your own targeted policies</h2>

By default httpd will use the /var/www as the Documetroot. However you might want to change this default to a new custom folder, e.g.:

<pre>
$ mkdir /webcontent
$ echo "hello world" > /webcontent/index.html
$ chown -R apache:apache /webcontent
</pre>

You would then update the httpd config file to point to this new "DocumentRoot" folder.

Now ensure that selinux mode is set to enabled:

<pre>
$ setenforce Enforcing
$ getenforce
Enforcing
</pre>

Now if you open up your servers homepage in a web browser you will find that you get an error message. That's because the security context of the /webcontent folder and it's children is wrong:

<pre>
$ ls -laZ /webcontent
drwxr-xr-x. apache apache unconfined_u:object_r:<strong>default_t</strong>:s0 .
dr-xr-xr-x. root   root   system_u:object_r:root_t:s0      ..
-rw-r--r--. apache apache unconfined_u:object_r:<strong>default_t</strong>:s0 index.html
</pre>

That's because <code>$ semanage fcontext -l</code> doesn't contain any specific policies for our custom made folder:

<pre>
$ semanage fcontext -l | grep webcontent
$
</pre> 


Therefore our folder and it's children just got a context with the generic type "default_t". 

However for the apache process to be able to access /webcontent folder and it's contents, there security context needs to be the same as that of the original default DocumentRoot folder:

<pre>
$ ls -laZ /var | grep www
drwxr-xr-x. root root system_u:object_r:<strong>httpd_sys_content_t</strong>:s0 www
</pre>

By reviewing the examples in the semanage-fcontext man page, we found that we need to run the following command to define our new security context for the /webcontent folder and it's children:

<pre>
$ semanage fcontext -l | grep webcontent
$ semanage fcontext -a -t httpd_sys_content_t "/webcontent(/.*)?"
</pre>

Now let's confirm this policy has been added to the catalog:

<pre>
$ semanage fcontext -l | grep webcontent
/webcontent(/.*)?         all files          system_u:object_r:httpd_sys_content_t:s0
</pre>

Finally we can now apply this policy to our folder and it's children.

First let's check again what it is currently:

<pre>
$ ls -laZ /webcontent
drwxr-xr-x. apache apache unconfined_u:object_r:default_t:s0 .
dr-xr-xr-x. root   root   system_u:object_r:root_t:s0      ..
-rw-r--r--. apache apache unconfined_u:object_r:default_t:s0 index.html
</pre>

Now apply our new policy to the /webcontent folder and it's children:

<pre>
$ restorecon -r /webcontent
</pre>

Now let's check again:

<pre>
$ ls -laZ /webcontent
drwxr-xr-x. apache apache unconfined_u:object_r:<strong>httpd_sys_content_t</strong>:s0 .
dr-xr-xr-x. root   root   system_u:object_r:root_t:s0      ..
-rw-r--r--. apache apache unconfined_u:object_r:<strong>httpd_sys_content_t</strong>:s0 index.html
</pre>

We should now be able to access the index.html via a web browser. 



]]></Content>
		<Date><![CDATA[2015-06-20]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - SELinux extended man pages]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What is the command to load in SELinux specific man pages?"]
$ sepolicy manpage -a -p /usr/share/man/man8
[/toggle]
[toggle title="What is the command you then need to run?"]
$ mandb
[/toggle]
[toggle title="What is the command to view all the newly available man pages?"]
$ man -K "_selinux"
[/toggle]
[/accordion]

<hr/>


So far we saw that the (target) policy dictates what security label each and every object should have, and it stores all the policy rules, i.e. which objects can access other objects based on the security labels. 


Another thing you might want to do is find out what a security label actually means. There are lots of man pages that hold this info. These man pages aren't loaded in by default and to load them in, you need to use the <strong>sepolicy</strong>. 

<pre>
$ whatis sepolicy
sepolicy (8)         - SELinux Policy Inspection tool
</pre>


First we run the following command (need to memorize this):


<pre>
$ sepolicy manpage -a -p /usr/share/man/man8
</pre>


Then run mandb to update the man pages and add in the new pages:

<pre>
$ mandb
</pre>

Finally you can view all the newly added SELinux man pages like this:


<pre>
<span style='font-size:11px'>$ man -k "_selinux" 
abrt_dump_oops_selinux (8) - Security Enhanced Linux Policy for the abrt_dump_oops processes
abrt_handle_event_selinux (8) - Security Enhanced Linux Policy for the abrt_handle_event processes
abrt_helper_selinux (8) - Security Enhanced Linux Policy for the abrt_helper processes
abrt_retrace_coredump_selinux (8) - Security Enhanced Linux Policy for the abrt_retrace_coredump processes
abrt_retrace_worker_selinux (8) - Security Enhanced Linux Policy for the abrt_retrace_worker processes
abrt_selinux (8)     - Security Enhanced Linux Policy for the abrt processes
.
.
.
...etc.</span>
</pre> 

There is quite a lot!

These man pages are quite comprehensive detailed info about various security contexts and booleans  are available here.





]]></Content>
		<Date><![CDATA[2015-06-21]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - SELinux Log Files]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What is the command to check the status of the service that logs all SELinux related log entries?"]
$ systemctl status auditd
[/toggle]
[toggle title="Which file stores all SELinux related log entries?"]
/var/log/audit/audit.log
[/toggle]
[toggle title="What is the (cat/grep) command that filters the above file to only display SELinux related log entries?"]
$ cat /var/log/audit/audit.log | grep "AVC"
[/toggle]
[toggle title="What is a better alternative to the above command?"]
$ sealert -a /var/log/audit/audit.log
[/toggle]
[toggle title="Where can you find additional SELinux related log entries?"]
$ vim /var/log/messages  # search for 'sealert'
[/toggle]
[/accordion]

<hr/>



When you are having SELinux access problems one of the first things to investigate are the log files. The auditd service is responsible for writing SELinux logs, so first we need to make sure it is running:

<pre>$ systemctl status auditd
auditd.service - Security Auditing Service
   Loaded: loaded (/usr/lib/systemd/system/auditd.service; enabled)
   Active: active (running) since Sat 2015-06-20 07:20:26 BST; 1 day 3h ago
 Main PID: 581 (auditd)
   CGroup: /system.slice/auditd.service
           ├─581 /sbin/auditd -n
           ├─587 /sbin/audispd
           └─589 /usr/sbin/sedispatch
</pre>


This service will write all SELinux related access denied messages to the <code>/var/log/audit/audit.log</code> file. However this file records lots of other things and not just SELinux related log entries. Therefore we grep for "AVC" to view just SELinux related log entries:

<pre>
$ cat /var/log/audit/audit.log | grep "AVC"
type=AVC msg=audit(1434247738.386:471): avc:  denied  { getattr } for  pid=1435 comm="httpd" path="/var/www/html/index.html" dev="dm-1" ino=71561345 scontext=system_u:system_r:httpd_t:s0 tcontext=unconfined_u:object_r:admin_home_t:s0 tclass=file
type=AVC msg=audit(1434247738.386:472): avc:  denied  { getattr } for  pid=1435 comm="httpd" path="/var/www/html/index.html" dev="dm-1" ino=71561345 scontext=system_u:system_r:httpd_t:s0 tcontext=unconfined_u:object_r:admin_home_t:s0 tclass=file
type=USER_AVC msg=audit(1434247801.772:476): pid=1 uid=0 auid=4294967295 ses=4294967295 subj=system_u:system_r:init_t:s0 msg='avc:  received setenforce notice (enforcing=1)  exe="/usr/lib/systemd/systemd" sauid=0 hostname=? addr=? terminal=?'
.
.
</pre>

This works because all SELinux log entries contains the string "AVC".  

However you can also parse this log file into the <strong>sealert</strong> utility which will then output the relevant parts of this log in a more human readable format:


<pre>
$ sealert -a /var/log/audit/audit.log
</pre>


If after viewing the logs, you are still unsure how to fix this, then you could also looking at the main log file, <code>/var/log/messages</code>, using the "less" command. In main log file, try searching for the keyword <code>sealert</code>. You should then find an entry in the log file that looks something like this:


<pre>
Jun 21 12:14:20 localhost setroubleshoot: SELinux is preventing /usr/sbin/httpd from read access on the file testfile.html. For complete SELinux messages. run sealert -l d72b5c45-af2d-4a29-95cd-8a7289594b85
Jun 21 12:14:20 localhost python: SELinux is preventing /usr/sbin/httpd from read access on the file testfile.html.

</pre>

This should hopefully give enough for you to determine what's wrong. You can also run the sealert command as suggested. So if we run it:


<pre>
 sealert -l d8dfacad-00f5-4796-a00d-c02bbfdc3a41
SELinux is preventing /usr/sbin/httpd from getattr access on the file /var/www/html/testfile.html.

*****  Plugin restorecon (<strong>99.5 confidence</strong>) suggests   ************************

If you want to fix the label.
/var/www/html/testfile.html default label should be httpd_sys_content_t.
Then you can run restorecon.
Do
<strong># /sbin/restorecon -v /var/www/html/testfile.html</strong>

*****  Plugin catchall (1.49 confidence) suggests   **************************

If you believe that httpd should be allowed getattr access on the testfile.html file by default.
Then you should report this as a bug.
You can generate a local policy module to allow this access.
Do
allow this access for now by executing:
# grep httpd /var/log/audit/audit.log | audit2allow -M mypol
# semodule -i mypol.pp


Additional Information:
Source Context                system_u:system_r:httpd_t:s0
Target Context                unconfined_u:object_r:admin_home_t:s0
Target Objects                /var/www/html/testfile.html [ file ]
Source                        httpd
Source Path                   /usr/sbin/httpd
Port                          <Unknown>
Host                          localhost.localdomain
Source RPM Packages           httpd-2.4.6-31.el7.centos.x86_64
Target RPM Packages
Policy RPM                    selinux-policy-3.13.1-23.el7_1.7.noarch
Selinux Enabled               True
Policy Type                   targeted
Enforcing Mode                Enforcing
Host Name                     localhost.localdomain
Platform                      Linux localhost.localdomain
                              3.10.0-229.4.2.el7.x86_64 #1 SMP Wed May 13
                              10:06:09 UTC 2015 x86_64 x86_64
Alert Count                   2
First Seen                    2015-06-21 12:14:16 BST
Last Seen                     2015-06-21 12:45:01 BST
Local ID                      d8dfacad-00f5-4796-a00d-c02bbfdc3a41

Raw Audit Messages
type=AVC msg=audit(1434887101.673:2151): avc:  denied  { getattr } for  pid=15563 comm="httpd" path="/var/www/html/testfile.html" dev="dm-1" ino=69211150 scontext=system_u:system_r:httpd_t:s0 tcontext=unconfined_u:object_r:admin_home_t:s0 tclass=file


type=SYSCALL msg=audit(1434887101.673:2151): arch=x86_64 syscall=lstat success=no exit=EACCES a0=7f61bd817c68 a1=7fff6a1c4ec0 a2=7fff6a1c4ec0 a3=0 items=0 ppid=1304 pid=15563 auid=4294967295 uid=48 gid=48 euid=48 suid=48 fsuid=48 egid=48 sgid=48 fsgid=48 tty=(none) ses=4294967295 comm=httpd exe=/usr/sbin/httpd subj=system_u:system_r:httpd_t:s0 key=(null)

Hash: httpd,httpd_t,admin_home_t,file,getattr


</pre>


sealert gives a lot of useful info, and it even suggests what command it thinks you need to run that will resolve this issue (with 99.5%) confidence. However as indicated by the confidence level, it's suggestion might not always be correct. 

 

]]></Content>
		<Date><![CDATA[2015-06-21]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Using Firewalld]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What is the command to check the status of the firewall's service?"]
$ systemctl status firewalld
[/toggle]
[toggle title="What is the command to list all the available zones?"]
$  firewall-cmd <span style="letter-spacing:1.5px">-</span>-get-zones
[/toggle]
[toggle title="What is the command to view the default zone?"]
$ firewall-cmd <span style="letter-spacing:1.5px">-</span>-get-default-zone
[/toggle]
[toggle title="What is the command to view the currently active zone?"]
$ firewall-cmd <span style="letter-spacing:1.5px">-</span>-get-active-zones
[/toggle]
[toggle title="What is the command to view info about the zone, 'public'?"]
$ firewall-cmd <span style="letter-spacing:1.5px">-</span>-zone=public <span style="letter-spacing:1.5px">-</span>-list-all
[/toggle]
[toggle title="What is the command to add an IP address (192.168.50.0/24) restriction to the zone, 'public'. Also this needs to be a persistant restriction?"]
$ firewall-cmd <span style="letter-spacing:1.5px">-</span>-zone=public <span style="letter-spacing:1.5px">-</span>-add-source=192.168.50.0/24 <span style="letter-spacing:1.5px">-</span>-permanent
# then restart the firewalld service to load in the change
$ systemctl restart firewalld
[/toggle]
[toggle title="What is the command to view a list of services that can be added to a zone?"]
$ firewall-cmd <span style="letter-spacing:1.5px">-</span>-get-services
[/toggle]
[toggle title="What are the commands to peristantly add the http service to the 'public' zone?"]
$ firewall-cmd <span style="letter-spacing:1.5px">-</span>-add-service=http <span style="letter-spacing:1.5px">-</span>-zone=public <span style="letter-spacing:1.5px">-</span>-permanent
$ systemctl restart firewalld     # load in the new change
[/toggle]
[toggle title="What are the commands to persistantly add port 80/tcp to the zone public?"]
$ firewall-cmd <span style="letter-spacing:1.5px">-</span>-add-port=80/tcp <span style="letter-spacing:1.5px">-</span>-permanent <span style="letter-spacing:1.5px">-</span>-zone=public
$ systemctl restart firewalld
[/toggle]
[toggle title="What is the command to check that the above changes have worked?"]
$ firewall-cmd <span style="letter-spacing:1.5px">-</span>-list-all
[/toggle]
[toggle title="Which directory contains xml representations of each 'firewalld' service, which can use as templates to create your own templates?"]
/usr/lib/firewalld/services/
[/toggle]
[toggle title="If you create your custom firewalld service xml files, then which directory do you need to store them in?"]
/etc/firewalld/services
[/toggle]
[toggle title="What is the command to open up firewall gui interface?"]
$ firewall-config
[/toggle]
[/accordion]

<hr/>


On your machine you are likely to have lots of service that are listening on various ports. For example sshd, httpd, vsftpd,...etc. In Linux there is a system called "Netfilter" which controls which the flow of traffic to/from these services. NetFilter is designed to only allow internet access to network based services, and on the port numbers that they require.  There are 2 systems which are available to configure Netfilter, they are "iptables" and "firewalld". Both of them can fully configure Netfilter, however firewalld is a lot more easier to use.  

Firewalld itself runs as a service, so to make sure it is working you need to run the following:


<pre>$ systemctl status firewalld
firewalld.service - firewalld - dynamic firewall daemon
   Loaded: loaded (/usr/lib/systemd/system/firewalld.service; enabled)
   Active: active (running) since Sat 2015-06-20 07:20:30 BST; 1 day 7h ago
 Main PID: 611 (firewalld)
   CGroup: /system.slice/firewalld.service
           └─611 /usr/bin/python -Es /usr/sbin/firewalld --nofork --nopid

Jun 20 07:20:26 localhost.localdomain systemd[1]: Starting firewalld - dynamic firewall daemon...
Jun 20 07:20:30 localhost.localdomain systemd[1]: Started firewalld - dynamic firewall daemon.
</pre>

The main command that we use to work with firewalld is <strong>firewall-cmd</strong>. In firewalld there are number of "zones" available, you can see a list of them using the <strong>firewalld-cmd</strong> command:


<pre>
$  firewall-cmd <span style="letter-spacing:1.5px">-</span>-get-zones
block dmz drop external home internal public trusted work
</pre>

Note: All the zones appear on a single line as a space separated string. In our case we have 9 zones. All your firewall rules are grouped inside a zone. Each zone specifies an ip address range whitelist. Therefore we want to send+data via ip address ranges, then we need to have at least 2 zones active. 

To see what is our default zone, we do:

<pre>
$ firewall-cmd <span style="letter-spacing:1.5px">-</span>-get-default-zone
public
</pre> 

This is the zone, our machine defaults to on boot up. However the zone that we are currently in can be different. To see what zones are currently active, we do:

<pre>
$ firewall-cmd <span style="letter-spacing:1.5px">-</span>-get-active-zones
public
  interfaces: enp0s3 enp0s8
</pre>

In our case, we only have one zone that is currently active zone...which is the default zone.  

It also shows here that only the "enp0s3" and "enp0s8" interfaces are allowed to send+receive network traffic data in this zone. 

Now to get all the info for a given zone we do:

<pre>$ firewall-cmd <span style="letter-spacing:1.5px">-</span>-zone=public <span style="letter-spacing:1.5px">-</span>-list-all
public (default, active)
  interfaces: enp0s3
  sources:
  services: dhcpv6-client ssh
  ports:
  masquerade: no
  forward-ports:
  icmp-blocks:
  rich rules:
</pre>

The --list-all option gives an overview of the entire zone, i.e. which interfaces are allowed to operate in this zone, what services are allowed to send+receive data in this zone. The cool thing here is that you don't need to specify what ports needs to be opened. Since the sshd service is allowed in this zone, it means that firewalld has port 22 opened for this zone automatically. Here's the "home" zone's info:

<pre>
$ firewall-cmd <span style="letter-spacing:1.5px">-</span>-zone=home <span style="letter-spacing:1.5px">-</span>-list-all
home
  interfaces:
  sources:
  services: dhcpv6-client ipp-client mdns samba-client ssh
  ports:
  masquerade: no
  forward-ports:
  icmp-blocks:
  rich rules:
</pre>  


In both the "home" and the "public" zones shown above, there isn't any ip address restriction, however we can add one, like this:

<pre>
$ firewall-cmd --zone=public --add-source=192.168.50.0/24
success
</pre>

Now let's check that an ip range restriction has now been applied:

<pre>
$ firewall-cmd --list-all --zone=public
public (default, active)
  interfaces: enp0s3 enp0s8
  <strong>sources: 192.168.50.0/24</strong>
  services: dhcpv6-client ssh
  ports:
  masquerade: no
  forward-ports:
  icmp-blocks:
  rich rules:
</pre>

However this change isn't persistent, and is reverted when the the firewalld service is restarted:


<pre>
$ systemctl restart firewalld.service
$ firewall-cmd --list-all --zone=public
public (default, active)
  interfaces: enp0s3
  sources:
  services: dhcpv6-client ssh
  ports:
  masquerade: no
  forward-ports:
  icmp-blocks:
  rich rules:
</pre>

To make this change persistant, you need to use the "--permenant" flag:

<pre>
$ firewall-cmd --zone=public --add-source=192.168.50.0/24 --permanent
success
</pre>

However this change will take affect after a firewalld service restart:


<pre>
$ firewall-cmd <span style="letter-spacing:1.5px">-</span>-list-all <span style="letter-spacing:1.5px">-</span>-zone=public
public (default, active)
  interfaces: enp0s3
  sources:
  services: dhcpv6-client ssh
  ports:
  masquerade: no
  forward-ports:
  icmp-blocks:
  rich rules:

$ systemctl restart firewalld.service
$
$ firewall-cmd --list-all --zone=public
public (default, active)
  interfaces: enp0s3
  sources: 192.168.50.0/24
  services: dhcpv6-client ssh
  ports:
  masquerade: no
  forward-ports:
  icmp-blocks:
  rich rules:
</pre>

Hence, using the "--permenant" setting is a bit like how systemctl's "enable", i.e. it only takes affect after a service restart. Therefore if you want to make a persistant change that also is active in the current firewalld.session, then you will need run the same firewall-cmd command twice, one without the --permenant option, and one with.  

Notice above that the httpd service is not currently permitted in this zone. This means that we'll get a connection timeout message when you try to open a web browser and access the vm's web server's homepage. 

To fix this issue, we need to add the appropriate services to the zone. Let's first see what services we have available:

<pre>
$ firewall-cmd --get-services
RH-Satellite-6 amanda-client bacula bacula-client dhcp dhcpv6 dhcpv6-client dns ftp high-availability <mark>http</mark> <mark>https</mark> imaps ipp ipp-client ipsec kerberos kpasswd ldap ldaps libvirt libvirt-tls mdns mountd ms-wbt mysql nfs ntp openvpn pmcd pmproxy pmwebapi pmwebapis pop3s postgresql proxy-dhcp radius rpc-bind samba samba-client smtp ssh telnet tftp tftp-client transmission-client vnc-server wbem-https
</pre>  

Now let's add the httpd related services to the zone:

<pre>
$ firewall-cmd <span style="letter-spacing:1.5px">-</span>-add-service=http <span style="letter-spacing:1.5px">-</span>-zone=public
success
$ firewall-cmd <span style="letter-spacing:1.5px">-</span>-add-service=http <span style="letter-spacing:1.5px">-</span>-zone=public <span style="letter-spacing:1.5px">-</span>-permanent
success
$ firewall-cmd <span style="letter-spacing:1.5px">-</span>-add-service=https
success
$ firewall-cmd <span style="letter-spacing:1.5px">-</span>-add-service=https <span style="letter-spacing:1.5px">-</span>-zone=public <span style="letter-spacing:1.5px">-</span>-permanent
success
$ firewall-cmd <span style="letter-spacing:1.5px">-</span>-list-all <span style="letter-spacing:1.5px">-</span>-zone=public
public (default, active)
  interfaces: enp0s3
  sources: 192.168.50.0/24
  services: dhcpv6-client <strong>http https</strong> ssh
  ports:
  masquerade: no
  forward-ports:
  icmp-blocks:
  rich rules:
</pre>
 
  
After that you should now be able to access the websites. 

Also instead of adding the http and https services to the zone, we could have added the ports, i.e.: 

<pre>
$ firewall-cmd <span style="letter-spacing:1.5px">-</span>-add-port=80/tcp <span style="letter-spacing:1.5px">-</span>-permanent <span style="letter-spacing:1.5px">-</span>-zone=public
success
$ firewall-cmd <span style="letter-spacing:1.5px">-</span>-add-port=443/tcp <span style="letter-spacing:1.5px">-</span>-zone=public
success
$ firewall-cmd <span style="letter-spacing:1.5px">-</span>-add-port=80/tcp <span style="letter-spacing:1.5px">-</span>-permanent <span style="letter-spacing:1.5px">-</span>-zone=public
success
$ firewall-cmd <span style="letter-spacing:1.5px">-</span>-add-port=443/tcp <span style="letter-spacing:1.5px">-</span>-permanent <span style="letter-spacing:1.5px">-</span>-zone=public
success
$ firewall-cmd <span style="letter-spacing:1.5px">-</span>-list-all
public (default)
  interfaces:
  sources:
  services: dhcpv6-client ssh
  ports: 443/tcp 80/tcp
  masquerade: no
  forward-ports:
  icmp-blocks:
  rich rules:
</pre>




Now in the context of firewalld, a service is something that is associated to a protocol and port, and it is not the traditional services such as httpd, sshd,...etc. Due to this new definition, a service can essentially be represented in the form of a an xml file.  You can view a list of the default services by looking in the following directory:


<pre>
$ ls /usr/lib/firewalld/services/
amanda-client.xml  ftp.xml                ipsec.xml        mdns.xml     pmcd.xml        radius.xml        tftp-client.xml
bacula-client.xml  high-availability.xml  kerberos.xml     mountd.xml   pmproxy.xml     rpc-bind.xml      tftp.xml
bacula.xml         https.xml              kpasswd.xml      ms-wbt.xml   pmwebapis.xml   samba-client.xml  transmission-client.xml
dhcpv6-client.xml  http.xml               ldaps.xml        mysql.xml    pmwebapi.xml    samba.xml         vnc-server.xml
dhcpv6.xml         imaps.xml              ldap.xml         nfs.xml      pop3s.xml       smtp.xml          wbem-https.xml
dhcp.xml           ipp-client.xml         libvirt-tls.xml  ntp.xml      postgresql.xml  ssh.xml
dns.xml            ipp.xml                libvirt.xml      openvpn.xml  proxy-dhcp.xml  telnet.xml
</pre>

You can use one of these xml files as a template to create your own custom services (i.e. xml file), however your service does not go inside the above folder, instead it has to be placed in <code>/etc/firewalld/services</code> folder. Here's an example of what one of these xml files looks like:

<pre>
$ cat /usr/lib/firewalld/services/ssh.xml
<?xml version="1.0" encoding="utf-8"?>
<service>
  <short>SSH</short>
  <description>Secure Shell (SSH) is a protocol for logging into and executing commands on remote machines. It provides secure encrypted communications. If you plan on accessing your machine remotely via SSH over a firewalled interface, enable this option. You need the openssh-server package installed for this option to be useful.</description>
  <strong><port protocol="tcp" port="22"/></strong>
</service>
</pre>

In the world of firewalld, this xml file is referred to as a "service" because it associates a name (which in this case is name, ssh) to a protocol and a port number. 


<h2>Using firewall-config</h2>

Finally you can do all of the above using the <strong>firewall-config</strong> GUI tool. From the terminal we run:

<pre>
$ firewall-config
</pre> 

<a href="http://codingbee.net/wp-content/uploads/2015/06/XQadD0h.png"><img src="http://codingbee.net/wp-content/uploads/2015/06/XQadD0h.png" alt="" width="895" height="581" class="alignnone size-full wp-image-4768" /></a>

Useful Tip: you can make the changes using firewall-cmd, and then check whether the changes are what you expected by viewing the firewall-config gui.  




See also:

http://www.tecmint.com/configure-firewalld-in-centos-7/]]></Content>
		<Date><![CDATA[2015-06-21]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Common SELinux network service related problems]]></Title>
		<Content><![CDATA[For services like ftp of httpd, you may configure them to use:

- different default directory for storing your data
- different port number to listen on


In these scenarios you need to add policy rules to make this work. 



For custom port numbers, you will need to run a command like this:


<pre>
$ semanage -port -a -t http_port_t -p tcp {custom port number}
</pre>

Note: sealert should be able to suggest this. 

]]></Content>
		<Date><![CDATA[2015-06-22]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Create your own yum repo on local machine]]></Title>
		<Content><![CDATA[mkdir 

/tmp/yumrepo/repodata


put all your rpms in this folder. 




https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Deployment_Guide/sec-Yum_Repository.html


clean cache, useful troubleshooting:
<pre>
yum clean all
</pre>]]></Content>
		<Date><![CDATA[2015-06-23]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Centos|RHEL|yum]]></Tags>
		<Status><![CDATA[draft]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Tip: avoid redownloading after each rspec run]]></Title>
		<Content><![CDATA[the rake's spec command does is comprised of the following:

spec = spec_prep + spec_standalone + spec_clean

So we need to avoid running spec_clean. To do this simply run:

<pre>$ bundle exec rake spec_prep</pre>

After that, always run


<pre>$ bundle exec rake spec_standalone</pre>

Instead of the "spec" command
]]></Content>
		<Date><![CDATA[2015-06-24]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>rspec-puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Setting up an FTP server]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What is the command to install the ftp server software?"]
$ yum install vsftpd
[/toggle]
[toggle title="What is the location of this packages main config file?"]
/etc/vsftpd/vsftpd.conf   
[/toggle]
[toggle title="What is the command to display help info about this config file?"]
$ man 5 vsftpd.conf
[/toggle]
[toggle title="What directory do you install the files in?"]
/var/ftp
[/toggle]
[toggle title="What is the command to check the ftp service?"]
$ systemctl status vsftpd
[/toggle]
[toggle title="What is the command to persistantly add the ftp service to the firewall?"]
$ firewall-cmd --permanent --add-service=ftp
$ systemctl restart firewalld
[/toggle]
[toggle title="What is the command to check that ftp service is listening on the ftp port?"]
$ ss -at | grep ftp
LISTEN     0      32        :::ftp         :::*
[/toggle]
[toggle title="What is the command to fix all file SELinux context issues?"]
$ restorecon -R /var/ftp
[/toggle]
[toggle title="What is the command to persistantly+verbosely enable the ftp_home_dir boolean?"]
$ setsebool -PV ftp_home_dir on
[/toggle]
[toggle title="What is the command to install the ftp client?"]
$ yum install lftp
[/toggle]
[toggle title="What is the command to start a new ftp interactive session remotely?"]
$ lftp localhost
[/toggle]
[/accordion]

<hr/>



First off, you might think configuring an ftp server will be similar to how we do it for httpd. For example, there is no equivalent "documentroot" setting defined in the main config file, but more about this later.  


<h2>Install vsftpd</h2>
The main package you need to install for setting up an ftp server for your machine is called "vsftpd":

<pre>
$ yum install vsftpd
</pre>

<h2>Configure vsfptd</h2>
Once that's done you'll find the main config file is:

<pre>
/etc/vsftpd/vsftpd.conf   
</pre>

This file is quite self explanatory, the default settings should be enough to start sharing folders right away, but you can find more help info here if you want to do any customisations:

<pre>
$ man 5 vsftpd.conf
</pre>


<h2>Add files into the ftp share directory</h2>
The next thing you need to do is place files in vsftp's "share folder". The only way to identify this is by looking at the ftp's service account's directory:

<pre>
$ cat /etc/passwd | grep ftp
ftp:x:14:50:FTP User:<mark>/var/ftp</mark>:/sbin/nologin
</pre>

Surprising this in the only way to locate this "share folder". In this folder you'll find the "pub":

<pre>
$ pwd
/var/ftp
$ ls -l
total 0
drwxr-xr-x. 2 root root 6 Jun 10  2014 pub
</pre>

Therefore this is folder where you drop files to make it available via ftp:

<pre>
/var/ftp/pub
</pre>

<h2>Start the vsftpd service</h2>
Now, the ftp service is:

<pre>
$ systemctl status vsftpd
vsftpd.service - Vsftpd ftp daemon
   Loaded: loaded (/usr/lib/systemd/system/vsftpd.service; disabled)
   Active: inactive (dead)
</pre>

So to start sharing we need to enable+start this service:

<pre>
$ systemctl enable vsftpd
ln -s '/usr/lib/systemd/system/vsftpd.service' '/etc/systemd/system/multi-user.target.wants/vsftpd.service'
$ systemctl start vsftpd
</pre>




<h2>Add FTP service to relevant firewalld zone</h2>

Now we need to tell the firewall to allow vsftpd to listen on the ftp port (port 21):

<pre>
$ firewall-cmd --list-all
public (default, active)
  interfaces: enp0s3
  sources:
  services: dhcpv6-client ssh
  ports:
  masquerade: no
  forward-ports:
  icmp-blocks:
  rich rules:
</pre>

Now lets add this service to the current firewalld zone:

<pre>$ firewall-cmd --add-service=ftp
success
$ firewall-cmd --permanent --add-service=ftp
success
</pre>



Next you should check that the vsftpd service is listening:

<pre>
ss -at | grep ftp
LISTEN     0      32        :::ftp         :::*
</pre>



<h2>Check that file SELinux contexts are ok</h2> 

Ensure the folders that contains the files/folders to be shared has the "public_content_t" type security context:

<pre>
$ ls -lZ /var/ | grep ftp
drwxr-xr-x. root root system_u:object_r:<strong>public_content_t</strong>:s0 ftp

$ ls -lZ /var/ftp/ | grep pub
drwxr-xr-x. root root system_u:object_r:<strong>public_content_t</strong>:s0 pub
</pre>

If it isn't then run the following to recursively correct them:

<pre>
$ restorecon -R /var/ftp
</pre>


<h2>SELinux Boolean Settings</h2>
SEBooleans is another aspect you SELinux that you might need to configure as part of setting up vsftpd. Also check SEboolean settings:

<pre>
$ getsebool -a | grep ftp
ftp_home_dir --> on
ftpd_anon_write --> off
ftpd_connect_all_unreserved --> off
ftpd_connect_db --> off
ftpd_full_access --> off
ftpd_use_cifs --> off
ftpd_use_fusefs --> off
ftpd_use_nfs --> off
ftpd_use_passive_mode --> off
httpd_can_connect_ftp --> off
httpd_enable_ftp_server --> off
sftpd_anon_write --> off
sftpd_enable_homedirs --> off
sftpd_full_access --> off
sftpd_write_ssh_home --> off
tftp_anon_write --> off
tftp_home_dir --> off
</pre>

Note, you can run the <code>semanage boolean -l | grep ftp</code> to find out what these settings means. 

Where necessary you can change settings, e.g to enable 'ftp_home_dir', we do:


<pre>
$ setsebool -PV ftp_home_dir on
</pre>

Where "P" is for persistant. And "V" is for verbose mode, in case there are any error messages. 











<h2>Test your vsftpd setup</h2>
Now to test whether our ftp service is working, we need to install an ftp client, this ftp client is called:

<pre>
$ yum install lftp
</pre>

After that, simply do:

<pre>
$ lftp localhost
</pre>

This starts an interactive shell. Here's an example of it in action:

<pre>
$ lftp localhost
lftp localhost:~> ls
drwxr-xr-x    2 0        0              25 Nov 10 21:06 pub
lftp localhost:/> cd pub/
lftp localhost:/pub> ls
-rw-r--r--    1 0        0              36 Nov 10 21:06 testfile.txt
lftp localhost:/pub> get testfile.txt
36 bytes transferred
lftp localhost:/pub> quit
$ ls -l
total 4
-rw-rw-r--. 1 tom tom 36 Nov 10 21:06 testfile.txt
</pre>









<h2>Other common configurations</h2>
There are basically 2 types of users who you can provide access to your ftp server:


<ul>
	<li>authenticated users - they do need to provide login credentials</li>
	<li>anonymous users - they don't need to provide login credentials</li>
</ul>


You have all kinds of possibilities:

<ul>
	<li>Restrict access to authenticated users only</li>
	<li>Provide anonymous access - this approach is ideal if you are setting up an ftp server to share files with the public</li>
	<li>tiered access - anonymous users can access some files, whereas authenticated can access all the files that anonymous user can, but they also have access to other restricted files too.</li>
</ul>



Also notice that "others" has r+x permissions. That is so to allow anonymous users to access this directory (enabled with "x") and view/download it's content (enabled with "r") 

]]></Content>
		<Date><![CDATA[2015-06-25]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - NTP, system time, and the Hardware clock]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="How many different types of clocks exists on your machine and what are they called?"]
There are 2 types:
- Hardware clock
- System clock
</ul>
[/toggle]
[toggle title="What is the command to install the ntp service?"]
$ yum install chrony
[/toggle]
[toggle title="What is ntp's main config file?"]
/etc/chrony.conf   # the only thing you might need 
                   # to update here are the ntp servers. 
[/toggle]
[toggle title="What is the command to check the NTP service's status?"]
$ systemctl status chronyd.service
[/toggle]
[toggle title="What is command to view all your time related settings, e.g. current time, timezone, whether ntp is enabled, daylight saving time....etc?"]
$ timedatectl status
[/toggle]
[toggle title="What is the command to enable ntp?"]
$ timedatectl set-ntp true
[/toggle]
[toggle title="What is the command to view a list of ntp servers that your machine is using?"]
$ chronyc sources -v
[/toggle]
[toggle title="What is the command to do a manual ntp sync?"]
$ systemctl restart chronyd
[/toggle]
[toggle title="What is the command to check how effectively our ntp syncing is working?"]
$ chronyc tracking
[/toggle]
[toggle title="What is the command to update your hardware clock?"]
$ hwclock --systohc
[/toggle]
[/accordion]

<hr/>



<h2>Install ntp</h2>
Here's the command you need to run:

<pre>
$ yum install chrony
</pre>

There are two main clocks in your machine:

<ul>
	<li><strong>hardware clock</strong> - This clock is operated on your machine's motherboard</li>
	<li><strong>system-clock (aka system-time)</strong> - You'r operating system's clock</li>
</ul>


So when you start your machine, the first clock that your machine uses for timekeeping is the hardware clock, while the OS is booting up, the system-clock set's gets it's time from the hardware-clock. After that the system-clock's time get's maintained by the OS. 

After the machine has been booted, you can then change the system-clock's time manually by using the <strong>timedatectl</strong> command:

<pre>
$ date
Tue 23 Jun 02:32:32 BST 2015

$ timedatectl set-time 04:00

$ date
Tue 23 Jun 04:00:02 BST 2015
</pre>

You can also use timedatectl to instruct your OS to accurately maintain the correct time by keeping it's time in sync with a another trusted remote "ntp" server. This is done by running the following command:

<pre>
$ timedatectl set-ntp yes
</pre>


We can view a list of trusted ntp servers that the chronyd is using to sync the system-time. You can view this list using the <strong>chronyc</strong> command: 

<pre>
$ chronyc sources -v
210 Number of sources = 4

  .-- Source mode  '^' = server, '=' = peer, '#' = local clock.
 / .- Source state '*' = current synced, '+' = combined , '-' = not combined,
| /   '?' = unreachable, 'x' = time may be in error, '~' = time too variable.
||                                                 .- xxxx [ yyyy ] +/- zzzz
||                                                /   xxxx = adjusted offset,
||         Log2(Polling interval) -.             |    yyyy = measured offset,
||                                  \            |    zzzz = estimated error.
||                                   |           |
MS Name/IP address         Stratum Poll Reach LastRx Last sample
===============================================================================
^? ns1.tomhek.net                3   9   377   208   -607us[ -607us] +/-   91ms
^+ ntp1.mediamatic.nl            2  10   377   399   +446us[ +446us] +/-   44ms
^* metronoom.dmz.cs.uu.nl        1   9   377   976   -298us[ -335us] +/-   22ms
^- mirror.muntinternet.net       2  10   377   126   +318us[ +318us] +/-   65ms

</pre>


The asterisk indicates the what our machine is currently using as the primary server. The primary NTP server can change every few minutes based on an algorithm that chronyd users. 

To get more info about our ntp status, we run: 

<pre>
$ chronyc tracking
Reference ID    : 213.136.0.252 (ntp4.bit.nl)
Stratum         : 2
Ref time (UTC)  : Sun Oct  4 16:13:07 2015
System time     : 0.001208134 seconds fast of NTP time
Last offset     : 0.002128782 seconds
RMS offset      : 0.003163246 seconds
Frequency       : 500.079 ppm fast
Residual freq   : 7.099 ppm
Skew            : 37.938 ppm
Root delay      : 0.038595 seconds
Root dispersion : 0.000447 seconds
Update interval : 64.3 seconds
Leap status     : Normal

</pre>

This shows how effectively our ntp service is working to keep the time in sync with the remote ntp server.
 
The main config file for our chronyd service is:


<pre>
$ cat /etc/chrony.conf
# Use public servers from the pool.ntp.org project.
# Please consider joining the pool (http://www.pool.ntp.org/join.html).
<strong>
server 0.centos.pool.ntp.org iburst
server 1.centos.pool.ntp.org iburst
server 2.centos.pool.ntp.org iburst
server 3.centos.pool.ntp.org iburst
</strong>
# Ignore stratum in source selection.
stratumweight 0

# Record the rate at which the system clock gains/losses time.
driftfile /var/lib/chrony/drift

# Enable kernel RTC synchronization.
rtcsync

# In first three updates step the system clock instead of slew
# if the adjustment is larger than 10 seconds.
makestep 10 3

# Allow NTP client access from local network.
#allow 192.168/16

# Listen for commands only on localhost.
bindcmdaddress 127.0.0.1
bindcmdaddress ::1

# Serve time even if not synchronized to any NTP server.
#local stratum 10

keyfile /etc/chrony.keys

# Specify the key used as password for chronyc.
commandkey 1

# Generate command key if missing.
generatecommandkey

# Disable logging of client accesses.
noclientlog

# Send a message to syslog if a clock adjustment is larger than 0.5 seconds.
logchange 0.5

logdir /var/log/chrony
#log measurements statistics tracking
</pre>

The important of this file are the server settings. These servers, e.g. 0.centos.pool.ntp.org are not remote ntp servers. Instead they are servers that gives the chronyd a list of available ntp remote servers that it can use (a bit like how yum mirrors work). You can edit this file directly and then just restart the chronyd service to load in the changes.  



If you want to do a manual time resync, you can simply do this by restarting the ntp service:


<pre>
$ systemctl restart chronyd
</pre>




If your system clock is correct, but your hardware-clock is wrong, then you can update the hardware clock using the <strong>hwclock</strong> command:

<pre>
$ hwclock --help

Usage:
 hwclock [function] [option...]

Functions:
 -h, --help           show this help text and exit
 -r, --show           read hardware clock and print result
     --set            set the RTC to the time given with --date
 -s, --hctosys        set the system time from the hardware clock
 -w, --systohc        set the hardware clock from the current system time
     --systz          set the system time based on the current timezone
     --adjust         adjust the RTC to account for systematic drift since
                        the clock was last set or adjusted
 -c, --compare        periodically compare the system clock with the CMOS clock
     --getepoch       print out the kernel's hardware clock epoch value
     --setepoch       set the kernel's hardware clock epoch value to the
                        value given with --epoch
     --predict        predict RTC reading at time given with --date
 -V, --version        display version information and exit

Options:
 -u, --utc            the hardware clock is kept in UTC
     --localtime      the hardware clock is kept in local time
 -f, --rtc <file>     special /dev/... file to use instead of default
     --directisa      access the ISA bus directly instead of /dev/rtc
     --badyear        ignore RTC's year because the BIOS is broken
     --date <time>    specifies the time to which to set the hardware clock
     --epoch <year>   specifies the year which is the beginning of the
                        hardware clock's epoch value
     --noadjfile      do not access /etc/adjtime; this requires the use of
                        either --utc or --localtime
     --adjfile <file> specifies the path to the adjust file;
                        the default is /etc/adjtime
     --test           do not update anything, just show what would happen
 -D, --debug          debugging mode

</pre>

Based on this info, you would need to run:


<pre>
$ hwclock --systohc
</pre>




<h2>The "timedatectl" command</h2>

The <strong>timedatectl</strong> command can actually do a lot of things, 




<pre>
$ timedatectl --help
timedatectl [OPTIONS...] COMMAND ...

Query or change system time and date settings.

  -h --help              Show this help
     --version           Show package version
     --adjust-system-clock
                         Adjust system clock when changing local RTC mode
     --no-pager          Do not pipe output into a pager
  -P --privileged        Acquire privileges before execution
     --no-ask-password   Do not prompt for password
  -H --host=[USER@]HOST  Operate on remote host

Commands:
  status                 Show current time settings
  set-time TIME          Set system time
  set-timezone ZONE      Set system timezone
  list-timezones         Show known timezones
  set-local-rtc BOOL     Control whether RTC is in local time
  set-ntp BOOL           Control whether NTP is enabled
</pre>



So let's try the status command:


<pre>
$  timedatectl status
      Local time: Tue 2015-06-23 04:28:07 BST
  Universal time: Tue 2015-06-23 03:28:07 UTC
        RTC time: Tue 2015-06-23 03:28:03
        Timezone: Europe/London (BST, +0100)
     NTP enabled: yes
NTP synchronized: yes
 RTC in local TZ: no
      DST active: yes
 Last DST change: DST began at
                  Sun 2015-03-29 00:59:59 GMT
                  Sun 2015-03-29 02:00:00 BST
 Next DST change: DST ends (the clock jumps one hour backwards) at
                  Sun 2015-10-25 01:59:59 BST
                  Sun 2015-10-25 01:00:00 GMT
</pre>



Note when you do either:


<pre>
$ timedatectl set-ntp true
</pre>

or 

<pre>
$ timedatectl set-ntp false
</pre>

then behind the scenes it will end up starting/stopping the <strong>chronyd</strong> service:


<pre>
$ systemctl status chronyd
chronyd.service - NTP client/server
   Loaded: loaded (/usr/lib/systemd/system/chronyd.service; enabled)
   Active: active (running) since Tue 2015-06-23 04:31:35 BST; 2 days ago
  Process: 9562 ExecStartPost=/usr/libexec/chrony-helper add-dhclient-servers (code=exited, status=0/SUCCESS)
  Process: 9556 ExecStart=/usr/sbin/chronyd -u chrony $OPTIONS (code=exited, status=0/SUCCESS)
 Main PID: 9560 (chronyd)
   CGroup: /system.slice/chronyd.service
           └─9560 /usr/sbin/chronyd -u chrony

Jun 23 04:31:35 localhost.localdomain chronyd[9560]: chronyd version 1.29.1 starting
Jun 23 04:31:35 localhost.localdomain chronyd[9560]: Linux kernel major=3 minor=10 patch=0
Jun 23 04:31:35 localhost.localdomain chronyd[9560]: hz=100 shift_hz=7 freq_scale=1.00000000 nominal_tick=10000 slew_delta_t...pll=2
Jun 23 04:31:35 localhost.localdomain chronyd[9560]: Frequency -9.935 +/- 3.661 ppm read from /var/lib/chrony/drift
Jun 23 04:31:35 localhost.localdomain systemd[1]: Started NTP client/server.
Jun 23 04:31:42 localhost.localdomain chronyd[9560]: Selected source 84.245.25.222
Jun 23 04:31:42 localhost.localdomain chronyd[9560]: System clock wrong by 227763.759989 seconds, adjustment started
Jun 25 19:47:46 localhost.localdomain chronyd[9560]: System clock was stepped by 227763.760 seconds
Jun 25 19:48:52 localhost.localdomain chronyd[9560]: Selected source 194.171.167.130
Hint: Some lines were ellipsized, use -l to show in full.
</pre>


The chronyd service has it's own config file is:



<pre>
$ cat /etc/chrony.conf
# Use public servers from the pool.ntp.org project.
# Please consider joining the pool (http://www.pool.ntp.org/join.html).
server 0.centos.pool.ntp.org iburst
server 1.centos.pool.ntp.org iburst
server 2.centos.pool.ntp.org iburst
server 3.centos.pool.ntp.org iburst

# Ignore stratum in source selection.
stratumweight 0

# Record the rate at which the system clock gains/losses time.
driftfile /var/lib/chrony/drift

# Enable kernel RTC synchronization.
rtcsync

# In first three updates step the system clock instead of slew
# if the adjustment is larger than 10 seconds.
makestep 10 3

# Allow NTP client access from local network.
#allow 192.168/16

# Listen for commands only on localhost.
bindcmdaddress 127.0.0.1
bindcmdaddress ::1

# Serve time even if not synchronized to any NTP server.
#local stratum 10

keyfile /etc/chrony.keys

# Specify the key used as password for chronyc.
commandkey 1

# Generate command key if missing.
generatecommandkey

# Disable logging of client accesses.
noclientlog

# Send a message to syslog if a clock adjustment is larger than 0.5 seconds.
logchange 0.5

logdir /var/log/chrony
#log measurements statistics tracking

</pre>

But's it's unlikely you need to make any changes on this file, since the main way to configure your system clock is by using the <strong>timedatectl</strong> command. The only thing you may need to configure here is adding/removing ntp servers. ]]></Content>
		<Date><![CDATA[2015-06-25]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Setting up VNC]]></Title>
		<Content><![CDATA[VNC is a way to access a machine's gui desktop interface remotely. 

There are a few steps involved in setting this up, starting with installing the vnc software:

<h2>Installing vnc</h2>

The first you need to is install the the following:


<pre>
$ yum install tigervnc tigervnc-server
</pre>


<h2>Set the vncpasswd</h2>
Switch to the user you are going to give remote desktop access to:


<pre>
$ su - mchowdhury
</pre>

Now we need to set a "vnc password", and then return back to root user:


<pre>
[mchowdhury@localhost ~]$ vncpasswd
Password:
Verify:
[mchowdhury@localhost ~]$ exit
logout
</pre>


 
<h2>Create a "vncserver@:{x}.service"</h2>

Navigate to the following directory:


<pre>
$ cd /usr/lib/systemd/system
</pre>


In this folder you should find a file called "vncserver@.service":


<pre>
$ ll /usr/lib/systemd/system | grep vnc
-rw-r--r--. 1 root root 1744 Jun 10  2014 vncserver@.service
</pre>


The content of this file is:


[html]
$ cat vncserver@.service
# The vncserver service unit file
#
# Quick HowTo:
# 1. Copy this file to /etc/systemd/system/vncserver@:&lt;display&gt;.service
# 2. Edit &lt;USER&gt; and vncserver parameters appropriately
#   (&quot;runuser -l &lt;USER&gt; -c /usr/bin/vncserver %i -arg1 -arg2&quot;)
# 3. Run `systemctl daemon-reload`
# 4. Run `systemctl enable vncserver@:&lt;display&gt;.service`
#
# DO NOT RUN THIS SERVICE if your local area network is
# untrusted!  For a secure way of using VNC, you should
# limit connections to the local host and then tunnel from
# the machine you want to view VNC on (host A) to the machine
# whose VNC output you want to view (host B)
#
# [user@hostA ~]$ ssh -v -C -L 590N:localhost:590M hostB
#
# this will open a connection on port 590N of your hostA to hostB's port 590M
# (in fact, it ssh-connects to hostB and then connects to localhost (on hostB).
# See the ssh man page for details on port forwarding)
#
# You can then point a VNC client on hostA at vncdisplay N of localhost and with
# the help of ssh, you end up seeing what hostB makes available on port 590M
#
# Use &quot;-nolisten tcp&quot; to prevent X connections to your VNC server via TCP.
#
# Use &quot;-localhost&quot; to prevent remote VNC clients connecting except when
# doing so through a secure tunnel.  See the &quot;-via&quot; option in the
# `man vncviewer' manual page.


[Unit]
Description=Remote desktop service (VNC)
After=syslog.target network.target

[Service]
Type=forking
# Clean any existing files in /tmp/.X11-unix environment
ExecStartPre=/bin/sh -c '/usr/bin/vncserver -kill %i &gt; /dev/null 2&gt;&amp;1 || :'
ExecStart=/sbin/runuser -l &lt;USER&gt; -c &quot;/usr/bin/vncserver %i&quot;
PIDFile=/home/&lt;USER&gt;/.vnc/%H%i.pid
ExecStop=/bin/sh -c '/usr/bin/vncserver -kill %i &gt; /dev/null 2&gt;&amp;1 || :'

[Install]
WantedBy=multi-user.target
[/html]

This file is actually a template, and isn't used directly. The only part of this file that you will need to substitute are the 2 <code><user></code> placeholders that are near the bottom of this file. .  

Now you need to use this file as a template to create a new file which has the filename convention of, "vncserver@:{x}.service", where "x" is a number that is at least equal to 1:


<pre>
[root@localhost system]# cp vncserver@.service vncserver@\:1.service
[root@localhost system]# ls -l vncserver*
-rw-r--r--. 1 root root 1744 Jun 26 19:08 vncserver@:1.service
-rw-r--r--. 1 root root 1744 Jun 10  2014 vncserver@.service
</pre>
  
Notice the "\", that is so to treat the ":" as a literal string, otherwise it will fail to set the filename. 

Now replace the placeholders with the username of an existing linux user (which is listed in /etc/passwd). 


<h2>Start and enable the new vnc service</h2>

Next we reload all the daemons:

<pre>
$ systemctl daemon-reload
</pre>

This is to make systemd aware of the new .service file. 

Now let's check our new service's status:



<pre>
$ systemctl status vncserver@\:1.service
vncserver@:1.service - Remote desktop service (VNC)
   Loaded: loaded (/usr/lib/systemd/system/vncserver@:1.service; disabled)
   Active: inactive (dead)
</pre>

From this we can see that we need to start the service and enable it so that it auto-starts at next boot time:


<pre>
$ systemctl status vncserver@\:1.service
vncserver@:1.service - Remote desktop service (VNC)
   Loaded: loaded (/usr/lib/systemd/system/vncserver@:1.service; disabled)
   Active: inactive (dead)
</pre>
 

<pre>
$ systemctl start vncserver@\:1.service
</pre>

Note: we have to always escape the ":" with a backslash. 

Next we enable it:
<pre>
$ systemctl enable vncserver@\:1.service
ln -s '/usr/lib/systemd/system/vncserver@:1.service' '/etc/systemd/system/multi-user.target.wants/vncserver@:1.service'
</pre>

Now let's recheck the new service's status:


<pre>
$ systemctl status vncserver@\:1.service
vncserver@:1.service - Remote desktop service (VNC)
   Loaded: loaded (/usr/lib/systemd/system/vncserver@:1.service; <strong>enabled</strong>)
   Active: <strong>activ</strong>e (running) since Fri 2015-06-26 19:39:19 BST; 7min ago
 Main PID: 2795 (Xvnc)
   CGroup: /system.slice/system-vncserver.slice/vncserver@:1.service
           ‣ 2795 /usr/bin/Xvnc :1 -desktop localhost.localdomain:1 (mchowdhury) -auth /home/mchowdhury/.Xauthority -geometry 102...

Jun 26 19:39:16 localhost.localdomain systemd[1]: Starting Remote desktop service (VNC)...
Jun 26 19:39:19 localhost.localdomain systemd[1]: Started Remote desktop service (VNC).
</pre>

So far so good, next we need to sort out the firewall. 

<h2>Add service to firewalld zone</h2>

First let's locate the service that need to be added to the zone:


<pre>
$ firewall-cmd --get-services
amanda-client bacula bacula-client dhcp dhcpv6 dhcpv6-client dns ftp high-availability http https imaps ipp ipp-client ipsec kerberos kpasswd ldap ldaps libvirt libvirt-tls mdns mountd ms-wbt mysql nfs ntp openvpn pmcd pmproxy pmwebapi pmwebapis pop3s postgresql proxy-dhcp radius rpc-bind samba samba-client smtp ssh telnet tftp tftp-client transmission-client <strong>vnc-server</strong> wbem-https
</pre>

Now let's see if that has been already added to a zone:


<pre>
$ firewall-cmd --list-all
public (default, active)
  interfaces: enp0s3
  sources:
  services: dhcpv6-client ssh
  ports:
  masquerade: no
  forward-ports:
  icmp-blocks:
  rich rules:

</pre>

Add the moment, it hasn't been added yet, so let add it now:

<pre>
$ firewall-cmd --permanent --add-service=vnc-server
success
</pre>

Now let's recheck:


<pre>
$ firewall-cmd --list-all
public (default, active)
  interfaces: enp0s3
  sources:
  services: dhcpv6-client ssh
  ports:
  masquerade: no
  forward-ports:
  icmp-blocks:
  rich rules:
</pre>

Since we used the permanent option, it hasn't it for the current run time, but will appear the next time we reboot the machine. so let's do the next best thing to a reboot by restarting the firewalld service, then check again:

<pre>
$ systemctl restart firewalld
$ firewall-cmd --list-all
public (default, active)
  interfaces: enp0s3
  sources:
  services: dhcpv6-client ssh vnc-server
  ports:
  masquerade: no
  forward-ports:
  icmp-blocks:
  rich rules:
</pre>
 

<h2>Start a new vnc session</h2>
Let's now try out our new vnc setup by trying to start a remote connection.

During the connection, you will be prompted to enter 2 passwords, the first password is the machine login password. That's because the vnc session operates inside a ssh tunnel. And the second password is the vnc password. Here's the command we need to run:


<pre>$ vncviewer via mchowdhury@localhost localhost:1</pre>

The "localhost:1" specifies which vnc service we want to connect to, which matches the number specified in the .service file's filename. 

You may then get prompted to enter the user's main machine password. 

You should then get a pop window prompting you to enter the user's vnc password, then then finally a new pop up window opens with the remote gui interface. 

]]></Content>
		<Date><![CDATA[2015-06-26]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|RedHat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Command Line shortcuts]]></Title>
		<Content><![CDATA[no quiz required

Here are some handy tips and tricks you can perform inside the bash terminal to help you speed up productivity


<h2>Retrieve and run an earlier command</h2>

This is done in few ways:

<ul>
	<li>By pressing up and down to scroll through previously run commands</li>
	<li>Hold down <code>ctrl+r</code>, this will bring up the "reverse-i-search" prompt. You can then type a part of the command, and this prompt will automatically return the most recent match from your command history, You can scroll back to earlier matches by doing <code>ctrl+r</code> repeatedly</li>
	<li>Using the "!{command}" notation. this will run the last matching command what ever that was. This is particurly handy for running long unwieldy commands, e.g. "!scp"</li>
        <li>Use <code>history</code> command - this will output a list of your previously run commands. You can then run a command by copy+pasting one of your historical commands. Or you can run !{history number}</li>
</ul>



<h2>Command editing shortcuts</h2>



While on the command line you can do the following:

<ul>
	<li><code>Ctrl+a</code> (or the <code>home</code> key ) : jump to the line's beginning</li>
	<li><code>Ctrl+e</code> (or the <code>end</code> key) : Jump to end of line</li>
	<li><code>Ctrl+u</code> : delete everything from start of line to cursor</li>
	<li><code>Ctrl+k</code> : delete everything from the cursor to end up line</li>
	<li><code>Ctrl+LeftArrow</code> : go to the next word on the command line</li>
	<li><code>Ctrl+RightArrow</code> : go to the previous word on the command line </li>
	<li>Press <code>tab</code> key twice midway through writing a command, this will try to autocomplete your command.</li>

</ul>
]]></Content>
		<Date><![CDATA[2015-06-28]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Packer - notes]]></Title>
		<Content><![CDATA[Note, when you want to create a box to be shared on atlas, you need to ensure yoru box has the following configurations:

http://docs.vagrantup.com/v2/boxes/base.html  (e.g. root user's password should be "vagrant")





https://atlas.hashicorp.com/help

https://atlas.hashicorp.com/help/vagrant/boxes/create
https://atlas.hashicorp.com/CodingBee

Install Packer on windows:

https://www.packer.io/downloads.html

this is a binary, so simply unzip it and place it a in a place like:


C:\HashiCorp\Packer


Then update windows "PATH" environment variable

Then open powershell termianl and run "packer --help".  


Now create your packer projects folder, e.g. :

C:\packer

than download an iso into it e.g. the centos dvd iso:

C:\packer


While it's downloading, create an empty json file, give it a meaningful name:

centos-dvd-iso-virtualbox.json



Enter the following in the json file:

<pre>
{
  "builders": [
    {
      "type": "virtualbox-iso",
      "guest_os_type": "RedHat_64",
      "iso_url": "CentOS-7-x86_64-DVD-1503-01.iso",
      "iso_checksum": "99E450FB1B22D2E528757653FCBF5FDC",
      "iso_checksum_type": "md5",
      "ssh_username": "packer",
      "ssh_password": "packer",
      "ssh_wait_timeout": "30s",
      "shutdown_command": "echo 'packer' | sudo -S shutdown -P now"
    }
  ],

  "provisioners": [
    {
      "type": "shell",
      "script": "setup_things.sh"
    }
  ]
}
</pre>

I created the above from the sample in:

https://www.packer.io/docs/builders/virtualbox-iso.html

You also need to find a checksum value which you can do in powershell v4 using:

<pre>
Get-FileHash c:\path\to\file -Algorithm MD5</pre>


Then in your powershell terminal, cd into this directory, and run:



<pre>
$ packer validate centos7-dvd-iso-virtualbox.json
</pre>


It will fail citing that setup_things.sh scripts doesn't exist, create an empty file in the cwd and try again.

Next do:

<pre>
$ packer build centos7-dvd-iso-virtualbox.json
</pre>


Need to use convert to vagrant:

https://www.packer.io/intro/getting-started/vagrant.html


Now upload your box to hashicorp-atlas:

https://atlas.hashicorp.com/




http://digitalsandwich.com/packer-built-centos-vagrant-base-box-automated-build/


]]></Content>
		<Date><![CDATA[2015-07-03]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[packer]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Vagrant -  Single/Multi box environments, i.e. "config.vm.box" verses "config.vm.define"]]></Title>
		<Content><![CDATA[So far we have come across:

<pre>
# -*- mode: ruby -*-
# vi: set ft=ruby :
VAGRANTFILE_API_VERSION = "2"
Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|
    config.vm.box ="puppetlabs/ubuntu-14.04-32-nocm"
end
</pre>

Using "config.vm.box" means that your vagrantfile is limited to be able to only spin up a single box. 

However instead of "config.vm.box" we could use the "config.vm.define"

<pre>
# -*- mode: ruby -*-
# vi: set ft=ruby :
VAGRANTFILE_API_VERSION = "2"
Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|
  config.vm.define "puppetmaster" do |puppetmaster|
    puppetmaster.vm.box ="puppetlabs/ubuntu-14.04-32-nocm"
  end
end
</pre>


this approach lets us label each box with a name that is used internally within the vagrant file, in this case I labelled it "puppetmaster". 


If you do a "vagrant up" for both of the above boxes, then the end result is the same, however with the "config.vm.define" approach it leaves the door open to add other boxes in future.

that's why it's best practice to always use the "config.vm.define" instead of "config.vm.box".

Follow along from the same pattern we end up with:


https://github.com/Sher-Chowdhury/centos7-packer-virtualbox-foreman/blob/251497d947c83cc2b0125d11d905f8cd21658bd0/Vagrantfile


Notice the "puppetmaster.vm.provider" pattern. 


Note we have replaced the word "config" with our label. But if you want to look anything up in vagrant docs for "puppetmaster.vm.provider", then you need to look for "config.vm.provider". For example search for "config.vm.provider" on the follwoing page:

http://docs.vagrantup.com/v2/vagrantfile/machine_settings.html





http://docs-v1.vagrantup.com/v1/docs/base_boxes.html]]></Content>
		<Date><![CDATA[2015-07-09]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[vagrant|VirtualBox]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Vagrant]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Access a linux gui interface using xming]]></Title>
		<Content><![CDATA[https://en.wikipedia.org/wiki/Xming]]></Content>
		<Date><![CDATA[2015-07-09]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[xming]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[foreman, vagrant, puppet and virtualbox]]></Title>
		<Content><![CDATA[https://programmaticponderings.wordpress.com/tag/virtualbox/

]]></Content>
		<Date><![CDATA[2015-07-14]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Foreman|Puppet|vagrant|VirtualBox]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Starting a "login shell" or "interactive shell" using the switch user (su) command]]></Title>
		<Content><![CDATA[interactive shell - This is a shell sessions used for running commands interactively via the command line. 

login shell - This is a shell sessions used running a shell script inside it.

When 

in the home directory you will find the following files:


<pre>
/etc/profile
       The systemwide initialization file, executed for login shells
~/.bash_profile
       The personal initialization file, executed for login shells
~/.bashrc
       The individual per-interactive-shell startup file
~/.bash_logout
       The individual login shell cleanup file, executed when a login shell exits

</pre> 



https://www.google.co.uk/webhp?sourceid=chrome-instant&ion=1&espv=2&ie=UTF-8#safe=off&q=interactive+login+shell

All the following let's you switch user, and switch into a login shell:

<ul>
	<li><code>su - {username}</code></li>
	<li><code>su -l {username}</code></li>
	<li><code>su --login {username}</code></li>
</ul>


Note: you can omit {username}, in which case the "su" command will use the default username, which is "root". 


If you want to su into an interactive shell, you do:


<pre>
$ su {username}
</pre>

Once again, you can omit {username} to imply the root user. 

In most cases you would log in using login shells. 


http://stackoverflow.com/questions/415403/whats-the-difference-between-bashrc-bash-profile-and-environment]]></Content>
		<Date><![CDATA[2015-07-25]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|RedHat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - User mask (umask)]]></Title>
		<Content><![CDATA[no quiz required

In linux, the default permissions for any newly created folder is 777 (rwxrwxrwx), and for any newly created file it is 666 (rw-rw-rw-).

However when you actually come to create a file or folder, you'll find that in practice that is not really the case:


<pre>
$ touch testfile.txt
$ mkdir testfolder
$ ls -l testfile.txt
-rw-rw-r--. 1 vagrant vagrant 0 Jul 25 14:31 testfile.txt    # here it is 664
$ ls-l | grep testfolder
drwxrwxr-x. 2 vagrant vagrant 6 Jul 25 14:31 testfolder      # here it is 775

</pre>


The reason for this descrepency is that the default permissions are restricted (i.e. masked) by something called a umask value, which you can view by running the the umask command:


<pre>
$ umask
0002
</pre>

This essentially works by being subtracted from the defaults, i.e. 666-002=664 and 777-002=775. 

Hence the umask value effectively adjusts the default values to arrive at new default values that are more restrictive.


We can update the user mask value like this:



<pre>
$ umask 0022
$ umask
0022
$ touch testfile1.txt
$ mkdir testfolder1
$ ls -l testfile1.txt
-rw-r--r--. 1 vagrant vagrant 0 Jul 25 14:53 testfile1.txt        # now it is 644
$ ls -l | grep testfolder1
drwxr-xr-x. 2 vagrant vagrant 6 Jul 25 14:53 testfolder1          # now it is 755

</pre>


Also the default umask value is different for a privileged user (e.g. root user) than it is for an ordinary user:


<pre>
[root@puppetmaster ~]# umask
0022
[root@puppetmaster ~]# su - vagrant
Last login: Sat Jul 25 14:57:15 BST 2015 on pts/0
[vagrant@puppetmaster ~]$ umask
0002
</pre>

These default umask values are specified in the /etc/profile file:



<pre>
$ cat /etc/profile
# /etc/profile

# System wide environment and startup programs, for login setup
# Functions and aliases go in /etc/bashrc

# It's NOT a good idea to change this file unless you know what you
# are doing. It's much better to create a custom.sh shell script in
# /etc/profile.d/ to make custom changes to your environment, as this
# will prevent the need for merging in future updates.

pathmunge () {
    case ":${PATH}:" in
        *:"$1":*)
            ;;
        *)
            if [ "$2" = "after" ] ; then
                PATH=$PATH:$1
            else
                PATH=$1:$PATH
            fi
    esac
}


if [ -x /usr/bin/id ]; then
    if [ -z "$EUID" ]; then
        # ksh workaround
        EUID=`id -u`
        UID=`id -ru`
    fi
    USER="`id -un`"
    LOGNAME=$USER
    MAIL="/var/spool/mail/$USER"
fi

# Path manipulation
if [ "$EUID" = "0" ]; then
    pathmunge /usr/sbin
    pathmunge /usr/local/sbin
else
    pathmunge /usr/local/sbin after
    pathmunge /usr/sbin after
fi

HOSTNAME=`/usr/bin/hostname 2>/dev/null`
HISTSIZE=1000
if [ "$HISTCONTROL" = "ignorespace" ] ; then
    export HISTCONTROL=ignoreboth
else
    export HISTCONTROL=ignoredups
fi

export PATH USER LOGNAME MAIL HOSTNAME HISTSIZE HISTCONTROL

# By default, we want umask to get set. This sets it for login shell
# Current threshold for system reserved uid/gids is 200
# You could check uidgid reservation validity in
# /usr/share/doc/setup-*/uidgid file
<strong>
if [ $UID -gt 199 ] && [ "`id -gn`" = "`id -un`" ]; then
    umask 002
else
    umask 022
fi
</strong>

for i in /etc/profile.d/*.sh ; do
    if [ -r "$i" ]; then
        if [ "${-#*i}" != "$-" ]; then
            . "$i"
        else
            . "$i" >/dev/null
        fi
    fi
done

unset i
unset -f pathmunge


</pre>


Note: "id -gn" outputs the group name, and "id -un" outputs the username. For non-root privileged user, their primary group tends to be the "wheel" group.  

However changing the umask value using the umask command isn't persistant, i.e. it reverts back to the original value when you close your terminal and reopen it again, that's because the /etc/profile script reset's the umask value.  

Hence to make it persistant you simply have to insert your umask command into:


<pre>
$ vim ~/.bashrc
</pre>

Note: the above works for both the login and interactive shells. 


The above makes it persistant for a given user. 


If however you want to make it persistant machine wide, then you need to edit both the /etc/bashrc and /etc/profile files. Both of these files contains the same if-else statement, as highlighted above. 

 




]]></Content>
		<Date><![CDATA[2015-07-25]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Foreman - triggering a puppet run from the gui interface]]></Title>
		<Content><![CDATA[https://github.com/ripienaar/mcollective-vagrant







On the agent, add the following near to the top of the file:

<pre>
# vim /etc/puppet/auth.conf
path /run
allow *
</pre>

Note: later on change the "*" to the fqdn of the puppetmaster fqdn. 


On the puppetmaster enable mcollective:



<pre>
$ cat /etc/foreman-proxy/settings.d/puppet.yml
.
.
.# valid providers:
#   puppetrun   (for puppetrun/kick, deprecated in Puppet 3)
#   mcollective (uses mco puppet)
#   puppetssh   (run puppet over ssh)
#   salt        (uses salt puppet.run)
#   customrun   (calls a custom command with args)
:puppet_provider: mcollective
.
.
.
</pre>





Foreman gui:
on foreman settings (More –> Settings –> “Puppet”tab), set puppetrun to “true”.

Edit the master sudoers file as described here:

http://theforeman.org/manuals/1.8/index.html#4.3.7Puppet


On the agent add the following to sudoers file:

<pre>
# /etc/sudoers
foreman-proxy ALL=(ALL) NOPASSWD: ALL
foreman ALL=(ALL) NOPASSWD: ALL


</pre>


On foreman, You might need to do the following for the first time only:

<pre>
# on the agent first do:
$ service puppet restart
# then on master, do:
puppet kick puppetagent01.local
# then on foreman gui, hit the puppet run button
</pre>


https://tickets.puppetlabs.com/browse/PUP-2659 

http://wiki.infn.it/progetti/cloud-areapd/best_practices/config_puppetrun

https://groups.google.com/forum/#!topic/foreman-users/vFViUPc8zok

http://serverfault.com/questions/638360/how-to-get-run-puppet-button-working-on-foreman


This might be best:
http://projects.theforeman.org/projects/foreman/wiki/_mcollective_






https://github.com/witlessbird/foreman_mco    # you can install this using:
 
<pre>
$ yum install ruby193-rubygem-foreman-mco.noarch
</pre>

I think on the agents you need to do:


<pre>
$ yum install mcollective-client
</pre>

Actually, I think the above is done on the foreman server itself, see:

http://www.pythian.com/blog/some-observations-of-puppetrun-with-foreman/ 

https://github.com/puppet-community/puppet-mcollective
]]></Content>
		<Date><![CDATA[2015-07-27]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Foreman]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - External Facts]]></Title>
		<Content><![CDATA[External facts is a great way to apply (arbitary) meta data to a vm during the vm provisioning stage (e.g. using vsphere/openstack/aws....etc).

These meta data can then be used by puppet to determine what needs to be added into the catalog. 

it's really simple to create an external fact:

step 1:

On the agent, create the following directory (if it doesn't already exist):

<pre>
$ mkdir -p /etc/facter/facts.d
</pre>

In this directory, create a shell script, of any name. In this example I created the following script:

<pre>
$ ls -l /etc/facter/facts.d
total 4
-rwxrwxrwx. 1 root root 65 Sep 18 13:11 external-facts.sh
$ cat /etc/facter/facts.d/external-facts.sh
#!/bin/bash

echo "hostgroup=dev"
echo "environment=development"

</pre>

Then make the script executable:

<pre>
$ chmod u+x /etc/facter/facts.d/external-facts.sh
</pre>

You will now see that these key/value pairs are now available via facter:


<pre>
$ facter hostgroup
dev
$ facter environment
development
</pre>



Note: An <a href="http://codingbee.net/tutorials/puppet/puppet-format-of-the-enc-output/#The_8220parameters8221_key">External Node Classifier, such as foreman can also set metadata</a>. But external facts is probably a better approach, becuase data gets stored on the agents, and mcollective can make use of facter data as well.  

]]></Content>
		<Date><![CDATA[2015-09-18]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Autostarting a vm after a reboot]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What tasks do you need to perform to ensure vm starts up at boot time?"]
1. start+enable the libvirtd.service
2. enter into virsh cli, and apply to the autostart setting to the vm
[/toggle]
[toggle title="What is the command to start the virtualisation service?"]
$ systemctl start libvirtd.service
$ systemctl enable libvirtd.service
[/toggle]
[toggle title="What is the command to enable the vm called 'puppetmaster'?"]
$ virsh
virsh # autostart puppetmaster
[/toggle]

[/accordion]

<hr/>



By default when you restart your host machine, then any guest VMs that were running at the time won't automatically start up again after the reboot. 

To make sure that your VMs do start up after a host machine's reboot, you need to do 2 things. First you need to ensure that the libvirtd daemon is enabled, so that it starts up after the reboot. 

<pre>
$ systemctl start libvirtd.service
$ systemctl enable libvirtd.service
</pre>


Then you need to start a virsh interactive session and apply the autostart setting: 

<pre>
# virsh
Welcome to virsh, the virtualisation interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh # list                     # only lists running vms. 
 Id    Name                           State
----------------------------------------------------

virsh # list --all               # lists all vms even those that are not currently running. 
 Id    Name                           State
----------------------------------------------------
 -     testVM                         shut off
virsh # autostart testVM
Domain testVM marked as autostarted
virsh #
</pre>

After this the vm will now autostart after a host machine's reboot. ]]></Content>
		<Date><![CDATA[2015-09-18]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - The Puppet Narrative (learning technique)]]></Title>
		<Content><![CDATA[The fastest way to learn Puppet is to get a full picture of the puppet infrastrucutre. The best way to do to this is if you have a story (aka narrative) to follow along that starts from the puppet basics to the advanced stuff. When you learn something new about puppet you can then link it back to the narrative. This will help you understand the bigger picture of using Puppet.  



<h2>Learning approach checklist</h2>

<ol>
	<li>Run a <a href="https://docs.puppetlabs.com/learning/ral.html#the-puppet-resource-command">Puppet resource command</a>, with a resource definition itself on the command line. Run this on all demo machines. E.g.:

<pre>
[root@puppetagent1 ~]# puppet resource file /tmp/testfile.txt ensure=present content="hello world"
Notice: /File[/tmp/testfile.txt]/ensure: created
file { '/tmp/testfile.txt':
  ensure  => 'file',
  content => '{md5}5eb63bbbe01eeed093cb22bb8f5acdc3',
}
[root@puppetagent1 ~]# cat /tmp/testfile.txt
hello world
[root@puppetagent1 ~]#

</pre>

This demonstrate how puppet is standardising how changes are implemented across all Linux distros. This solves: The need to keep up to date with the subtle command differences between each distros. New problem: tedious to run all this from the command line, and also no record of this. A better approach is write the resource definition into a puppet file (aka manifest).</li>

	<li>Copy earlier resource into a file called "MyFirstManifest.pp", then use the puppet apply command, on the master, and the agents". This solves: writing things in the command line, also lets your write lots of resources in a single file. New Problem: You'll have lots of manifests files spread across all your agents which will be difficult to keep track of. It would be much better if all your manifests are stored on one server, aka the puppetmaster</li>
	<li>Copy the code from MyFirstManifest.pp and place it into the master's site.pp file, then delete all existing testmanifest.pp. The site.pp file will look something like this:
<pre>
[root@puppetmaster tmp]# cat /etc/puppet/manifests/site.pp
node default {
  file { '/tmp/matestfile.txt':
    ensure => file,
    content => "hello world",
  }
}
[root@puppetmaster tmp]#

</pre>
Note, we used a special node definition called "default" . 

 and then do "puppet agent -t" from on all machines. This solves: All your puppet code is now on one machine, i.e. it is now in one place. What needs to improve: You may not want to apply the same resources to all agents. </li>
	<li>Cut and paste resource definitions into node definitions. This solves: resources are now targeting specific agents. New Problem: code duplication.</li>
	<li>remove duplicate code and convert code into class, then use the include statement. This solve: reduced code duplication. What needs to improve: classes are not versatile and quite static.</li>
	<li>Parameterise the class - Also replace the include-statements with resource-like class declaration.  This solves: classes are more versatile. New Problem: We could end up writing hundreds of class parameters, which makes class parameters quite complicated to declare</li>

	<li>Use Facter - This minimises the need of creating unnecessary class parameters. New Problem: We could end up writing hundreds of classes in the site.pp file, Hence need a way to organise these modules.</li>

	<li>Move the class into it's own init.pp manifest and create a module - This solve: moves classes out of site.pp and organise them into module, which ends up reducing the size of site.pp file. Also taken advantage of auto-loading. Existing Problem: We still have the problem that resource like class declarations still takes up a lot of lines in the site.pp. This is solved by utilising hiera</li>
	<li>Utilize Hiera - this will allow us to return back to using the includ-statements again in the site.pp. It also separates out the logic (that is stored in the puppet module) from the data (that is stored in the yaml files). New Problem: over time each node-definition could end up having a long list of class declarations, in the form of a series of "include" statements. It would be better if we could just have one include statement, this is possible by implementing a design approach called "Roles and Profiles"</li>
	<li>Implement Roles and Profiles - This doesn't introduce any new puppet feature, instead it introduces a way to organise all your puppet modules into 2 new modules called roles and profiles. This solves: now only have one include statement per node definition. Also all include statements align to business logic. The technical details, e.g. dbs, os security patches,....and etc, are hidden away in the profiles module. Things that could be improved: So far we have all data into yaml files. However these is still more data left in the site.pp, i.e. site.pp contains data about what nodenames are, and what classes are included in each node definitions.</li>
	<li>Implement ENC - this will result in a completely empty site.pp file. This is what we have been culminating too. Here we introduce Foreman.</li>

</ol>



The difference between a beginner puppet user, and an expert puppet user is that a beginner has a lots of lines of code in their site.pp file, and an expert user keeps their site.pp file completely empty.    

]]></Content>
		<Date><![CDATA[2015-09-19]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - 5 Critical things you need to know]]></Title>
		<Content><![CDATA[There are a few things that can really catch you out in your day to day work, if you are new to RedHat: 


<ol>
	<li>Regain root access - If you forget what the root password is then you would really struggle do anything.</li>
	<li>Fix Grub2 problems</li>
	<li>Fix start up problems, e.g. corrupt /etc/fstab</li>
	<li>Resolve network access issues</li>
	<li>Set up yum repos</li>
	<li>Using built-in help info effectively</li>
	<li>Everything must survive a reboot</li>

</ol>

]]></Content>
		<Date><![CDATA[2015-09-19]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Redhat|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Scheduling jobs with Anacron]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What is the command to install the anacron system's package?"]
$ yum install cronie-anacron
[/toggle]
[toggle title="If you have a shell script thay you want to run daily, but you don't mind exactly when that script is run (e.g. beginning, middle, or end of the day) as long as it is run sometime within that day, then what directory do you place this file in?"]
/etc/cron.daily
[/toggle]
[toggle title="What is the anacron's config file?"]
/etc/anacrontab
[/toggle]
[toggle title="What are the full paths to all the anacron timestamps files?"]
/var/spool/anacron/cron.daily
/var/spool/anacron/cron.monthly
/var/spool/anacron/cron.weekly
[/toggle]
[toggle title="Which cron file contains the cron that manages the hourly anacron system?"]
$ cat /etc/cron.d/0hourly
[/toggle]
[/accordion]

<hr/>



If you have a shell script thay you want to run daily, but you don't mind exactly when that script is run (e.g. beginning, middle, or end of the day) as long as it is run sometime within that day, then you can achieve this by simply dropping that script into the <code>/etc/cron.daily</code> folder. You then leave it to crond service to pick a convenient time to run that script.

There's a few equivalent folders for setting up hourly, weekly, and monthly jobs. In total we have four folders: 
   

<pre>
$ ls -l /etc | grep cron
-rw-------.  1 root root      541 Jul 30  2014 anacrontab
drwxr-xr-x.  2 root root       72 Apr  3 16:31 cron.d
drwxr-xr-x.  2 root root       76 Apr  3 16:24 cron.daily     # this one
-rw-------.  1 root root        0 Jul 30  2014 cron.deny      
drwxr-xr-x.  2 root root       44 Apr  3 16:24 cron.hourly    # this one 
drwxr-xr-x.  2 root root        6 Jun  9  2014 cron.monthly   # this one
-rw-r--r--.  1 root root      451 Jun  9  2014 crontab
drwxr-xr-x.  2 root root        6 Jun  9  2014 cron.weekly    # this one
</pre>


In practice, scripts are dropped into these folders when installing various rpm packages that require the use of cron for it to function. Don't forget to give these scripts execute permissions in order for them to run. 


This approach to setting cron jobs is provided by the cronie-anacron (aka anacron) package:

<pre>
$ rpm -ql cronie-anacron
/etc/anacrontab
/etc/cron.hourly/0anacron
/usr/sbin/anacron
/usr/share/man/man5/anacrontab.5.gz
/usr/share/man/man8/anacron.8.gz
/var/spool/anacron
/var/spool/anacron/cron.daily
/var/spool/anacron/cron.monthly
/var/spool/anacron/cron.weekly
</pre>



<h2>The Anacrontab config file</h2>

Let's say that you have RHEL vm that has been left switched on for the whole week, and you only switch it on at around 10am on Sunday morning (i.e. last day of the week). Now let's say you have jobs in the cron.weekly folder. Then will anacron realise that these jobs needs to be run as matter of urgency since the machine has been switched-off for nearly the whole week? The the cool thing with anacron, is that it keeps track of the last time each anacron job has been run (we'll show you where this is stored further down). After that anacron will determine whether it has enough time left in the current window (which in this case is the week) to run the job, and if so then it starts the job.     

If you want to make adjustments to this behavior, then you need edit the <code>/etc/anacrontab</code> config file:

<pre>
$  cat /etc/anacrontab
# /etc/anacrontab: configuration file for anacron

# See anacron(8) and anacrontab(5) for details.

SHELL=/bin/sh
PATH=/sbin:/bin:/usr/sbin:/usr/bin
MAILTO=root
# the maximal random delay added to the base delay of the jobs
RANDOM_DELAY=45
# the jobs will be started during the following hours only
START_HOURS_RANGE=3-22

#period in days   delay in minutes      job-identifier          command
1                  5                    cron.daily              nice run-parts /etc/cron.daily
7                 25                    cron.weekly             nice run-parts /etc/cron.weekly
@monthly          45                    cron.monthly            nice run-parts /etc/cron.monthly

</pre>

Note: the hour.daily entry is missing here. We'll explain why that is later. 

Note: The "delay in minutes" column is to do with the minimum number of minutes of delay in minutes to run the job, after the machine is booted up. 

This "delay in minutes" combined with "RANDOM_DELAY" is designed to stagger out the anacron job so to avoid all the anacron jobs getting triggered at once when the machine is booted up.   

The "job-identifier" is just a name given to particular job. 

The "command" is the command to run. In the above example we prefixed each command by "<a href="http://codingbee.net/tutorials/rhcsa/rhcsa-setting-process-priorities-by-setting-nice-values/">nice</a>". The actual command that is run is "run-parts". run-parts is a simple utility that runs all the scripts in the given directory. 

What if you want to run a cron job once in an any time 3 day period, then you can create your own custom anacron job entry in this file:



<pre>
#period in days   delay in minutes      job-identifier          command
4                 0                     myscript.sh             /path/to/myscript.sh
</pre>
  



Now as mentioned earlier, anacron keep track of last job run time stamps, you can find them here:


<pre>
$ ls -l /var/spool/anacron/
total 12
-rw-------. 1 root root 9 Sep 20 09:18 cron.daily
-rw-------. 1 root root 9 Sep 19 23:09 cron.monthly
-rw-------. 1 root root 9 Sep 19 22:49 cron.weekly


$ date
Sun 20 Sep 11:00:52 BST 2015
$ cat /var/spool/anacron/cron.weekly
20150919                                  # YYYYMMDD
</pre>



As you saw earlier, the "cron.hourly" entry was missing from the anacrontab file. That's because the hourly jobs are handled differently due to the limitations of how anacron works. They are instead handled as part of a normal cron:

<pre>
$ cat /etc/cron.d/0hourly
# Run the hourly jobs
SHELL=/bin/bash
PATH=/sbin:/bin:/usr/sbin:/usr/bin
MAILTO=root
01 * * * * root run-parts /etc/cron.hourly
</pre>

As you can see, hourly cron jobs are run on the first minute of every hour. 






See also: http://www.thegeekstuff.com/2011/05/anacron-examples/ 










<strong>See also:</strong>

<a href="http://www.centos.org/docs/2/rhl-cg-en-7.2/autotasks.html">http://www.centos.org/docs/2/rhl-cg-en-7.2/autotasks.html</a>

<a href="https://access.redhat.com/knowledge/docs/en-US/Red_Hat_Enterprise_Linux/3/html/System_Administration_Guide/ch-autotasks.html">https://access.redhat.com/knowledge/docs/en-US/Red_Hat_Enterprise_Linux/3/html/System_Administration_Guide/ch-autotasks.html</a>

<a href="https://access.redhat.com/knowledge/docs/en-US/Red_Hat_Enterprise_Linux/6/html/Deployment_Guide/ch-Automating_System_Tasks.html">https://access.redhat.com/knowledge/docs/en-US/Red_Hat_Enterprise_Linux/6/html/Deployment_Guide/ch-Automating_System_Tasks.html</a>

<a href="http://www.thegeekstuff.com/2009/06/15-practical-crontab-examples/">http://www.thegeekstuff.com/2009/06/15-practical-crontab-examples/</a>

<a href="http://www.thegeekstuff.com/2011/07/cron-every-5-minutes/">http://www.thegeekstuff.com/2011/07/cron-every-5-minutes/</a>

<a href="http://www.thegeekstuff.com/2011/12/crontab-command/">http://www.thegeekstuff.com/2011/12/crontab-command/</a>

<a href="http://www.cyberciti.biz/faq/linux-when-does-cron-daily-weekly-monthly-run/">http://www.cyberciti.biz/faq/linux-when-does-cron-daily-weekly-monthly-run/</a>

<a href="http://www.howtogeek.com/101288/how-to-schedule-tasks-on-linux-an-introduction-to-crontab-files/">http://www.howtogeek.com/101288/how-to-schedule-tasks-on-linux-an-introduction-to-crontab-files/</a>

<a href="http://helpdeskgeek.com/linux-tips/crontab-howto-tutorial-syntax/">http://helpdeskgeek.com/linux-tips/crontab-howto-tutorial-syntax/</a>

<a href="http://www.unixgeeks.org/security/newbie/unix/cron-1.html">http://www.unixgeeks.org/security/newbie/unix/cron-1.html</a>

<a href="http://www.cyberciti.biz/faq/linux-show-what-cron-jobs-are-setup/">http://www.cyberciti.biz/faq/linux-show-what-cron-jobs-are-setup/</a>]]></Content>
		<Date><![CDATA[2015-09-19]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Controlling who is allowed to schedule jobs]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="How do you prevent the user 'david', from creating their own cron jobs (using the crontab command)?"]
Append their username to the following file:
$ cat /etc/cron.deny
david
[/toggle]
[toggle title="By default everyone can create their own cron jobs (via the crontab command), how do you reverse this so that by default no one can create their own jobs?"]
$ touch /etc/cron.allow

[/toggle]
[toggle title="How do you prevent the user 'david', from creating their own 'at' jobs (using the 'at' command)?"]
# Append their username to the following file:
/etc/at.deny
[/toggle]
[toggle title="By default everyone can create their own 'at' jobs (via the 'at' command), how do you reverse this so that by default no one can create their own jobs?"]
/etc/at.allow
[/toggle]
[/accordion]

<hr/>



<h2>Controlling who can create cron jobs</h2>          
By default all users are allowed to use the crontab command to manage their respective cron jobs. To restrict a user from using the crontab command, we can add their names to the <code>/etc/cron.deny</code> file:

<pre>
$ cat /etc/passwd | egrep 'tom|jerry'
tom:x:1003:1003::/home/tom:/bin/bash
jerry:x:1004:1004::/home/jerry:/bin/bash
$ cat /etc/cron.deny
jerry
tom
</pre>

Notice, each user name is on it's own line. 

After that, the listed users can no longer use the crontab command, which in turn means that they can't manage their cron jobs:


<pre>
$ su - jerry
Last login: Sat May  9 21:48:35 BST 2015 on pts/1
$ crontab -e
You (jerry) are not allowed to use this program (crontab)
See crontab(1) for more information
$ crontab -l
You (jerry) are not allowed to use this program (crontab)
See crontab(1) for more information
</pre>

Notice, you can't even using crontab for viewing a list of cron jobs. 


By default everyone has crontab access, and you remove access by adding usernames to the /etc/cron.deny. If you have a lot of users and the majority of them are not supposed to have crontab access, then a better option is to change the default, so that by default user's don't have crontab access. This is done by simply creating the /etc/cron.allow file:


<pre>
$ su - homer
Last login: Sat Apr 11 18:23:26 BST 2015 from powershellpc.codingbee.dyndns.org on pts/7
$ crontab -l
no crontab for homer
$ exit
logout
$ touch /etc/cron.allow
$ su - homer
Last login: Sat May  9 22:09:19 BST 2015 on pts/1
$ crontab -l
You (homer) are not allowed to use this program (crontab)
See crontab(1) for more information
</pre>

Now you can give crontab access to users by explicitly adding their name to the <code>/etc/cron.allow</code> file.  




The equivalent privelege system exists for the "at" system. By default, anyone can create an "at" job unless their name is added to:

<pre>/etc/at.deny</pre>

If you want to change the default so that no one can set "at" jobs, then you need to delete the above file and replace it with:

 <pre>/etc/at.allow</pre>

Note: You can either have the .allow file or the .deny file, but you can't have them both exist at the same time. I think the same applies to cron.allow and cron.deny. ]]></Content>
		<Date><![CDATA[2015-09-19]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - The "target" unit]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What is the command list all the units that the multi-user.target starts up?"]
$ systemctl list-dependencies multi-user.target
[/toggle]
[toggle title="Which directory do you change the contents of to override the contents of /usr/lib/systemd/system?"]
/etc/systemd/system/
[/toggle]
[toggle title="What is the command to list all the units that are started up when the 'graphical.target' is activated?"]
$ ls -l /usr/lib/systemd/system/graphical.target.wants
[/toggle]
[toggle title="What is the command to list the default target in symbolic link form?"]
$ ls -l /etc/systemd/system/default.target
[/toggle]
[toggle title="What is the main command to display the default target?"]
$ systemctl get-default
[/toggle]
[toggle title="What are the 3 ways to switch to a different target?"]
- The systemctl command's set-default subcommand. This is persistant
- Use systemctl's "isolate" subcommand (this is non-persistant)
- From the grub menu during machine bootup (this is non-persistant)
[/toggle]
[toggle title="What is the command to set the 'multi-user.target' as the default target?"]
$ systemctl set-default multi-user.target
[/toggle]
[toggle title="What is the systemctl command to switch to 'rescue.target' target non-persistantly?"]
$ systemctl isolate rescue.target
[/toggle]
[toggle title="How do you switch to the 'multi-user.target' via the grub menu?"]
1. Wait for the grub menu, then hit "e",
2. scroll down to the "linux16" line then press the 'end' key
3. type the following:
systemd.unit=multi-user.target
4. press ctrl+x
[/toggle]
[/accordion]
<hr/>



There will be times when you want to boot your machine and only want the command line running. Whereas there will be other times you'll want to boot your machine with the whole GNOME gui running. This is possible   

In RHEL 7, we have a concept known as "target". A target is something that lets you define your preferences on what units (i.e. services, mounts, sockets,..etc) should be started/activated when you boot your machine.   

In RHEL, targets are a type of unit, and therefore each target is represented by it's own .target file which resides in the <code>/usr/lib/systemd/system</code> directory. 

Here's a list of all the targets:


<pre>
$ ls -l /usr/lib/systemd/system | grep "target$"
-rw-r--r--. 1 root root  312 Jul  2  2014 anaconda.target
-rw-r--r--. 1 root root  546 Jun 10  2014 basic.target
-rw-r--r--. 1 root root  379 Jun 10  2014 bluetooth.target
-rw-r--r--. 1 root root  366 Jun 10  2014 cryptsetup.target
lrwxrwxrwx. 1 root root   13 Mar 14 19:16 ctrl-alt-del.target -> reboot.target
lrwxrwxrwx. 1 root root   16 Mar 14 19:16 default.target -> graphical.target
-rw-r--r--. 1 root root  431 Jun 10  2014 emergency.target
-rw-r--r--. 1 root root  440 Jun 10  2014 final.target
-rw-r--r--. 1 root root  460 Jun 10  2014 getty.target
-rw-r--r--. 1 root root  522 Jun 10  2014 <mark>graphical.target</mark>
-rw-r--r--. 1 root root  487 Jun 10  2014 halt.target
-rw-r--r--. 1 root root  447 Jun 10  2014 hibernate.target
-rw-r--r--. 1 root root  468 Jun 10  2014 hybrid-sleep.target
-rw-r--r--. 1 root root  536 Jun 10  2014 initrd-fs.target
-rw-r--r--. 1 root root  509 Jun 10  2014 initrd-root-fs.target
-rw-r--r--. 1 root root  691 Jun 10  2014 initrd-switch-root.target
-rw-r--r--. 1 root root  654 Jun 10  2014 initrd.target
-rw-r--r--. 1 root root  501 Jun 10  2014 kexec.target
-rw-r--r--. 1 root root  395 Jun 10  2014 local-fs-pre.target
-rw-r--r--. 1 root root  489 Jun 10  2014 local-fs.target
-rw-r--r--. 1 root root  524 Jun 10  2014 <mark>multi-user.target</mark>
-rw-r--r--. 1 root root  443 Jun 10  2014 network-online.target
-rw-r--r--. 1 root root  433 Jun 10  2014 network.target
-rw-r--r--. 1 root root  193 Jun 10  2014 nfs.target
-rw-r--r--. 1 root root  514 Jun 10  2014 nss-lookup.target
-rw-r--r--. 1 root root  473 Jun 10  2014 nss-user-lookup.target
-rw-r--r--. 1 root root  354 Jun 10  2014 paths.target
-rw-r--r--. 1 root root  500 Jun 10  2014 poweroff.target
-rw-r--r--. 1 root root  377 Jun 10  2014 printer.target
-rw-r--r--. 1 root root  493 Jun 10  2014 reboot.target
-rw-r--r--. 1 root root  396 Jun 10  2014 remote-fs-pre.target
-rw-r--r--. 1 root root  482 Jun 10  2014 remote-fs.target
-rw-r--r--. 1 root root  486 Jun 10  2014 rescue.target
-rw-r--r--. 1 root root  500 Jun 10  2014 rpcbind.target
lrwxrwxrwx. 1 root root   15 Mar 14 19:16 runlevel0.target -> poweroff.target
lrwxrwxrwx. 1 root root   13 Mar 14 19:16 runlevel1.target -> rescue.target
lrwxrwxrwx. 1 root root   17 Mar 14 19:16 runlevel2.target -> multi-user.target
lrwxrwxrwx. 1 root root   17 Mar 14 19:16 runlevel3.target -> multi-user.target
lrwxrwxrwx. 1 root root   17 Mar 14 19:16 runlevel4.target -> multi-user.target
lrwxrwxrwx. 1 root root   16 Mar 14 19:16 runlevel5.target -> graphical.target
lrwxrwxrwx. 1 root root   13 Mar 14 19:16 runlevel6.target -> reboot.target
-rw-r--r--. 1 root root  402 Jun 10  2014 shutdown.target
-rw-r--r--. 1 root root  362 Jun 10  2014 sigpwr.target
-rw-r--r--. 1 root root  420 Jun 10  2014 sleep.target
-rw-r--r--. 1 root root  409 Jun 10  2014 slices.target
-rw-r--r--. 1 root root  380 Jun 10  2014 smartcard.target
-rw-r--r--. 1 root root  356 Jun 10  2014 sockets.target
-rw-r--r--. 1 root root  380 Jun 10  2014 sound.target
-rw-r--r--. 1 root root   49 Jun  9  2014 spice-vdagentd.target
-rw-r--r--. 1 root root  441 Jun 10  2014 suspend.target
-rw-r--r--. 1 root root  353 Jun 10  2014 swap.target
-rw-r--r--. 1 root root  540 Jun 10  2014 sysinit.target
-rw-r--r--. 1 root root  652 Jun 10  2014 system-update.target
-rw-r--r--. 1 root root  355 Jun 10  2014 timers.target
-rw-r--r--. 1 root root  509 Jun 10  2014 time-sync.target
-rw-r--r--. 1 root root  417 Jun 10  2014 umount.target
</pre>

Note: the highlighted targets are the most common ones. 

In total there are about 60 targets: 

<pre>
$ systemctl list-unit-files --type=target | wc -l
59  
</pre>


To see which targets are currently active, we do:



<pre>
$ systemctl list-units --type target
UNIT                 LOAD   ACTIVE SUB    DESCRIPTION
basic.target         loaded active active Basic System
cryptsetup.target    loaded active active Encrypted Volumes
getty.target         loaded active active Login Prompts
graphical.target     loaded active active Graphical Interface
local-fs-pre.target  loaded active active Local File Systems (Pre)
local-fs.target      loaded active active Local File Systems
multi-user.target    loaded active active Multi-User System
network.target       loaded active active Network
paths.target         loaded active active Paths
remote-fs-pre.target loaded active active Remote File Systems (Pre)
remote-fs.target     loaded active active Remote File Systems
slices.target        loaded active active Slices
sockets.target       loaded active active Sockets
swap.target          loaded active active Swap
sysinit.target       loaded active active System Initialization
timers.target        loaded active active Timers

LOAD   = Reflects whether the unit definition was properly loaded.
ACTIVE = The high-level unit activation state, i.e. generalization of SUB.
SUB    = The low-level unit activation state, values depend on unit type.

16 loaded units listed. Pass --all to see loaded but inactive units, too.
To show all installed unit files use 'systemctl list-unit-files'.

</pre>

All the units that are currently active on the machine represented to the collective sum of all the units in the above list of targets. 

 

Now to make a target responsible for starting a unit at boot time, we need to <em>enable</em> that unit using the systemctl command. 

<pre>
$ systemctl enable httpd
ln -s '/usr/lib/systemd/system/httpd.service' '/etc/systemd/system/multi-user.target.wants/httpd.service'
</pre>

This essentially creates a soft link in the "multi-user.target.wants" folder.

The systemctl command knew that it needed to create the link in the multi-user.target.wants folder because that is what's defined in the httpd.service unit file's [Install] sections. 

This is systemctl's way of telling the multi-user.target that in future it now needs to start up the httpd.servie as part of it's start up process. 


Due to this nature of having the startup of one unit dependent on another means that end with a bit of a dependency like tree structure. You can view this dependency using systemctl's list-dependencies command:

<pre>
$ systemctl list-dependencies multi-user.target
multi-user.target
├─abrt-ccpp.service
├─abrt-oops.service
├─abrt-vmcore.service
├─ModemManager.service
├─network.service
├─NetworkManager.service
├─plymouth-quit-wait.service
├─vmtoolsd.service
├─xinetd.service
├─basic.target
│ ├─alsa-restore.service
│ ├─alsa-state.service
│ ├─microcode.service
│ ├─rhel-autorelabel-mark.service
│ ├─rhel-autorelabel.service
│ ├─rhel-configure.service
│ ├─rhel-dmesg.service
│ ├─rhel-loadmodules.service
│ ├─paths.target
│ ├─slices.target
│ │ ├─-.slice
│ │ └─system.slice
│ ├─sockets.target
│ │ ├─avahi-daemon.socket
.
...etc
</pre>


<h2>Adjusting a unit's settings</h2> 
The <code>/usr/lib/systemd/system</code> directory is supposed to hold default settings only. This means that you're not supposed to make any changes in this directory. Unit files are usually dropped into this folder when you install rpm packages, e.g. the httpd package. If you want to override anything or create your own <em>units</em>, then you should do that in the the <code>/etc/systemd/system/</code> directory instead:


<pre>
$ ls -l /etc/systemd/system/
total 16
drwxr-xr-x. 2 root root   54 Mar 14 19:20 basic.target.wants
drwxr-xr-x. 2 root root   30 Mar 14 19:18 bluetooth.target.wants
lrwxrwxrwx. 1 root root   41 Mar 14 19:18 dbus-org.bluez.service -> /usr/lib/systemd/system/bluetooth.service
lrwxrwxrwx. 1 root root   41 Mar 14 19:17 dbus-org.fedoraproject.FirewallD1.service -> /usr/lib/systemd/system/firewalld.service
lrwxrwxrwx. 1 root root   44 Mar 14 19:17 dbus-org.freedesktop.Avahi.service -> /usr/lib/systemd/system/avahi-daemon.service
lrwxrwxrwx. 1 root root   44 Mar 14 19:20 dbus-org.freedesktop.ModemManager1.service -> /usr/lib/systemd/system/ModemManager.service
lrwxrwxrwx. 1 root root   46 Mar 14 19:17 dbus-org.freedesktop.NetworkManager.service -> /usr/lib/systemd/system/NetworkManager.service
lrwxrwxrwx. 1 root root   57 Mar 14 19:17 dbus-org.freedesktop.nm-dispatcher.service -> /usr/lib/systemd/system/NetworkManager-dispatcher.service
lrwxrwxrwx. 1 root root   36 Mar 14 19:23 default.target -> /lib/systemd/system/graphical.target
drwxr-xr-x. 2 root root   85 Mar 14 19:16 default.target.wants
lrwxrwxrwx. 1 root root   35 Mar 14 19:19 display-manager.service -> /usr/lib/systemd/system/gdm.service
drwxr-xr-x. 2 root root   31 Mar 14 19:16 getty.target.wants
drwxr-xr-x. 2 root root 4096 Mar 14 19:26 graphical.target.wants
drwxr-xr-x. 2 root root 4096 Jun  6 12:44 multi-user.target.wants
drwxr-xr-x. 2 root root   29 Mar 14 19:17 nfs.target.wants
drwxr-xr-x. 2 root root   25 Mar 14 19:17 printer.target.wants
drwxr-xr-x. 2 root root 4096 Mar 14 19:17 sockets.target.wants
drwxr-xr-x. 2 root root   35 Mar 14 19:20 spice-vdagentd.target.wants
drwxr-xr-x. 2 root root 4096 Mar 14 19:17 sysinit.target.wants
drwxr-xr-x. 2 root root   83 Mar 14 19:16 system-update.target.wants
</pre>
 

====================================================
need to clean up the rest of this article. 


From the systemctl, command's status info you'll also see that the service is represented in the form of a file:



<pre>
$ systemctl status httpd.service
httpd.service - The Apache HTTP Server
   Loaded: loaded (<mark>/usr/lib/systemd/system/httpd.service</mark>; enabled)
   Active: active (running) since Sat 2015-06-06 11:08:52 BST; 5min ago
 Main PID: 4336 (httpd)
   Status: "Total requests: 0; Current requests/sec: 0; Current traffic:   0 B/sec"
   CGroup: /system.slice/httpd.service
           ├─4336 /usr/sbin/httpd -DFOREGROUND
           ├─4337 /usr/sbin/httpd -DFOREGROUND
           ├─4338 /usr/sbin/httpd -DFOREGROUND
           ├─4339 /usr/sbin/httpd -DFOREGROUND
           ├─4340 /usr/sbin/httpd -DFOREGROUND
           └─4341 /usr/sbin/httpd -DFOREGROUND

Jun 06 11:08:52 centos7.codingbee.dyndns.org systemd[1]: Starting The Apache HTTP Server...
Jun 06 11:08:52 centos7.codingbee.dyndns.org systemd[1]: Started The Apache HTTP Server.
</pre>
 




This folder houses a *.service file for each available service. An rpm that comes with a service, (e.g. the apache web server, httpd)  will drop a *.service into this folder when you install the rpm. Therefore to find out whether an rpm package has installed a service, and if so, what services it has installed, you can do:


<pre>
$ rpm -ql httpd | grep ".service$"
/usr/lib/systemd/system/htcacheclean.service
/usr/lib/systemd/system/httpd.service
</pre>




 





<h2>Target dependencies</h2>

Let's take a look at all the services are assigned to the graphical target:

<pre>
$ ls -l /usr/lib/systemd/system/graphical.target.wants
total 0
[root@centos7 system]# ls -l /etc/systemd/system/graphical.target.wants/*.service
lrwxrwxrwx. 1 root root 47 Mar 14 19:16 /etc/systemd/system/graphical.target.wants/accounts-daemon.service -> /usr/lib/systemd/system/accounts-daemon.service
lrwxrwxrwx. 1 root root 51 Mar 14 19:23 /etc/systemd/system/graphical.target.wants/firstboot-graphical.service -> /usr/lib/systemd/system/firstboot-graphical.service
lrwxrwxrwx. 1 root root 44 Mar 14 19:17 /etc/systemd/system/graphical.target.wants/rtkit-daemon.service -> /usr/lib/systemd/system/rtkit-daemon.service
</pre>

As you can see, there is no services in the default folder, and only three under the etc folder. 

This appears far lower than expected. However that is because targets have a hierarchial like dependencies on one another. To understand what I mean you need to take a look at a target's unit file:


<pre>
$ cat /usr/lib/systemd/system/graphical.target
#  This file is part of systemd.
#
#  systemd is free software; you can redistribute it and/or modify it
#  under the terms of the GNU Lesser General Public License as published by
#  the Free Software Foundation; either version 2.1 of the License, or
#  (at your option) any later version.

[Unit]
Description=Graphical Interface
Documentation=man:systemd.special(7)
<strong>Requires=multi-user.target</strong>
After=multi-user.target
Conflicts=rescue.target
Wants=display-manager.service
AllowIsolate=yes

[Install]
Alias=default.target

</pre>

Note, AllowIsolate means you can switch to this target from another target.  This setting is usually set to "yes". 

Here it shows that the "graphical" target can only be loaded after the multi-user target has been loaded, as indicated by the "Require" setting. This means that if you load the graphical target, then systemd will read this definition and then first load the multi-user targets. Note, the multi-user target may itself has it's own dependencies, and so on.

By the way, service definition, e.g. the httpd.service shown above, can specify dependencies on targets as well, if so then they are defined in the "unit" section, e.g.:


<pre>
cat /usr/lib/systemd/system/httpd.service
[Unit]
Description=The Apache HTTP Server
<strong>After=network.target remote-fs.target nss-lookup.target</strong>

[Service]
Type=notify
EnvironmentFile=/etc/sysconfig/httpd
ExecStart=/usr/sbin/httpd $OPTIONS -DFOREGROUND
ExecReload=/usr/sbin/httpd $OPTIONS -k graceful
ExecStop=/bin/kill -WINCH ${MAINPID}
# We want systemd to give httpd some time to finish gracefully, but still want
# it to kill httpd after TimeoutStopSec if something went wrong during the
# graceful stop. Normally, Systemd sends SIGTERM signal right after the
# ExecStop, which would kill httpd. We are sending useless SIGCONT here to give
# httpd time to finish.
KillSignal=SIGCONT
PrivateTmp=true

[Install]
WantedBy=multi-user.target


</pre> 
      

This offers a further granularity of control on when a service should be started and loading dependencies before hand.  



<h2>Default target</h2>

Due to the hierarchial dependency structure of targets, it means that there is a top level target. You can choose the default top level target by editing the "default.target" symbolic link, which is located at:


<pre>
$ ls -l /etc/systemd/system/default.target
lrwxrwxrwx. 1 root root 36 Mar 14 19:23 /etc/systemd/system/default.target -> /lib/systemd/system/graphical.target
</pre> 


As you can see, at the moment the the default target is set to "graphical", which will start up the gnome desktop gui interface when you boot the machine. 

You can also get this info using systemctl:


<pre>
$ systemctl get-default
graphical.target
</pre> 

Similarly you can change the default target like this:

<pre>
$ systemctl set-default multi-user.target
rm '/etc/systemd/system/default.target'
ln -s '/usr/lib/systemd/system/multi-user.target' '/etc/systemd/system/default.target'
$ systemctl get-default
multi-user.target
</pre>
 




<h2>Commonly used targets</h2>

The most commonly used targets are those that are equivalent to the old style runlevels:


<pre>
$ ls -l /usr/lib/systemd/system  | grep runlevel | grep "^l"
lrwxrwxrwx. 1 root root   15 Mar 14 19:16 runlevel0.target -> poweroff.target
lrwxrwxrwx. 1 root root   13 Mar 14 19:16 runlevel1.target -> rescue.target
lrwxrwxrwx. 1 root root   17 Mar 14 19:16 runlevel2.target -> multi-user.target
lrwxrwxrwx. 1 root root   17 Mar 14 19:16 runlevel3.target -> multi-user.target
lrwxrwxrwx. 1 root root   17 Mar 14 19:16 runlevel4.target -> multi-user.target
lrwxrwxrwx. 1 root root   16 Mar 14 19:16 runlevel5.target -> graphical.target
lrwxrwxrwx. 1 root root   13 Mar 14 19:16 runlevel6.target -> reboot.target
</pre>



poweroff.target - if your machine enters this state, then it will turn off your machine. 
rescue.target - this enters troubleshooting mode. A bit like single user mode I think. 
multi-user.target - Fully operational mode, this is the normal mode but without Gnome desktop UI. 
graphical.target - Same as multi-user.target but with the Gnome desktop UI enabled. 
reboot.target - entering this state ends up rebooting the machine

There is also another one of interest, which is:

emergency.target - this is more minimal version of the rescue.target. 





One thing to bear in mind is that since systemd does a lot more than just managing services, this means that targets do a lot more than just ensuring the state of services, it also ensures the state of all unit types, i.e. *.mount, *.socket....etc. 





<h2>Switching betweeen systemd targets</h2>

There's a few ways to switch between (top level) targets. First ways to do this:



<ul>
	<li>Change the default target, and then reboot the machine.</li>
	<li>Use systemctl's "isolate" subcommand (this is non-persistant)</li>
	<li>From the grub menu, hit e, then specify the target (this is non-persistant)</li>
</ul>

<h3>Switching targets using the "isolate" subcommand</h3>

We can switch using the isolate subcommand:


<pre>
$ systemctl --help | grep isolate
  isolate [NAME]                  Start one unit and stop all others
</pre>


Therefore to switch to the rescue mode, we do:

<pre>
$ systemctl isolate rescue.target
</pre>

This results in:


<a href="http://codingbee.net/wp-content/uploads/2015/06/reNpfwk1.png"><img src="http://codingbee.net/wp-content/uploads/2015/06/reNpfwk1.png" alt="" width="740" height="418" class="alignnone size-full wp-image-4584" /></a>


You can then check that you are in rescue.target state like this:

<a href="http://codingbee.net/wp-content/uploads/2015/06/Ez5hdu6.png"><img src="http://codingbee.net/wp-content/uploads/2015/06/Ez5hdu6.png" alt="" width="738" height="418" class="alignnone size-full wp-image-4585" /></a>


Notice that we only have a small number of targets active, since we are in rescue mode. Also even sshd service is disabled.


We can return back to mult-user.target using the isolate subcommand again:


<pre>
$ systemctl isolate multi-user.target
</pre>

By the way, emergency.target looks like this:


<a href="http://codingbee.net/wp-content/uploads/2015/06/pIqSAEC.png"><img src="http://codingbee.net/wp-content/uploads/2015/06/pIqSAEC.png" alt="" width="744" height="426" class="alignnone size-full wp-image-4587" /></a>


Note: due to emergency mode being barebones. You can't use isolate to switch to another target while being in emergency.target state. 





<h3>Switching targets via the grub menu</h3>

This is something that you will commonly do if for some reason you have trouble booting up into graphical or mult-user targets. In which you can use grub to boot into a trouble shooting mode, such as rescue.target or emergency.target.


While your machine is booting, as soon as you see the grub menu, select the (e)dit option:


<a href="http://codingbee.net/wp-content/uploads/2015/06/vdsRWtz.png"><img src="http://codingbee.net/wp-content/uploads/2015/06/vdsRWtz.png" alt="" width="739" height="415" class="alignnone size-full wp-image-4589" /></a>



Then navigate to the line that starts with "linux16", press the 'end' key, and then append the following setting:

<pre>
systemd.unit=rescue.target
</pre>

So that we have:

<a href="http://codingbee.net/wp-content/uploads/2015/06/WIyCkNw.png"><img src="http://codingbee.net/wp-content/uploads/2015/06/WIyCkNw.png" alt="" width="735" height="419" class="alignnone size-full wp-image-4590" /></a>


Then control+x to resume the boot process with the modified settings. 

In this example we have gone into the rescue.target, this means that after that has been done, you can use systemctl's isolate command to return to one of the fully operational normal targets, mult-user or graphical. 


]]></Content>
		<Date><![CDATA[2015-09-20]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - The anatomy of a unit file]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="Which directory houses all the unit files?"]
/usr/lib/systemd/system
[/toggle]
[toggle title="Where can you find help info about units?"]
$ man systemd.unit
[/toggle]
[/accordion]

<hr/>


A "unit" is a resource that systemd can manage. 

If you look inside the <code>/usr/lib/systemd/system</code> directory, you will find files with different extensions, e.g. .socket, .target, .mount, ...etc. These are all the resources that systemd can manage, and in systemd they are referred by the generic term "unit". There are several different types of resources, however the files (i.e. units) that we are interested at the moment are the unit files ending with ".service":

<pre>
$ ls -l /usr/lib/systemd/system | grep ".service$"
-rw-r--r--. 1 root root  275 Jun 18  2014 abrt-ccpp.service
-rw-r--r--. 1 root root  380 Jun 18  2014 abrtd.service
-rw-r--r--. 1 root root  361 Jun 18  2014 abrt-oops.service
.
.
.
...etc
</pre>


Each unit file contains all the information that's required for systemd to start/stop/enable (i.e. manage) that service.

The structure of a unit file varies slightly from one unit file to another, but in general it looks something like this:


<pre>
cat /usr/lib/systemd/system/sshd.service
[Unit]
Description=OpenSSH server daemon
After=network.target sshd-keygen.service    # these units must be activated before this unit
Wants=sshd-keygen.service                 # systemd will attempt to start this at the same time as this unit. 
                                          # but can still function without it.  

[Service]
EnvironmentFile=/etc/sysconfig/sshd
ExecStart=/usr/sbin/sshd -D $OPTIONS
ExecReload=/bin/kill -HUP $MAINPID
KillMode=process
Restart=on-failure
RestartSec=42s

[Install]
WantedBy=multi-user.target
</pre>

You can find documentation about these directives here:

<pre>
$ man systemd.unit
</pre>


The [Unit] sections gives a description of the service. It can also contain other settings, e.g. in this case it has an "After" setting. 

The [Service] section defines the main definition of the service, which gives the systemctl command the info it needs to start/stop the service along with other things. This section is only found in .service files. 

The [Install] section shows that this services has been assigned to a target. A target is essentially a way to group together a collection of services. 



The <strong><em>WantedBy</em></strong> directive is particularly important. That's because it tells systemctl which "target"  it should make responsible for starting up the resource during boot time (if this unit is enabled).   

Note: A <em>target</em> is something that lets you define what units (e.g. services, mounts, sockets,.....etc, and even includes other targtes) should be started/activated when you boot your machine. More about targets in the next article. 

In the next article we'll cover exactly how systemctl makes a target responsible for starting up a unit. 






<strong>Also see:</strong>

https://www.digitalocean.com/community/tutorials/understanding-systemd-units-and-unit-files]]></Content>
		<Date><![CDATA[2015-09-20]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Adjusting a unit's settings]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="How do you change the httpd service's default 'WantedBy=multi-user.target' setting to 'WantedBy=graphical.target'"]
# First make a template
cp /usr/lib/systemd/system/httpd.service /etc/systemd/system/httpd.service
# Now remove all the things you don't want to override and 
# then apply the adjustment you want, so that you end up with: 
$ cat /etc/systemd/system/httpd.service
[Install] 
 
WantedBy=graphical.target
[/toggle]
[/accordion]

<hr/>


Let's say we want to adjust the httpd's service, e.g. make it dependent on the graphical.target, rather that multi-user.target. 


In that case let's first check the httpd service's status:

<pre>
$ systemctl status httpd.service
httpd.service - The Apache HTTP Server
   Loaded: loaded (<mark>/usr/lib/systemd/system/httpd.service</mark>; enabled)
   Active: active (running) since Sat 2015-06-06 11:08:52 BST; 5min ago
 Main PID: 4336 (httpd)
   Status: "Total requests: 0; Current requests/sec: 0; Current traffic:   0 B/sec"
   CGroup: /system.slice/httpd.service
           ├─4336 /usr/sbin/httpd -DFOREGROUND
           ├─4337 /usr/sbin/httpd -DFOREGROUND
           ├─4338 /usr/sbin/httpd -DFOREGROUND
           ├─4339 /usr/sbin/httpd -DFOREGROUND
           ├─4340 /usr/sbin/httpd -DFOREGROUND
           └─4341 /usr/sbin/httpd -DFOREGROUND

Jun 06 11:08:52 centos7.codingbee.dyndns.org systemd[1]: Starting The Apache HTTP Server...
Jun 06 11:08:52 centos7.codingbee.dyndns.org systemd[1]: Started The Apache HTTP Server.
</pre>


Here we can see which file needs that contains the setting we want to adjust.Another way to locate this file is:

<pre>
$ rpm -ql httpd | grep '.service'
/usr/lib/systemd/system/htcacheclean.service
/usr/lib/systemd/system/httpd.service
</pre>



The <code>/usr/lib/systemd/system</code> directory is where you find all your unit's along with their default settings. Unit files are usually dropped into this folder when you install rpm packages, e.g. the httpd package.

It's not best practice to manually edit any of these unit files in this directory. If you want to change any of these unit files, then you do this by overriding the settings. This is done by creating/editing unit files in the <code>/etc/systemd/system/</code> directory instead:

<pre>
$ ls -l /etc/systemd/system/
total 16
drwxr-xr-x. 2 root root   54 Mar 14 19:20 basic.target.wants
drwxr-xr-x. 2 root root   30 Mar 14 19:18 bluetooth.target.wants
lrwxrwxrwx. 1 root root   41 Mar 14 19:18 dbus-org.bluez.service -> /usr/lib/systemd/system/bluetooth.service
lrwxrwxrwx. 1 root root   41 Mar 14 19:17 dbus-org.fedoraproject.FirewallD1.service -> /usr/lib/systemd/system/firewalld.service
lrwxrwxrwx. 1 root root   44 Mar 14 19:17 dbus-org.freedesktop.Avahi.service -> /usr/lib/systemd/system/avahi-daemon.service
lrwxrwxrwx. 1 root root   44 Mar 14 19:20 dbus-org.freedesktop.ModemManager1.service -> /usr/lib/systemd/system/ModemManager.service
lrwxrwxrwx. 1 root root   46 Mar 14 19:17 dbus-org.freedesktop.NetworkManager.service -> /usr/lib/systemd/system/NetworkManager.service
lrwxrwxrwx. 1 root root   57 Mar 14 19:17 dbus-org.freedesktop.nm-dispatcher.service -> /usr/lib/systemd/system/NetworkManager-dispatcher.service
lrwxrwxrwx. 1 root root   36 Mar 14 19:23 default.target -> /lib/systemd/system/graphical.target
drwxr-xr-x. 2 root root   85 Mar 14 19:16 default.target.wants
lrwxrwxrwx. 1 root root   35 Mar 14 19:19 display-manager.service -> /usr/lib/systemd/system/gdm.service
drwxr-xr-x. 2 root root   31 Mar 14 19:16 getty.target.wants
drwxr-xr-x. 2 root root 4096 Mar 14 19:26 graphical.target.wants
drwxr-xr-x. 2 root root 4096 Jun  6 12:44 multi-user.target.wants
drwxr-xr-x. 2 root root   29 Mar 14 19:17 nfs.target.wants
drwxr-xr-x. 2 root root   25 Mar 14 19:17 printer.target.wants
drwxr-xr-x. 2 root root 4096 Mar 14 19:17 sockets.target.wants
drwxr-xr-x. 2 root root   35 Mar 14 19:20 spice-vdagentd.target.wants
drwxr-xr-x. 2 root root 4096 Mar 14 19:17 sysinit.target.wants
drwxr-xr-x. 2 root root   83 Mar 14 19:16 system-update.target.wants
</pre>
 
For example if we take a look at the default httpd.service file, we'll see that by default target is set as the multiuser target:

<pre>
cat /usr/lib/systemd/system/httpd.service
[Unit]
Description=The Apache HTTP Server
After=network.target remote-fs.target nss-lookup.target

[Service]
Type=notify
EnvironmentFile=/etc/sysconfig/httpd
ExecStart=/usr/sbin/httpd $OPTIONS -DFOREGROUND
ExecReload=/usr/sbin/httpd $OPTIONS -k graceful
ExecStop=/bin/kill -WINCH ${MAINPID}
# We want systemd to give httpd some time to finish gracefully, but still want
# it to kill httpd after TimeoutStopSec if something went wrong during the
# graceful stop. Normally, Systemd sends SIGTERM signal right after the
# ExecStop, which would kill httpd. We are sending useless SIGCONT here to give
# httpd time to finish.
KillSignal=SIGCONT
PrivateTmp=true

[Install]
WantedBy=multi-user.target

</pre>

Hence the multi-user targets get's affected when you enable/disable the httpd service:

<pre>
$ systemctl enable httpd
ln -s '/usr/lib/systemd/system/httpd.service' '/etc/systemd/system/multi-user.target.wants/httpd.service'
$ systemctl disable httpd
rm '/etc/systemd/system/multi-user.target.wants/httpd.service'
</pre>

Now if we want to change this "WantedBy" directive to another target, e.g. the graphical.target, then we need to have a file that contains the following:

<pre>
$ cat /etc/systemd/system/httpd.service
[Install]
WantedBy=graphical.target
</pre>

In my case this file didn't exist, so I had to create it.  

Now when we enable/disable this service we now see:

<pre>
$ systemctl enable httpd
ln -s '/usr/lib/systemd/system/httpd.service' '/etc/systemd/system/multi-user.target.wants/httpd.service'
$ systemctl disable httpd
rm '/etc/systemd/system/multi-user.target.wants/httpd.service'
</pre> 

As you can see the directive settings specified in the <code>/etc/systemd/system</code> takes precedance over the corresponding settings in <code>/usr/lib/systemd/system</code>


In our case we only wanted to override just this one directive. But you can add more custom to your unit files to add/override other directives. 
]]></Content>
		<Date><![CDATA[2015-09-25]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Creating swap disks]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What is the command to check your ram status?"]
$ free -m

[/toggle]
[toggle title="What is the command to create a new partition from /dev/sdb, which is used for creating a swap?"]
$ fdisk /dev/sdb   # ensure you set the type to set the type as "Linux swap"
[/toggle]
[toggle title="What is the command to install the 'swap filesystem' on /dev/sb3, and name it extramemory?"]
$ mkswap -L extramemory /dev/sdb3
[/toggle]
[toggle title="What is the command to activate the swap, /dev/sdb3?"]
$ swapon /dev/sdb3
[/toggle]
[toggle title="What is the command to check if the above has worked?"]
$ free -m
[/toggle]
[toggle title="What is the command to list all active swaps?"]
$ swapon -s
[/toggle]
[toggle title="What is the command to deactivate the swap, /dev/sdb3?"]
$ swapoff /dev/sdb3
[/toggle]
[toggle title="What is the entry to add into /etc/fstab to auto activate the swap, with the label, 'extramemory'?"]
LABEL=extramemory  swap  swap  defaults  0  0
[/toggle]
[toggle title="What is the command to test that the above entry is correct?"]
$ swapon -a
[/toggle]
[toggle title="What is the command to deactivate all swaps?"]
$ swapoff -a
[/toggle]
[/accordion]

<hr/>


Swap disks are a special type of storage that is designed to act like extra ram on a machine. This is handy if your machine is running low on ram. To see your ram info, you use the <em>free</em> command:


<pre>
$ free -m
              total        used        free      shared  buff/cache   available
Mem:            993         517         306          13         169         314
Swap:          1023         217         806
</pre>

Here you can see how much swap memory is currently active. 


You can make a swap disk out of either a disk partition, or from an LV. The approach varies slight with each approach. 

<h2>Create a Swap disk from a disk partition</h2>

First you need to create a partition, that is of the "Linux swap" partition type:

<pre>
Command (m for help): l

 0  Empty           24  NEC DOS         81  Minix / old Lin bf  Solaris
 1  FAT12           27  Hidden NTFS Win <mark>82  Linux swap / So</mark> c1  DRDOS/sec (FAT-
 2  XENIX root      39  Plan 9          83  Linux           c4  DRDOS/sec (FAT-
 3  XENIX usr       3c  PartitionMagic  84  OS/2 hidden C:  c6  DRDOS/sec (FAT-
 4  FAT16 <32M      40  Venix 80286     85  Linux extended  c7  Syrinx
 5  Extended        41  PPC PReP Boot   86  NTFS volume set da  Non-FS data
 6  FAT16           42  SFS             87  NTFS volume set db  CP/M / CTOS / .
 7  HPFS/NTFS/exFAT 4d  QNX4.x          88  Linux plaintext de  Dell Utility
 8  AIX             4e  QNX4.x 2nd part 8e  Linux LVM       df  BootIt
 9  AIX bootable    4f  QNX4.x 3rd part 93  Amoeba          e1  DOS access
 a  OS/2 Boot Manag 50  OnTrack DM      94  Amoeba BBT      e3  DOS R/O
 b  W95 FAT32       51  OnTrack DM6 Aux 9f  BSD/OS          e4  SpeedStor
 c  W95 FAT32 (LBA) 52  CP/M            a0  IBM Thinkpad hi eb  BeOS fs
 e  W95 FAT16 (LBA) 53  OnTrack DM6 Aux a5  FreeBSD         ee  GPT
 f  W95 Ext'd (LBA) 54  OnTrackDM6      a6  OpenBSD         ef  EFI (FAT-12/16/
10  OPUS            55  EZ-Drive        a7  NeXTSTEP        f0  Linux/PA-RISC b
11  Hidden FAT12    56  Golden Bow      a8  Darwin UFS      f1  SpeedStor
12  Compaq diagnost 5c  Priam Edisk     a9  NetBSD          f4  SpeedStor
14  Hidden FAT16 <3 61  SpeedStor       ab  Darwin boot     f2  DOS secondary
16  Hidden FAT16    63  GNU HURD or Sys af  HFS / HFS+      fb  VMware VMFS
17  Hidden HPFS/NTF 64  Novell Netware  b7  BSDI fs         fc  VMware VMKCORE
18  AST SmartSleep  65  Novell Netware  b8  BSDI swap       fd  Linux raid auto
1b  Hidden W95 FAT3 70  DiskSecure Mult bb  Boot Wizard hid fe  LANstep
1c  Hidden W95 FAT3 75  PC/IX           be  Solaris boot    ff  BBT
1e  Hidden W95 FAT1 80  Old Minix

</pre> 


Next we need to install the "swap filesystem":

<pre>
$ mkswap -L {swap-name} /dev/sdb1
mkswap: /dev/sdb1: warning: wiping old xfs signature.
Setting up swapspace version 1, size = 102396 KiB
no label, UUID=2e80abd8-ee86-4d06-b1b5-09ad66d17cc0
$
</pre>
Note: assigning a name with "-L" is optional, but is definitely recommended. Next we can activate the swap using the swapon command:

<pre>
$ swapon /dev/sdb1
</pre>

You can then check this has worked using the <code>free -m</code> command.

To list all your active swap disks, you can do:

<pre>
$ swapon -s
Filename                                Type            Size    Used    Priority
/dev/dm-1                               partition       1048572 224464  -1
/dev/sdb1                               partition       102396  0       -2
</pre> 

Note: this doesn't list inactive swaps. It only list active swaps only. 
 

You can also deactivate the swap with the swapoff command:

<pre>
$ swapoff /dev/sdb1
</pre>

Another way to deactivate the swap space is by rebooting the machine. That's because activating a swap using "swapon" isn't persistant. To make it persistant, you have to add it to the <code>/etc/fstab</code> file. More about this later.  


<h2>Create a Swap disk from a Logical Volume</h2>

First you create an LV, unless you already have one available.
 
<pre>
$ mkswap /dev/vg-name/lv-name
</pre>

Then you can use enable/disable swaps as shown earlier. 


<h2>Automouniting swap disks</h2>
If you want the new swap to become active at boot time, then you need add the new swap to the fstab file, here are the entries that you need to add:


col 1: “LABEL={swap’s nickname}”  or "UUID=..."

col 2: swap       # since mount-points are not applicable for swaps.

col 3: swap       # Since filesystems like ext4 are not applicable.               

col 4: defaults

col 5: 0          # since you don’t need to keep backups of temp data.

col 6: 0          # since you don’t need to do fsck because it will only contain temp data


You can test whether you have entered everything in fstab correctly by running the following command:


<pre>
$ swapon -a
</pre>

Here swapon will read the fstab file and start all swaps it can find. 

You can then use <code>free -m</code> and <code>swapon -s</code> to check everything is as expected. 

Likewise, to turn off all swaps listed in the fstab file, you do:

<pre>
$ swapoff -a
</pre>
 

 






Creating a swap partition is quite similar to how you create a filesystem partition, here's a side-by-side comparison of the 2 processes:

&nbsp;

&nbsp;
[table id=1 /]
&nbsp;

&nbsp;

<strong>Must survive reboot:</strong>

]]></Content>
		<Date><![CDATA[2015-09-27]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCE - Set up a CIFS Server]]></Title>
		<Content><![CDATA[CIFS is a system that's designed to let you share folders over the internet between machines. It is cross platform, i.e. you can share a Microsoft windows folder with a Linux machine.    


First you need to install: 

<pre>
$ yum install -y samba
</pre>

This package installs a few dependent rpms. One of which is the samba-common rpm. This rpm in turn installs the following config file:


<pre>
$ cat /etc/samba/smb.conf 
# This is the main Samba configuration file. You should read the
# smb.conf(5) manual page in order to understand the options listed
# here. Samba has a huge number of configurable options (perhaps too
# many!) most of which are not shown in this example
#
# For a step to step guide on installing, configuring and using samba,
# read the Samba-HOWTO-Collection. This may be obtained from:
#  http://www.samba.org/samba/docs/Samba-HOWTO-Collection.pdf
#
# Many working examples of smb.conf files can be found in the
# Samba-Guide which is generated daily and can be downloaded from:
#  http://www.samba.org/samba/docs/Samba-Guide.pdf
#
# Any line which starts with a ; (semi-colon) or a # (hash)
# is a comment and is ignored. In this example we will use a #
# for commentry and a ; for parts of the config file that you
# may wish to enable
#
# NOTE: Whenever you modify this file you should run the command "testparm"
# to check that you have not made any basic syntactic errors.
#
.
.
.
</pre>


At the very end of your <code>/etc/samba/smb.conf</code>, append the following:


<pre>
[{folder_name}]
path = /path/to/{folder_name}
available = yes
valid users = {user_name}
read only = no
browseable = yes
public = yes
writable = yes
</pre>

In the world of CIFS, we use existing machine login user accounts, as a means to authenticate yourself. In our example, we will use an existing account called "vagrant":

<pre>
$ cat /etc/passwd | grep vagrant
vagrant:x:1000:1000:vagrant:/home/vagrant:/bin/bash
</pre>

Let's say we want to share a folder called "/CIFS/Shared-Folder". Therefore in our scenario we will end up appending the following: 

<pre>
[Shared-Folder]
path = /CIFS/Shared-Folder
available = yes
valid users = vagrant
read only = no
browseable = yes
public = yes
writable = yes
</pre>


Next create the folder you want to share. In my case I create the following folder. 

<pre>
$ mkdir -p /CIFS/Shared-Folder
$ chmod 777 /CIFS/Shared-Folder
</pre>

Note: The "vagrant" user needs to have the appropriate permission for this folder.   


When using samba, we have to create a different password for our Linux account, which we do like this:

<pre>
$ smbpasswd -a vagrant
New SMB password:
Retype new SMB password:
Added user vagrant.
</pre>


Next we enable + restart the service:


<pre>
$ systemctl enable smb
ln -s '/usr/lib/systemd/system/smb.service' '/etc/systemd/system/multi-user.target.wants/smb.service'
$ systemctl start smb      
</pre>

Now from another server that has the smblcient installed, check if it can see the new share:


<pre>
$ smbclient -L puppetagent01 --user vagrant
Enter vagrant's password:
Domain=[MYGROUP] OS=[Unix] Server=[Samba 4.1.12]

        Sharename       Type      Comment
        ---------       ----      -------
        Shared-Folder   Disk
        IPC$            IPC       IPC Service (Samba Server Version 4.1.12)
        vagrant         Disk      Home Directories
Domain=[MYGROUP] OS=[Unix] Server=[Samba 4.1.12]

        Server               Comment
        ---------            -------

        Workgroup            Master
        ---------            -------
</pre>


Success, you can access the CIFS folder. 














https://rbgeek.wordpress.com/2012/05/25/how-to-install-samba-server-on-centos-6/]]></Content>
		<Date><![CDATA[2015-09-28]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCE|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCE]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCE - About this course]]></Title>
		<Content><![CDATA[placeholder]]></Content>
		<Date><![CDATA[2015-09-28]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCE|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCE]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Quiz Template]]></Title>
		<Content><![CDATA[<h2>Quiz Time</h2>
By the end of this article you should be able to answer the following questions:

[expand title="Question"  alt=" "]
<pre></pre>
[/expand]
&nbsp;

[expand title="Question" alt=" "]
<pre></pre>
[/expand]
&nbsp;

[expand title="Question" alt=" "]
<pre></pre>
[/expand]
&nbsp;

[expand title="Question" alt=" "]
<pre></pre>
[/expand]
&nbsp;

[expand title="Question" alt=" "]
<pre></pre>
[/expand]

&nbsp;
[expand title="Question" alt=" "]
<pre></pre>
[/expand]
&nbsp;

<hr/>
]]></Content>
		<Date><![CDATA[2015-09-29]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[quiz]]></Tags>
		<Status><![CDATA[private]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCE - Set up an NFS server]]></Title>
		<Content><![CDATA[If you have directories on your machine that you want to share out to other machines so that they can access these folders as well (via a network connection), then one way to to do is by setting up nfs on your machine. 


To do this, you first need to ensure the following package is installed:

<pre>
$ yum install nfs-utils
</pre> 


Next identify the directories you want to share, or create them if they don't exist them. I am going to create the following folder that I want to share:

<pre>
$ mkdir -p /nfs/Shared-Folder
$ chmod -R 777 /nfs/Shared-Folder
</pre>

Notice we have created a meaningful folder name to make it obvious what folder we are sharing, and with what technology (i.e. nfs) 

Now lets create some dummy content in this directory, which will get shared:


<pre>
$ touch /nfs/Shared-Folder/testfile1.txt
$ mkdir /nfs/Shared-Folder/subfolder
$ touch /nfs/Shared-Folder/subfolder/testfile2.txt
$ tree /nfs/Shared-Folder
/nfs/Shared-Folder
├── subfolder
│   └── testfile2.txt
└── testfile1.txt

1 directory, 2 files
</pre>

Now to share this folder, you need to create/open the the following nfs config file:

<pre>
$ vim /etc/exports
</pre>

Note this file might not exist, or does exist but is empty. 

In this file, you need to add an entry for each folder that you want to share. Each entry is made up 3 fields. For more info, checkout:

<pre>
$ whatis exports
exports (5)          - NFS server export table
$ man 5 exports
</pre>


In this example I inserted:


<pre>
[root@puppetagent01 ~]# cat /etc/exports
/nfs/Shared-Folder      -rw        *
</pre>

<strong>First field</strong>: we specify which local folder we want to share.
<strong>Second field</strong>: Here we specify the mount options. In this example we want remote servers to have read-write access to this server. 
<strong>Third field</strong>: Here we specify remote server ip numbers that can access this shared folder. Here we stated asterisk to indicate that all servers in the local network can access this. Note, nfs is only able to share folders within a private local network, this means you can't share a folder over the internet even if you wanted to. If wanted to do that then you need to use samba instead of nfs. 


Now start and enable the nfs-server service:

<pre>
$ systemctl enable nfs-server
$ systemctl restart nfs-server
</pre>

Now back on the nfs client server, we check what :

<pre>
[root@puppetmaster ~]# showmount -e puppetagent01
Export list for puppetagent01:
/data *
</pre>   


Now let's manually mount this folder on a remote server. To do this, we first create our mount point:

<pre>
[root@puppetmaster ~]# mkdir /tmp/mount-point
[root@puppetmaster ~]# tree /tmp/mount-point
/tmp/mount-point

0 directories, 0 files
</pre>


Then we use the mount command to mount the shared folder:


<pre>
[root@puppetmaster ~]# mount puppetagent01:/data /tmp/mount-point/
</pre>

This doesn't give a success message, which is a good sign. 

Now let's use the df command to check this has worked:

<pre>
$ df -h
Filesystem                            Size  Used Avail Use% Mounted on
/dev/mapper/centos_puppetmaster-root   38G  4.4G   34G  12% /
devtmpfs                              488M     0  488M   0% /dev
tmpfs                                 497M   80K  497M   1% /dev/shm
tmpfs                                 497M   14M  484M   3% /run
tmpfs                                 497M     0  497M   0% /sys/fs/cgroup
/dev/sda1                             497M  149M  349M  30% /boot
none                                  224G  158G   66G  71% /vagrant
//192.168.50.11/vpublicshare           37G  1.4G   34G   4% /tmp/cifs_share
<strong>puppetagent01:/data                    37G  1.4G   34G   4% /tmp/mount-point</strong>

</pre>


This output doesn't indicate that this is an nfs mount, to check the mount type we can instead use the mount command:

<pre>
$ mount | grep "mount-point"
localhost:/data on /tmp/mount-point type <mark>nfs4</mark> (rw,relatime,vers=4.0,rsize=131072,wsize=131072,namlen=255,hard,proto=tcp6,port=0,timeo=600,retrans=2,sec=sys,clientaddr=::1,local_lock=none,addr=::1)
</pre>







Now if you want, you can unmount this shared directory like this:

<pre>
$ umount /tmp/mount-point/
$ tree /tmp/mount-point
/tmp/mount-point

0 directories, 0 files
</pre>


<h2>Setting up automount</h2>
So far we have mounted the folder manually, let's now automount this. In this situation, we will use /tmp/mount-point/ as a folder that hosts other mount points.  

So let's first add the following entry to the /etc/auto.master file:

<pre>
$ cat /etc/auto.master | grep auto.mount-point
/tmp/mount-point     /etc/auto.mount-point
</pre>

Now lets create the new auto.mount-mount file and add the following line to it:

<pre>
$ cat /etc/auto.mount-point
sharedfolder1      -rw        localhost:/data
</pre>


Based on the above two config files, it means that the new mount point's location will be at:

<pre>
/tmp/mount-point/sharedfolder1
</pre>


Now let's go to the mount points's hosting folder (which in this case is /tmp/mount-point) and see what it contains:

<pre>
ls -l /tmp/mount-point
total 0
$
$ cd /tmp/mount-point/sharedfolder1
-bash: cd: /tmp/mount-point/sharedfolder1: No such file or directory
</pre>


That's expected since we don't have the shared folder mounted yet:


<pre>
$ mount | grep sharedfolder1
$
</pre>

To fix this we need to restart the autofs service in order to load in the new config data:

<pre>
$ systemctl restart autofs
</pre>

Now let's retry:

<pre>
$ ls -l /tmp/mount-point
total 0
$
</pre>

Still no luck, that's because this service is called automounting, which means it will automount when you actually try to cd into the mountpoint itself, so let's try that:


<pre>
[root@localhost /]# cd /tmp/mount-point/sharedfolder1
[root@localhost sharedfolder1]# tree
.
├── subfolder
│   └── testfile2.txt
└── testfile1.txt

1 directory, 2 files

</pre>

Success!

Now if we cd back out of the mount point, and check if that mount point still exist:


<pre>
[root@localhost sharedfolder1]# cd /
[root@localhost /]# ls -l /tmp/mount-point
total 0
drwxr-xr-x. 3 root root 42 Apr 11 00:26 sharedfolder1
[root@localhost /]#

</pre>

Now you can see, that the mount point is still active. That's because if a automounted mount points isn't being used, then it stays mounted for up to 5 minutes (300 seconds) before automatically getting unmounted again. This default 5 mins get be changed, by editing the main autofs config file, in particular this entry:

<pre>
$ cat /etc/autofs.conf | grep "^timeout"
timeout = 300
</pre> 


 Here's another check that our mount point is currently active:


<pre>$ mount | grep sharedfolder1
/dev/mapper/centos-root on /tmp/mount-point/sharedfolder1 type xfs (rw,relatime,seclabel,attr2,inode64,noquota)
</pre>


Some things to keep in mind:

<ul>
	<li>any automounted mount points are unmounted if you stop the autofs service</li>
	<li>You can manually create the mount points. But this unnecessary becuase autofs will create the mount point for you. In fact if you do manually create it, then autofs will assume full control of it, once automounting happens and will delete this folder when you unmount, or stop the autofs service. Hence no point manually creating the mount point folder.</li>
	<li>You don't need to manually create the mount point's hosting folder, autofs can create that for you (but only when you try cd into the hosting folder), however if you do let autofs create it for you, then it will also delete this folder when you stop the autofs service. But if you create it manually, then the folder will state intact after stopping the autofs service. Hence it might be a good idea to only create this folder manually.</li>
</ul>


https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Storage_Administration_Guide/nfs-serverconfig.html]]></Content>
		<Date><![CDATA[2015-10-01]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCE|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCE]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Deleting a Physical Volume]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What is the command to redistribute all the data out of the physical volume, /dev/sdb1?"]
$ pvmove /dev/sdb1    # this can take a few seconds to run
[/toggle]
[toggle title="What is the command remove the physical volume /dev/sdb1, from the volume group vgdata1vg?"]
$ vgreduce vgdata1vg /dev/sdb1
[/toggle]
[/accordion]

<hr/>

There will be times when you want to decommission a block device (e.g. hdd). To do this you first need to delete any physical volumes that is the hdd houses. One of the big advantages of lvm is that you can delete these physical volumes without needing to unmount any filesystems. 

Let's say we want to remove the the /dev/sdb device and it comprises of only one primary partition (/dev/sdb1) which is being using as physical volume. Lets also assume that this physical volume is assigned to the volume group vgdata1vg. Now before we can delete the physical volume we first have to ensure that the volume group it is assigned to has enough capacity to carry on after losing the physical volume, if not then add a another physical volume to it from another block device.  

After that you have to move date out of the /dev/sdb1 redistribute it to all the other physical volumes in the volume group, this is done by running the following command:

<pre>
$ pvmove /dev/sdb1    # this can take a few seconds to run
</pre>

Now we can remove this physical volume from vgdata1vg by running the following command:

<pre>
$ vgreduce vgdata1vg /dev/sdb1
</pre>]]></Content>
		<Date><![CDATA[2015-10-03]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Networking basics]]></Title>
		<Content><![CDATA[One of the first things you need to realise is that "interfaces" and "connections" are not the same thing. They are closely related to each other, but they refer to different things. 

<h2>Network Interfaces</h2>
An <strong>interface</strong> is actually the slots on your physical network card. Some of these interfaces can be virtual slots. But ultimately they are like tunnels from your machine to the outside world. You can view a list of your interfaces using the ip command like this:


<pre>
$ ip link show
1: <mark>lo</mark>: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: <mark>enp0s3</mark>: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT qlen 1000
    link/ether 08:00:27:d4:f9:99 brd ff:ff:ff:ff:ff:ff
3: <mark>enp0s8</mark>: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT qlen 1000
    link/ether 08:00:27:46:bf:38 brd ff:ff:ff:ff:ff:ff
</pre> 

As you can see we have 3 interfaces on our machine. 

These interfaces on their own don't know what connection config data to use, e.g. ip address, and that's where "connections" comes into play.  

<h2>Connections</h2>
A <strong>connection</strong> is acutally a config file that contains information that an interface uses to connect to a network. All these connections files resides in the following directory, and their file name is in the form of ifcfg-{interface-name}:

<pre>
$ ls -l /etc/sysconfig/network-scripts | grep ifcfg
-rw-r--r--. 1 root root   318 Sep 16 19:31 ifcfg-enp0s3
-rw-r--r--. 1 root root   215 Oct  3 07:58 ifcfg-enp0s8
-rw-r--r--. 1 root root   254 Jan 15  2015 ifcfg-lo
</pre>

As you can see, each interface has it's own (connection) file. 







<h2>Network Manager</h2>
This is a collection of gui, tui, and cli utilities that are used to configure your network settings. All these utilities interacts with the NetworkManager service:

<pre>
$ systemctl status NetworkManager
NetworkManager.service - Network Manager
   Loaded: loaded (/usr/lib/systemd/system/NetworkManager.service; enabled)
   Active: active (running) since Sat 2015-10-17 00:12:08 BST; 2 weeks 0 days ago
 Main PID: 590 (NetworkManager)
   CGroup: /system.slice/NetworkManager.service
           ├─ 590 /usr/sbin/NetworkManager --no-daemon
           └─4595 /sbin/dhclient -d -q -sf /usr/libexec/nm-dhcp-helper -pf /var/run/dhclient-enp0s3.pid -lf /var/lib/NetworkManag...

Oct 31 18:31:14 puppetmaster.local NetworkManager[590]: <info>  (enp0s3): Activation: Stage 5 of 5 (IPv4 Commit) complete.
</pre>


<h2>The Network service</h2>
There is a service called "network" which uses a connection's info to establish an internet session on an interface. 

<pre>
$ systemctl status network
network.service - LSB: Bring up/down networking
   Loaded: loaded (/etc/rc.d/init.d/network)
   Active: active (exited) since Sat 2015-10-17 00:12:11 BST; 2 weeks 0 days ago
  Process: 693 ExecStart=/etc/rc.d/init.d/network start (code=exited, status=0/SUCCESS)

Oct 17 00:12:08 puppetmaster.local systemd[1]: Starting LSB: Bring up/down networking...
</pre>


    ]]></Content>
		<Date><![CDATA[2015-10-03]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Configuring hostnames and DNS]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What is the command to display just your machine's hostname?"]
$ hostname -s
[/toggle]
[toggle title="What is the command to display your machine's fqdn?"]
$ hostname -f
[/toggle]
[toggle title="What is the command to just display your machine's ip address?"]
$ hostname -I
[/toggle]
[toggle title="What is the command to display your machine's hostname along with other related info?"]
$ hostnamectl status
[/toggle]
[toggle title="What is the file you need to edit to change your machine's hostname?"]
/etc/hostname
[/toggle]
[toggle title="Assuming you don't want to edit this file directly, then what is the command to (persistantly) change your machine's hostname to puppet01.local?"]
$ hostnamectl set-hostname puppet01.local
[/toggle]
[toggle title="What file is used to lookup dns locally?"]
/etc/hosts
[/toggle]
[/accordion]

<hr/>





<h2>Hostname</h2>

All machines in a network have two ways to identify itself to the other machines on the network, they are the machine's ip address and it's name (aka hostname).  The hostname is your machine's name. You can view this name, using the hostname command:

<pre>
$ hostname -s
puppetmaster
</pre>
 
This is machine's short name. It's full name is:

<pre>
$ hostname -f
puppetmaster.local
</pre>

This full name is more commonly known as the machine's <strong>Fully Qualified Domain Name</strong>, (aka <strong>FQDN</strong>). 

To view a machine's ip address we can do:
<pre>
$  hostname -I
192.168.1.124
</pre> 




Another way to view your machine's hostname is by using the <strong>hostnamectl</strong> command:


<pre>
$ hostnamectl status
   Static hostname: puppetmaster.local
         Icon name: computer-vm
           Chassis: vm
        Machine ID: bb2262658ee64941afea091071b78f45
           Boot ID: c59311743d9940b1981d065179eeccf3
    Virtualization: oracle
  Operating System: CentOS Linux 7 (Core)
       CPE OS Name: cpe:/o:centos:centos:7
            Kernel: Linux 3.10.0-229.14.1.el7.x86_64
      Architecture: x86_64

</pre>


You can change your machine's hostname by editing the following file:

<pre>
$ cat /etc/hostname
puppetmaster.local
</pre>

Or you can use the hostnamectl command:

<pre>
$ hostnamectl set-hostname puppetmaster.local.test
$ cat /etc/hostname
puppetmaster.local.test
</pre>

As you can see the hostnamectl command just edits the same file behind the scene. 



<h2>Domain Name Servers (DNS)</h2>
Hostnames are human friendly names for a machine. However machines can't identify themselves on a network using hostnames. Instead they have to identify themselves using an IP number. Machines use IP numbers pretty much the same way that phones connect to each other through phone numbers. 

If we try to connect to a machine, e.g. via ssh, then we normally use it's fqdn:

<pre>
ssh root@puppetagent01.local
</pre>

That's because ip numbers are too difficult to remember. However before a connection to puppetagent01.local can be established, the fqdn will first need to be resolved to an ip number. This resolution can be done locally on your machine by adding an entry to the <code>/etc/hosts</code> file:

<pre>
$ cat /etc/hosts
127.0.0.1 localhost
127.0.1.1 puppetmaster.local puppetmaster
192.168.50.10 puppetmaster puppetmaster.local
192.168.50.11 puppetagent01 puppetagent01.local
192.168.50.12 puppetagent02 puppetagent02.local
</pre>     


If your machine can't find a matching entry here, then your machine will contact the dns server to resolve it instead.  

<h2>DNS Server</h2>
A dns server is a server that receives requests for an ip address for a given a fqdn. The dns server then sends back the ip number.  

The dns server's details is specified in the <code>/etc/resolv.conf</code> file:


<pre>
$ cat /etc/resolv.conf
# Generated by NetworkManager
search local
nameserver 10.0.2.3
nameserver 8.8.8.8
</pre>

This file is actually not used anymore and is only used internally be the NetworkManager. So please don't make any changes to this file, otherwise it will get overwritten by NetworkManager service. 

This file is actually combined list of all the dns entries specified in the individual ifcfg-* config files. We will cover these files later. 


 
Finally you can query a dns server using nslookup command. 

<pre>
$ nslookup google.com
Server:         10.0.2.3
Address:        10.0.2.3#53

Non-authoritative answer:
Name:   google.com
Address: 216.58.208.46
</pre>




Also, here is an interesting setting:


<pre>
$ cat /etc/nsswitch.conf | grep "^hosts:"
hosts:      files dns myhostname
</pre>

This shows the order in which dns lookup occurs, first "files" which refers to /etc/hosts, then it is the dns server. 













]]></Content>
		<Date><![CDATA[2015-10-03]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - List of all resources that needs to be persistant]]></Title>
		<Content><![CDATA[There are some changes that you can make to a machine that get's reverted back to what the were before. 

It is really important to know what you need to do so that these changes can survive a reboot, i.e. make them persistant.

For  

<ul>
	<li><a href="http://codingbee.net/tutorials/rhcsa/rhcsa-network-interface-card-nic/">network interfaces (done using nmcli)</a></li>
	<li>systemd units </li>
	<li>default target</li>
	<li>Setting the default kernel version</li>
	<li></li>
</ul>


kvm vms
network interfaces (done using nmcli)
filesystem mounts
Selinux stuff
]]></Content>
		<Date><![CDATA[2015-10-04]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Create a local Yum repo from an ISO file]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What are three commands needed to mount a cd containing yum repo, and then configure yum to access it?"]
1. Create the mount point
2. mount the iso onto the mount point
3. create a repo file pointing to this new mount point
[/toggle]
[toggle title="What is the command to mount the iso, /tmp/library.iso onto the mount point, /data/iso?"]
$ mount -o loop /tmp/library.iso /data/iso
[/toggle]
[toggle title="What is an indicator that a folder is actually a yum repo?"]
It contains a folder called "repodata"
[/toggle]
[toggle title="What is the command to create a yum repo pointing to this newly mounted iso (assuming the folder, /data/iso/repodata exists)?"]
$ yum-config-manager --add-repo=file:///data/iso
[/toggle]
[/accordion]

<hr/>




If you explore the contents of your CentOS/RHEL installation dvd you would find that it contains really big yum repos. You can identify a yum repo by any directory that contains a folder called <strong>repodata</strong>

If you have used an ISO file to install your OS, then that ISO file is simply the file equivalent of the installation DVD (for this article we will ignore DVDs and assume you have an ISO). 

We can therefore configure our ISO/DVD to act as a local yum repo. To do this, you first need to mount the iso.

To mount the iso, we first create a mount point:

<pre>
$ mkdir -p /data/iso
</pre>

Then we mount the iso using the mount command:

<pre>
$ mount -o loop /vagrant/CentOS-6.6-x86_64-minimal.iso /data/iso/
mount: /dev/loop0 is write-protected, mounting read-only
</pre>

Note: we need to use the "loop" option when mounting an ISO or dvd. 


We can then check this has worked using df:

<pre>
$ df -h
Filesystem                            Size  Used Avail Use% Mounted on
/dev/mapper/centos_puppetmaster-root   38G  4.7G   33G  13% /
devtmpfs                              488M     0  488M   0% /dev
tmpfs                                 497M   80K  497M   1% /dev/shm
tmpfs                                 497M  7.0M  490M   2% /run
tmpfs                                 497M     0  497M   0% /sys/fs/cgroup
/dev/sda1                             497M  149M  349M  30% /boot
none                                  224G  156G   68G  70% /vagrant
<strong>/dev/loop0                            383M  383M     0 100% /data/iso</strong>
</pre>

or we can check using the mount command:

<pre>
$ mount | grep '/data/iso'
/vagrant/CentOS-6.6-x86_64-minimal.iso on /data/iso type iso9660 (ro,relatime)
</pre>

Obviously another way to check is by going into the mount point itself, and see what's there:

<pre>
$ ls -l /data/iso/
total 82
-r--r--r--. 1 root root    14 Oct 24  2014 CentOS_BuildTag
dr-xr-xr-x. 3 root root  2048 Oct 24  2014 EFI
-r--r--r--. 1 root root   212 Nov 27  2013 EULA
-r--r--r--. 1 root root 18009 Nov 27  2013 GPL
dr-xr-xr-x. 3 root root  2048 Oct 24  2014 images
dr-xr-xr-x. 2 root root  2048 Oct 24  2014 isolinux
dr-xr-xr-x. 2 root root 40960 Oct 24  2014 Packages
-r--r--r--. 1 root root  1354 Oct 19  2014 RELEASE-NOTES-en-US.html
<strong>dr-xr-xr-x. 2 root root  4096 Oct 24  2014 repodata
</strong>-r--r--r--. 1 root root  1706 Nov 27  2013 RPM-GPG-KEY-CentOS-6
-r--r--r--. 1 root root  1730 Nov 27  2013 RPM-GPG-KEY-CentOS-Debug-6
-r--r--r--. 1 root root  1730 Nov 27  2013 RPM-GPG-KEY-CentOS-Security-6
-r--r--r--. 1 root root  1734 Nov 27  2013 RPM-GPG-KEY-CentOS-Testing-6
-r--r--r--. 1 root root  3380 Oct 24  2014 TRANS.TBL
</pre>

Now to set up the local repo we can manually create the .repo file. Byt it is easier to just use the <strong>yum-config-manager</strong> command:

<pre>
$ yum-config-manager --add-repo=file:///data/iso
</pre>
Note the three forward slashes. 

Now we can check this has worked by using <strong>yum repolist</strong>:

<pre>
$ yum repolist
Loaded plugins: fastestmirror, langpacks
Loading mirror speeds from cached hostfile
 * base: mirrors.clouvider.net
 * epel: fedora.cu.be
 * extras: mirrors.clouvider.net
 * updates: centos.muzzy.org.uk
repo id                     repo name                              status
base/7/x86_64               CentOS-7 - Base                        8,652
<strong>data_iso                    added from: file:///data/iso             248
</strong>epel/x86_64                 Extra Packages for Enterprise Linux 7  8,545
extras/7/x86_64             CentOS-7 - Extras                        214
foreman/x86_64              Foreman 1.9                              367
foreman-plugins/x86_64      Foreman plugins 1.9                      216
puppetlabs-deps/x86_64      Puppet Labs Dependencies El 7 - x86_64    17
puppetlabs-products/x86_64  Puppet Labs Products El 7 - x86_64       191
rhscl-ruby193-epel-7-x86_64 Ruby193 - epel-7-x86_64                  405
rhscl-v8314-epel-7-x86_64   V8 3.14.5.10 - epel-7-x86_64              21
updates/7/x86_64            CentOS-7 - Updates                     1,501
repolist: 20,377

</pre>

 ]]></Content>
		<Date><![CDATA[2015-10-04]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Yum Repository GPG keys]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What is the command to create a repo using the baseurl as 'https://dl.fedoraproject.org/pub/epel/7/x86_64/'?"]<span style='font-size:15px'>$ yum-config-manager <span style='letter-spacing:0.1px'>-</span>-add-repo=https://dl.fedoraproject.org/pub/epel/7/x86_64/</span>[/toggle]
[toggle title="What is the command?"]
/etc/pki/rpm-gpg/
[/toggle]
[toggle title="What is the command?"]


[/toggle]
[toggle title="What is the command?"]


[/toggle]
[toggle title="What is the command?"]


[/toggle]
[toggle title="What is the command?"]


[/toggle]
[toggle title="What is the command?"]


[/toggle]
[toggle title="What is the command?"]


[/toggle]
[toggle title="What is the command?"]


[/toggle]
[toggle title="What is the command?"]


[/toggle]
[/accordion]

<hr/>




Yum makes use of GPG keys as a way to ensure that our machine downloads rpm packages from an authenticated source. 

To set up GPG keys for a yum repo, let's first get a url for an yum repo, in our example, we'll use the <a href="https://fedoraproject.org/wiki/EPEL">epel yum repo</a>:

<pre>
https://dl.fedoraproject.org/pub/epel/7/x86_64/
</pre>

Let's first create a .repo file for this yum repo using the yum-config-manager:


<pre>
$ <strong>yum-config-manager --add-repo=https://dl.fedoraproject.org/pub/epel/7/x86_64/</strong>
Loaded plugins: fastestmirror, langpacks
adding repo from: https://dl.fedoraproject.org/pub/epel/7/x86_64/

[dl.fedoraproject.org_pub_epel_7_x86_64_]
name=added from: https://dl.fedoraproject.org/pub/epel/7/x86_64/
baseurl=https://dl.fedoraproject.org/pub/epel/7/x86_64/
enabled=1
</pre>

This command generated the following .repo file:

<pre>
$ cat /etc/yum.repos.d/dl.fedoraproject.org_pub_epel_7_x86_64_.repo

[dl.fedoraproject.org_pub_epel_7_x86_64_]
name=added from: https://dl.fedoraproject.org/pub/epel/7/x86_64/
baseurl=https://dl.fedoraproject.org/pub/epel/7/x86_64/
enabled=1
</pre>

By default, yum-config-manager creates a repo that isn't GPG secured. Although at this point our epel repo is in a usable state. To enable GPG for our new repo, We'll need to append a couple more lines to our new .repo file. 

To add these line, we first need to locate the epel repo's GPG key. In this case the gpg key is located further up epel's directory tree:


<pre>
https://dl.fedoraproject.org/pub/epel/
</pre>

It is best practice to download all gpg keys to the <code>/etc/pki/rpm-gpg/</code> directory:

<pre>
$ ls -l /etc/pki/rpm-gpg/
total 32
-rw-r--r--. 1 root root 1690 Mar 31  2015 RPM-GPG-KEY-CentOS-7
-rw-r--r--. 1 root root 1004 Mar 31  2015 RPM-GPG-KEY-CentOS-Debug-7
-rw-r--r--. 1 root root 1690 Mar 31  2015 RPM-GPG-KEY-CentOS-Testing-7
-rw-r--r--. 1 root root 1662 Nov 25  2014 RPM-GPG-KEY-EPEL-7
-rw-r--r--. 1 root root 3140 Sep 11 10:35 RPM-GPG-KEY-foreman
-rw-r--r--. 1 root root 5567 Aug 22  2014 RPM-GPG-KEY-nightly-puppetlabs
-rw-r--r--. 1 root root 1716 Aug 22  2014 RPM-GPG-KEY-puppetlabs
</pre>

Therefore let's download the epel GPG key into this directory using <strong>wget</strong>:


<pre>
$ cd /etc/pki/rpm-gpg/
$ <strong>wget https://dl.fedoraproject.org/pub/epel/RPM-GPG-KEY-EPEL-7</strong>
--2015-10-06 20:28:49--  https://dl.fedoraproject.org/pub/epel/RPM-GPG-KEY-EPEL-7
Resolving dl.fedoraproject.org (dl.fedoraproject.org)... 209.132.181.24, 209.132.181.23, 209.132.181.26, ...
Connecting to dl.fedoraproject.org (dl.fedoraproject.org)|209.132.181.24|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 1662 (1.6K)
Saving to: ‘RPM-GPG-KEY-EPEL-7’

100%[=============================================================>] 1,662       --.-K/s   in 0s

2015-10-06 20:28:50 (18.0 MB/s) - ‘RPM-GPG-KEY-EPEL-7’ saved [1662/1662]

</pre>

Now we have to append the following lines to our new .repo file:

<pre>
$ cat /etc/yum.repos.d/dl.fedoraproject.org_pub_epel_7_x86_64_.repo

[dl.fedoraproject.org_pub_epel_7_x86_64_]
name=added from: https://dl.fedoraproject.org/pub/epel/7/x86_64/
baseurl=https://dl.fedoraproject.org/pub/epel/7/x86_64/
enabled=1
<mark>gpgcheck=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7</mark>
</pre>

That's all you need to do.


Tip, some popular repos, such as the epel repo can be installed in the form of an rpm package themselves. So instead of doing all the above, you can simply do: 


<pre>
$ yum install epel-release
</pre>

This will effectively drop in the gpg key and the .repo file in the relevant directories. 



]]></Content>
		<Date><![CDATA[2015-10-06]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Setting the default kernel version]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What is the technique to list all your grub related commands?"]
$ grub2-  {tab+tab}
[/toggle]
[toggle title="What is the command to set the kernel to most recent version?"]
$ grub2-set-default 0
[/toggle]
[toggle title="How do you check the above command has worked?"]
# reboot your machine, then run 
$ uname -r
[/toggle]
[/accordion]

<hr/>

By default, when your machine is starting up, it will automatically default to starting up the latest installed kernel. That is your bootloader's default behaviour. 

However there are lot of commands available that can adjust your bootloader's behavior: 

<pre>
$ grub2-  {tab+tab}
grub2-bios-setup       grub2-macbless         grub2-mkpasswd-pbkdf2  grub2-render-label
grub2-editenv          grub2-menulst2cfg      grub2-mkrelpath        grub2-rpm-sort
grub2-file             grub2-mkconfig         grub2-mkrescue         grub2-script-check
grub2-fstest           grub2-mkfont           grub2-mkstandalone     <mark>grub2-set-default</mark>
grub2-glue-efi         grub2-mkimage          grub2-ofpathname       grub2-sparc64-setup
grub2-install          grub2-mklayout         grub2-probe            grub2-syslinux2cfg
grub2-kbdcomp          grub2-mknetdir         grub2-reboot
</pre>

The <strong>grub2-set-default</strong> commands let's you set the default grub version. For example if you run the following:

<pre>
$ grub2-set-default 0
</pre>

This set's the most recent kernel version as the default kernel. 


However if you run:


<pre>
$ grub2-set-default 1
</pre>

Then the second most recent kernel version becomes the default....and so forth and so on. 

Unfortunately this command doesn't let you specify a specific kernel version. So if you want to continue using a particular kernel version, then it means that you have to use this command every time a newer kernel version is installed. 

To confirm that this has worked you can run the <code>uname -r</code> command to check with kernel version is currently running. 


Luckily there is a package that you can use that let's you set specific kernel version.  
]]></Content>
		<Date><![CDATA[2015-10-06]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[nginx - Reverse Proxy Server setup]]></Title>
		<Content><![CDATA[<pre>
$ yum install nginx
</pre>

<pre>
$ cp /etc/nginx/conf.d/default.conf /etc/nginx/conf.d/default.conf.orig
</pre>

<pre>
$ mv /etc/nginx/conf.d/default.conf /etc/nginx/conf.d/foreman.conf
</pre>


<pre>

$ cat foreman.conf
server {
       listen       80;
       server_name  lv793.ordsvy.gov.uk;
       return       301 https://lv793.ordsvy.gov.uk;
}
 
server {
       listen       443;
       server_name  lv793.ordsvy.gov.uk;
 
       ssl                  on;
       ssl_certificate      /etc/nginx/ssl/cert.crt;
       ssl_certificate_key  /etc/nginx/ssl/cert.key;
 
       location / {
           proxy_pass  https://lv650.ordsvy.gov.uk;
       }
}
 
# this to get the dns alias to work
server {
       server_name  testforeman;
       return       301 https://testforeman.ordsvy.gov.uk;
}

</pre>



Explanation:
If you use the nginx server's host name it will connect to nginx reverse proxy server (lv793) and redirect to foreman server which is https://lv650.ordsvy.gov.uk.


Start the nginx service. It listens http only.

<pre>
$ service nginx start
</pre>

To make our reverse proxy more secure, we now need to setup ssl. check openssl package is available or not in the system using below command.


<pre>
$ rpm -qa | grep openssl
</pre>


If not installed then install it:


<pre>$ yum install openssl</pre>


Next generate the self-signed SSL certificate and key:


<pre>
$ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /etc/nginx/cert.key -out /etc/nginx/cert.crt   (Certificate & Key will be created in /etc/nginx location)
  
Here -x509, Self-signed certificate for testing.
     -days=365, self-signed certificate valid for 365 days.
     -rsa=2048, rsa 2048-bit certificate.
      /etc/nginx/cert.crt - ssl certificate location.
      /etc/nginx/cert.key - ssl key location.
</pre>


Now open ssl.conf file add below lines.

<pre>server {
    listen                443;
    server_name           https://nginx.ordsvy.gov.uk;       #nginx server hostname
    ssl                   on;
    ssl_certificate       /etc/nginx/ssl/cert.crt;
    ssl_certificate_key   /etc/nginx/ssl/cert.key;
     
    location / {
        proxy_pass        https://lv650.ordsvy.gov.uk:443;     #foreman server hostname
    }
}
</pre>


 Restart the nginx service.
<pre>service nginx restart</pre>


After that both:

- http://nginx-fqdn
- https://nginx-fqdn

will work. ]]></Content>
		<Date><![CDATA[2015-10-08]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[nginx]]></Tags>
		<Status><![CDATA[private]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA Objectives]]></Title>
		<Content><![CDATA[

<h5>Understand and use essential tools</h5>
[expand title="Access a shell prompt and issue commands with correct syntax" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Use input-output redirection" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Use grep and regular expressions to analyze text" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Access remote systems using ssh" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Log in and switch users in multiuser targets" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Archive, compress, unpack, and uncompress files using tar, star, gzip, and bzip2" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Create and edit text files" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Create, delete, copy, and move files and directories" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Create hard and soft links" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="List, set, and change standard ugo/rwx permissions" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Locate, read, and use system documentation including man, info, and files in /usr/share/doc" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]







<h5>Operate running systems</h5>
[expand title="Boot, reboot, and shut down a system normally" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Boot systems into different targets manually" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Interrupt the boot process in order to gain access to a system" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Identify CPU/memory intensive processes, adjust process priority with renice, and kill processes" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Locate and interpret system log files and journals" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Access a virtual machine's console" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Start and stop virtual machines" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Start, stop, and check the status of network services" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Securely transfer files between systems" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]










<h5>Configure local storage</h5>
[expand title="List, create, delete partitions on MBR and GPT disks" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Create and remove physical volumes, assign physical volumes to volume groups, and create and delete logical volumes" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Configure systems to mount file systems at boot by Universally Unique ID (UUID) or label" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Add new partitions and logical volumes, and swap to a system non-destructively" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]



<h5>Create and configure file systems</h5>
[expand title="Create, mount, unmount, and use vfat, ext4, and xfs file systems" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Mount and unmount CIFS and NFS network file systems" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Extend existing logical volumes" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Create and configure set-GID directories for collaboration" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Create and manage Access Control Lists (ACLs)" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Diagnose and correct file permission problems" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]










<h5>Deploy, configure, and maintain systems</h5>
[expand title="Configure networking and hostname resolution statically or dynamically" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Schedule tasks using at and cron" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Start and stop services and configure services to start automatically at boot" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Configure systems to boot into a specific target automatically" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Install Red Hat Enterprise Linux automatically using Kickstart" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Configure a physical machine to host virtual guests" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Install Red Hat Enterprise Linux systems as virtual guests" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Configure systems to launch virtual machines at boot" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Configure network services to start automatically at boot" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Configure a system to use time services" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Install and update software packages from Red Hat Network, a remote repository, or from the local file system" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Update the kernel package appropriately to ensure a bootable system" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Modify the system bootloader" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]






<h5>Manage users and groups</h5>
[expand title="Create, delete, and modify local user accounts" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Change passwords and adjust password aging for local user accounts" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Create, delete, and modify local groups and group memberships" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Configure a system to use an existing authentication service for user and group information" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]












<h5>Manage security</h5>
[expand title="Configure firewall settings using firewall-config, firewall-cmd, or iptables" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Configure key-based authentication for SSH" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Set enforcing and permissive modes for SELinux" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="List and identify SELinux file and process context" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Restore default file contexts" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Use boolean settings to modify system SELinux settings" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Diagnose and address routine SELinux policy violations" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
]]></Content>
		<Date><![CDATA[2015-10-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[RHCSA]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Exam Objectives]]></Title>
		<Content><![CDATA[In this course we will cover every single RHCSA exam objects. Here's a copy the RHCSA exam objectives (as of October 2015). We have mapped each objective with tutorials that covers them:

<h4>Understand and use essential tools</h4>
[expand title="Access a shell prompt and issue commands with correct syntax" alt=" "]

-- <a href="http://codingbee.net/tutorials/rhcsa/rhcsa-the-bash-terminal/">RHCSA – The Bash Terminal</a>
[/expand]
<br />
[expand title="Use input-output redirection" alt=" "]

-- <a href="http://codingbee.net/tutorials/rhcsa/rhcsa-input-and-output-redirection/">RHCSA – Standard Outputs, Inputs, and Errors</a>
-- <a href="http://codingbee.net/tutorials/rhcsa/rhcsa-standard-inputoutputerror-redirection-and-piping/">Piping and Redirection</a>
[/expand]
<br />
[expand title="Use grep and regular expressions to analyze text" alt=" "]

-- <a href="http://codingbee.net/tutorials/rhcsa/rhcsa-the-grep-command-and-regular-expressions/">RHCSA - The grep command</a>
[/expand]
<br />
[expand title="Access remote systems using ssh" alt=" "]

-- <a href="http://codingbee.net/tutorials/rhcsa/rhcsa-the-bash-terminal/">RHCSA - The Bash Terminal</a>
-- <a href="http://codingbee.net/tutorials/rhcsa/rhcsa-secure-shell-ssh/">The Secure Shell (SSH)</a>
-- <a href="http://codingbee.net/tutorials/rhcsa/rhcsa-ssh-fingerprint-authentication/">RHCSA – SSH fingerprint authentication</a>
-- <a href="http://codingbee.net/tutorials/rhcsa/rhcsa-create-ssh-session-without-typing-a-password-publicprivate-key-authentication/">RHCSA – Create SSH session without typing a password (Public/Private Key Authentication)</a>
[/expand]
<br />
[expand title="Log in and switch users in multiuser targets" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Archive, compress, unpack, and uncompress files using tar, star, gzip, and bzip2" alt=" "]

-- <a href="http://codingbee.net/tutorials/rhcsa/rhcsa-compressing-files-and-folders-using-gzip-and-tar/">RHCSA – Compressing files and folders using gzip and tar</a>
[/expand]
<br />
[expand title="Create and edit text files" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Create, delete, copy, and move files and directories" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Create hard and soft links" alt=" "]

-- <a href="http://codingbee.net/tutorials/rhcsa/rhcsa-hard-and-soft-links/">RHCSA - Hard and Soft Links</a>
[/expand]
<br />
[expand title="List, set, and change standard ugo/rwx permissions" alt=" "]
-- <a href="http://codingbee.net/tutorials/rhcsa/rhcsa-understanding-usergroupother-permissions/">RHCSA - File and Folder Permissions</a>
-- <a href="http://codingbee.net/tutorials/rhcsa/rhcsa-special-file-and-folder-permissions/">RHCSA – Special File and Folder Permissions</a>
-- <a href="http://codingbee.net/tutorials/rhcsa/rhcsa-special-file-and-folder-permissions/">RHCSA – Access Control List (ACL)</a>
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Locate, read, and use system documentation including man, info, and files in /usr/share/doc" alt=" "]

-- <a href="http://codingbee.net/tutorials/rhcsa/rhcsa-access-linux-help-guides/">RHCSA – Access Linux help guides</a>
[/expand]







<h4>Operate running systems</h4>
[expand title="Boot, reboot, and shut down a system normally" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Boot systems into different targets manually" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Interrupt the boot process in order to gain access to a system" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Identify CPU/memory intensive processes, adjust process priority with renice, and kill processes" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Locate and interpret system log files and journals" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Access a virtual machine's console" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Start and stop virtual machines" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Start, stop, and check the status of network services" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Securely transfer files between systems" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]










<h4>Configure local storage</h4>
[expand title="List, create, delete partitions on MBR and GPT disks" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Create and remove physical volumes, assign physical volumes to volume groups, and create and delete logical volumes" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Configure systems to mount file systems at boot by Universally Unique ID (UUID) or label" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Add new partitions and logical volumes, and swap to a system non-destructively" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]



<h4>Create and configure file systems</h4>
[expand title="Create, mount, unmount, and use vfat, ext4, and xfs file systems" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Mount and unmount CIFS and NFS network file systems" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Extend existing logical volumes" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Create and configure set-GID directories for collaboration" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Create and manage Access Control Lists (ACLs)" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Diagnose and correct file permission problems" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]










<h4>Deploy, configure, and maintain systems</h4>
[expand title="Configure networking and hostname resolution statically or dynamically" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Schedule tasks using at and cron" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Start and stop services and configure services to start automatically at boot" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Configure systems to boot into a specific target automatically" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Install Red Hat Enterprise Linux automatically using Kickstart" alt=" "]
-- <a href="http://codingbee.net/tutorials/rhcsa/rhcsa-automating-rhel-installation-with-kickstart/">RHCSA - Automating RHEL installations using Kickstart</a>
-- <a href="http://codingbee.net/tutorials/rhcsa/rhcsa-how-to-create-a-kickstart-file/">How to create kickstart files</a>
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Configure a physical machine to host virtual guests" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Install Red Hat Enterprise Linux systems as virtual guests" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Configure systems to launch virtual machines at boot" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Configure network services to start automatically at boot" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Configure a system to use time services" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Install and update software packages from Red Hat Network, a remote repository, or from the local file system" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Update the kernel package appropriately to ensure a bootable system" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Modify the system bootloader" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]






<h4>Manage users and groups</h4>
[expand title="Create, delete, and modify local user accounts" alt=" "]

-- <a href="http://codingbee.net/tutorials/rhcsa/rhcsa-adding-managing-and-removing-users/">RHCSA – Adding, modifying, and deleting users</a>
[/expand]
<br />
[expand title="Change passwords and adjust password aging for local user accounts" alt=" "]

-- <a href="http://codingbee.net/tutorials/rhcsa/rhcsa-password-properties-passwd-and-chage-commands/">RHCSA – Password properties (passwd and chage commands)</a>

[/expand]
<br />
[expand title="Create, delete, and modify local groups and group memberships" alt=" "]

-- <a href="http://codingbee.net/tutorials/rhcsa/rhcsa-creating-modifying-and-deleting-groups/">RHCSA – Creating, modifying, and deleting groups</a>
[/expand]
<br />
[expand title="Configure a system to use an existing authentication service for user and group information" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]












<h4>Manage security</h4>
[expand title="Configure firewall settings using firewall-config, firewall-cmd, or iptables" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Configure key-based authentication for SSH" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Set enforcing and permissive modes for SELinux" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="List and identify SELinux file and process context" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Restore default file contexts" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Use boolean settings to modify system SELinux settings" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />
[expand title="Diagnose and address routine SELinux policy violations" alt=" "]
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
-- articlelink
[/expand]
<br />



The above objectives were taken from <a href="https://www.redhat.com/en/services/training/ex200-red-hat-certified-system-administrator-rhcsa-exam">Red Hat</a> as of October 2015.]]></Content>
		<Date><![CDATA[2015-10-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - How does SELinux work?]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What are objectives in the world of SELinux?"]
Apply a SELinux label to absolutely everything, e.g. files, folders, processes, user accounts...etc. 
[/toggle]
[toggle title="What are the SELinux labels called?"]
- SELinux contexts   
# or
- Security contexts
[/toggle]
[toggle title="What is the command to displays the SELinux contexts of the file /tmp/testfile.txt?"]
$ ls -lZ testfile.txt
[/toggle]
[toggle title="What is the format of an SELinux context?"]
An SELinux context comprised of 4 colon delimited strings. Each string is referred to as a "security attribute"
[/toggle]
[toggle title="What gives SELinux Contexts meaning?"]
The Policy Type
[/toggle]
[toggle title="What is a policy type?"]
You can think of it as really big text book, and this book contains an entry for each security attribute, followed by a list of other security attributes that it is allowed to access.
[/toggle]
[toggle title="What is each entry in the policy type referred to as?"]
- policy rule    
# or simply: 
- policy
[/toggle]
[toggle title="What is the command to view 'allow' policy rules for the security attribute, 'httpd_content_type'?"]
$ sesearch <span style="letter-spacing:1.5px">-</span>-allow | grep httpd_content_type
# this outputs:
allow httpd_t httpd_content_type : file { ioctl read getattr lock open } ;
allow httpd_t httpd_content_type : dir { getattr search open } ;
allow httpd_suexec_t httpd_content_type : dir { getattr search open } ;
allow httpd_t httpd_content_type : file { ioctl read getattr lock open } ;
allow httpd_t httpd_content_type : dir { ioctl read getattr lock search open } ;
[/toggle]
[toggle title="What does the first line of the output means?"]
Allow an object of the type 'httpd_t' to access other objects of the type 'httpd_content_type', as long as these objects are 'file' objects. Furthermore they are allowed to have { ioctl read getattr lock open } types of access. 
[/toggle]
[/accordion]

<hr/>


SELinux operates at the kernel level. This means that when one objects (e.g. a process) attempts to access another object (e.g. config file), the kernel checks the relevant "policy" for the given objects to see if the access should be permitted, the kernel then either grants or deny access. 

The heart of SELinux comprises of 2 parts:

<ul>
	<li>Security Contexts</li>
	<li>Policy Type</li>
</ul>



<h2>Security Contexts</h2>
SELinux assigns a label to every single "object" on your machine. An "object" can be absolutely anything, including:

<ul>
	<li>user accounts</li>
	<li>user groups</li>
	<li>files</li>
	<li>folders</li>
	<li>processes</li>
</ul>



These labels are referred to as <strong>SELinux Contexts</strong>, aka <strong>security contexts</strong>. 

A lot of commands have a "-Z" option for displaying security contexts. For example the ls command has a -Z option:


<pre>
$ touch testfile.txt
$ ls -lZ testfile.txt
-rw-r--r--. root root <strong>unconfined_u:object_r:admin_home_t:s0</strong> testfile.txt
</pre> 

Notice that SELinux automatically assigns a security context to newly created objects.  

As you can see each SELinux context is made up of (colon delimited parts) 4 parts, which in this case are:

<ul>
	<li>unconfined_u</li>
	<li>object_r</li>
	<li>admin_home_t</li>
	<li>s0</li>
</ul>


These items are referred to as "Security Attributes". We will cover more about <a href="http://codingbee.net/tutorials/rhcsa/rhcsa-selinux-security-contexts-aka-labels/">Security contexts and Security attributes</a> later. 

These Security Contexts on their own are useless, and it is the "policy types" that gives them meaning.


<h2>Policy Type</h2>
You can think of a policy type as really big text book, and this book contains an entry for each security attribute, followed by a list of other security attributes that it is permitted to access. 

<pre>
$ sesearch --all | wc -l
ERROR: Cannot get avrules: Neverallow rules requested but not available
70620
[root@puppetmaster ~]# sesearch --all | head
ERROR: Cannot get avrules: Neverallow rules requested but not available

Found 17669 semantic te rules:
   type_transition sosreport_t sssd_initrc_exec_t : process initrc_t;
   type_transition certmonger_unconfined_t sssd_initrc_exec_t : process initrc_t;
   type_transition piranha_pulse_t prelude_audisp_exec_t : process prelude_audisp_t;
   type_transition authconfig_t NetworkManager_initrc_exec_t : process initrc_t;
   type_transition dbadm_sudo_t sudo_exec_t : process dbadm_t;
   type_transition anaconda_t sssd_initrc_exec_t : process initrc_t;
   type_transition ricci_modservice_t ddclient_initrc_exec_t : process initrc_t;
   type_transition kdumpctl_t lsassd_exec_t : process lsassd_t;

</pre>

<provide an example>


Each entry in the policy type is referred to as a <strong>policy rule</strong>, or just <strong>policy</strong>. You can modify these mapping at a higher level by configuring <a href="http://codingbee.net/tutorials/rhcsa/rhcsa-selinux-booleans/">sebooleans</a>, we'll cover this later.

There's also 3 different policy types to choose from, (i.e. three different books). We'll cover these 3 <a href="http://codingbee.net/tutorials/rhcsa/rhcsa-selinux-policy-types-and-selinux-attributes/">policy types</a> later. There are several types of of policy rules, which you learn about via sesearch's man page. One of the type is the "allow" policy rule.      


Here are several allow policy rules relating to httpd_content_type:

<pre>
$ sesearch --allow | grep httpd_content_type
<strong>   allow httpd_t httpd_content_type : file { ioctl read getattr lock open } ;
</strong>   allow httpd_t httpd_content_type : dir { getattr search open } ;
   allow httpd_suexec_t httpd_content_type : dir { getattr search open } ;
   allow httpd_t httpd_content_type : file { ioctl read getattr lock open } ;
   allow httpd_t httpd_content_type : dir { ioctl read getattr lock search open } ;
   allow httpd_t httpd_content_type : lnk_file { read getattr } ;

</pre>

Let's break down the very first rules in this list. This rule says that an object of the type <strong>httpd_t</strong> is allowed to access an object of the type <strong>httpd_content_type</strong>, as long as that object is a <strong>file</strong>. Further more it can only have <strong>{ ioctl read getattr lock open }</strong> types of access.

All "allow" policy rules are structured like this.  






]]></Content>
		<Date><![CDATA[2015-10-14]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Interactive shells and login shells]]></Title>
		<Content><![CDATA[https://app.pluralsight.com/library/courses/lfcs-linux-user-group-management/description

<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What are three possible commands to start a full login-shell for the user called david?"]
$ su - david     # note: option l is implicitedly assumed here.   
# or 
$ su -l david
# or 
$ ssh david@centos7machine

[/toggle]
[toggle title="Which shell scripts gets triggered as part of this command?"]
-  /etc/profile        # which in turn calls all scripts inside /etc/profile.d folder, and then it calls:
-  ~/.bash_profile    # (If for any reason this doesn't exist, then it will look for ~/.profile) which in turn calls:
-  ~/.bashrc   # which in turn calls:
-  /etc/bashrc   

[/toggle]
[toggle title="While logged in as the root user, What is the command to start a non-login shell for the user called david?"]
$ su david
# or:
$ bash
[/toggle]
[toggle title="Which shell scripts gets triggered as part of this command?"]
-  ~/.bashrc   # which in turn calls:
-  /etc/bashrc
Note: all the profile related scripts are ignored. Hence privilege is set but environment variable are unset.    
[/toggle]
[toggle title="What is the command to view info about the package 'nmap'?"]
$ yum info nmap

[/toggle]
[toggle title="What is the command to install the package 'nmap'?"]
$ yum install nmap

[/toggle]
[toggle title="What are the 3 commands to list all packages, installed packages, and available packages?"]
$ yum list all
$ yum list installed
$ yum list available
[/toggle]
[toggle title="What is the command to install all the latest packages including security updates?"]
$ yum update
[/toggle]
[toggle title="What is the command to install the 'tree' package from a repo recalled 'epel' repo rather than any other repo?"]
$ yum --enablerepo=epel install tree
[/toggle]
[toggle title="What is the command to list all package groups?"]
$ yum grouplist

[/toggle]
[toggle title="What is the command to install a the tree.rpm file which is in the current directory?"]
$ yum localinstall tree.rpm

[/toggle]
[/accordion]

<hr />











Whenever a new user is created, by default, a set of (usually hidden) config files will get copied to the user’s new home folder. These files are actually duplicate of all the files contained in the following directory:

/etc/skel

i.e. :

$ ls -l /etc/skel/

total 6

-rw-r--r--   1 root     sys              136 Nov 9 2006 local.cshrc

-rw-r--r--   1 root     sys              157 Nov 9 2006 local.login

-rw-r--r--   1 root     sys              174 Nov 9 2006 local.profile

the useradd commands automatically copies all the content from this folder and places it in the user’s home directory.

when a user logs into a RHEL machine (using username and password), the following scripts are executed automatically (in this order):

/etc/profile       # (this sets up user env variables, e.g. path, hostname, histsize….etc)…this script will also trigger:

/etc/profile.d     # (directory containing lots of other files.

{user’s home folder}/.bash_profile # can be used to add your own stuff including overriding what was set in the /etc/profile script.

{user’s home folder}/.bashrc     # (used for setting local variables e.g. the PS1 variables)

&nbsp;

However these does get executed when a non-login shell is generated. a non-login shell is a shell that gets opened buit without getting a username/password prompt. Here are some ways to start a non-login shell:

-         Applications=&gt;system tools=&gt;terminal

-        When you start a shell script, linux will automatically create a new shell for that script to run in, and this new shell is a non-login shell.

-         At the command line, type ksh, followed by bash.

In a non-login shell the following scripts are run:

&nbsp;

1. {user’s home folder}/bashrc,

2. /etc/bashrc,

3. /etc/profile.d (have a look through these script to familiarize yourself)
]]></Content>
		<Date><![CDATA[2015-10-19]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Redhat|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Automounting remotely shared folders (NFS and CIFS)]]></Title>
		<Content><![CDATA[<h2>Overview of automounting NFS and CIFS shares</h2>

Mounting CIFS and NFS shares using the mount command, won't survive a reboot. Luckily there are three different ways to auto-mount CIFS and NFS shares when the machine boots up:

<ul>
	<li>Adding entries to /etc/fstab</li>
	<li>via autofs</li>
	<li>via systemd</li>
</ul>

By the end of this article you should be able to answer the following questions:
 
[accordion]
[toggle title="01. How many different ways can you automount, and what are they?"]
There are 3 ways, the are:
- Adding entries to /etc/fstab
- via autofs
- via systemd
[/toggle]
[toggle title="02. What is the fstab entry you need to add to mount the The-Server's, NFS shared folder, '/nfs/Shared-Folder' onto the mountpoint '/nfs/The-Server/Shared-Folder'?"]
<span style='font-size:12px'>The-Server:/nfs/Shared-Folder   /nfs/The-Server/Shared-Folder   nfs   defaults   0   0</span>[/toggle]
[toggle title="03. What is the fstab entry  you need to add to mount the The-Server's, CIFS shared folder, 'Shared-Folder' onto the mountpoint '/cifs/The-Server/Shared-Folder', where the user credentials for accessing the cifs is username=vagrant and password=admin?"]
<span style='font-size:12px'><mark>//</mark>The-Server/Shared-Folder   /cifs/The-Server/Shared-Folder   cifs   username=vagrant,password=admin  0  0</span>
# remember to use the "//" syntax again. 
[/toggle]
[toggle title="04. What is the command to check that your fstab entries have worked?"]
$ mount -a
[/toggle]
[toggle title="05. What are 2 commands to check that your nfs/cifs have been successfully mounted?"]
$ mount
# or 
$ df -h
[/toggle]
[toggle title="06. What is the command to install the automatic mounting software?"]
$ yum install autofs
[/toggle]
[toggle title="07. Then what do you need to do?"]
# start the service
$ systemctl start autofs
[/toggle]
[toggle title="08. Where is this package's main config file located?"]
/etc/auto.master
[/toggle]
[toggle title="09. What entry do you need to add to this to indicate that the folder /nfs/The-Server will host remote folders, and this folder's configuration will be stored in our custom config file, which will be /etc/autofs/nfs/The-Server.conf?"]
/nfs/The-Server   /etc/autofs/nfs/The-Server.conf
[/toggle]
[toggle title="10. What entry do you need to add to /etc/autofs/nfs/The-Server.conf, in order to automount the nfs share, /nfs/Shared-Folder, that is being shared by 'The-Server' onto the mountpoint /nfs/The-Server/A-Shared-Folder?"]
A-Shared-Folder  -fstype=nfs,rw  The-Server:/nfs/Shared-Folder
[/toggle]
[toggle title="11. Then what do you need to do?"]
# restart the service
$ systemctl restart autofs
[/toggle]
[toggle title="12. Then what do you need to do?"]
# cd into the mount point
$ cd /nfs/The-Server/A-Shared-Folder
[/toggle]
[toggle title="13. What is the command to check this has worked?"]
$ mount 
# or 
$ df -h
[/toggle]
[toggle title="14. What entry do you need to add to /etc/auto.master to indicate that the folder /cifs/The-Server will host remote folders, and this folder's configuration will be stored in our custom config file, which will be /etc/autofs/cifs/The-Server.conf?"]
/cifs/The-Server /etc/autofs/cifs/The-Server.conf
[/toggle]
[toggle title="15. What entry do you need to add to /etc/autofs/cifs/The-Server.conf, in order to automount the cif share, cifshare1, that is being shared by 'The-Server' onto the mountpoint /cifs/The-Server/Shared-Folder?"]<span style="font-size:12px">Shared-Folder  -fstype=cifs,rw,username=vagrant,password=admin  <mark>://</mark>The-Server/cifshare1
# note don't forget to include the "//", also notice that this time you also need to prefix the colon as well ":". Therefore you now end up with "://". It looks wrong but that's how it is.</span> 
[/toggle]
[toggle title="16. Then what do you need to do?"]
# restart the service
$ systemctl restart autofs
[/toggle]
[toggle title="17. Then what do you need to do?"]
# cd into the mount point
$ cd /cifs/The-Server/Shared-Folder
[/toggle]
[toggle title="18. What is the command to check this has worked?"]
$ mount 
# or 
$ df -h
[/toggle]
[toggle title="What is the command - need to add questions for the systemd approach?"]


[/toggle]
[toggle title="What is the command?"]


[/toggle]
[toggle title="What is the command?"]


[/toggle]
[toggle title="What is the command?"]


[/toggle]
[toggle title="What is the command?"]


[/toggle]
[/accordion]
<hr/>

<h2>Automounting using fstab</h2>

Quick recap, here's how we manually mounted the shared folders from our previous article's examples for nfs and cifs respectively:

<pre>
<span style='font-size:12px'>$ mount -t nfs The-Server:/nfs/Shared-Folder /nfs/The-Server/Shared-Folder
$ mount -t cifs -o user=vagrant,password=admin //The-Server/Shared-Folder /cifs/The-Server/Shared-Folder</span>
</pre>


<h3>Automount NFS via the fstab file</h3>

Following on from the earlier example. 

To automatically mount an export during boot time, you need to add the following entry into the <code>/etc/fstab</code> config file.

<pre>
<span style='font-size:12px'>The-Server:/nfs/Shared-Folder /nfs/The-Server/Shared-Folder nfs defaults 0 0</span>
</pre>

To check that we haven't made a mistake, we should run the following to confirm we have done everything correctly:


<pre>
$ mount -a
</pre>


<h3>Automount CIFS using fstab</h3>


Now here's the entry we need to make in the <code>/etc/fstab</code> file for this example:


<pre>
<span style='font-size:12px'>//The-Server/Shared-Folder  /cifs/The-Server/Shared-Folder  cifs  username=vagrant,password=admin  0 0</span>
</pre>


However to ensure you haven't made a mistake. Check that the CIFS share is currently mounted. And then run:

<pre>
$ mount -a
</pre>

Then check if the CIFS share is now mounted. If it has then your fstab cif entry is correct. 

Note, in this approach, we have added credentials as plain texts in the fstab file. To make it a bit more secure, you can take this approach instead:

<pre>
<span style='font-size:12px'>//The-Server/Shared-Folder  /cifs/The-Server/Shared-Folder  cifs  credentials=~/.secret.txt  0 0</span>
</pre>

Where the credentials files contains:


<pre>$ cat /root/.secret.txt
username=vagrant
password=admin
</pre>


<h2>Automounting using autofs</h2>
Autofs is a more intelligent alternative to using fstab. That's because Autofs has 2 main advantages:

<ol>
	<li>mounted remote resources are automatically unmounted after a period of inactivity.</li>
	<li>unmounted remote resources are automatically mounted again when you try to access the mount-point.</li>
	<li>You don't have to create the mount point before hand, only the mountpoint's parent directories</li>

</ol>

To be able to do automounting, you need to first install the automounting package which is called autofs:

<pre>
$ yum install autofs
</pre>


The main autofs config file is auto.master:


<pre>
$ cat /etc/auto.master
#
# Sample auto.master file
# This is a 'master' automounter map and it has the following format:
# mount-point [map-type[,format]:]map [options]
# For details of the format look at auto.master(5).
#
/misc   /etc/auto.misc
#
# NOTE: mounts done from a hosts map will be mounted with the
#       "nosuid" and "nodev" options unless the "suid" and "dev"
#       options are explicitly given.
#
/net    -hosts
#
# Include /etc/auto.master.d/*.autofs
# The included files must conform to the format of this file.
#
+dir:/etc/auto.master.d
#
# Include central master map if it can be found using
# nsswitch sources.
#
# Note that if there are entries for /net or /misc (as
# above) in the included master map any keys that are the
# same will not be seen as the first read key seen takes
# precedence.
#
+auto.master
</pre>

Now we'll take a look at how to configure autofs to automount nfs and cifs shares, starting with nfs.

<h3>Automount NFS via autofs</h3>
The auto.master file itself doesn't contain mounting related info. Instead it's a file that maps each mountpoint's parent folder along with location of a custom config file that houses actual mounting info. That's why the entries in this file is in the form of 2 column entries. Therefore for this example, we added the following line to the /etc/auto.master file:

<pre>
/nfs/The-Server   /etc/autofs/nfs/The-Server.conf
</pre>


Note: The "/nfs/The-Server" folder isn't a mount point itself, but houses mount points. This folder is therefore sometime srefered to as a "automount" folder, i.e. the mount point's parent folder.   



The second entry could be anything to your liking. However it's best practice to try to make it meaningful. Now let's create our custom file:

<pre>
$ mkdir -p /etc/autofs/nfs
$ touch /etc/autofs/nfs/The-Server.conf
</pre>

Now in this file custom file, we add the following:

<pre>
$ cat /etc/autofs/nfs/The-Server.conf
A-Shared-Folder  -fstype=nfs,rw  The-Server:/nfs/Shared-Folder
</pre>

Here we just specify the mountpoint folder, and not the full path. In this example our mountpoint's name is "A-Shared-Folder"

Now we restart autofs service:

<pre>
$ systemctl enable autofs
ln -s '/usr/lib/systemd/system/autofs.service' '/etc/systemd/system/multi-user.target.wants/autofs.service'
$ systemctl start autofs
</pre>

Now let's check our mount point. 
 
<pre>
$ pwd
/nfs/The-Server
$ ls -l
total 0
$ cd A-Shared-Folder
$ ls -l
total 0
drwxr-xr-x. 2 root root 26 Oct 24 20:14 subfolder
-rw-r--r--. 1 root root  0 Oct 24 20:14 testfile1.txt
</pre>

Notice how we can magically cd into the mountpoint, even though it didn't originally exist. 


At first it seems a bit unusual, the structure of the auto.master file being a mapping to a mountpoint's parent and it's corresponding config file. This is designed for easier scalability. For example, let's say we have 2 nfs shares available on The-Server, instead of one:


<pre>
$ showmount -e The-Server
Export list for The-Server:
/nfs/Shared-Folder2 *             # this is a new nfs share.  
/nfs/Shared-Folder  *
</pre>


Then automount this, we don't need to edit auto.master. Instead we add another entry to our custom file:

<pre>
$ cat /etc/autofs/nfs/The-Server.conf
A-Shared-Folder  -fstype=nfs,rw  The-Server:/nfs/Shared-Folder
Another-Shared-Folder -fstype=nfs,rw  The-Server:/nfs/Shared-Folder2
</pre>

As you can see, the mountpoint's parent folder defined in /etc/auto.master actually becomes the home of multiple possible NFS mountpoints. In our case both NFS shares originated from the same server, we could have had different nfs servers, in which case it would instead be better for a different parent mount folder name, e.g. just "/nfs".  


Now let's test this out:


<pre>
$ pwd
/nfs/The-Server
$ ls -l
total 0
$ cd A-Shared-Folder
$ ls
subfolder  testfile1.txt
$ pwd
/nfs/The-Server/A-Shared-Folder
$ cd ..
$ ls
A-Shared-Folder
$ cd Another-Shared-Folder
$ pwd
/nfs/The-Server/Another-Shared-Folder
$ ls -l
total 4
-rw-r--r--. 1 root root 6 Oct 28 21:05 yet-more-stuff.txt
$ cd ..
$ pwd
/nfs/The-Server
$ ls -l
total 0
drwxr-xr-x. 2 root root 31 Oct 28 21:05 Another-Shared-Folder
drwxr-xr-x. 3 root root 42 Oct 24 20:14 A-Shared-Folder

</pre>


<h3>Automount CIFS via autofs</h3>

Quick recap:

<pre>
$ smbclient -L The-Server --user vagrant
Enter vagrant's password:
Domain=[MYGROUP] OS=[Unix] Server=[Samba 4.1.12]

        Sharename       Type      Comment
        ---------       ----      -------
        Shared-Folder   Disk
        IPC$            IPC       IPC Service (Samba Server Version 4.1.12)
        vagrant         Disk      Home Directories
Domain=[MYGROUP] OS=[Unix] Server=[Samba 4.1.12]

        Server               Comment
        ---------            -------

        Workgroup            Master
        ---------            -------
</pre>

In a similar fashion to nfs, we first add the following line to /etc/auto.master:

<pre>
/cifs/The-Server /etc/autofs/cifs/The-Server.conf
</pre> 

And in the custom config file we add:

<pre>
Shared-Folder  -fstype=cifs,rw,username=vagrant,password=admin  ://The-Server/Shared-Folder
</pre>


Now, let's test if it this worked:



<pre>
$ pwd
/cifs/The-Server
$ ls -l
total 0
$ cd Shared-Folder
$ pwd
/cifs/The-Server/Shared-Folder
$ ls -l
total 0
-rwxrwxrwx. 1 root    root    0 Oct 24 22:31 hello-world-cifs.txt
drwxrwxrwx. 2 root    root    0 Oct 24 22:31 subfolder
-rw-r--r--. 1 vagrant vagrant 0 Oct 28 21:51 testfile.txt

</pre>

Success!

<h2>Automounting using systemd</h2>

This is relatively new approach. With this approach you can't have any "-" characters in the full path of your mount point. Then it's just a case of creating the .mount file, and using systemctl to start+enable your new .mount unit. 


<h3>Automounting NFS using systemd</h3>


First we create the mountpoint:

<pre>
$ mkdir -p /nfs/TheServer/SharedFolder
</pre>


Now we create the following unit file:



<pre>
$ cat /etc/systemd/system/nfs-TheServer-SharedFolder.mount
[Unit]
Description=NfS share from The-Server

[Mount]
What=The-Server:/nfs/Shared-Folder
Where=/nfs/TheServer/SharedFolder
Type=nfs
Options=rw

[Install]
WantedBy=multi-user.target
</pre>


Finally we start+enable this mount unit:

<pre>
$ systemctl start nfs-TheServer-SharedFolder.mount
$ systemctl enable nfs-TheServer-SharedFolder.mount
$ systemctl status nfs-TheServer-SharedFolder.mount
nfs-TheServer-SharedFolder.mount - Nfs share form The-Server
   Loaded: loaded (/etc/systemd/system/nfs-TheServer-SharedFolder.mount; enabled)
   Active: active (mounted) since Wed 2015-10-28 23:14:23 GMT; 2min 13s ago
    Where: /nfs/TheServer/SharedFolder
     What: The-Server:/nfs/Shared-Folder

Oct 28 23:14:23 puppetmaster.local systemd[1]: Mounting Nfs share form The-Server...
Oct 28 23:14:23 puppetmaster.local systemd[1]: Mounted Nfs share form The-Server.
Oct 28 23:16:29 puppetmaster.local systemd[1]: Mounted Nfs share form The-Server.
</pre>


Now, we can go in to check it has worked:

<pre>
$ cd /nfs/TheServer/SharedFolder
$ ls -l
total 0
drwxrwxrwx. 2 root      root      26 Oct 24 20:14 subfolder
-rw-r--r--. 1 nfsnobody nfsnobody  0 Oct 28 23:02 testfile
-rwxrwxrwx. 1 root      root       0 Oct 24 20:14 testfile1.txt
</pre>
 
Also to confirm it has worked, we can do:

<pre>
$ df -h
Filesystem                            Size  Used Avail Use% Mounted on
/dev/mapper/centos_puppetmaster-root   38G  4.7G   33G  13% /
devtmpfs                              487M     0  487M   0% /dev
tmpfs                                 497M   80K  497M   1% /dev/shm
tmpfs                                 497M  6.9M  490M   2% /run
tmpfs                                 497M   16K  497M   1% /sys/fs/cgroup
The-Server:/nfs/Shared-Folder          38G  3.9G   34G  11% /nfs/TheServer/SharedFolder
$ mount | grep The-Server
/etc/autofs/nfs/The-Server.conf on /nfs/The-Server type autofs (rw,relatime,fd=-1,pgrp=15130,timeout=300,minproto=5,maxproto=5,indirect)
The-Server:/nfs/Shared-Folder on /nfs/TheServer/SharedFolder type nfs4 (rw,relatime,vers=4.0,rsize=131072,wsize=131072,namlen=255,hard,proto=tcp,port=0,timeo=600,retrans=2,sec=sys,clientaddr=192.168.50.10,local_lock=none,addr=192.168.50.11)

</pre>


<h3>Automounting CIFS using systemd</h3>

First we create the mount point: 

<pre>
$ mkdir -p /cifs/TheServer/SharedFolder
</pre>


Now we create the mount.unit:



<pre>
$ cat /etc/systemd/system/cifs-TheServer-SharedFolder.mount
[Unit]
Description=CIFS share from The-Server

[Mount]
What=//The-Server/Shared-Folder
Where=/cifs/TheServer/SharedFolder
Type=cifs
Options=rw,username=vagrant,password=admin

[Install]
WantedBy=multi-user.target

</pre>


Once again you name this unit after the full mountpath with the "/" replaced by "-". 


Next we start+enable it:


<pre>
$ systemctl start cifs-TheServer-SharedFolder.mount
$ systemctl enable cifs-TheServer-SharedFolder.mount
ln -s '/etc/systemd/system/cifs-TheServer-SharedFolder.mount' '/etc/systemd/system/multi-user.target.wants/cifs-TheServer-SharedFolder.mount'
$ systemctl status cifs-TheServer-SharedFolder.mount
cifs-TheServer-SharedFolder.mount - CIFS share from The-Server
   Loaded: loaded (/etc/systemd/system/cifs-TheServer-SharedFolder.mount; enabled)
   Active: active (mounted) since Wed 2015-10-28 23:31:07 GMT; 5min ago
    Where: /cifs/TheServer/SharedFolder
     What: //The-Server/Shared-Folder

Oct 28 23:31:07 puppetmaster.local systemd[1]: Mounting CIFS share from The-Server...
Oct 28 23:31:07 puppetmaster.local systemd[1]: Mounted CIFS share from The-Server.
Oct 28 23:36:36 puppetmaster.local systemd[1]: Mounted CIFS share from The-Server.
</pre>

Now we confirm that it is working:

<pre>
$ cd /cifs/TheServer/SharedFolder
$ ls -l
total 0
-rwxrwxrwx. 1 root    root    0 Oct 24 22:31 hello-world-cifs.txt
drwxrwxrwx. 2 root    root    0 Oct 24 22:31 subfolder
-rw-r--r--. 1 vagrant vagrant 0 Oct 28 21:51 testfile.txt
$ df -h
Filesystem                            Size  Used Avail Use% Mounted on
/dev/mapper/centos_puppetmaster-root   38G  4.7G   33G  13% /
devtmpfs                              487M     0  487M   0% /dev
tmpfs                                 497M   80K  497M   1% /dev/shm
tmpfs                                 497M  6.9M  490M   2% /run
tmpfs                                 497M   16K  497M   1% /sys/fs/cgroup
The-Server:/nfs/Shared-Folder          38G  3.9G   34G  11% /nfs/TheServer/SharedFolder
//The-Server/Shared-Folder             38G  3.9G   34G  11% /cifs/TheServer/SharedFolder
$ mount | grep cifs
//The-Server/Shared-Folder on /cifs/TheServer/SharedFolder type cifs (rw,relatime,vers=1.0,cache=strict,username=vagrant,domain=PUPPETAGENT01,uid=0,noforceuid,gid=0,noforcegid,addr=192.168.50.11,unix,posixpaths,serverino,acl,rsize=1048576,wsize=65536,actimeo=1)
</pre>


]]></Content>
		<Date><![CDATA[2015-10-24]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - NFS and CIFS (remote folders)]]></Title>
		<Content><![CDATA[In Linux you can share a folder from one Linux machine with another Linux machine, over a network. There are 2 different technologies that allows you to do this, NFS and CIFS. In the next few articles we are going to show you how to use these technology. Along the way we will compare and contrast them as we go, since NFS and CIFS are both designed the same kind of job. .
<h2>Network File System (NFS)</h2>
It is also a useful place to store backups. Because if your machine suffers a massive hdd failure, then the backups have not been lost because they actually reside on another machine.

This article doesn't cover how to <a href="http://codingbee.net/tutorials/rhce/rhce-set-up-an-nfs-server/">setup an NFS server</a>, since that is covered in the RHCE course. Instead we look at how we can connect to an existing shared folder (and make it look like a normal folder on the local machine) that is on a remote NFS server.

NFS has a couple of limitations:
<ul>
	<li>You can't share folders over the internet. NFS can only be used for sharing folders across a local network.</li>
	<li>You can't share folders across platforms. For example you can't share a folder that is on a Microsoft windows machine with a RHEL machine, or vice version.</li>
</ul>
<h2>CIFS (aka Samba)</h2>
NFS has 2 big limitations as outlined above. However CIFS (Common Internet File System) doesn't have these limitation. That means that with CIFS you can share folder across the internet, as well as over a local network. CIFS is also cross platform. This means that you can share a folder that is on a Microsoft windows machine, with a RHEL machine, or vice version.

This makes CIFS a more powerful alternative to NFS.

This article doesn't cover how to <a href="http://codingbee.net/tutorials/rhce/rhce-set-up-a-cifs-server/">Setup a Samba server</a>, since that is covered in the RHCE course. Instead we look at how we can access an existing shared CIFS folder.
<h2>The Scenario</h2>
To help understand how NFS and CIFS work, we’ll refer to a scenario throughout the next few articles to help explain what’s going on. In this scenario we have the 2 Linux machines with the following hostnames:

<strong>The-Server</strong> – This machine has 2 folders available for sharing. The first folder is an "NFS share", and the path to this folder is ...... The second folder is a "CIF share". We’ll refer to this folder as the “Share-Folder”. Aside from these 2 folders, we can’t access this machine in any other way, i.e. we can’t access this machine via ssh, scp, ftp, http,….etc.
<strong>The-Client</strong> – In this scenario we are sitting in front of this machine. In the next few articles we’ll cover how to configure this machine so that it can access The-Server’s "NFS share" and "CIFS share". We can configure The-Client in such a way that the Share-Folder appears as an ordinary folder on the The-Client machine. Therefore to access the Shared-Folder, you simply navigate into it using just the <code>cd</code> command. Any files you create in the Shared-Folder will actually end up being stored on The-Server and not on The-Client.

In the next few articles we will be running a few commands, but these commands will be run on the The-Client machine only. ]]></Content>
		<Date><![CDATA[2015-10-24]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[yum error - Public key for *.rpm is not installed]]></Title>
		<Content><![CDATA[the dirty way aroung this is doing:
<pre>$ yum install {package-name} --nogpgcheck</pre>
But best practice is:

first cd to:
<pre>$ cd /etc/pki/rpm-gpg</pre>
Then do a wget command, here's an example:
<pre>$ wget -O http://yum.theforeman.org/releases/1.8/RPM-GPG-KEY-foreman</pre>
then import it into rpm db like this:
<pre>$ rpm --import RPM-GPG-KEY-foreman</pre>
You can list all the imported keys like this:
<pre>$ rpm -qa gpg*</pre>
then to check it have worked, you can do:
<pre>$ rpm -qi gpg-pubkey-225c9b71-54fda121
 Name : gpg-pubkey Relocations: (not relocatable)
 Version : 225c9b71 Vendor: (none)
 Release : 54fda121 Build Date: Wed 28 Oct 2015 11:41:38 AM GMT
 Install Date: Wed 28 Oct 2015 11:41:38 AM GMT Build Host: localhost
 Group : Public Keys Source RPM: (none)
 Size : 0 License: pubkey
 Signature : (none)
 Summary : gpg(Foreman Release Signing Key (1.8) &lt;packages@theforeman.org&gt;)
 Description :
 -----BEGIN PGP PUBLIC KEY BLOCK-----</pre>]]></Content>
		<Date><![CDATA[2015-10-28]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Centos|RHEL|yum]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Bash - If-Statement based on exit codes]]></Title>
		<Content><![CDATA[
<pre>
{some command}
if [ $? -eq 0 ]; then
  ...
else
  ...
fi
</pre>
]]></Content>
		<Date><![CDATA[2015-10-29]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Services that makes use of a network]]></Title>
		<Content><![CDATA[Here we'll cover network based services. 

We have already come across sshd, which is already set up out of the box, but now we'll cover other onese:


httpd
vnc
ntp]]></Content>
		<Date><![CDATA[2015-11-01]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - The ss utility]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What is the command to view all running and established connections?"]
$ ss -a
[/toggle]
[toggle title="What is the command to view all running and established tcp connections?"]
$ ss -at
[/toggle]
[toggle title="What is the command to view all running and established tcp connections, and displays port numbers in instead of service names?"]
ss -atn
[/toggle]
[toggle title="What does the output's right column show?"]
The right column actually shows internal mapping ports. 
[/toggle]
[/accordion]
<hr/>



The ss utility is the successor to the netstat utility. It is a great tool for troubleshooting networking related issues. 


Here's how to list all listening and established connections:

<pre>
$ ss -a
</pre>

As above, but filtered to only display tcp sockets:

<pre>
$ ss -at
</pre>
Note: you can use "u"

The above shows the service name (e.g. "ssh") instead of the port number (e.g. 22). If you want to display port number, then do:


<pre>
ss -atn
</pre>

Note it is displayed in the middle column. The right column actually shows internal mapping ports. 

Here's the help summary:



<pre>
$ ss --help
Usage: ss [ OPTIONS ]
       ss [ OPTIONS ] [ FILTER ]
   -h, --help           this message
   -V, --version        output version information
   -n, --numeric        don't resolve service names
   -r, --resolve       resolve host names
   -a, --all            display all sockets
   -l, --listening      display listening sockets
   -o, --options       show timer information
   -e, --extended      show detailed socket information
   -m, --memory        show socket memory usage
   -p, --processes      show process using socket
   -i, --info           show internal TCP information
   -s, --summary        show socket usage summary
   -b, --bpf           show bpf filter socket information

   -4, --ipv4          display only IP version 4 sockets
   -6, --ipv6          display only IP version 6 sockets
   -0, --packet display PACKET sockets
   -t, --tcp            display only TCP sockets
   -u, --udp            display only UDP sockets
   -d, --dccp           display only DCCP sockets
   -w, --raw            display only RAW sockets
   -x, --unix           display only Unix domain sockets
   -f, --family=FAMILY display sockets of type FAMILY

   -A, --query=QUERY, --socket=QUERY
       QUERY := {all|inet|tcp|udp|raw|unix|packet|netlink}[,QUERY]

   -D, --diag=FILE     Dump raw information about TCP sockets to FILE
   -F, --filter=FILE   read filter information from FILE
       FILTER := [ state TCP-STATE ] [ EXPRESSION ]
</pre>




]]></Content>
		<Date><![CDATA[2015-11-01]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Package management with yum and rpm]]></Title>
		<Content><![CDATA[Yum is a wrapper around rpm. It can handle rpm dependencies. ]]></Content>
		<Date><![CDATA[2015-11-01]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Job Scheduling]]></Title>
		<Content><![CDATA[There will be times when you will want to schedule some task to automatically occur in the future. There are 2 possible scenrario. 

<strong>Scenario 1</strong> - In the first scenario you might want to run tasks periodically, e.g. once a month. In which case the best way to achieve is by setting up a cron job. 

<strong>Scenario 2</strong> - The second scenario is that you want to schedule a one-off task to take place at some point in the future, e.g. perform a patch installation at 2am on early Sunday morning. The best solution to achieve this is by using the "at" utility.

In the next few articles we will cover cron (in its various forms) as well as the "at" utility.]]></Content>
		<Date><![CDATA[2015-11-03]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - The emergency.target]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="How do you switch to the 'emergency.target' via the grub menu?"]
1. (Re)boot the machine
2. Wait for the grub menu to appear, then hit "e",
3. scroll down to the "linux16" line then press the 'end' key
4. type the following:
systemd.unit=emergency.target
5. press ctrl+x
[/toggle]
[toggle title="What is the command to make the root directory wrieable?"]
$ mount -o rw,remount /
[/toggle]
[/accordion]
<hr/>




If you have problems booting up your machine, then you can boot into the emergency.target. In this target you can explore your filesystem for possible issues. 

In this filesystem, the root filesystem (/) has been mounted as root only mode. Therefore before you can make any fixes, then while in the emergency target, you will need to first run the following command:


<pre>
$ mount -o rw,remount /
</pre> 

After that you can edit your config files, e.g. /etc/fstab or /etc/sudoers.

 ]]></Content>
		<Date><![CDATA[2015-11-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - A hello world ruby script example]]></Title>
		<Content><![CDATA[<pre>
$ cat testscript.rb
#!/usr/bin/env ruby

a_string = "Hello World!"
puts a_string
puts a_string.class
</pre>

The first line is slightly unusual because:

https://en.wikibooks.org/wiki/Ruby_Programming/Hello_world#Using_env

Next make it an executable:
<pre>
$ chmod 777 testscript.rb
</pre>

then run the script:
<pre>
$ ./testscript.rb
Hello World!
String
</pre>
]]></Content>
		<Date><![CDATA[2015-11-12]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby Recipe - Store contents of a file in a variable]]></Title>
		<Content><![CDATA[


<pre>
$ cat testscript.rb
#!/usr/bin/env ruby

data = File.read("/tmp/testfile.txt")
puts data
$
$ cat /tmp/testfile.txt
Hello!
here is some content.
Goodbye.
$
$ ./testscript.rb
Hello!
here is some content.
Goodbye.
$
</pre>]]></Content>
		<Date><![CDATA[2015-11-12]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ruby - Passing a code block into a method using the "yield" keyword]]></Title>
		<Content><![CDATA[<pre>
$ cat testscript.rb
#!/usr/bin/env ruby

def normal_welcome
   puts "about to execute:"
   yield
   puts "finished executing"
end

normal_welcome do
    puts 1 + 1
    puts 2 + 2
    puts 3 + 3
    puts 4 + 4
end
</pre>

The above outputs: 

<pre>
$ ./testscript.rb
about to execute:
2
4
6
8
finished executing
</pre>





]]></Content>
		<Date><![CDATA[2015-11-12]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ruby]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ruby]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Foreman - Render provisioning templates]]></Title>
		<Content><![CDATA[#!/usr/bin/env ruby


require "erb"
require 'ostruct'


class Basicerb

  def initialize name
    @name = name
    @template = File.read('/root/index.erb')
  end

  def render
    @person = OpenStruct.new
    @person.name    = "John Smith"
    @person.age     = 70
    @person.pension = 300
    ERB.new("<h1>Hello ERB World!! </h1><h3><%= @person.name %></h3>").result( binding )
  end
end

test = Basicerb.new "wow"


puts test.render
]]></Content>
		<Date><![CDATA[2015-11-12]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Foreman]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA -  missing topics]]></Title>
		<Content><![CDATA[missing topics:


superuser  - su command

emergency mode to fix /etc.fstab issues. 

rsync (1st book, page 301)

fix incorrect grub parameters (2nd book, page 278)

managing temp files (2nd book 74)]]></Content>
		<Date><![CDATA[2015-11-13]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[private]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Applying ACLs to directories]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="Lets say you have a folder called /tmp/research-team-folder and you want to give the user jerry, 'rw-' permissions. Note, jerry is not the owner nor one of the group owners of this folder. What are the five steps you need to do achieve this"]
1. Update user+group ownerships using chown (if necessary).
2. Update the normal ugo+rwx permissons using chmod (if necessary).
3. Update suid, sgid, and sticky bit special permissions, if necessary.  
4. Apply ACL settings recursively to all existing child files/folders (using the setfacl command)
5. Set default ACL settings on the folder (using setfacl command)
[/toggle]
[toggle title="What is the command to apply specials permissions to give jerry, 'rw-' to the folder /tmp/research-team-folder and all it's existing child files and folders?"]
$ setfacl -R -m user:jerry:rw<mark>X</mark> /tmp/research-team-folder
[/toggle]
[toggle title="What does the capital 'X' mean?"]
This mean execute permissions for child folder but not on files. 
[/toggle]
[toggle title="What is the command to give jerry, 'rw-' priveleges for any new files+folders that are created in the folder, /tmp/research-team-folder?"]
$ setfacl -m default:user:jerry:rwX /tmp/research-team-folder
[/toggle]
[toggle title="What command do you run copy across acl setting from /tmp/file1 to /tmp/file2?"]
$ getfacl /tmp/file1 | setfacl <span style="letter-spacing:0.1px">-</span>-set-file=- /tmp/file2
[/toggle]
[/accordion]

<hr/>




<h2>Using ACL on directories</h2>


So far we have seen how to give a user/group special permissions for a particular file. However you can also give a user/group special permissions to a folder. This is very common scenario, and for this scenario it is very common to:

<ul>
	<li>Recursivley apply the same special permissions to everything inside this folder</li>
	<li>Set default permissions on the folder, so that all future new files+folders created inside it automatically inherits the same special permissions.</li>
</ul>

To achieve this, we need to take the following steps
<ol>
	<li>First set directory's standard permissions using the chmod and chown command.</li>
	<li>Apply ACL settings recursively to all existing child files/folders</li>
	<li>Set default ACL settings on the folder</li>
</ol>

Now let's take a look at the permissions of the "research-team-folder":


<pre>
$ ls -l /tmp | grep "research-team-folder"
drwxr-xr-x. 2 nobody research  6 Nov 15 22:32 research-team-folder
</pre>

At the moment it doesn't have any special permissons 

<pre>
$ getfacl /tmp/research-team-folder/
getfacl: Removing leading '/' from absolute path names
# file: tmp/research-team-folder/
# owner: nobody
# group: research
user::rwx
group::r-x
other::r-x
</pre>

Let's say we We have a user called "jerry" that isn't a member of the "research" group but we want to give it access to this folder via special permissions.  Now let's apply the acl permission:

<pre>
$ setfacl -R -m user:jerry:rw<mark>X</mark> /tmp/research-team-folder
</pre>

Here we are saying (R)ecursively (m)odify the "user" permission jerry to "rwX" for the research-team-folder. Now let's see what the acl permission have become:

Note: the capital "X" indicates to apply "execute" on child folders only and not child files. 

<pre>
$ getfacl /tmp/research-team-folder
# file: research-team-folder/
# owner: nobody
# group: research
user::--x
group::rwx
<strong>user:jerry:rwx</strong> 
mask::rwx
other::--x
</pre>

All the existing folders+files that exist inside the research-team-folder folder all have the same special permissions assigned them. However any new files+folders created inside research-team-folder won't have any special permissions applied to them.  To overcome this we need to set <strong>default special permissions</strong>. 

Now we apply the default permission

<pre>
$ setfacl -m default:user:jerry:rwx /tmp/research-team-folder
</pre>

Note: "default acls" are something that you can only apply to folders and not files. In fact the concept of default special permissions on files wouldn't make any sense anyway.      

Now we end up with:

<pre>
$ getfacl /tmp/research-team-folder
getfacl: Removing leading '/' from absolute path names
# file: tmp/research-team-folder
# owner: nobody
# group: research
user::rwx
user:jerry:rwx
group::r-x
mask::rwx
other::r-x
default:user::rwx
default:user:jerry:rwx
default:group::r-x
default:mask::rwx
default:other::r-x
</pre>

]]></Content>
		<Date><![CDATA[2015-11-15]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - The ACL's mask setting]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What are the 2 commands you can use for setting mask values?"]
- chmod
- setfacl
[/toggle]
[toggle title="What is the command that indirectly applies the mask, '---', to the file /tmp/testfile.txt?"]
$ chmod 000 /tmp/testfile.txt 	 
[/toggle]
[toggle title="What is the command to explictly apply the mask, 'r-x'?, to the file /tmp/testfile.txt?"]
$ setfacl -m m<mark>::</mark>r-x /tmp/testfile.txt
# notice the double colon syntax. 
[/toggle]
[/accordion]

<hr/>




In the previous example we had:
<pre>$ getfacl /tmp/testfile.txt
getfacl: Removing leading '/' from absolute path names
# file: tmp/testfile.txt
# owner: root
# group: root
user::rw-
user:homer:rw-
group::r--
<mark>mask::rw-</mark>
other::r--
</pre>
Here we have the "mask" setting. This acts as safety-mechanism. To understand how this works, lets first create a few more users:
<pre>
$ useradd bart	 
$ useradd marge	 
$ useradd lisa	 
$ useradd maggie
</pre>	 
Now let's all give them various special permissions to /tmp/testfile.txt:	
<pre>	 
$ setfacl -m u:homer:rwx /tmp/testfile.txt	
$ setfacl -m u:marge:rwx /tmp/testfile.txt	
$ setfacl -m u:lisa:rw- /tmp/testfile.txt	
$ setfacl -m u:bart:r-- /tmp/testfile.txt	
$ setfacl -m u:maggie:r-- /tmp/testfile.txt	
</pre>	 	
Now our special permissions for /tmp/testfile.txt should now look something like this:	
<pre>	 
$ getfacl /tmp/testfile.txt
getfacl: Removing leading '/' from absolute path names
# file: tmp/testfile.txt
# owner: root	
# group: root	
user::r--	 	
user:homer:rwx	
user:bart:r--	
user:marge:rwx	
user:lisa:rw-	
user:maggie:r--	
group::r--	 	
mask::rwx	 	
other::r--	 	
</pre>	 
Now what if you suspect that /tmp/testfile.txt might contain a virus! In this situation you might want to delete all the special permissions (using the --remove-all option) so that none of these might accidentally trigger the virus. However removing all the special persmissions isn't a good approach. That's because after inspecting the testfile.txt, if you discover that this file is safe after all, then you need to re-apply all the special permissions again. That's why a better option is to lock down the file but at same time preserve the special permissions settings (in a suppressed mode) to be reactivated in future again. This is made possible using the "mask" setting. You can apply it implicitly by simply using the chmod command:
<pre>	
$ chmod 000 /tmp/testfile.txt 	 
</pre>	

This results in a bunch of "effective" settings:	

<pre>
$ getfacl /tmp/testfile.txt	 
getfacl: Removing leading '/' from absolute path names	
# file: tmp/testfile.txt
# owner: root	
# group: root	
user::---
user:homer:rwx    #effective:---	
user:bart:r--     #effective:---	
user:marge:rwx    #effective:---	
user:lisa:rw-     #effective:---	
user:maggie:r--   #effective:---	
group::r--        #effective:---	 	
mask::---	
other::---	
</pre>

The mask setting is set to the maximum allowed setting for all users. This "effectively" override special permissions. 

The mask setting will automatically update again indirectly when you modify permissions using either the chmod or setfacl command. But you can also directly change the mask setting as well. E.g. if you wan to set the mask to "r-x", then you do:

<pre>
$ setfacl -m m::r-x /tmp/testfile.txt
</pre> 

Notice that we have "::" since it is empty as the mask setting is not something particular to a user or group.  ]]></Content>
		<Date><![CDATA[2015-11-15]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[spacewalk - notes]]></Title>
		<Content><![CDATA[deregister vm from spacewalk:

rm /etc/sysconfig/rhn/systemid

<h2>Upload rpm to spacewalk</h2>

<pre>$ Rpm --addsign name.rpm
$ Rhnpush -c {channel's-label} --server localhost name.rpm</pre>
 
https://fedorahosted.org/spacewalk/wiki/UploadFedoraContent]]></Content>
		<Date><![CDATA[2015-11-19]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[spacewalk]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[ssh encryption with prime numbers and subprime numbers]]></Title>
		<Content><![CDATA[https://np.reddit.com/r/math/comments/3tn1xq/what_intuitively_obvious_mathematical_statements/cx7np4t?context=3]]></Content>
		<Date><![CDATA[2015-11-22]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ssh]]></Tags>
		<Status><![CDATA[private]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - SELInux Summary]]></Title>
		<Content><![CDATA[The policy book can be thought of as a really big text book, which contains 3 chapters.

Chapter 1 - Contains a list of all available security attributes. There are actually  4 lists, one for each type of the differnt types of suecurity attributes user:role:type:level. In the targeted policy, the "type" list is by far the longest, containing about 4500 entries.  
Chapter 2 - lists mapping rules about which security attribute can access to which other security attributes. 
chapter 3 - list selinux context assignment rules. This chapter gives info about what SELinux needs, in order to determine what selinux context values every object in your machine should have.
   


<h2>Overview</h2>
By the end of this article you should be able to answer the following questions:

[accordion]
[toggle title="What is the command to list all available security attributes?"]
seinfo          # this retrieves info from the policy book
[/toggle]
[toggle title="What command do you use to see which selinux attributes are allowed access to objects of which other selinux attributes?"]
sesearch
[/toggle]
[toggle title="What is the command to see the SELinux logic for assigning selinux contexts to objects?"]
semanage        # this retrieves info from the policy book
[/toggle]
[toggle title="What is the command to modify selinux context assignment rules (i.e. chapter 3)?"]
semaage
[/toggle]
[toggle title="What is the command?"]

[/toggle]
[toggle title="What is the command?"]

[/toggle]
[toggle title="What is the command?"]

[/toggle]
[toggle title="What is the command?"]

[/toggle]
[toggle title="What is the command?"]

[/toggle]
[toggle title="What is the command?"]

[/toggle]
[toggle title="What is the command?"]

[/toggle]
[toggle title="What is the command?"]

[/toggle]
[toggle title="What is the command?"]

[/toggle]
[/accordion]

<hr/>
]]></Content>
		<Date><![CDATA[2015-11-28]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - stuff I failed on]]></Title>
		<Content><![CDATA[https://www.google.com/search?q=fstab+vfat&ie=utf-8&oe=utf-8#safe=off&q=fstab+vfat+busy

autofs+ldap

I also didn't have a spare block device. Instead I had to use existing block device to create an extended partition, followed by a logical partition, then run partprobe command. ]]></Content>
		<Date><![CDATA[2015-12-02]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Centos|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[private]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[linux quick quide - The sshpass command]]></Title>
		<Content><![CDATA[https://www.google.com/webhp?sourceid=chrome-instant&ion=1&espv=2&ie=UTF-8#safe=off&q=sshpass

]]></Content>
		<Date><![CDATA[2015-12-02]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Centos|RHEL|sshpass]]></Tags>
		<Status><![CDATA[private]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCE - Kerberos]]></Title>
		<Content><![CDATA[http://www.roguelynn.com/words/explain-like-im-5-kerberos/]]></Content>
		<Date><![CDATA[2015-12-03]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCE|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCE]]></Categories>
	</post>
	<post>
		<Title><![CDATA[gem - trollop]]></Title>
		<Content><![CDATA[trollop lets you easily create script parameters with meaningful names, instead of useing ARGV[0], ARGV[1],...etc. 

If you have the following in your ruby script:


<pre>
cat ./testscript.rb
#!/usr/bin/env ruby

require 'trollop'

opts = Trollop::options do
  opt :hostname, "Server FQDN.", :type => :string
end

puts opts[:hostname]

</pre>

Next we install the gem:


<pre>$ gem install trollop</pre>

Now we have builtin "help" feature:


<pre>$ ruby ./testscript.rb --help
Options:
  -h, --hostname=<s>    Server FQDN.
  -e, --help            Show this message</pre>

Now if we do:

<pre>
$ ruby ./testscript.rb --hostname=test.com
test.com
</pre>

]]></Content>
		<Date><![CDATA[2015-12-04]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[gem|ruby|rubygem]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Rubygems]]></Categories>
	</post>
	<post>
		<Title><![CDATA[gem - yaml]]></Title>
		<Content><![CDATA[Yaml actually comes as part of the ruby core, therefore there is no ruby gem for it.  

Lets say you have the following yaml file:


<pre>$ cat /etc/credentials.yaml
---
:foreman:
  :username: 'codingbee'
  :password: 'liverpool'
</pre>
now you want use these content, then you do:


<pre>
cat yamldemo.rb
require 'yaml'

credentials = YAML.load_file("/etc/credentials.yaml")
@username = credentials[:foreman][:username]
@password = credentials[:foreman][:password]

puts @username
puts @password


</pre>

Which leads to:

<pre>
$ ruby yamldemo.rb
codingbee
liverpool

</pre>]]></Content>
		<Date><![CDATA[2015-12-04]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[gem|ruby|rubygem]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Rubygems]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Setting up an LDAP client]]></Title>
		<Content><![CDATA[<h2>Overview</h2>
There are 2 main server-side ldap software you can use to set up an ldap server:
<ul>
 	<li>OpenLDAP</li>
 	<li>Microsoft Active Directory (AD)</li>
</ul>
Unfortunately we don't have a single client-side ldap software that can connect to both of these types of LDAP servers. Instead we have the following 2 options:
<ul>
 	<li><strong>openldap-clients</strong>: This ldap client-side software is used for connecting to an OpenLDAP based LDAP server</li>
 	<li><strong>realmd</strong>: This ldap client-side software is used for connecting to an Microsoft AD based LDAP server</li>
</ul>
In most cases you won't know which type of LDAP server you are dealing with. If that is the case, then the only way to do figure it out is by first try one approach, if that doesn't work then try the other approach.

One of the RHCSA exam object is:
<blockquote>Configure a system to use an existing authentication service for user and group information</blockquote>
You don't need to know how to set up openldap server itself, however you do need an an existing dummy openldap server as a target machine to try connecting to from your CentOS7 machine.

In that respect you do need to set up a dummy openldap server, which is quite complicate and a pain. However the good news is that I have created a complete openldap sandpit environment in the form of a vagrant project:

<a href="https://github.com/Sher-Chowdhury/vagrant-openldap">https://github.com/Sher-Chowdhury/vagrant-openldap</a>

If you don't know what <a href="https://www.vagrantup.com/docs/">vagrant</a> is then I strongly recommend reading up about it. Vagrant isn't something you need to know about for the RHCSA exam, but is popular tool to become familiar with as it's a massive time saver in the long run. Also you only need basic familiarity with vagrant to start using this vagrant project.
<h2>Pre-reqs</h2>
In order to setup your machine as an openldap client, you need to have the following:
<ul>
 	<li>openldap server's fqdn and "base dn". They look something like this:Base DN: dc=codingbee,dc=net
fqdn: ldap://ldap.codingbee.netAs you can see, if you did end up with one of the above info, you can derive the other without too much difficulty.</li>
 	<li>.pem file</li>
</ul>
<h2>Setting up LDAP client (authconfig approach)</h2>
Let's assume we have a openldap based ldap server. In that case we first install:
<pre>$ yum install -y openldap-clients nss-pam-ldapd nss_ldap authconfig-gtk</pre>
Once authconfig-gtk is installed, start the gui interface like this:
<pre>$ authconfig-gtk
</pre>
A window should now pop up:

<a href="http://codingbee.net/wp-content/uploads/2015/04/g5oQadP.png"><img class="alignnone size-full wp-image-3725" src="http://codingbee.net/wp-content/uploads/2015/04/g5oQadP.png" alt="" width="493" height="556" /></a>

Then select ldap from the dropdown list and you should then see:

<a href="http://codingbee.net/wp-content/uploads/2015/04/5VdZJKb.png"><img class="alignnone size-full wp-image-3726" src="http://codingbee.net/wp-content/uploads/2015/04/5VdZJKb.png" alt="" width="487" height="726" /></a>

The select "ldap password" from the authentication method drop down list:

<a href="http://codingbee.net/wp-content/uploads/2015/04/3Hy5Kaf.png"><img class="alignnone size-full wp-image-3727" src="http://codingbee.net/wp-content/uploads/2015/04/3Hy5Kaf.png" alt="" width="491" height="555" /></a>

Now fill in the <em>LDAP Search Base DN</em> and <em>ldap server</em> fields. Both these info will be provided in the rhcsa exam. However it may be in the form a single piece of info, e.g you are just given in the "ldap server" info:
<pre>Next you need to check the tls certificate checkbox, if you are given the location of a .pem file.</pre>
Then in the download cert box, enter the location of the certificate, e.g.:
<pre>ftp://certs.codingbee.net/resources/cert.pem
</pre>
Finally click "apply" and then wait a few minutes.

after a few minutes the window closes and you end up back at the terminal.

Then finally we restart sshd service:
<pre>$ systemctl restart sshd
</pre>
<h2>Setting up LDAP client (realmd approach)</h2>
We can configure all other Linux servers to authenticate user login credentials against this LDAP server. This kind of setup is known as "single sign-on", aka SSO. This is one of the main purposes of using ldap.

For the RHCSA and RHCE exam, you only need to know how to set up your Linux machine as an LDAP client that can connect to an existing LDAP server. You don't need to know how to setup the LDAP server itself.

There's 2 ways to configure an ldap client, they are <strong>realmd</strong> and <strong>authconfig-gtk</strong>. In this article we will take a look at the realmd approach.

First off, in order for the realmd ldap client to work, the ldap software running on the ldap server needs to be realmd compatible. The only way to find out is by running the "realmd discover", which you can only do part way through setting up the realmd cleint software.

Let's say our ldap server is:
<pre>puppetagent01.local 
</pre>
Let's first see if we can ping to it:
<pre>$ ping -c 3 puppetagent01.local
PING puppetagent01 (192.168.50.11) 56(84) bytes of data.
64 bytes from puppetagent01 (192.168.50.11): icmp_seq=1 ttl=64 time=0.412 ms
64 bytes from puppetagent01 (192.168.50.11): icmp_seq=2 ttl=64 time=0.728 ms
64 bytes from puppetagent01 (192.168.50.11): icmp_seq=3 ttl=64 time=0.825 ms

--- puppetagent01 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2006ms
rtt min/avg/max/mdev = 0.412/0.655/0.825/0.176 ms

</pre>
Now install the realmd package:
<pre>$ yum install -y realmd
</pre>
This package is able to discover realmd based LDAP servers. Once we have info we can then try to do a handshake to the LDAP server like this:
<pre>$ realm discover puppetagent01.local
See: journalctl REALMD_OPERATION=r4373.10347
realm: No such realm found: puppetagent01.local
</pre>
Here it didn't work, this is most likely because the ldap server's ldap software doesn't support realmd type connections. In this situation, you need to set up the ldap client using authconfig-gtk instead of realmd.

####### the rest of this article will need more work.

Assuming the above worked, we can continue.

When you run this command, you might get prompted to install some other package. In which install those as you well using yum.

Next we "join" the ldap server:
<pre>$ realm join ldap.codingbee.net
Password for Administrator: 
</pre>
Here you'll get prompted to enter the password.

To confirm this has worked, we simply run the discover command again:
<pre>$ realm discover ldap.codingbee.net
</pre>
This time it will give more info indicating that you have successfully connected to the server. It will also indicate the type of ldap server, this is usually "kerberos"

You may also need to configure the <code>/etc/ssh/sshd_config</code>. Here you just find all the kerberos setting, enable them, and then set them to 'yes'.
<pre>$ cat /etc/ssh/sshd_config | grep Kerberos
# Kerberos options
#KerberosAuthentication no
#KerberosOrLocalPasswd yes
#KerberosTicketCleanup yes
#KerberosGetAFSToken no
#KerberosUseKuserok yes
</pre>
Also don't forget to restart sshd:
<pre>$ systemctl restart sshd</pre>
&nbsp;

&nbsp;
<h2>The login binary (extra/background reading)</h2>
When a user attempts to login to a vm, they get prompted to enter a username followed by the password. This login/password prompting feature is provided by the login binary:
<pre>$ which login
/usr/bin/login
$ file /usr/bin/login
/usr/bin/login: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked (uses shared libs), for GNU/Linux 2.6.32, BuildID[sha1]=0xb64a9cff8ba34aa4586e27a77909ef8fbc4a9769, stripped
</pre>
This binary has dependencies on the following shared libraries:
<pre>[root@localhost ~]# ldd /usr/bin/login
        linux-vdso.so.1 =&gt;  (0x00007fff091fe000)
        libpam.so.0 =&gt; /lib64/libpam.so.0 (0x00007fbf0de08000)
        libpam_misc.so.0 =&gt; /lib64/libpam_misc.so.0 (0x00007fbf0dc04000)
        libaudit.so.1 =&gt; /lib64/libaudit.so.1 (0x00007fbf0d9dc000)
        libselinux.so.1 =&gt; /lib64/libselinux.so.1 (0x00007fbf0d7b7000)
        libc.so.6 =&gt; /lib64/libc.so.6 (0x00007fbf0d3f6000)
        libdl.so.2 =&gt; /lib64/libdl.so.2 (0x00007fbf0d1f1000)
        libpcre.so.1 =&gt; /lib64/libpcre.so.1 (0x00007fbf0cf90000)
        liblzma.so.5 =&gt; /lib64/liblzma.so.5 (0x00007fbf0cd6b000)
        /lib64/ld-linux-x86-64.so.2 (0x00007fbf0e02a000)
        libpthread.so.0 =&gt; /lib64/libpthread.so.0 (0x00007fbf0cb4e000)

</pre>
You can think of all of these libraries as plugins that adds extra functionality to the login tool.

The "libpam.so.0" is listed second here, which is part of Pluggable Authentication Module (pam).

All of pam's config files are listed under the following directory:
<pre>$ [root@localhost ~]# ls -l /etc/pam.d/
total 156
-rw-r--r--. 1 root root  272 Oct  7  2014 atd
-rw-r--r--. 1 root root  192 Mar  6 05:58 chfn
-rw-r--r--. 1 root root  192 Mar  6 05:58 chsh
-rw-r--r--. 1 root root  232 Mar  6 04:55 config-util
-rw-r--r--. 1 root root  293 Jul 30  2014 crond
-r--r--r--. 1 root root  146 Mar  5 22:51 cups
lrwxrwxrwx. 1 root root   19 Mar 14 19:23 fingerprint-auth -&gt; fingerprint-auth-ac
-rw-r--r--. 1 root root  702 Mar 14 19:23 fingerprint-auth-ac
-rw-r--r--. 1 root root  590 Mar  5 23:41 gdm-autologin
-rw-r--r--. 1 root root  605 Mar  5 23:41 gdm-fingerprint
-rw-r--r--. 1 root root  341 Mar  5 23:41 gdm-launch-environment
-rw-r--r--. 1 root root  829 Mar  5 23:41 gdm-password
-rw-r--r--. 1 root root  844 Mar  5 23:41 gdm-pin
-rw-r--r--. 1 root root  597 Mar  5 23:41 gdm-smartcard
-rw-r--r--. 1 root root   70 Mar  6 01:41 ksu
-rw-r--r--. 1 root root   97 Mar 26 10:43 liveinst
-rw-r--r--. 1 root root  796 Mar  6 05:58 login
-rw-r--r--. 1 root root  154 Mar  6 04:55 other
-rw-r--r--. 1 root root  188 Jun 10  2014 passwd
lrwxrwxrwx. 1 root root   16 Mar 14 19:23 password-auth -&gt; password-auth-ac
-rw-r--r--. 1 root root  974 Mar 14 19:23 password-auth-ac
-rw-r--r--. 1 root root  510 Mar  6 03:39 pluto
-rw-r--r--. 1 root root  155 Jun  9  2014 polkit-1
lrwxrwxrwx. 1 root root   12 Mar 14 19:23 postlogin -&gt; postlogin-ac
-rw-r--r--. 1 root root  330 Mar 14 19:23 postlogin-ac
-rw-r--r--. 1 root root  144 Jun 10  2014 ppp
-rw-r--r--. 1 root root  681 Mar  6 05:58 remote
-rw-r--r--. 1 root root  143 Mar  6 05:58 runuser
-rw-r--r--. 1 root root  138 Mar  6 05:58 runuser-l
-rw-r--r--. 1 root root  145 Jun  9  2014 setup
lrwxrwxrwx. 1 root root   17 Mar 14 19:23 smartcard-auth -&gt; smartcard-auth-ac
-rw-r--r--. 1 root root  752 Mar 14 19:23 smartcard-auth-ac
lrwxrwxrwx. 1 root root   25 Mar 14 19:20 smtp -&gt; /etc/alternatives/mta-pam
-rw-r--r--. 1 root root   76 Jun 10  2014 smtp.postfix
-rw-r--r--. 1 root root  643 Mar  6 04:44 sshd
-rw-r--r--. 1 root root  540 Mar  6 05:58 su
-rw-r--r--. 1 root root  202 Mar  6 05:42 sudo
-rw-r--r--. 1 root root  187 Mar  6 05:42 sudo-i
-rw-r--r--. 1 root root  137 Mar  6 05:58 su-l
lrwxrwxrwx. 1 root root   14 Mar 14 19:23 system-auth -&gt; system-auth-ac
-rw-r--r--. 1 root root 1015 Mar 14 19:23 system-auth-ac
-rw-r--r--. 1 root root  181 Mar 26 13:03 systemd-user
-rw-r--r--. 1 root root   84 Mar  6 03:11 vlock
-rw-r--r--. 1 root root  297 Mar 12 15:06 vmtoolsd
-rw-r--r--. 1 root root  163 Mar  6 06:11 xserver

</pre>
Most config files here have the same name as the utility that needs to make use of PAM. These commands are referred to as "PAM aware". Hence you can reconfigure one of these files in order to change the ways the utility uses pam to verify the user.

Here you'll find a file called "login", this configures pam's set up when being called by the login binary, if we take a look at this:
<pre>[root@localhost pam.d]# cat login
#%PAM-1.0
auth [user_unknown=ignore success=ok ignore=ignore default=bad] pam_securetty.so
auth       substack     system-auth
auth       include      postlogin
account    required     pam_nologin.so
account    include      system-auth
password   include      system-auth
# pam_selinux.so close should be the first session rule
session    required     pam_selinux.so close
session    required     pam_loginuid.so
session    optional     pam_console.so
# pam_selinux.so open should only be followed by sessions to be executed in the user context
session    required     pam_selinux.so open
session    required     pam_namespace.so
session    optional     pam_keyinit.so force revoke
session    include      system-auth
session    include      postlogin
-session   optional     pam_ck_connector.so
[root@localhost pam.d]#


</pre>
Here the first line (<a href="http://linux-pam.org/Linux-PAM-html/sag-pam_securetty.html">pam_securetty.so</a>) means that the root user can only login from secure terminals as defined by the /etc/securetty file.

Hence this line makes use of a module called pam_securetty.so.

The next line however doesn't call a module, instead it calls another config file, called "system-auth" which also sits in the same directory.

This file is actually is more of generic file that most of the service/tool specific config calls on and the content of this generic file is essentially inserted into the service/tool specific files, which reduces repitition in individual pam config files. If we take a look at this file we find:

You can configure a machine to become an ldap client from the command-line or using a gui tool. We'll explain each approach next.

&nbsp;
<h2>Test your ldap connection</h2>
There are a few checks you can perform to make sure that your ldap

<strong>Need to learn more about:</strong>

<a href="https://access.redhat.com/knowledge/docs/en-US/Red_Hat_Enterprise_Linux/5/html/Deployment_Guide/ch-ldap.html">https://access.redhat.com/knowledge/docs/en-US/Red_Hat_Enterprise_Linux/5/html/Deployment_Guide/ch-ldap.html</a>

http://www.learnitguide.net/2016/01/configure-openldap-server-on-rhel7.htmlhttp://www.zytrax.com/books/ldap/http://www.zytrax.com/books/ldap/ch15/#tls]]></Content>
		<Date><![CDATA[2015-12-06]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - PuppetDB]]></Title>
		<Content><![CDATA[http://docs.puppetlabs.com/puppetdb/latest/

https://github.com/spotify/puppetexplorer]]></Content>
		<Date><![CDATA[2015-12-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[wp booking plugins]]></Title>
		<Content><![CDATA[https://wordpress.org/plugins/booking-system/

https://www.google.com/search?q=booking+block+of+days+calendar+wordpress+plugin&ie=utf-8&oe=utf-8

https://www.google.com/search?q=booking+block+of+days+calendar+wordpress+plugin&ie=utf-8&oe=utf-8#safe=off&q=booking+group+days+calendar+wordpress+plugin

]]></Content>
		<Date><![CDATA[2015-12-20]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Centos]]></Tags>
		<Status><![CDATA[private]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - useful utilities]]></Title>
		<Content><![CDATA[puppet modulesync: https://github.com/voxpupuli/modulesync

puppet blacksmith: https://github.com/voxpupuli/puppet-blacksmith
]]></Content>
		<Date><![CDATA[2016-01-06]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[private]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Git - rebasing + squashing process, followed by merging via pull request]]></Title>
		<Content><![CDATA[Let's say I have a feature branch called "feature-x" which I want to first rebase onto the "develop" branch. Then it is first best practice to squash all my feature-branch's commits into a single commit and then merge it into the develop branch via a pull request.

first checkout the develop branch and pull in the latest changes.

<pre>
$ git checkout develop
$ git pull
</pre>

Now do the rebasing:

<pre>
$ git checkout feature-x
$ git rebase -i develop 
</pre>

At this stage just use vim's "wq" option to save and quit. This does the rebasing without any squashing.

Now we do the squashing, using the "f" option, for each commit:

<pre>
$ git rebase -i develop
</pre>

Now we use the following to create a new overall commit message:

<pre>
$ git commit -v --amend
</pre>

Tip: Here are some advice on <a href="http://chris.beams.io/posts/git-commit/#imperative" rel="nofollow">how to write good commit messages</a>.


Now check everything looks ok:

<pre>
$ git log -p
$ git log -p -n1 # displays info about only the latest commit.
</pre>

Now update your git commit's timestamp:

<pre>
$ git commit --amend --date="$(date -R)"
</pre>

Now push your changes to your bitbucket/github server. the following overwrites the entire history of the current branch. Hence it replaces the individual commits with squashed commits.

<pre>
$ git push --force origin feature-x
</pre>

# login into your git server using firefox, and then create the pull request.

Some useful links:
https://git-scm.com/book/en/v2/Git-Tools-Rewriting-History
https://www.atlassian.com/git/tutorials/rewriting-history

The aim of this approach is that it you don't end up cluttering up your main master branch with lots of commits.]]></Content>
		<Date><![CDATA[2016-01-07]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[git]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>git]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - About this course]]></Title>
		<Content><![CDATA[AWS CSA (associate level) - Amazaon Web Services Certified Solutions Architect

You can book this exam at:

http://aws.amazon.com/certification

After getting the AWS CSA (associate level), you can then move onto getting the AWS CSA (professional level) certification

https://www.expeditedssl.com/aws-in-plain-english

https://read.acloud.guru/what-you-need-to-get-aws-certified-5937e613b10f#.ddmu8gmn0

https://cloudacademy.com/quiz/
<h2>Core Architecture Best Practices</h2>
You want to design your product along with the infrastructure they sit on, so that they are:
<ul>
 	<li><strong>Scalable</strong> - i.e. you app isn't designed to only run on one vm, it can run on cluster of VMs, but still appear as a single service.</li>
 	<li><strong>Elastic</strong> - i.e. your system recognises when it is under heavy load, and spin up according. Similar it reduces in size when needed.</li>
 	<li><strong>Fault Tolerant</strong> - e.g. if an vm fails then ELB identifies this and stops send jobs to failing vms. Instead redistributes to remaining vms</li>
 	<li><strong>Self Healing</strong> - e.g. if an ec2 instance fails, then AWS identifies this and automatically builds new replacement EC2 instance. Also if files get corrupted then, where possible, an automated system is in place to regenerate that data.</li>
 	<li><strong>Highly Available</strong> - If one AZ goes down, then another AZ in the same region can takes on the load. Also use load-balanced cluster vms.</li>
 	<li><strong>Cost Efficient</strong> - Use as little as possible without signficantly compromising the best practices mentioned above.</li>
</ul>
&nbsp;]]></Content>
		<Date><![CDATA[2016-01-13]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - The CSA exam (Associate level)]]></Title>
		<Content><![CDATA[https://aws.amazon.com/certification/certified-solutions-architect-associate/

https://aws.amazon.com/certification/faqs/


Syllabus - https://d0.awsstatic.com/training-and-certification/docs-sa-assoc/AWS_certified_solutions_architect_associate_blueprint.pdf]]></Content>
		<Date><![CDATA[2016-01-13]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Common terminologies]]></Title>
		<Content><![CDATA[AWS in it's physical form is made up of a large number of data centers across the world. These data centers are independent of each other, meaning that if one data center goes down, it will not impact the others.

A data center is essentially a really big build that houses hundreds/thousands of physical servers. In AWS these data centres are referred to as <strong>Availability Zones</strong>. These Availability Zones are organised into groups, based on geographical proximity, and each of these groups are referred to as <strong>regions</strong>.

Making use of all the <strong>Availability Zones</strong> in a region has several advantages:
<ul>
	<li>low latency - if a AZ is close to an end user.</li>
	<li>fault tolerance</li>
</ul>
&nbsp;

The AWS console is designed to only interact with one region at a time, you can navigate between regions using the console.

Some AWS services are not restricted to a region, instead they operate worldwide, For example, "Edge Locations"

<strong>Edge Location</strong> - This is an AWS datacenter which is primarily used for caching content, so that it can deliver content to the requester more quickly. E.g. Cloudfront, which is a Content Delivery Network (CDN). These data centers are used for covering parts of the world that are not covered by any AZs. Hence Edge Locations are data centres that are primarily used for caching static content, e.g. html+css files, audio files, video files, ...etc.

scalability - able to add/remove resources to a service as and when needed. e.g. logical volumes are scalable compared to using a simple partition. Scalability has the following advantages:
<ul>
	<li>Resilient - automatically add more cpu resources when demand increases.</li>
	<li>Efficient</li>
	<li>Cost effective - allocate just enough amount of resources to meet demands.</li>
</ul>
Using vm's is an example of scalability.

<strong>Fault Tolerant</strong> (aka failovers, or redundancies) - this means that the your system carries on working even if there is a service failure, e.g. when an AZ goes down. E.g. if one server fails, then there is another server that will take over.

<strong>Elasticity</strong> - Pretty much means that same thing as scalability, i.e. can automatically scale up and scale down. This means we pay end up paying for only what we need.

Some amazon resources have elasticity built in, e.g. Amazon S3, whereas others are not, e.g. EC2.

Ther are 3 main types of scaling:
<ul>
	<li><strong>proactive Cycle Scaling</strong>: automatically start up and shutdown vms during peak periods, e.g. 9am-5pm mon-fri.</li>
	<li><strong>proactive Event-Based Scaling</strong>: Can automatiically scale in anticapation peaks caused by certain events. E.g. black-friday, boxing day, half price sale days.</li>
	<li><strong>Auto-Scaling Based on Demand</strong>: scale up based on things like cpu, ram, bandwidth, reaches a certain predefined upper limit, e.g. 80%. This type of scaling has small delay when scaling up/down. I.e. it can take a couple of minutes for new EC2 register and become part of the applicaiton. Hence, end users may experience minor outages, if there is a very sharp peak. Another way to scale is based on jobs in the queue (e.g. queueing jobs in the Jenkins setup)</li>
</ul>]]></Content>
		<Date><![CDATA[2016-01-13]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Overview of AWS Services and categories]]></Title>
		<Content><![CDATA[<a href="http://codingbee.net/wp-content/uploads/2016/01/aws-dashboard-screenshot.png" rel="attachment wp-att-6687"><img class="alignnone size-full wp-image-6687" src="http://codingbee.net/wp-content/uploads/2016/01/aws-dashboard-screenshot.png" alt="aws-dashboard-screenshot" width="1128" height="930" /></a>

&nbsp;

&nbsp;

&nbsp;

All the AWS services are grouped into the following catagories:

http://d0.awsstatic.com/whitepapers/aws-overview.pdf

Here's the list,
<ul>
	<li>Compute
<ul>
	<li>Amazon EC2 (elastic cloud computing). This has these key features:
<ul>
	<li>Auto Scaling</li>
	<li>Elastic Load Balancing (ELB)</li>
</ul>
</li>
</ul>
</li>
	<li>Storage and Content Delivery
<ul>
	<li>Elastic Block Store (EBS) Volumes - these are network attached block storage devices, e.g. SAN.</li>
</ul>
</li>
	<li>Database</li>
</ul>
<ul>
	<li>Networking
<ul>
	<li>Virtual Private Cloud (VPC)</li>
	<li>Amazon Route 53 (domain name system web service)</li>
</ul>
</li>
</ul>
<ul>
	<li>Developer Tools</li>
	<li>Management Tools</li>
	<li>Security and Identity</li>
	<li>Analytics</li>
	<li>Internet of Things</li>
	<li>Mobile Services</li>
	<li>Application Services</li>
	<li>Enterprise Applications</li>
</ul>]]></Content>
		<Date><![CDATA[2016-01-13]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Amazon Elastic Cloud Compute (EC2) overview]]></Title>
		<Content><![CDATA[<a href="https://aws.amazon.com/ec2/" rel="nofollow">EC2</a> is arguably the most important service available on the AWS platform, EC2 is used for creating VMs on the AWS platform. These vms are referred to as EC2 "<strong>instances</strong>". There are <a href="https://aws.amazon.com/ec2/pricing/" rel="nofollow">three pricing models</a> available under EC2:

&nbsp;
<h2>Pricing</h2>
<ul>
	<li><strong>On-Demand Instances</strong>: This lets you pay for computing capacity by the hour with no long-term commitments.</li>
	<li><strong>Reserved Instances</strong>: This reserves an instance's resource exclusively for your use. You get charged even if you don't use them. But it garauntees that this resource is available in the given AZ. You can book your instance a period of one month, to up to 3 years. You can also sell it if you don't need it anymore. You also have an option to ensure this vm runs on the same physical machine throughout the term.</li>
	<li><strong>Spot Instances</strong> - Spot Instances enable you to bid for unused Amazon EC2 capacity. Only recommended to use this for doing things like processing non-production stuff.</li>
</ul>
You don't pay for the EC2 while it is switched off. However you still have to pay for any EBS devices attached to your EC2 instance.
Later on you will find the <a href="http://calculator.s3.amazonaws.com/index.html">aws cost calculator</a> really handy.

&nbsp;
<h2>EC2 features</h2>
<ul>
	<li>Autoscaling/elastic</li>
	<li>highly availablle</li>
	<li>fault tolerant</li>
	<li>Tightly integrates with Elastic Load Balancer, so that an ELB can evenly distirbute work to a cluster of servers, that can reside across multiple AZs, within the same region.</li>
	<li>Tightly integrates with Elastic Block Storage (EBS) - which is how we added network based block devices to the vm.</li>
</ul>
&nbsp;
<h2>Instance Types</h2>
Since EC2 instance have built in auto-scaling it means that we don't need to build an 10 CPU vm, where 4 cpus are required for nearly the whole year and only 10 CPU is require during the Christmas period. Instead, we only request for what we need.

Instead the specs we need should be based on the applicaton's needs. AWS provides the following predefined profiles to choose from. These profiles are referred to as <a href="https://aws.amazon.com/ec2/instance-types/">instance types</a>, here are the main ones:
<ul>
	<li>T2 - These are comparitively quite low spec, but they have Burstable Performance Instance, which means that can handle occassional high demands. You have a few T2 specs to choose from:
<ul>
	<li>t2.nano</li>
	<li>t2.micro</li>
	<li>t2.small</li>
	<li>t2.medium</li>
	<li>t2.large</li>
</ul>
</li>
	<li>M4 - These are more general purpose VMs. These are EBS storage only, and no SSD. This means that this is also EBS optimised i.e. bigger bandwidth</li>
	<li>M3 - like M4, but with SSD</li>
	<li>C4 - These vms have emphasis on cpu power and EBS storage only, and no SSD. This means that this is also EBS optimised i.e. bigger bandwidth</li>
	<li>C3 - Like C4 but with SSD</li>
	<li>R3 - these have more emphasis on RAM</li>
	<li>G2 - these have more emphasis on GPU pcrocessor</li>
	<li>I2 - these have more emphasis on storage. I.e. large ssd instead of EBS for fast I/0</li>
	<li>D2 - Like I2 but with even more storage but in HDD form. This is EBS optimised as well.</li>
</ul>
&nbsp;
<h2>Elastic Network Interfaces</h2>
When you create a new instance, it normally has one default interface, which is eth0, however you can add additional Interfaces, e.g. eth1, et2...etc. There is a limit to how many interfaces you can add, and this limit depends on the instance size. e.g. you can attach more instances on a large instance compared to a nano instance.

&nbsp;
<h2>Elastic IP address</h2>
When you create a new instance, you instance automatically gets to ip addresses:
<ul>
	<li>public ip address - this address is not persistant and will change if you shutdown your instance and then start it up again.</li>
	<li>primary private ip address - this is persistant and used by other resources that are inside your vpc to communicate with each other.</li>
</ul>
If you want to have a persistent public IP address, then you can achieve this by creating an "Elastic IP Address" and link it up to your private IP address.

&nbsp;

&nbsp;
<h2><strong>Instance Storage</strong></h2>
Your EC2 instance's primary block device can be either one of the following types of block device:
<ul>
	<li>Instance Store-backed instances (aka ephemeral storage)</li>
	<li>EBS (Elastic block store) backed storage</li>
</ul>
In AWS, the primary block device is referred to as <strong>/dev/sda1</strong>. In fact /dev/sda doesn't exist in the world of aws.

The block device type that your instance ends up is dictated by choice of AMI.

&nbsp;
<h3><strong>Instance Store-backed instances (aka ephemeral storage)</strong></h3>
This is temporary storage for the life of the instance. Here are the main features:
<ul>
	<li>Data in this storage gets deleted when the instance is switched-off/shutdown. In fact, the switched-off/shutdown option is actually grayed out, and you can only terminate them.  However the data does survive a reboot.</li>
	<li>They are virtual storage that is creating on top of actual SSD devices.</li>
	<li>These SSD hardware is actually physically attached to the hardware that is hosting your instance. In order to maximise performance.</li>
	<li>If there are things you want to persist when booting up a power-offed instance, then you should prebake those into the AMI that your instance was built from. Alternatively you can run a bunch of shell scripts after bootup, this kind of "bootstrapping" is possible using a technology called <a href="http://codingbee.net/tutorials/aws/aws-bootstrapping-ec2-using-cloud-init/">cloud-init</a>.</li>
</ul>
To create an instance with an ephemeral root device, you nee to choose an ami which is compatible to this setup:
http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/RootDeviceStorage.html#choose-an-ami-by-root-device

In reality, most of the AMIs in the AMI marketplaces are EBS based. Very few are ephemaral storage based.

&nbsp;
<h3><strong>EBS (Elastic block store) backed storage</strong></h3>
These are network attached storage.
<ul>
	<li>You can take snapshots of this and store it in Amazon S3.</li>
	<li>T2, M4, and C4 uses an EBS for their primary block device. This means you can bootup a powered-off instance without any data loss.</li>
	<li>If your instance needs to do a lot of reading/writing to the EBS, then this can be a bottleneck. However you can overcome this by one of the following ways:
<ul>
	<li>allocating additional IOPS to your instance</li>
	<li>Create an EBS optimised instance to begin with.</li>
	<li>ebs sizes can be anywhere between 1gb to 16TB (16000gb)</li>
	<li>ebs must resided be in the same AZ as the instance that it is connected to.</li>
	<li>ebs can only be attached to one instance at a time.</li>
</ul>
</li>
</ul>
&nbsp;
<h2><strong>EBS-optimized Instances</strong></h2>
These are instance that have greater network priority when transferring data to/from EBS.

Instance are EBS-optimised by default for C4, M4, and D2 types. But for the others there is additional fees to enable this feature.

C4, M4, and D2 instances can be shutdown with no loss of data, on it's primary device, since the device is EBS rather then ephemeral. You can even detach it from one instance and attach it to another, which can be handy for things like realising you needed a C4 instance but currently using and M4 instance.

&nbsp;

&nbsp;
<h2>AMI Images</h2>
This is a vm template. This is the amazon's equivalent to vagrant's .box file.

All instances are created from an AMI.

You can create your own AMIs using <a href="https://www.packer.io/docs/builders/amazon.html">packer</a>.

&nbsp;

&nbsp;
<h2>Security Groups</h2>
This is a collection of port numbers you group together and give it a name. e.g. you can create a group called "web-server". And this group you have whitelisted ports 22, 80, and 443.

You can then assign this to multiple instances. This is a network level security. You still also need to do instance level security, e.g. using RHEL's firewalld.

&nbsp;]]></Content>
		<Date><![CDATA[2016-01-13]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Elastic Load Balancer (ELB) overview]]></Title>
		<Content><![CDATA[ELB is used for distributing traffic to a group of servers. Hence a group of servers shares the same load so that each server doesn't get overwhelmed.

It commonly uses the round robin approach to distribute work amongst the servers. However this approach can be adjusted by enabling "sticky sessions". You can add EC2 instances to an elb. ELB recieves traffic via <strong>route 53</strong> domain aliases. ELB can actually distribute traffic across EC2 instances that are in different availability zones, which maximizes redundancies. Hence if an instance (or AZ) goes down for whatever reason, then the ELB simply stop sending requests to that instance (or AZ).

ELB combined with autoscaling gives the following benefits:
<ul>
	<li>fault tolerance (i.e. prevents an instance from burn out)</li>
	<li>scalability</li>
	<li>elasticity</li>
	<li>high-availability (if one AZ goes down then remaining AZs jumps in to help out.)</li>
	<li>Automatically stop sending traffics to instances that it has identified that have become unhealthy</li>
	<li>Can configure ssl certificates on the ELB rather than each EC2 instances.</li>
</ul>
After you create an ELB, you will find that your elb has it's own dedicated url, e.g.:
<p id="sxyJORJ"><img class="alignnone size-full wp-image-6741 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56bf719574953.png" alt="" /></p>
&nbsp;

&nbsp;
<h2>Public Vs Internal ELBs</h2>
Internal ELBs are ELBs which can't accept traffic from the public, instead it receives traffic from other resources inside AWS and passes them on.

Private EC2 instance: instance that don't have a public ip address, and consequently don't belong to a public instance. Instead they only have a private ip address and can only recieve traffic from other internal sources, that are also inside the same vpc.

An EC2 instance can recieve public traffic in one of 2 ways:

- by Having a public ip address, and consequently be part of a public subnet. This approach isn't a good idea, sometimes you don't want your EC2 to be directly exposed to the public, i.e. you want EC2 instances to be private EC2 instances.
- Create a public ELB, which receives traffic from outside, and then redistributes to one or more private EC2 instance. A public ELB has a internet gateway attached to it.

Now if you have private EC2 instance that you want to send public traffic to, then you need to create a public ELB. This public ELB need 3 things:

- One or more private EC2 instances. Let's assume we only have one for the time being, which resides in AZ "London"
- A private subnet which the EC2 instance is attached to (this is done by createing a VPC).
- A public subnet, that must also be in the same AZ as the private EC2 instance (this is also done by creating )

For the ELB to work, it needs to be

&nbsp;
<h2>Create an ELB</h2>
This is done by going in the EC2 section of the aws web console.
<p id="YOxDeHl"><img class="alignnone size-full wp-image-6734 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56bf6b087f01f.png" alt="" /></p>
&nbsp;
<p id="abFgDNc"><img class="alignnone size-full wp-image-6735 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56bf6b40abbd4.png" alt="" /></p>
&nbsp;

An ELB can only be created inside a vpc. So if you select a vpc from the dropdown list.

As part of the elb process, you have to select one subnet, choose public-subnet.

Also as part creating the elb, you have to specify which ec2 instance the elb should be attached to, i.e. send traffic to.

After you have created the elb, your elb has a url to access it with:
<p id="GwYlGxO"><img class="alignnone size-full wp-image-6743 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56bf72a6c539c.png" alt="" /></p>
Which is connected to our instance:

&nbsp;
<p id="uUqoscY"><img class="alignnone size-full wp-image-6744 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56bf72f010520.png" alt="" /></p>
&nbsp;

At this point, you should be able to access your ec2 via it's ip address:

&nbsp;
<p id="NznWcGy"><img class="alignnone size-full wp-image-6745 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56bf7365f2250.png" alt="" /></p>
Or the elb's url:

&nbsp;
<p id="ZEGngZV"><img class="alignnone size-full wp-image-6746 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56bf73b399395.png" alt="" /></p>
&nbsp;

&nbsp;

Both of this is working because our ec2 instance is attached to the public via the elb, and via the subnet, private-subnet, which is still attached to a route table that is connected to an internet gateway.

We now want to close off direct access to our ec2 instance (i.e. ip based access), so that it can only be accessible via the ELB's url. To do this we need to first create a new route table that isn't attache to an internet gateway:
<p id="DpvfiTL"><img class="alignnone size-full wp-image-6747 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56bf75e372e2c.png" alt="" /></p>
Now let's confirm that our route table isn't attached to an internet gateway:

&nbsp;
<p id="rsDkcAK"><img class="alignnone size-full wp-image-6748 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56bf761ff0d5d.png" alt="" /></p>
&nbsp;

Note, the public route table looks like this:

&nbsp;
<p id="ugZkWIt"><img class="alignnone size-full wp-image-6749 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56bf7647008d4.png" alt="" /></p>
&nbsp;

Now let's switch our private-subnet to use this new private route-table:

&nbsp;
<p id="wDyBBPo"><img class="alignnone size-full wp-image-6750 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56bf768f6fadc.png" alt="" /></p>
So now we have:

&nbsp;
<p id="gEdhBhn"><img class="alignnone size-full wp-image-6751 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56bf76b47ba6a.png" alt="" /></p>
&nbsp;

Now when we try to access via ip address we get:

&nbsp;
<p id="iEwkeWy"><img class="alignnone size-full wp-image-6752 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56bf776b70013.png" alt="" /></p>
&nbsp;

&nbsp;

&nbsp;

But it still works via the elb's url:

&nbsp;
<p id="UXdoSOB"><img class="alignnone size-full wp-image-6753 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56bf77c480f88.png" alt="" /></p>
&nbsp;

&nbsp;

&nbsp;
<h2>ELB setup process</h2>
There are 4 tabs to fill in when setting up a new elb.

Define Load balancer:
<a href='http://codingbee.net/wp-content/uploads/2016/04/img_57193ae59e4fc.png'><img alt='' class='alignnone size-full wp-image-6946 pastedimages' src='http://codingbee.net/wp-content/uploads/2016/04/img_57193ae59e4fc.png' /></a>



Assign security groups:
<a href='http://codingbee.net/wp-content/uploads/2016/04/img_57193b331beb3.png'><img alt='' class='alignnone size-full wp-image-6947 pastedimages' src='http://codingbee.net/wp-content/uploads/2016/04/img_57193b331beb3.png' /></a>

Configure security settings:
<a href='http://codingbee.net/wp-content/uploads/2016/04/img_57193bb6e1e19.png'><img alt='' class='alignnone size-full wp-image-6949 pastedimages' src='http://codingbee.net/wp-content/uploads/2016/04/img_57193bb6e1e19.png' /></a>

The above is mandatory to fill in if you have specified that the elb needs to listen on port 443. If you have opted to listen only on port 80, then this section just shows:

<a href='http://codingbee.net/wp-content/uploads/2016/04/img_57193c462ca20.png'><img alt='' class='alignnone size-full wp-image-6950 pastedimages' src='http://codingbee.net/wp-content/uploads/2016/04/img_57193c462ca20.png' /></a>


Assign security groups:
<a href='http://codingbee.net/wp-content/uploads/2016/04/img_57193e79ae989.png'><img alt='' class='alignnone size-full wp-image-6954 pastedimages' src='http://codingbee.net/wp-content/uploads/2016/04/img_57193e79ae989.png' /></a>

You need to also configure this on your ec2 instances too, so to open up communication between the elb and it's ec2 instances. Note you can think of an elb as an ec2 instance. in which case the ports it is listening too is anologous to those ports listed via 'ss -atn' command. Where as security groups are the wrapper around the ec2 instance. Your security group needs to have the same ports open as the ports that it is listening too. Your ec2 instance's security group needs to allow traffic that are coming from elb's security group.   


Configure health check:
<a href='http://codingbee.net/wp-content/uploads/2016/04/img_57193c7965800.png'><img alt='' class='alignnone size-full wp-image-6951 pastedimages' src='http://codingbee.net/wp-content/uploads/2016/04/img_57193c7965800.png' /></a>


Your ELB will constantly ping it's ec2 instances. Be careful here, if you are too strict then a newly added ec2 instance might have to wait longer before elb starts sending traffic to it. 




Add ec2 instance:
<a href='http://codingbee.net/wp-content/uploads/2016/04/img_57193d25c3d8c.png'><img alt='' class='alignnone size-full wp-image-6952 pastedimages' src='http://codingbee.net/wp-content/uploads/2016/04/img_57193d25c3d8c.png' /></a>


One thing to note here is that in the autoscaling section, you can attach an ELB to an autoscaling group. In this situation you wouldn't need to add any instances here since the autoscaling group will automatically add them to the ELB for you.

then finally you have tagging:

<a href='http://codingbee.net/wp-content/uploads/2016/04/img_57193d5617164.png'><img alt='' class='alignnone size-full wp-image-6953 pastedimages' src='http://codingbee.net/wp-content/uploads/2016/04/img_57193d5617164.png' /></a>

Note, the autoscaling feature will automatically add/remove instance from elb for you. Also it's best practive to enable cross zone load balancing. This will improve high availability and fault tolerence. 




And then you review everything:

<a href='http://codingbee.net/wp-content/uploads/2016/04/img_5719417ab9eae.png'><img alt='' class='alignnone size-full wp-image-6955 pastedimages' src='http://codingbee.net/wp-content/uploads/2016/04/img_5719417ab9eae.png' /></a>

Once the elb is created, you should then find it has a dns url assigned to it. This url is referred to as a "CName". 

This url is usually complicated looking, however you can give it a friendly name in route 53. you do this by setting up 2 route53 (alias) record sets, one for www, and the other without. 

Note, by design it is impossible to do 'www.{cname}' although is is possible to do 'http://{cname}'. That's why it's better to setup alias record types, as opposed to cname record types for elb urls. 

Note, wait a few minutes for route53 changes to take effect. 




 


<h2>Important info</h2>
- Your ELB should send traffic to EC2 instance that are in different AZs. This will improve fault tolerance and high availability. 
- EC2 instance that are connected to an ELB doesn't need to have public/elastic ip addresses. since it will receive traffic via the private ip address, which the elb will use to send traffic to it. However the ec2 still needs to be in a public subnet. 
- when creating an ELB, you will find a checkbox setting for whether you want your ELB to be an <a href="https://docs.aws.amazon.com/ElasticLoadBalancing/latest/DeveloperGuide/elb-internal-load-balancers.html?icmpid=docs_elb_console">internal ELB</a>. This is useful if you want an ELB that isn't accessible from the internet, and it used to distribute internal traffic only. 
- when creating a new ELB, you have to specify what port numbers to receive traffic from. You can specify one or more port numbers. However you can't specify source IP. 
- If you specify port 443 i.e. https, then new forms opens up prompting you to provide ssl info. It is best practice to <a href="http://docs.aws.amazon.com/ElasticLoadBalancing/latest/DeveloperGuide/elb-create-https-ssl-load-balancer.html">set up ssl on the elb</a> rather than on the ec2 instances themselves.
- elb is a fully managed service, however behind the scenes elbs are essentially ec2 instances that has load balancing software installed on it. These behind the scene instance are not big instances, but they autoscale depending on incoming traffic load. However if there is a sharp spike in traffic then the elb might lag a little while the behind the scenes instances are converted to bigger instance types to handle the higher traffic. If you are expecting a spike like this, then you can contact aws and let them know so that they can increase the size of the elb's internal ec2 instance ahead of time. This concept of increasing elb's internal ec2s ahead of time is called 'pre-warming' 
- The ec2 instance apache httpd's access logs will show all traffic source as the elb's ip address, since requests passes through elb, and therefores mask the original source IP. However to help mitigate this problem, you can <a href="http://docs.aws.amazon.com/ElasticLoadBalancing/latest/DeveloperGuide/enable-access-logs.html">enable access logs</a> for your ELB. These logs are then stored in a s3bucket.
- ELBs setup stateful connectons with ec2 instantces. which allows incoming and outgoing traffic even though the ec2 instance may not have a public/elastic ip address. However you can't do commands like yum install ... if EC2 instance doens't have public/elastic ip address, or ec2 instance. That's because the outbound routing will work, but the outbound packet will only have the private ip address attached to it, so return packet won't even reach the vpc, since it doesn't have public ip address on the envelope.      


&nbsp;
]]></Content>
		<Date><![CDATA[2016-01-14]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Security considerations]]></Title>
		<Content><![CDATA[It is up to you to ensure your instances are secure (at a software level). This means things like:

&nbsp;
<ul>
	<li>setting strong password, set selinux to enforcing mode, enabling firewalld,...etc</li>
	<li>EBS encrypted, this is a feature provided by AWS</li>
	<li>Also encrypting snapshots with EBS encryption</li>
	<li>EBS encryption makes use of the AWS Key Management Service.</li>
	<li>As altnernative to using EBS encryption is to encrypt your entire file system, which is possible in rhel.#</li>
	<li>EBS encryption is only for instances that are of size "M3" or bigger.  That's because EBS encryption needs quite a lot of cpu resources.</li>
	<li>Install SSL certificate to the ELB. That way all instances that the ELB distributes traffic to are secured.</li>
</ul>
&nbsp;

AWS by default also provide the following additional security protection features:
<ul>
	<li>DDOS protection (Distributed Denial of Service)</li>
	<li>Port scanning protection - e.g. using the "nmap" command to see what ports an instance is listening on. This even includes using nmap inside your own AWS environment. If you want to do this then you will need to contact Amazon to request this.</li>
	<li>Ingress network filtering - this validates the source destination of the data packets.</li>
</ul>
&nbsp;

&nbsp;]]></Content>
		<Date><![CDATA[2016-01-16]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Virtual Private Cloud (VPC) overview]]></Title>
		<Content><![CDATA[VPC is a way to group all your resources (e.g. ec2 instances) into a network, i.e. a virtual network. This means that resources in the same vpc can communicate with each other via their private ip addresses (irrespective of which subnet's their in).

Consequently when you create a new VPC, you have to specify a range of ip addresses, in the form of a CIDR block.
<p id="fwvyYdP"><a href="http://codingbee.net/wp-content/uploads/2016/02/img_56c377631ac95.png"><img class="alignnone size-full wp-image-6792 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56c377631ac95.png" alt="" /></a></p>
Note: the tenancy setting here, let's you define at the vpc level whether resources should be fixed to specific hardware.

For example a cidr block of 10.0.0.0/16 equates to an IP range of:
<p id="AfRIWjl"><a href="http://codingbee.net/wp-content/uploads/2016/02/img_56c3784800f9e.png"><img class="alignnone size-full wp-image-6793 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56c3784800f9e.png" alt="" /></a></p>
Source: <a href="http://www.subnet-calculator.com/cidr.php" rel="nofollow">subnet-calculator</a>

Note, the range you pick can't overlap with the range of one of your other VPCs.

Within this VPC range, you can break down your range into smaller ranges, called subnets.

When you create a new resource, e.g. instance, e.g. an EC2 instance, you will get prompted to provide what vpc it should be created in followed by which subnet it is created in:
<p id="SjpnKKm"><a href="http://codingbee.net/wp-content/uploads/2016/02/img_56c37accbbccf.png"><img class="alignnone size-full wp-image-6794 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56c37accbbccf.png" alt="" /></a></p>
Once you have selected the subnet, and started the launch, your instance will get an IP address assigned to it that's within the subnet range. This address is called the "private IP address".

Note: Private IP address are only unique inside your aws account.

You can also assign a public ip address to your vm, so that your instance ends up with 2 ip address, a public ip address, and a private ip address.

The public address is unique across the internet and is used to access your instance form the internet, whereas the private address is used by your other aws resource to connect to eachother.

Your resources can communicate with eachother via public adddresses as well, but it is better to use private ip address for performance reasons and security reasons.

&nbsp;
<h2>Public and Private Subnets</h2>
You have the option to attach an "Internet gateway" to your subnet. A subnet that has an internet gateway attached to it, is referred to as a <strong>public subnet</strong>. Whereas a <strong>private subnet</strong> is a subnet that doesn't have an internet gateway attached it. Also you attach internet gateway to a subnet via the subnet's <strong>route table</strong>.

Before you can associate an Internet Gateway to a a vpc's subnet. You need to first associate the internet gateway to the vpc itself. You can only associate one Internet gateway to a VPC. Onc'e that's done, you can then associate this Internet Gateway to one or more of the VPC's route table, which turns them into "<strong>public route tables</strong>". You can attach a route table to a subnet. Note, all subnets must have exactly one route table, you can't have more than that, or less than that. If you attach a public route table to a subnet, then that subnet becomes a public subnet. However if you want to ssh into an instance in this public subnet, then you also need to meet the following conditions:
<ul>
	<li>Necessary firewalls are open at the Network ACL level, so public traffic can enter the public subnet on the allowed port numbers.</li>
	<li>Necessary firewalls are open at the security group level</li>
	<li>instance has a public ip or elastic public ip address attache to it</li>
	<li>Instance level: necessary firewalld allow rules are in place</li>
	<li>Instance level: necessary services, e.g. httpd, ssh, ..etc are listening on the appropriate ports.</li>
</ul>
&nbsp;

If you want to ssh into an ec2 instance (from your home laptop) then your instance must be inside a public subnet and have a public ip address assigned to it.

If you have to use a private subnet, for security reasons, then you can still ssh into your instance by one of the following ways:
<ul>
	<li>ssh into another (publically accessible) ec2 instance that happens to reside in the same vpc, and then ssh into your target instance, using the private IP address. This middle instance can be thought of as a <a href="http://codingbee.net/uncategorized/setting-up-bastion-hosts-in-aws-and-connect-to-ec2-instance-using-tags/">bastion host</a>.</li>
	<li>set up a vpn, so that your home laptop effectively is a member of the vpc, i.e. it is already part of the internal network.</li>
</ul>
&nbsp;
<h2></h2>
<h2>Virtual Private Network (VPN)</h2>
With a VPC, it is possible to connect your home router to the vpc, so that it appears like just another subnet in your vpc. Doing this essentially ends up creating a virtual private network.

To create a VPN, you need to attach a <a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_VPN.html" rel="nofollow">Virtual Private Gateway</a> to your VPC. Therefore the only main gateways into a vpc is the Virtual Private Gateaway, or a Internet Gateway, since you can only have one Internet Gateway attached to a VPC, see diagram in:

http://blog.celingest.com/en/2013/04/19/aws-virtual-private-cloud-vpc-security/

&nbsp;
<h2>Benefits of VPC</h2>
<ul>
	<li>Good way to organise your resources inside subnets. It's a bit like how you organise files into folders</li>
	<li>You can allow resources from different subnets to communicate with eachother by configuring route tables</li>
	<li>You can control which subnet's resources have internet access, by setting up public subnets.</li>
	<li>You can set which subnets are private, which improves security</li>
	<li>we can  make our home routeer part of the vpc, i.e. create a vpn</li>
	<li>You can improve security by using instnace security groups</li>
</ul>
&nbsp;
<h2>The Default VPC</h2>
A VPC can be a bit tricky to setup for a a begginer. That's why a newly created AWS account comes with a default vpc created for you.

The default vpc comes with a several subnets already created for you, and each of these subnets, are public subnets, which makes life easier for beginners.

New instances created inside the default vpc automatically gets assigned with 2 ip address, a public ip address and a private ip address.

&nbsp;
<h2>Elastic Network Interface</h2>
A normal linux vm, has network interfaces, e.g. eth0.

However instance inside a vpc comes with <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html" rel="nofollow">Elastic Network Interfaces</a> (ENI) instead. These ENI's only have private ip addresses associated with them. Traffic going to/from an instance's public ip address is actually rerouted (behind the scenes) through to the private ip address. Therefore public IP addresses are not actually attached to any network instances, that explains why public ip addresses doesn't appear when you do an "ip addr show":

&nbsp;
<pre>[root@ip-10-0-0-159 ~]# ip addr show
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN
 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
 inet 127.0.0.1/8 scope host lo
 valid_lft forever preferred_lft forever
 inet6 ::1/128 scope host
 valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9001 qdisc pfifo_fast state UP qlen 1000
 link/ether 0e:a6:d2:74:06:57 brd ff:ff:ff:ff:ff:ff
 inet <span style="color: #ff6600;"><strong>10.0.0.159</strong></span>/24 brd 10.0.0.255 scope global dynamic eth0
 valid_lft 3191sec preferred_lft 3191sec
 inet6 fe80::ca6:d2ff:fe74:657/64 scope link
 valid_lft forever preferred_lft forever
</pre>
Here we only see the private ip address.

&nbsp;

This auto-rerouting of public ip address to the private ip address is referred to as <strong>Network Address Translation (NAT)</strong>.

&nbsp;

&nbsp;
<h2>Default VPC limits</h2>
Each aws account comes with the following limits, all of which can be increased by sending a request to aws:
<ul>
	<li>5 VPCs per region</li>
	<li>5 internet gateways per region, since you are only allowed one internet gateway per VPC, this means that all public subnets in the same vpc must all have the same internet gateway attached. Note, that a</li>
	<li>50 VPN connections per region</li>
	<li>50 customer gateways per region - customer gateways are used to set up VPN connections</li>
	<li>200 route tables per region</li>
	<li>50 entries per route table</li>
	<li>5 Elastic IP Addresses</li>
	<li>100 security groups</li>
	<li>50 rules per security groups</li>
	<li>5 security groups per network interface</li>
</ul>
&nbsp;

&nbsp;

&nbsp;

&nbsp;

Inside this vpc, it will get assigned with an ip address that's fall within this range.

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

When creating a new resource, e.g. ec2 instance, you'll get prompted to specify what vpc to create it in.

The main advantage of vpc is that resources that are created in a single vpc will be part of the same network and can communicate internally without any further netowrking configuration.

VPC themselve does not cost anything, you only pay for the costs of the resources that a vpc houses.

&nbsp;

Here's an overview example:

&nbsp;

<a href="http://codingbee.net/wp-content/uploads/2016/01/architecture_linux.png" rel="attachment wp-att-6421"><img class="alignnone size-full wp-image-6421" src="http://codingbee.net/wp-content/uploads/2016/01/architecture_linux.png" alt="" width="603" height="436" /></a>

&nbsp;

Source: <a href="http://docs.aws.amazon.com/gettingstarted/latest/wah-linux/web-app-hosting-intro.html">Amazon</a>

When creating VPC via the aws web console, you need to provide just the following info:
<p id="fCDPPuH"><a href="http://codingbee.net/wp-content/uploads/2016/02/img_56c0e222b3120.png"><img class="alignnone size-full wp-image-6789 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56c0e222b3120.png" alt="" /></a></p>
Note: you can only attach one Internet Gateway to a vpc at a time.

&nbsp;

&nbsp;
<h2>DNS server</h2>
By default each VPC that you create comes with a dns server. This dns server assigns hostnames to all the hosts that are created inside your VPC:

&nbsp;
<p id="pqYxhRJ"><a href="http://codingbee.net/wp-content/uploads/2016/02/img_56c617798cdd5.png"><img class="alignnone size-full wp-image-6818 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56c617798cdd5.png" alt="" /></a></p>
This means the that an instance's fqdn looks like:
<pre>[centos@ip-10-0-0-159 ~]$ hostnamectl
 Static hostname: <strong>ip-10-0-0-159.ec2.internal</strong>
 Icon name: computer
 Chassis: n/a
 Machine ID: 0af04d3c78a943ae8f3cc26602e374f2
 Boot ID: 6fd874e6b35b40568f53c7c0540259cd
 Operating System: CentOS Linux 7 (Core)
 CPE OS Name: cpe:/o:centos:centos:7
 Kernel: Linux 3.10.0-229.14.1.el7.x86_64
 Architecture: x86_64
[centos@ip-10-0-0-159 ~]$</pre>
&nbsp;

&nbsp;

If you don't want to use this default dns server, and instead you have your own dns server, then you can use this by creating a custom DHCP option set:

&nbsp;
<p id="NVDYsMa"><a href="http://codingbee.net/wp-content/uploads/2016/02/img_56c6189b66d19.png"><img class="alignnone size-full wp-image-6819 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56c6189b66d19.png" alt="" /></a></p>
&nbsp;
<h2>Route tables</h2>
A <a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Route_Tables.html" rel="nofollow">route table</a> helps your vpc to traffic.

When you create a VPC, a route table automatically get's created along with it and this route table is the vpc's "main" route table.
<p id="oqUkWvS"><a href="http://codingbee.net/wp-content/uploads/2016/02/img_56c61d53cb191.png"><img class="alignnone size-full wp-image-6821 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56c61d53cb191.png" alt="" /></a></p>
&nbsp;

By being "main" it means that this route table get's automatically attached to each private/public subnet that get's created in the vpc. This Main route table has an inital route rule called "local" as shown above. This rule makes it makes it possible for your VPC's resources to communicate with each other via the private IP, irrespective of which private/public subnets the resources are in.

You can't delete this "local" route rulem and any new route tables that you create will always have this vpc wide "local" route rule.

Note: A subnet always has exactly one route table attached to it, it can't have more than that, or less than that, this means that you can switch route tables.

You can only attach one Internet Gateway to a route table. In doing so, any private subnet associated with this route table, become public subnets.

Note, you can only have one active internet gateway per vpc.

&nbsp;

<a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Introduction.html" rel="nofollow">Amazon vpc</a>]]></Content>
		<Date><![CDATA[2016-01-16]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Route 53 (DNS) overview]]></Title>
		<Content><![CDATA[<h3>General DNS fundamentals:</h3>
&nbsp;

<strong>Time to Live TTL</strong> - Global dns servers and desktop pcs caches dns resolutions results. The default is 2 days. So if you want to migrate to new a new server, then the best thing to do is first reduce TTL to 300 seconds (5 mins), then wait about 4 days for the new caching to propagate through. Then update entry in route 53. This will reduce intermittent issues, and people will start using the new servers more quickly.

<strong>Top Level Domain Names</strong> - All Domain names are organised into top level domain names, e.g. .com, .net, .uk,....etc. All these Top Level Domain names are controlled by a central organisation called <a href="http://www.iana.org/" rel="nofollow">Iana</a>. They have a <a href="http://www.iana.org/domains/root/db" rel="nofollow">full list of top level domain names</a>. There are Top Level Domain Servers (aka a type of dns server) around the world that redirects traffic to a lower level DNS server. Note: the "co.uk" subdomain is owned by a company called <a href="http://www.nominet.uk/domains/our-domains/uk-domains/" rel="nofollow">nominet</a>, according to the <a href="http://whois.domaintools.com/co.uk" rel="nofollow">whois entry</a>.

<strong>Domain Registrars</strong>: These are companies that have the authority to register subdomains under a top level domains. E.g. godaddy is one of the biggest companies that does this. Domain Registrars registers a subdomain on behalf of it's clients, via a service called <strong>InterNIC</strong>. This service, InterNIC, registers a subdomain, as long as it is unique.

<strong>WhoI</strong><strong>S Database</strong>: This is a central Database that the InterNIC service registers new (successfully processed) subdomains to.

<strong>NS Records</strong>: These is a record that is stored in dns servers, and is made up 2 "Nameserver" urls. It is used by dns servers to forward dns resolution requests to, if it can resolve it itself.

<strong>CNAME</strong>: This a record stored on dns servers that simply is used to resolve one domain name to another. E.g. create a cname record that resolves codingbee.co.uk to codingbee.net

<strong>Alias Records</strong>: This is something that's specific to AWS. It is similar to CNAME, however when creating one in route53, if you selected alias from dropdown when creating the record, then the actual field becomes a dropown list of the following:
<ul>
 	<li><strong>ELB's dns address</strong> - this is useful because ELBs don't have a static ip address, it is always changing, and there are several. You can check this by running a dig command to an ec2 instance that sits behind an ELB.</li>
 	<li>Cloudfront Distributions</li>
 	<li>S3 buckets</li>
</ul>
The main reason that Alias Records exists is so to be able to link up the domain name to an ELB, since ELBs can have several constantly changing IP addresses, due to the nature of how ELBs work internally. This also means that an ELB's ip addresses are never visible from the AWS web console.

One big difference between CNAME and ALIAS records is that it's free to use ALIAS, but you will be charged for each CNAME entry. So always using ALIAS records wherever possible.

&nbsp;

<strong>Here's an example</strong>:

E.g. if a Top level domain server get's a resolution request for the url "codingbee.co.uk", then it will forward this request to a ".uk" dns server, which in turn will forward it onto a "co.uk" dns server, which will get forwarded to a nominet owned server. My domain register, "domainmonster" has it's nameservers registered with nominet, so the nominet server will forward the request to a domain monster server. My webhost provider (justhostme) provided me with there 2 nameserver urls, which I have provided to domainmonster. Hence domainmonster will then resolve the url request to this nameserver.

&nbsp;

<strong>Key points</strong>

When you create a new "Hosted Zone", in route 53, you will find after it is created, it has 2 entries by default, first one is an NS record, and the second one is an SOA record. The NS usually comes with 4 domain addresses by default, for even higher redundancies. You can copy and paste this into your corresponding entry in your domain register's online portal, e.g. godaddy or domainmonster.

&nbsp;

After that's done, the domain registrar's dns servers will start forwarding dns resolution requests to route 53.

&nbsp;
<h3>Route 53 intro</h3>
&nbsp;

AWS primarily uses IPV4, and IPV6 isn't properly supported yet.

route 53 acts as an internal+external DNS. It is commonly used with ELB to route traffic requested from a dommain, to the ELB.

Route 53 provides 2 nameserver values that you can input into a domain registrar (e.g. domainmonster.com).

Route 53 is the name of the dns solution provided by AWS.

Route 53 also lets you register a domain (i.e. no longer need to use domainmonster.com)

Also you can transfer domains to aws.

Hence in summary:

&nbsp;

route 53 offers:
<ul>
 	<li>dns server - resolve urls to ip address</li>
 	<li>register domains</li>
 	<li>transfer in domains</li>
 	<li>latency, GEO, basic, and failover routing policies - these maximises performance and high-availability</li>
 	<li>failover to S3 or Cloudfront -  this serves static content until vpc is back up again. It could deliver static cached data (partial service), or "website is currently down we will be back soon" (no service)</li>
</ul>
&nbsp;]]></Content>
		<Date><![CDATA[2016-01-16]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Amazon S3]]></Title>
		<Content><![CDATA[<ul>
 	<li>it is object based storage (key-value pairs)</li>
 	<li>Read-After-Write consistency for PUTS of new objects. This means you can access the file straight away after uploading the file to S3 for the first time. However it has "eventual consistency" for files that you are updating/deleting. That's because there is a small time lag while AZs are being synced up.</li>
</ul>
&nbsp;

Each object stored in S3 compromises of the following subcomponents:
<ul>
 	<li>Key (name)</li>
 	<li>Value (data)</li>
 	<li>Version ID (used for tracking object versions)</li>
 	<li>metadata (e.g. tags)</li>
 	<li>ACLs</li>
</ul>
&nbsp;
<h2>Bucket Versioning</h2>
This is a boolean setting that you can enable for a given bucket. After it is enabled, you can't disable it.

S3 does versioning based on a snapshot style. I.e. it doesn't do incremental version. This means if you have a 1GB file, and have 10 version of it, then you will end up using 10GB worth of storage space, hence versioning becomes expensive for big files.

&nbsp;

&nbsp;
<h2>Cross Region replication</h2>
By default each object is replicated across all AZs inside a region, in order to achieve 11 9s durability (with the exception of RRS). You can increase durability further by enabling Cross Region Replication boolean on a bucket. Note this requires versioning to be enabled on the bucket.

You need to create a new bucket in the new region, so that the 2 buckets can keep in sync with eachother. A bit like how rsync works.

Note: existing objects in the bucket won't get replicated, just the newly created objects only.

Replication also requires an IAM role, this will be added for you. You just have to acknowledge it.

&nbsp;

&nbsp;

&nbsp;

&nbsp;

Amazon's Simple Storage Service (aka S3), can store unlimited "objects".

S3 Objects: These are essentially static files with metadata attached to it.

The metadata is in the form of key-value pairs. Some of these key-value pairs are assigned by aws, whereas others can be user-defined. An example of aws metadata is whether storage type is (reduced reduncany storage) RRS or glacier.

S3 works a bit like how your dropbox folder works, but S3 also has the following features:
<ul>
 	<li>send content via CDN to Cloudfront</li>
 	<li>control who can access to specific objects</li>
 	<li>version control objects (a bit like github)</li>
 	<li>can act as a simple static website, when linked up with route 53.</li>
</ul>
Here are the main characteristics of S3:
<ul>
 	<li>High Availability - Objects stored in s3 are actually synchronized across all AZs in the given region. However it's not synced across regions. However for the biggest region, <strong>US-east-1 (aka US Standard Region)</strong>, the syncing takes a few second longer because all the AZs are so far apart, geographically. So all the AZs will eventually become consistent, after a few seconds, this phenomenon is called "<strong>eventual consistency</strong>". This means when you upload a file, it could get uploaded to one AZ but if an app immediately tries to access it, it might get an error because it is trying to download it from another AZ that is lagging a few seconds behind. All the other regions don't have this problem.</li>
 	<li>S3 provides a 99.999999999% guaranteed uptime. This also means none of your files will get lost or corrupted.</li>
 	<li>S3 provides 99.99% availability. This means that S3 rarely ever goes down for maintenance purposes.</li>
 	<li>S3 is mainly used for file storage</li>
 	<li>In S3, instead of storing files in folders, you store them in things called "<strong>buckets</strong>". bucket names are unique across all regions around the world, i.e. in the same way that a website address is unique.</li>
 	<li>S3 provides unlimited storage, however the more storage space you use, the more you pay.</li>
 	<li>The size of the file can range from 1Byte to up to 5TB.</li>
</ul>
&nbsp;
<h2><strong>Buckets</strong></h2>
In buckets:
<ul>
 	<li>you can set up "<strong>sub namespaces</strong>", these basically like folders that you can use to organise your objects into. In fact inside AWS web consoles they are referred to as folders, e.g. there is a "create folder" button</li>
 	<li>You can only create a maximum of 100 buckets per aws account.</li>
 	<li>Bucket ownership cannot be changed after the bucket has been created.</li>
 	<li>At the time of creating a bucket, you can choose which region  to create it in, across the world. That main reason for this is because the content on S3 buckets may need to be requested by certain parts of the world, therefore to minimize latency, you need to be able to create the bucket that is geographically closer to where it is needed.</li>
 	<li>EC2 instances can transfer data to/form S3 buckets for free, as long as both the EC2 instance and S3 bucket are in the same region. If the S3 bucket is in a different region then it will cost money.</li>
 	<li>Each bucket is owned by whoever created the bucket.</li>
 	<li>each bucket has it's own url (aka endpoint) so to be accessible via a web browser. The structure of this url is "{bucket-name}.s3-website-{region-name}.amazonaws.com". This useful if you want to use your bucket as a static website hoster</li>
 	<li>You can activate logging on a bucket to track all crud related activities.</li>
 	<li>Buckets like all resources can be "tagged" (i.e. labelled) with something meaningful e.g. the name of the app. You can tag your other resources with the same name. In aws this will end up creating "resource groups", which you can then manage as a collective. This is useful because you can have several applications running inside a single aws account.</li>
 	<li>Each object in a bucket has it's own unique download url, which you can use to download, provided you have the necessary permissions.</li>
</ul>
&nbsp;
<h2>Durability and Availability</h2>
S3 comes in three forms, each of which offers a different levels of durability+availability. This in turn is reflected by the pricing:
<ul>
 	<li>Standard S3 - This is the most expensive form - and provides 99.999999999% durability, and 99.99% availability</li>
 	<li>Reduced Redundancy Storage - 99.99% durability and 99.99% availability. The pricing of this is in the middle of standard S3 and glacier.</li>
 	<li>Glacier - This is the cheapest form. objects stored here can take 3-5 hours to retrieve. Glacier works really well in conjunction to S3 Lifecycle policies, which is covered further down.</li>
</ul>
&nbsp;
<h2>Amazon S3 pricing structure</h2>
The costs is calculated based on a combination of:
<ul>
 	<li>charges per GB</li>
 	<li>Number of requests</li>
 	<li>data transfers in/out of region. This means that instances can write/read from it's own S3 for free since they are within the same region.</li>
 	<li>works out cheaper if you bulk buy</li>
 	<li>Whether you opt for <strong>RRS (Reduced Redundancy Storage)</strong>. This is cheaper version of S3 which is disabled by default but you can enable it. The main difference is that it only offers 99.99% durability compared to standards S3's 99.999999999% durability. If you decide to use RRS, then Amazon will send notification if and when an object is lost/corrupted. In reality this could mean that your objects are synchronised across some AZs rather than all of them in the region</li>
 	<li>If we have enabled object versioning, then we have to pay for each versioned object.</li>
 	<li>In a bucket's settings section, you have the option to enable payment requests when another aws user attempts to access the bucket's content. Note, this feature is not compatible with anonymous access.</li>
</ul>
<h2>Amazon S3 security</h2>
Here are some of S3 security features:
<ul>
 	<li>Objects stored in S3 can be encrypted using the "S3 encryption" option. data transfers in/out of S3 can be done via HTTPS protocol</li>
 	<li>All buckets are private by default - and aws users can be granted access by adding access permissions via IAM, they can then access the content by first logging into aws</li>
 	<li>Publicly share bucket content - You can set up downloadable urls</li>
 	<li>share content temporarily using specially generated unique urls that expires after a certain period of time, e.g. 24 hours.</li>
 	<li>You can share buckets between 2 different aws accounts using Access Control Lists (ACLs)</li>
 	<li>You can apply permissions directly to the bucket itself, e.g.
<ul>
 	<li>e..g set up anonymous access, so that the bucket's content is accessible to the whole public</li>
 	<li>Allow access from certain source IP ranges</li>
 	<li>Allow access based on the http requester. E.g. hotlinking of images to display on a website</li>
</ul>
</li>
</ul>
&nbsp;
<h2>lifecycle policies and object versioning</h2>
In s3 you can do unlimited object versioning. Versioning is something that is disabled by default and so if you want to use it then you need to enable it on your s3 bucket. Versioning also keeps track of deleted object, I.e. you can restore deleted objects. Also to save money you can delete particular versions.

Lifecycle policy can be applied at the bucket level, or folder level.

In the same way you enable versioning on an s3 bucket, you can also enable "lifecycle policy" feature, which is used to eventually delete unwanted objects. Here are some common examples:
<ul>
 	<li>So that all versions of an object that's older than 3 months, must be deleted</li>
 	<li>Delete the object (along with all it's versions) if it hasn't been accessed in the last 3 months</li>
 	<li>Archive all versions of an object that are older than 3 months, by sending it to Amazon Glacier</li>
 	<li>Archive by sending to Amazon Glacier, all objects (along with all it's versions) if it hasn't been accessed in the last 3 months</li>
 	<li>You can selectively choose which objects should be actively lifecycled, based on them having a certain prefix in it's filename, or placing them in in sub-namespaces that you have selected to be lifecycled.</li>
</ul>
Lifecycle policy is used to automate the regular housecleaning, which in turn helps reduce the cost of using S3.

Note: once you enable versioning on a bucket, you can't disable it again. The only way around this, is to create a new bucket, migrate content from the old bucket, and then delete the old bucket.
<h2>How can S3 be integrated into application</h2>
S3 isn't just a storage solution. It can act as a place where application can store/retrieve/manage files, similar to how applications store/retrieve/manage data in a database.

here are some of the main uses:
<ul>
 	<li>S3 can be used for hosting static websites, with the help of Route 53</li>
 	<li>S3 can act as the source content for the Cloudfront CDN (Content Delivery Networks). Cloudfront can then cache this data around the world in <strong>edge locations</strong>.</li>
 	<li>Allow people to upload objects to S3 indirectly when using an app, e.g. Dropbox.</li>
 	<li>Make use of the <strong>Multipart</strong> feature, this is a feature you can build into an S3 connected app that allows users to upload files by first splitting the file into smaller parts, and then upload them. This can:
<ul>
 	<li>speed up uploads, if the parts are being uploaded in simultaneously (bandwidth permitting)</li>
 	<li>stop and resume upload</li>
 	<li>Multipart is actually required for uploading really big files, i.e. 5GB or bigger. However Amazon recommends to use Multipart of uploading files of 100MB and bigger.</li>
 	<li>Multipart is feature that you utilize via the the sdk and aws cli</li>
</ul>
</li>
</ul>
&nbsp;
<h2>Event notifications</h2>
You can set up automatic notifications to issue when certain events occur in an S3 bucket (via each bucket's settings section). A common example is when a file gets lost/corrupted while it is in Reduced Reduncancy Storage, RRS. AWS by defualt automatically recognizes these situations and issue a notification. This notification can be sent to number of places including:
<ul>
 	<li>SNS - which could then issue out emails, or send the notification to a website that could regenerate the file, if possible</li>
 	<li>SQS Queue - which could then be use processed in order to regenerate the lost file, if that is possible</li>
 	<li>Lamda - not sure what this is.</li>
</ul>
Another notification that S3 can issue is when an object is created using the API, i.e. when creating via web console, CLI, or SDK. The particular API calls that can create an object (and hence trigger a notification), are:
<ul>
 	<li>copy</li>
 	<li>put</li>
 	<li>post</li>
 	<li>CompleteMultiPartUpload</li>
</ul>
&nbsp;

&nbsp;]]></Content>
		<Date><![CDATA[2016-01-16]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Amazon Glacier]]></Title>
		<Content><![CDATA[This is similar to s3, i.e. it is also used for storing data.

The key difference is that it can take several hours to retrieve data out of Amazon glacier.

Another big difference is that it is a lot cheaper compared to s3 or rrs version of s3.

it is £0.01/GB per month.

or

£1.00/TB per month, which works out to be £12 for the whole year.

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;]]></Content>
		<Date><![CDATA[2016-01-16]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Amazon Storage Gateway]]></Title>
		<Content><![CDATA[Amazon Storage Gateway is a service that's commonly used in conjunction with Amazon S3.

A Storage gateway is a virtual device that you install in your onsite premise, this device then sends data to be stored in AWS, usually S3 or Glacier.

It is service the monitors a local copy of your objects and keeps a copy of them in S3. If your local copy changes, then the copy that is held in S3 changes as well in order to stay in sync. This lets you have a realtime back up of your local data held in S3.

This virtual device is essentially a vm image. To set this up, you do:
<ul>
 	<li>start up a vm from this image.</li>
 	<li>configure the vm to connect to your aws account</li>
 	<li>perform the activation process, e.g. handshake.</li>
</ul>
In the aws console, you then need to choose what type of storage gateway you want to use.

&nbsp;

You can configure "Amazon Storage Gateway" to work in 1 of 2 ways:
<ul>
 	<li><b>Gateway-Cached Volumes:</b> Here you store your data in Amazon S3, and retain your frequently accessed data locally. This is a cheaper option because it cuts down data traffic to/from S3 and reduces the amount of local storage you need. However the downside is that there is higher latency if you try to access an object that isn't locally cached.</li>
 	<li><b>Gateway-Stored Volumes:</b> Here all data are stored locally, however Amazon Storage Gateway will take an initial snapshot followed by regular periodic incremental snapshots and store these snapshots to S3.</li>
 	<li><strong>Gateway Virtual tape Library (VTL)</strong>:  Used for doing backups via other third party apps, e.g. Netbackup</li>
</ul>
To summarize, <b>Gateway-Cached Volumes </b>essentially works like a cloud based storage solution whereas <b>Gateway-Stored Volumes</b> whereas works more like a backup/Distaster-recovery solution.

&nbsp;

&nbsp;

&nbsp;

https://aws.amazon.com/storagegateway/

&nbsp;

This I think is how dropbox works.]]></Content>
		<Date><![CDATA[2016-01-16]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Amazon Import/Export]]></Title>
		<Content><![CDATA[If you have 20 billion terrabytes of data stored locally and you want to upload them AWS, then that will take a long time and also hog all your network bandwith as well.

That's why in this situation it is better to copy all your data onto hdds and post the hdds to Amazon using courier companies like, Fedex, UPS, DHL...etc. There's 2 ways to to do this:
<ul>
 	<li>Import/Export Disk - you buy a hdd from shop, load it with data, post it to AWS. AWS then loads data into S3/Glacier/EBS then post back your hdd.</li>
 	<li>Import/Export Snowball - Aws posts you with a big heavy duty storage device, which you then load up and post back. Ideal for petabytes of data.</li>
</ul>
Amazon will then import the data into one of the following:
<ul>
 	<li>S3 (disk and snowball)</li>
 	<li>EBS  (disk only)</li>
 	<li>Glacier storage (disk only)</li>
</ul>
Amazon will do this withing 24 hours of receiving the delivery.

&nbsp;

You can also do the reverse, i.e. get Amazon to export your data out of AWS and post it back to you, but this only limited to exporting from S3. This avoids hogging the network bandwith.

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;]]></Content>
		<Date><![CDATA[2016-01-16]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Amazon Relational Database Services (RDS)]]></Title>
		<Content><![CDATA[Amazon RDS is a service that makes it easy to set up, operate, and scale a relational database. Amazon RDS supports the following database engines:
<ul>
 	<li><a href="https://aws.amazon.com/rds/aurora/">Amazon Aurora</a> - this is a fork of mysql. this means that if you are familiar with mysql, then this works exactly the same way. It is 5 times faster then mysql. This isn't free, but it is about a 10% of the price of other commercial alternatives such as oracle db.</li>
 	<li>Oracle</li>
 	<li>Microsoft SQL Server</li>
 	<li>PostgreSQL</li>
 	<li>MySQL</li>
 	<li>MariaDB.</li>
</ul>
&nbsp;
<h2>Fully Managed service</h2>
With Amazon RDS, you get a "Fully Managed" service. A fully managed service is a service where:
<ul>
 	<li>Your DB software of choice is preinstalled and configured for you.</li>
 	<li>You are not allowed to access the operating system (e.g. via ssh) that the database is running on. Instead AWS will maintain the OS for you, e.g. installing OS security patches.</li>
 	<li>RDS will automatically take regular db backups for you - These are point in time snapshots. However you can also take manual backups via your db's cli. These automated backups are also incremental backups</li>
 	<li>RDS will automatically upgrade your db software for you, you can choose to enable either automatic minor, major, or all upgrades</li>
 	<li>The underlying instance that is provideing the service will automatically scale up/down, so that you don't have to worry about it.</li>
 	<li>It has built-in fault tolerance</li>
 	<li>It has built-in high-availability</li>
 	<li>High scalability - e.g. can add extra cpu cores, ram, or diskspace while db is running.</li>
 	<li>Automatic recovery if there is a failover</li>
 	<li>You can enable "multi-availability-zone deployments" with a single click</li>
 	<li>You can make use of <a href="https://aws.amazon.com/rds/details/read-replicas/" rel="nofollow">read replica</a>. This makes a copy of the main db, that is kept in sync with the main db (but not in real-time), and it's main purpose is to respond to read related queries. This eases the burden on the main db. Read replica is available for:
<ul>
 	<li>mysql</li>
 	<li>postgres</li>
 	<li>aurora</li>
</ul>
</li>
</ul>
&nbsp;

&nbsp;
<h2>Multi-Availability-Zone deployments</h2>
Multi-AZ is a Disaster recovery feature. I.e. if a DB fails, then there is another DB in standby mode that can aws will automatically fail over to. Although there are side benefits of Multi-Az too, e.g. perform db upgrades with zero downtime.

In a <a href="https://aws.amazon.com/rds/details/multi-az/" rel="nofollow">RDS Multi-AZ deployment</a> setup you have 2 databases, the first db is a the active db that acts as your application's main db. The second db is in a different availability zone, and acts as a passive db, aka standby db. The passive db is kept in sync with the active db in real-time, which is made possible because each db (sql) transtaction is applied to both active and passive db simultanuously. This setup provides the following benefits
<ul>
 	<li>You can change your disk types with no downtime</li>
 	<li>you can change your rds's instance type with no downtime</li>
 	<li>This feature is really important for building highly available, fault tolerant systems - bacause the passive db acts as a failover db, which becomes active if the main db fails for any reason.</li>
 	<li>We can initiate a manual failover if needed for any reason</li>
 	<li>backups are taken agains the passive db, so that active db's performance is not affected and there is no downtime.</li>
 	<li>Do OS updates, or db software updates with no downtime - this happens in the following setups
<ul>
 	<li>1. failover to passive db</li>
 	<li>2. Take primary db offline, perform os/db-software updates to primary instance</li>
 	<li>3. replicate passive db's data back to primary db</li>
 	<li>4. switch back to primary db</li>
 	<li>5. Take passive db offline, and perform os/db-software updates to primary instance</li>
 	<li>6. Get passive db to sync up with primary db again</li>
</ul>
</li>
</ul>
Here are a few thinkgs to remember about this setup:
<ul>
 	<li>The primary db and it's passive db(s) are in the same region (to minimise latency), but in different availability zones (to maximize fault-tolerance/high-availability).</li>
 	<li>You don't have to worry about managing your passive dbs, aws will automatically manage all this for you behind the scenes.</li>
 	<li>If there is a main db failure, then aws straight away updates  the db's "cname" to point to the passive db's endpoint. Hence this appears seamless and downtime for the end user.</li>
</ul>
&nbsp;
<h2>RDS Underlying instance</h2>
When creating an rds:
<ul>
 	<li>you have a large choice of intance types to choose from</li>
 	<li>choose a disk size ranging from 5GB to 3TB</li>
 	<li>Can choose disk type, (burstable SSD) or Provisioned IOPS (EBS)</li>
</ul>
&nbsp;
<h2>RDS backups</h2>
There are 2 types of RDS backups:
<ul>
 	<li>Automated backups</li>
 	<li>Database Snapshots</li>
</ul>
Whether you are using <em>RDS multi-AZ deployment</em> or not, db's backup are still made.

In both cases, When you restore (aka create) a db from an existing backup, then the new db ends up with a new endpoint.

&nbsp;
<h3>Automated Backups</h3>
These are performed daily by AWS automatically. You can specify a retention period for this time of backup, which can range from 1 day to 35 days. If you set the retention period to 35 days, then it will let you 'rewind' your db to any point in time within the last 35 days. The default retention period is 7 days.  These backups are full daily backups, and transaction logs are also captured throughout the day.

Some key facts:
<ul>
 	<li>All the backups gets deleted when you delete the database itself</li>
 	<li>All backups are stored in S3 for free</li>
 	<li>When doing a point-in-time restore, you don't select snapshot to restore with, instead you choose the db itselt, and select point-in-time restore under actions. RDS will then choose the appropriate snapshot along with transaction logs to restore from.</li>
 	<li>For backups to occur, it's recommended that our db uses a <a href="https://en.wikipedia.org/wiki/Database_engine" rel="nofollow">transactional db engine</a>, e.g. for mysql, use InnoDB. This type of db engine, essentially logs every sql crud queries into log files. This means that are you have restored a snapshot, you can then rewind to a particular transaction (and consequently point-in-time) within the snapshot itself.</li>
 	<li>Backup are taken during a particular window - e.g. 3am in the morning. You can choose when this window is.</li>
 	<li>Backups are performed without stopping the DB, however sql queries are paused to allow the backup to take place. This can cause a slightly longer latency while the backup is occuring.</li>
</ul>
&nbsp;
<h2>Database Snapshots</h2>
These are backups created manually. They exist even after the db is deleted. Hence you have to manually delete them if you no longer need them.

&nbsp;

&nbsp;

&nbsp;

&nbsp;
<h2>Read Replicas</h2>
Unlike Multi-AZ, read replicas are used for scaling.

Read replicas are secondary db's that supports the main db (in the sense that they can be used to respond to read only sql queries). each sql query gets applied to the main db first, and the resulting changes are then applied to the replica db. This means it is asynchronous, rather than synchronous (i.e. simultaneous). Read replica's main purpose is to reduce the load on the primary db instance

Here are the key points about read replicas:
<ul>
 	<li>Only mysql, postgres, aurora, and mariadb offer read replica setup</li>
 	<li>read replicas can be created from other read replicas, although this will cause even more latency. Hence, it's best to create read replicas direct from a db.</li>
 	<li>each main db can have multiple read replicas, which can be in different AZs. This makes read-replicas ideally suited for applications that does db "reading" most of the time, and comparatively little writing, e.g. wikipedia website. I.e. we can use read replica to effectively scale out read activities. Each DB can have a maximum of 5 read replicas associated with it.</li>
 	<li>We can monitor replication lag between main db and read replicas, using cloudwatch</li>
 	<li>Read replica is only compatible with dbs that uses a transactional db engine. E.g. you have to use InnoDB iwth msyql</li>
 	<li>You can use read replicas to feed data to data warehouses, rather than main db feeding this data. Hence minimises the main db's burden.</li>
 	<li>We can promote a read-replica to become the primary db</li>
 	<li>You must have automated backups enabled to use read replicas.</li>
 	<li>Additinonal benefits that applies to Mysql only:
<ul>
 	<li>You can create read replicas in different regions, which is useful for high availability, and reduced lags. You can also do this with mariadb too.</li>
 	<li>You can create a read replica for a db that is actually outside of aws, e.g. an on premise db server.</li>
</ul>
</li>
</ul>
When to use read replicas:
<ul>
 	<li>Use when application has a lot of read requests, compared to write requests</li>
 	<li>Provide various systems, e.g. datawarehouse, it's own dedicated read replica to pull data from.</li>
 	<li>importing/exporting data between database outside aws, and a read replica, you can then promote this read replica to become primary db</li>
 	<li>If you wan to do db reindexing. Do this on a read replica, then promote it to become main db, and demote main db to read replica</li>
</ul>
&nbsp;
<h2>Monitoring RDS instances</h2>
There are 2 ways to monitor your RDS setup:
<ul>
 	<li>SNS notifications</li>
 	<li>Cloudwatch</li>
</ul>
<h3>SNS Notifications</h3>
You can easily configure RDS to link up with the SNS service, so that you get notifications when any of the following occurs:
<ul>
 	<li>snapshots</li>
 	<li>parameter group changes</li>
 	<li>option changes</li>
 	<li>Security Group changes</li>
</ul>
<h3></h3>
<h2>Cloudwatch</h2>
Hardware and performance is monitored via cloudwatch.  For a normal EC2 instances, we need to install a few scripts into our instances to get access to a fuller range of cloudwatch monitors. However with RDS, all these scripts are already set up for you. This means we use cloudwatch to monitor/track:
<ul>
 	<li>CPU usage</li>
 	<li>ram usage</li>
 	<li>freeable memory (possible with preinstalled scripts)</li>
 	<li>swap usage</li>
 	<li>disk usage</li>
 	<li>read/write IOPS</li>
 	<li>db connections</li>
 	<li>read replicate latency logs</li>
 	<li>read/write throughput</li>
</ul>
&nbsp;

&nbsp;
<h2>Subnet Groups</h2>
As part of creating an RDS instance, you have to specify which VPC it needs to be created in. Now if you enable multi-AZ deployment (or create read replicas in other AZs), then before creating an RDS instance, you need to first ensure your VPC has subnets attached to it which in turn belongs to different AZs.

However you might have several subnets across different AZs, and you don't want RDS to use all those subnets. In this situation, you need to apply a restriction, and this is done by creating "subnet group":
<p id="PTtRqKA"><a href="http://codingbee.net/wp-content/uploads/2016/02/img_56c0c44fc03c3.png"><img class="alignnone size-full wp-image-6778 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56c0c44fc03c3.png" alt="" /></a></p>
Now you can create a rds db, and specify which vpc, followed by which group of subnets (i.e. subnet group) you want RDS to have access to.

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;
<h2>The drawbacks of RDS compared to EC2 based databases</h2>
<ul>
 	<li>you have less control+freedom, e.g. can't ssh into instance.</li>
 	<li>you can't use some of your db's features, e.g. setting up mysql clusters</li>
</ul>
&nbsp;
<h2>The RDS creation process</h2>
&nbsp;
<p id="nwMmEty"><a href="http://codingbee.net/wp-content/uploads/2016/02/img_56c09b23201ee.png"><img class="alignnone size-full wp-image-6768 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56c09b23201ee.png" alt="" /></a></p>
Here I selected postgresql, then
<p id="KnoDfAM"><a href="http://codingbee.net/wp-content/uploads/2016/02/img_56c09b03be5dc.png"><img class="alignnone size-full wp-image-6767 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56c09b03be5dc.png" alt="" /></a></p>
Notice we can enable  Multi-AZ Deployment feature by simply enabling the correct checkbox.
<p id="gDykmyZ"><a href="http://codingbee.net/wp-content/uploads/2016/02/img_56c09d259b8bc.png"><img class="alignnone size-full wp-image-6771 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56c09d259b8bc.png" alt="" /></a></p>
&nbsp;

Notice you can choose yes/no for multi-AZ.
<p id="JIAEiZd">You also have a long list of isntance types</p>
storage type is either general ssd or provisioned IOPS, or magnetic:
<p id="tCvHDXt"><a href="http://codingbee.net/wp-content/uploads/2016/02/img_56c09d82dfac6.png"><img class="alignnone size-full wp-image-6772 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56c09d82dfac6.png" alt="" /></a></p>
If we end choosing the wrong storage, then it can be easily switched to another (if we have Multi-AZ enabled) with only a max of a couple of minutes downtime.

The next screen is:
<p id="XBdIoIt"><a href="http://codingbee.net/wp-content/uploads/2016/02/img_56c0a39a24d1f.png"><img class="alignnone size-full wp-image-6773 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56c0a39a24d1f.png" alt="" /></a></p>
<p id="mOmTTua"><a href="http://codingbee.net/wp-content/uploads/2016/02/img_56c0a41b18166.png"><img class="alignnone size-full wp-image-6774 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56c0a41b18166.png" alt="" /></a></p>
Once you click on the launch instance button. you get:
<p id="siHJLSN"><a href="http://codingbee.net/wp-content/uploads/2016/02/img_56c0cecdf3854.png"><img class="alignnone size-full wp-image-6780 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56c0cecdf3854.png" alt="" /></a></p>
Notice, that a ip address isn't displayed. Instead we have an endpoint. This is the url we use to connect to the db from a remote ec2 instance. It is unique to our db, and is referred to as a c-name. You can give this name a nicer alias, via route 53.

Now we are ready to connect to this db.

&nbsp;

&nbsp;
<h2>Connecting to RDS db from an EC2 instance</h2>
This is quite straight forward, there are 3 things you need to ensure:
<ul>
 	<li>the security group that is attached to the rds instance is listening on the db's port open, this port can accept source connections from the ec2 instance's private ip address, security group, or subnet range. You just have to specify one of these 3 as the source.</li>
 	<li>The EC2 instance is in the same vpc as the rds</li>
</ul>
&nbsp;

Then you install the relevant db client software, and test your connection, here is an example of what to do inside an ec2 instance:

&nbsp;
<pre># The following link was found on: http://yum.postgresql.org/repopackages.php
$ wget http://yum.postgresql.org/9.5/redhat/rhel-7-x86_64/pgdg-centos95-9.5-2.noarch.rpm
$ yum localinstall pgdg-centos95-9.5-2.noarch.rpm
# the following is the client software only
$ yum install postgresql95-9.5.1   # Note: the server softare is something like: postgresql95-server 
$  psql --version
psql (PostgreSQL) 9.5.1
$ psql -h cb1-postgresql.c2tfibpbfgeb.us-east-1.rds.amazonaws.com -U postgrescb1 -d cb1mydb
Password for user postgrescb1:
psql (9.5.1, server 9.4.5)
SSL connection (protocol: TLSv1.2, cipher: ECDHE-RSA-AES256-GCM-SHA384, bits: 256, compression: off)
Type "help" for help.

cb1mydb=&gt;
cb1mydb=&gt; \l
                                      List of databases
   Name    |    Owner    | Encoding |   Collate   |    Ctype    |      Access privileges
-----------+-------------+----------+-------------+-------------+-----------------------------
 cb1mydb   | postgrescb1 | UTF8     | en_US.UTF-8 | en_US.UTF-8 |
 postgres  | postgrescb1 | UTF8     | en_US.UTF-8 | en_US.UTF-8 |
 rdsadmin  | rdsadmin    | UTF8     | en_US.UTF-8 | en_US.UTF-8 | rdsadmin=CTc/rdsadmin
 template0 | rdsadmin    | UTF8     | en_US.UTF-8 | en_US.UTF-8 | =c/rdsadmin                +
           |             |          |             |             | rdsadmin=CTc/rdsadmin
 template1 | postgrescb1 | UTF8     | en_US.UTF-8 | en_US.UTF-8 | =c/postgrescb1             +
           |             |          |             |             | postgrescb1=CTc/postgrescb1
(5 rows)

cb1mydb=&gt; \du
                                  List of roles
   Role name   |                   Attributes                   |    Member of
---------------+------------------------------------------------+-----------------
 postgrescb1   | Create role, Create DB                        +| {rds_superuser}
               | Password valid until infinity                  |
 rds_superuser | Cannot login                                   | {}
 rdsadmin      | Superuser, Create role, Create DB, Replication+| {}
               | Password valid until infinity                  |
 rdsrepladmin  | No inheritance, Cannot login, Replication      | {}
cb1mydb=&gt; \q
$
</pre>
&nbsp;

&nbsp;

https://aws.amazon.com/rds/]]></Content>
		<Date><![CDATA[2016-01-16]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Amazon ElastiCache]]></Title>
		<Content><![CDATA[Databases can often get the same query to proces, and each time it returns the same output over and over again. This can slow down the db's performance.

To overcome this, you can use a caching service. This service will intercept the repetitive queries and send back the cached output. The database will keep the cacheing service up to date with the latest data.

&nbsp;

There are  2 main caching software, they are:
<ul>
	<li><a href="http://redis.io/" rel="nofollow">redis</a></li>
	<li><a href="http://memcached.org/" rel="nofollow">memcached</a></li>
</ul>
Amazon ElastiCache is a fully managed in-memory cache engine. Once again, you can't access the underlying OS that the cache engine is running on. You can set up Amazon ElastiCache to run using either of above 2 caching engines only.

Note, your application needs to be written in a way to support Redis/Memcached.

&nbsp;

&nbsp;

&nbsp;

https://aws.amazon.com/elasticache/

&nbsp;

&nbsp;

&nbsp;

https://github.com/theforeman/foreman_memcache]]></Content>
		<Date><![CDATA[2016-01-16]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Amazon DynamoDB]]></Title>
		<Content><![CDATA["DynamoDB" is a "NoSQL" database rather than a relational db.

A relational db stores data about an object across a number of tables to avoid duplicate data. However in NoSQL, each object is stored in it's own document. This means that NoSQL performs faster compared to relational dbs since it doesn't need to construct a "joined table". sql db's performance gets worse the more data it houses whereas with NoSQL it can scale more easily. However there are lots of data duplication in NoSQL therefore a  NoSQL db requires more disk space than sql db to store the same information.

Note: <a href="https://en.wikipedia.org/wiki/MongoDB">MongoDB </a>is an example of a NoSQL database.

&nbsp;

"Amazone DynamoDB" is a NoSQL based fully-managed db managed service. As mentioned earlier, fully-managed covers:
<ul>
	<li>auto-scaling up and down</li>
	<li>underlying OS maintenance done by AWS</li>
	<li>AWS does the fault-tolerance and high-availability for you.</li>
</ul>
&nbsp;

&nbsp;

https://aws.amazon.com/dynamodb/getting-started/]]></Content>
		<Date><![CDATA[2016-01-16]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Amazon Redshift]]></Title>
		<Content><![CDATA[&nbsp;

This is amazon's datawarehousing service]]></Content>
		<Date><![CDATA[2016-01-16]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Amazon Elastic MapReduce (EMR)]]></Title>
		<Content><![CDATA[3Companies often wants to analyse huge amount of data obtain statistics/metrics, identify trends, or find meaningful insights. These can then be used to influence business decisions.

All this data crunching will require a lot of computing resources. One way to do this is to use a software like <a href="http://www.tutorialspoint.com/hadoop/" rel="nofollow">Hadoop</a> to manage this for you. Hadoop will distribute the data crunching task evenly across a cluster of servers. However setting up a Hadoop cluster can be difficult, time consuming and expensive. That's because:
<ul>
 	<li>You need to buy the servers to make up your cluster</li>
 	<li>Install your OS on them</li>
 	<li>install, configure, and tune Hadoop in your cluster</li>
</ul>
You need to do all this before you can start doing any data crunching.

However with Amazon Elastic MapReduce (EMR), you get a fully managed hadoop service already set up for you.

With Amazon EMR, all your raw data is stored in Amazon S3, and Amazon EMR starts up a Hadoop cluster of instances to crunch through all the data. The output (aka results) from all the number crunching then gets stored in Amazon S3. EMR can use other AWS based service sources/destinations aside from S3, e.g. DynamoDB or Redshift (datawarehouse).

Amazon EMR creates the hadoop cluster for you (i.e. it creates all the EC2 instance that makes up the cluster), and automatically destroys the cluster as soon as it is no longer required (or you can leave it running for future data crunching job). That basically means you only pay for what you use.

With Amazon EMR you can decide how quickly you want your output/results, and EMR will spin up/down EC2 in order to meet your timeframe.

You can also have multiple EMR clusters for multiple data crunching jobs using the same data set thats stored in S3 (or other AWS services)

In terms of pricing, you only pay for the instances that make up each of your cluster. Also you have to pay for the S3 costs too.

SInce EMR makes use of ec2 instances (that are running Hadoop) behind the scene, it means you can ssh into these instances.  Aside for this exception, EMR is fully managed service too.

&nbsp;

As mentioned before, a Hadoop cluster is made up of EC2 instances. One of these instances acts the Hadoop master, and the rest acts as Hadoop slaves.
<h2>Hadoop Master/Slaves</h2>
If you have a 100GB text file and your not using hadoop, but instead just using a normal EC2 instance that has 16GB of RAM. Then when it attempts to process this file, it first tries to load the whole text file into memory. This will fail since your EC2 instance only has 16GB of ram. when in fact you need 100GB of ram. In this scenario, your EC2 instance will try to use swapdisks which again would cause more problems and performance issues.

That's why Hadoop uses master/slave archictecture. The master breaks down the data into smaller chunks and distribute the chunks to the slaves. EMR has builtin logic on what the size of the chunks should be, which can either be 64MB or 128MB. However EMR lets you set this as well, by setting the "input split size" setting. The master then distributes these chunks to the slaves.

The hadoop slave have 2 types of process that are running, "mappers" and "reducers"

EMR is usually used to process huge amounts of unordered data. This data could be in the form of a 10GB text file. EMR is used to process this high amount of data so that we can then run queries to get meaningful info and trends from this data.

To use EMR, you need to write two types of processes (which are a bit like shell scripts). They are called:
<ul>
 	<li><strong>A mapper</strong>: This loads the chunk of data in the RAM, then starts processing it. This processing  involves scanning the data and then categorises it. It then sorts all the categories based on size. This process is run the Hadoop slave nodes. You can have multiple mappers running on a slave. EMR decides on this optimum number. However you can override this setting if you want. Having multiple mappers running on a slave, means the hadoop slave can process multiple 64MB/128MB chunks of data in parrallel.</li>
 	<li><strong>A reducer</strong> - Each category of data that is created by the mapper process is then passed onto a process called a 'reducer'. This process analyzes the data in the category in greater details in order to derive useful metrics about the data. You can also have multiple reducers running on each slave. However the lion share of the processing is performed by mappers, so you normally have at least twice as many mappers than reducers running on an instance.</li>
</ul>
These 2 processes are the core of the big-data processing concept, hence this server is called Elastic MapReduce.

EMR automatically shutdown slaves that become idle

&nbsp;

There are a few open source apps that are designed to run on Hadoop clusters:
<ul>
 	<li><a href="https://hive.apache.org/" rel="nofollow">Apache Hive</a></li>
 	<li><a href="https://pig.apache.org/" rel="nofollow">Apache Pig</a></li>
</ul>
&nbsp;
<h2>What does EMR offer</h2>
When you create an EMR resource, EMR will spin up EC2 instance using AMIs that already has the hadoop software pre-installed. You can log into these EC2 instances.

EMR can make the following AWS service as the big data sources (our output destinations):
<ul>
 	<li>S3</li>
 	<li>Redshift (Datawarehouse)</li>
 	<li>DynamoDB</li>
 	<li>RDS</li>
 	<li>AWS Glacier</li>
</ul>
You can also feed in a "Bootstrapper" into EMR. This passes in configurations into your EMR as part of it's initialization process. This is a bit like an EC2 user-data script, but at the EMR/Hadoop level. The bootstrap is executed just before the hadoop cluster starts, so you can specify custom hadoop configurations to override any defaults.

EMR has builtin logic to determine chunk sizes, and number of mappers that can run on each slaves, that's so that all chunks are loaded into a slave's RAM as much as possible to get the best possible performance. As mentioned earlier you can override the chunk size settings and number of mappers per slave settings.

&nbsp;
<h2>Creating an new EMR cluster</h2>
Here is what you see when creating a new cluster:

&nbsp;
<p id="grbPjNE"><a href="http://codingbee.net/wp-content/uploads/2016/05/img_5725b9641950e.png"><img class="alignnone size-full wp-image-7007 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/05/img_5725b9641950e.png" alt="" /></a></p>
<p id="GjyBFzM"><a href="http://codingbee.net/wp-content/uploads/2016/05/img_5725b98697416.png"><img class="alignnone size-full wp-image-7008 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/05/img_5725b98697416.png" alt="" /></a></p>
Notice you can specify the size of your cluster, and you must have exactly 1 node acting as the master, and all the rest are slaves.

You can also specify the instance types. <a href="http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/TaskConfiguration_H1.0.3.html" rel="nofollow">Note the bigger the instance the more mappers/reducers it can have available per slave</a> (although you can override these), but also more pricey.

&nbsp;

In a well optimized EMR setup, you should have the maximum amount of mappers that uses up all the available ram without resorting to swaps or EBS. The master's job queue has to be as short as possible. Also you shouldn't have any idle mappers.

One option could be to work out exactly how many chunks there will be, then from this set up an EMR instance to have the same number of mappers available (maxing out the ram in each slave). That way all the chunks end up being processed in parrallel, and you have a zero job queue. This ends up doing the processing in the fastest possible time.
<h2>Use S3 as a filesystem instead of HDFS</h2>
HDFS: HaDoop FileSystem

By default EMR uses it's local filesystems, called HDFS for storing reducer outputs (however the S3 filesystem is also installed by default too, but it's muli-part upload feature isnt enabled, but can be enabled via bootstrap). There are 2 problems with this:
<ul>
 	<li>The outputs isn't persistant, i.e. the outputs gets deleted when you delete the EMR cluster. The way round this is to copy the output to S3.</li>
 	<li>Copying outputs to S3 is an unnecessary extra step that takes up time. A better option is mounting S3 as a filesystem, as discussed next</li>
</ul>
If your data source is in S3, then you can kill several birds with one stone by mounting an S3 bucket as a filesystem on your slaves. The benefits of this is:
<ul>
 	<li>eliminates the extra step of copying data from S3 to Hadoops internal filesystems, hence saves time and money.</li>
 	<li>eliminates the extra step of copying output Hadoops internal filesystems to S3, hence saves time and money.</li>
 	<li>Avoids losing the outputs that are stored in HDFS</li>
</ul>
Even if the data source is elsewhere, e.g. rds, it's still better to S3 for 2 of three benefits listed above.

The Master only responsibility is to know where the data is coming, break it into chunks and distribute the chunks to all available mappers on slaves.

&nbsp;

https://aws.amazon.com/elasticmapreduce/ (watch the video)]]></Content>
		<Date><![CDATA[2016-01-16]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Amazon Simple Workflow Service (SWF)]]></Title>
		<Content><![CDATA[In companies you can document business processes in the form of swim-lane diagrams.

Some lanes requires human interaction (e.g. clicking the "approve" button), whereas other lanes are automated.

For example you went on a business meeting to another city and you want to claim for hotel/travel expenses. Then you submit it online. Then it goes to manager to approve it, then it goes to hr to approve it as well, then an automated system credits your bank account with a refund. All this can span over a couple of weeks.

SWF is designed to automate this as far as possible. SWF is long term processing workflow solution.

SWF has built in auto-scaling up/down (i.e. auto-create/destroy EC2 instance as and when necessary).

SWF can be configured to use onsite servers rather than EC2 instances.

SWF garantees execution of workflow no matter how long it takes, i.e. it doesn't hang. This means it always reaches the end of the swim-lane diagram.

&nbsp;
<h2>Distinctions between SQS and SWF</h2>
In SQS you have "workers" that regularly polling the SQS for new work, when they are idle.

In SWF however, the SWF service delegates "task" to the "workers" which then performs the work and notifies SWF that it has been completed. There are 2 types of tasks that can be delegated to the workers to perform:
<ul>
 	<li><strong>Activity tasks</strong> - The worker performs a routine piece of work and reports back to the SWF service once it is done. SWF, then moves on to the next step of the workflow.</li>
 	<li><strong>Decision tasks</strong> - This type of task contains information but the current state of the workflow. The worker that receives this type of task (is often referred to as a 'decider'), reviews the state of the workflow before deciding on what to do next.</li>
</ul>
A worker/decider can come in many forms, such as an EC2 instance, or even a human (e.g. has to press the approve/reject button).

So in SQS we have "messages" in the queue, whereas in SWF we have "tasks" which are performed in specific order through the workflow.

SWF doesn't have a GUI, instead it is an API the programmers integrates workers/deciders into. This means that workers/deciders can be something outside of aws, e.g. a vm running on microsoft azure.

A workflow execution is allowed to last up a maximum of one year.

.

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

https://aws.amazon.com/swf/]]></Content>
		<Date><![CDATA[2016-01-16]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Amazon Simple Queue Service (SQS)]]></Title>
		<Content><![CDATA[If you have incoming work requests, then you can queue them up if the existing EC2 instances are pre-occupied, rather than auto-scale up your EC2 resource.

This service works by a new message (in the form of a json file) being created by each incoming work request. This json file get's sent to the SQS service. We then configure all our ec2 instances to poll the sqs when they are idle, and process the next json file (if any) that is in the queue.

E.g. when a user uploads a video to youtube, that video would then need to be transcoded. The youtube website could upload the video to S3 and then "publish a message" to a "topic" in SNS. SNS in turn will then send "push" notifications to all subscription, one of which can be an SQS service. The SQS service can queue all these incoming messages in json form. .

You can also set up auto-scaling based on the length of the queue. If the queue gets too long, then new ec2 instances are created to help cope with the demand.

SQS has built-in high availability. meaning that the same queue exists and in sync across two or more Availability Zones.

SQS is a hosted fully managed service from AWS.  Behind the scene, SQS runs on EC2 instances, and these instance are configured to be highly, by having multiple internal SQS based instance running in different AZs inside a region. All these internal SQS instance keep the queue in sync across the region.

&nbsp;

Some key SQS characteristics:
<ul>
 	<li>Each message in the queue is up to 256KB, which is a quarter of an MB.</li>
 	<li>The message's content can be of any format, text, json, xml, yaml...etc</li>
 	<li>SQS gaurantees to deliver each message to the polling requesters at least once.</li>
 	<li>SQS tries to deliver the messages in the order SQS receives them. But can't gaurantee this order. In a lot of cases the exact ordering doesn't matter, because each message isn't really dependant each other. But if they are then maybe SQS isn't the best option.</li>
 	<li>Some SQS messages might be duplicates. Although SQS will do it's best to keep duplicates to a minimum.</li>
 	<li>By default a message stays in the queue for up to 4 days. However this limit can be increased to a maximum of 14 days. After the message exceeded the limt, AWS will automatically delete the message.</li>
</ul>
&nbsp;

Usually applications (including AWS based apps, such as sqs) are developed to send messages to the SQS queue. You then have separate application (which can be running on EC2 instances) that polls SQS's queue when they are idle, to retrieve the next message in the queue.

AWS charges for SQS in terms of API requests from the workers that are polling the SQS queue.

There are 2 main style that worker (EC2 instance) can poll an SQS for new messages in the queue:
<ul>
 	<li>Long polling -
<ul>
 	<li>the worker connects to SQS for a predefined period of time, e.g. 10 seconds. It will pull down the entire queue of messages along with any new messages that arrived during those 10 seconds. Then it will process all the messages in the queue.</li>
 	<li>The long polling duration can be anywhere from 1 seconds to a maximum of 20 seconds. All these messages can then be taken off the queue.</li>
 	<li>PRO: AWS charges a cheaper price for long polling, because it reduces the number of api requests.</li>
 	<li>CON: If worker dies, then it will have a bigger negative impact.</li>
</ul>
</li>
 	<li>short polling -
<ul>
 	<li>Queries for a quick list of messages in the queue. SQS has to give a quick response so only queries a subset of it's internal sqs instance to compile the queue and return that. Hence this queue is unlikely to be a full queue of messages.</li>
 	<li>PRO: worker gets a faster response from SQS</li>
 	<li>CON: AWS charges more, since there are more frequent API requests.</li>
</ul>
</li>
</ul>
&nbsp;
<h2>Types of coupling</h2>
Tightly coupled - When 2 components are not only linked but are dependent on each other in order to work. This type of coupling should be avoided.

Loosely Coupled - When 2 components are linked to eachother but not dependant. This means you can replace faulty components with minimal impact on other components. E.g. if a jenkins slave fails, then all the jobs stays in the queue until jenkins is replaced.

You can build architecture that are loosely decoupled by implementing the following into your architecture:
<ul>
 	<li>SWF (Simple Workflow Service)</li>
 	<li>SQS (Simple Queue Service)</li>
</ul>
&nbsp;

&nbsp;

&nbsp;

https://aws.amazon.com/sqs/]]></Content>
		<Date><![CDATA[2016-01-16]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Simple Notification Service (SNS)]]></Title>
		<Content><![CDATA[This is AWS's push notification service.

&nbsp;

SNS supports the following target destinations (aka endpoints):
<ul>
 	<li>Amazon SQS</li>
 	<li>Email</li>
 	<li>Email-json</li>
 	<li>SMS</li>
 	<li>HTTPS</li>
 	<li>HTTP</li>
 	<li>Application - e.g. Android App, such as a whatsapp notification</li>
 	<li>AWS Lambda</li>
</ul>
&nbsp;

A lot of AWS own services internally makes use of SNS as well for example S3 RRS makes use of this to send notifications when an object (e.g. file) gets lost or corrupted.

&nbsp;

&nbsp;

The way SNS works is that in the SNS section of the aws console, create create a new SNS resource called 'topic'. You can think of each topic as a noticeboard, or a forum thread. You can then add subscribed to this topic (e.g. by providing your email address, mobile number,...etc), so that when new updates arrives to the topic, sns will send out messages to all the subscriptions attached to the topic.

This could be a good way to regenerate new puppet certificates, if existing S3 stored puppet certificates gets corrupted.

SNS is a good way to loosely decouple your architecture.

&nbsp;

You publish new messages to your topic by writing code to send the messages to your SNS topic. Probably done via the use of

&nbsp;

http://docs.aws.amazon.com/sns/latest/dg/GettingStarted.html

https://aws.amazon.com/sns/]]></Content>
		<Date><![CDATA[2016-01-16]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Amazon Elastic Beanstalk]]></Title>
		<Content><![CDATA[Elastic Beanstalk service automates the building of a middleware servers (e.g. httpd servers, nginx servers,...etc) and then deploys your app into it. I.e. it is perfect for setting up a vanilla httpd server, or Rails server,...etc. This is ideal if you have a relatively simple application you want to deploy, that doesn't require a lot of middleware configurations.

Elastic Beanstalk basically does the following:
<ul>
 	<li>deploys  - install middleware and deploys apps into it.</li>
 	<li>manages - e.g. OS patching, configuring firewalld, ..etc.</li>
 	<li>auto scale up/down - add/removes instance to manage demand.</li>
</ul>
&nbsp;

Elastic Beanstalk can install middleware in order to deploy the following types of applications:
<ul>
 	<li>.net</li>
 	<li>java</li>
 	<li>php</li>
 	<li>python</li>
 	<li>ruby</li>
 	<li>docker containers</li>
 	<li>node.js</li>
</ul>
some of the middleware that Elastic Beanstalk can install+configure are:
<ul>
 	<li>httpd server</li>
 	<li>nginx server</li>
 	<li>passenger server</li>
 	<li>apache tomcat</li>
 	<li>Microsoft IIS</li>
</ul>
&nbsp;

Elastic Beanstalk achieves its job by making use of:
<ul>
 	<li>EC2</li>
 	<li>Simple Notication Service SNS</li>
 	<li>S3</li>
 	<li>ELB</li>
 	<li>Auto-Scaling</li>
</ul>
&nbsp;

All you need to tell the Elastic Beanstalk service is:
<ul>
 	<li>what middleware you want</li>
 	<li>what type of app you have, e.g. ruby app</li>
 	<li>what instance size you want, e.g. t2.micro</li>
</ul>
you can also configure it to deploy applications from git repos.

Elastic Beanstalk tool comes with it's own <a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/eb-cli3-install.html?icmpid=docs_elasticbeanstalk_console" rel="nofollow">cli called eb</a>. This CLI can be used to build dev/test/prod environments based on your dev/test/prod git branches. In fact this is the preferred way using beanstalk.

It's easy to deploy software updates, e.g. just do a git pull.

As an alternative to storing the code in git, you can store it in S3, e.g. in the form of a targz file.

&nbsp;

To start using Beanstalk, you first have to create a new "Application":
<p id="JNWZZcV"><a href="http://codingbee.net/wp-content/uploads/2016/05/img_5725cfd573f45.png"><img class="alignnone size-full wp-image-7016 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/05/img_5725cfd573f45.png" alt="" /></a></p>
You just have to specify the type of middleware (ruby. php, etc.) that you want to deploy.

It will then get started by building a 'default' environment. You can choose to build multiple environments (dev/test/prod) inside your beanstalk based application.

There are 2 types of environments you can create inside your beanstalk 'application' resource:
<p id="APIhnVq"><a href="http://codingbee.net/wp-content/uploads/2016/05/img_5725d17aa2a60.png"><img class="alignnone size-full wp-image-7017 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/05/img_5725d17aa2a60.png" alt="" /></a></p>
If you want an environment that directly handles web requests (e.g. http or https), then you pick web server. However if you want an environment that runs background jobs, then you choose <span class="emphasis"><em>worker environment</em></span>. Worker environment also links well with SQS, i.e. it polls the queue to processes queue jobs.

You can create multiple 'production' webserver+worker environments and then link them to create a single multi-tiered environment.

When creating a new web server environment, you will be prompted for the following:
<ul>
 	<li>middleware type, e.g. PHP, Tomcat, Docker, ...etc.</li>
 	<li>Environment type, here you have the option to set up environment as a single instance, or autoscaling+loadbalancing</li>
 	<li>You then get the option to upload your code, either by specifying an s3 based url to a zip file, or upload a zip file from your local desktop</li>
 	<li>you get a checkbox, to specify whether you want an RDS instance to be created as part of your environment</li>
 	<li>Instance specific details:
<ul>
 	<li>instance type, e.g. t2.medium</li>
 	<li>EC2 key pair, in order to ssh into your EC2 instance.</li>
 	<li>Application healthckeck url - what url to ping to check for health</li>
</ul>
</li>
 	<li>Which VPC to create the environment in, along with ticking which subnets to build the ELB and EC2 instances in.</li>
</ul>
&nbsp;

Each web server environment comes with it's own unique url. You can use this to view your Elastic Beanstalk environment's homepage, which is the php apache's home page, if you selected apache.

You can then add a user friendly url name to point to your environment's ELB. The ELB should be listed in the dropdown list when your creating a route 53, alias based entry.

&nbsp;

&nbsp;

https://aws.amazon.com/elasticbeanstalk/ (watch the video)]]></Content>
		<Date><![CDATA[2016-01-17]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - CloudFormation (Infrastructure As Code)]]></Title>
		<Content><![CDATA[You can manage all your AWS stuff using the AWS gui web console. However everything that you can do using the gui console, can also be done by using either the:
<ul>
 	<li>AWS API, which are accessible via a choice of <a href="https://aws.amazon.com/tools/" rel="nofollow">SDKs</a>.</li>
 	<li><a href="https://aws.amazon.com/tools/">AWS CLI</a> (Command Line Interface) - Linux or Powershell</li>
</ul>
CloudFormation on the other hand lets you document your entire AWS infrastructure (i.e. a vpc) in the form of a json file.

In other words, CloudFormation is the AWS equivalent of writing a vagrantfile for vagrant. The only difference being that your are writing about aws stuff and it is written using json syntax.

You can then store this json file in github and quickly build/rebuild new vpc instances from it. The json file can specify only part of a vpc environment rather than a whole environment, e.g. it could document just a single EC2 instance.

<strong>Stack</strong>: this is an instance of a CF template. You can use the template to created multiple stacks, e.g. a dev stack and a prod stack. A stack is essentially a collection of resources (ec2 instances, rds instances,...etc).

&nbsp;

A CF template can create these resources inside an existing VPC, in which case you need to add a parameter in your CF template for specifying the VPC's ID, or you can write a CF template to create the entire VPC along with all the resources that goes inside it.

<a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html">Anatomy of a template</a>.

&nbsp;

http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/gettingstarted.templatebasics.html

http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/GettingStarted.Walkthrough.html

https://aws.amazon.com/cloudformation/aws-cloudformation-articles-and-tutorials/

http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-using-cloudformer.html

https://aws.amazon.com/cloudformation/aws-cloudformation-templates/

http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/GettingStarted.Walkthrough.html

https://aws.amazon.com/cloudformation/

https://cloudacademy.com/library/?q=cloudformation&amp;label=course

https://www.youtube.com/watch?v=fVMlxJJNmyA

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;]]></Content>
		<Date><![CDATA[2016-01-17]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Managing AWS activities]]></Title>
		<Content><![CDATA[<h2>Identity Access Management (IAM)</h2>
IAM is a service that lets you set user and group permissions on what they are allowed/denied to do.

&nbsp;

It lets you set permissions for resources that belong to the following service categories:
<ul>
	<li>computing</li>
	<li>storage</li>
	<li>databases</li>
	<li>applications</li>
</ul>
These permissions are set specified against particular API-calls/CLI-options/web-gui-console

Here are some examples:
<ul>
	<li>give a user (or group) permission to create new EC2 instances</li>
	<li>deny user (or group) permssions to delete an particular EC2 instance</li>
</ul>
&nbsp;

IAM is very granular and let's you set all kinds of permissions.

http://docs.aws.amazon.com/IAM/latest/UserGuide/introduction.html

&nbsp;
<h2>CloudTrail</h2>
This is a service that logs all aws-console/cli/api activities and who performed them.

It is a logging solution to help identify any security issues.

https://aws.amazon.com/cloudtrail/

&nbsp;
<h2>Cloudwatch</h2>
This is a monitoring service that monitors various service and resources. It can collect and track metrics. It can collect logs for various resources, e.g. cpu utilisation on a given EC2 instance, network bandwith usage....etc.

Cloudwatch ties in with auto-scaling quite closely. E.g. you can instruct cloudwatch to scale-up if cpu usage exceeds 80% or if queue size exceeds 5000 jobs.

&nbsp;

https://aws.amazon.com/cloudwatch/

&nbsp;
<h2>Directory services</h2>
This service allows you to create and sync AWS users and groups based on local Microsoft Active-Directory Server. Alternatively you can create a new Microsoft Active Directory (AD) service inside AWS and sync it up with a local Microsoft AD server.

This makes single sign on possible.

https://aws.amazon.com/directoryservice/]]></Content>
		<Date><![CDATA[2016-01-17]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Identity Access Management (IAM)]]></Title>
		<Content><![CDATA[<h2>Identity Access Management (IAM)</h2>
IAM is a service that lets you
<ul>
	<li>create child aws user accounts,</li>
	<li>create groups, and organise the child accounts into groups</li>
	<li>set user and group permissions on what they are allowed/denied to do.</li>
</ul>
IAM works at the very top AWS level. This means that it lets us create users at the global level, i.e. when you create a user, then that user automatically exists in all regions.

&nbsp;

It lets you set permissions for resources that belong to the following service categories:
<ul>
	<li>computing</li>
	<li>storage</li>
	<li>databases</li>
	<li>applications</li>
</ul>
These permissions are set specified against particular API-calls/CLI-options/web-gui-console

Here are some examples:
<ul>
	<li>give a user (or group) permission to create new EC2 instances</li>
	<li>deny user (or group) permssions to delete an particular EC2 instance</li>
	<li>Give user (or group) access to resources in specific regions</li>
	<li>Give user (or group) access to resources in specific Availability Zones</li>
</ul>
&nbsp;

IAM is very granular and let's you set all kinds of permissions.

Main benefits of IAM are:
<ul>
	<li>Centralized control who which users/groups can interact with what resources</li>
	<li>Customised billing for user or groups, e.g. a group only pays for the resources it has used. Good way to pay for IT project budgets to fund AWS usage.</li>
	<li>restrict access based on originating IP address. E.g. you can't log into aws unless you are connected to your company's network via cable or wifi. This is achieved by specifying ip ranges.</li>
	<li>Let's you manage security credentials - e.g. API access keys.</li>
	<li> Provide temporary user access as and when necessary - e.g. <a href="http://docs.aws.amazon.com/STS/latest/APIReference/Welcome.html" target="_blank">simple token service</a>.</li>
	<li>Integrate with Microsoft Active Directorym which means you don't have to manually create IAM user accounts, and also achieve single-sign-on.</li>
	<li>Create roles, (e.g. read-only roles) with specific permissions. then attach that role user IAM user accounts or groups.</li>
	<li>You can even assign roles to a resource, e.g. EC2 instance, so that it can access the AWS API like a user.</li>
	<li>You can specify password policy (e.g. password must contain at least one digit)</li>
	<li>Set Multi-Factor Authentication on a per user basis. This could mean senior admin user must own a calculator like device that generates a pin to use to login, on top of username+password. This is commonly used in banks.</li>
	<li>Provide pre-designed policy templates out of the box, e.g.:
<ul>
	<li>Admin access</li>
	<li>Power user access - Can do everything accept manage IAM</li>
	<li>Read only access - E.g. for project managers to monitor how much AWS is costing.</li>
</ul>
</li>
</ul>
&nbsp;

In AWS, the "root user" account is the email address you used to create your AWS account. It is best practice not to use this account for day to day task. Instead you should create an "admin" IAM user account asap, then use that account from that point forward.

By default, a user is denied access to everything, until they are allocated permission to access what you allow them to access.

It is best practice to never store or pass security credentials on EC2 instances. Instead IAM does the authentication for you, in the form of IAM roles. It's similar to how <a href="http://software.dell.com/solutions/privileged-management/" rel="nofollow">PAM</a> works. So in practice it will be a bit like opening up a putty session, and entering the hostname, then you log in automatically with your AD credentials without needing type in your username/password.

In other words, IAM roles is a bit like single-sign-on when logging into an EC2 instance.

&nbsp;
<h2>Groups and Roles</h2>
Lets say you  want to grant a specific permission (e.g. create ec2 instance) to a 100 users. Then you can do this by adding it to each user one at a time. Now let's say a couple of weeks later, you want to grant another permission to these 100 users. Once again you would need to assign each user one at a time.  This can get tedious.

A better way is to create a group (with an appropriate name), and then add the 100 users to this group. After that you can then assign permissions to the group as a whole, rather then individually. You can also assign even more permissions to the individuals of the group as well. E.g. if one member in the group is the team leader who requires higher priveleges.

Let's now say that you don't have just one group, but actually 25 groups. Now let's say all these groups needs to have the same set of 15 different permissions. Then adding each permission to each group can get tedious.

A better approach is to group together all the permissions. This grouping of permissions is called a  "role". You need to give this role an appropriate name, e.g. "developers". You can then assign this role to each of the groups.

An IAM <span class="emphasis"><em>role</em></span> is a series of permission policies that are grouped together.

This means that if in a couple of weeks, a new permission needs to be added/revoked from all the groups, then instead of editing this for each group, you can instead edit the single role.

Using groups and roles gives you complete flexibility in how you manage users and permissions:
<ul>
	<li>You can still add individual permissions to a user</li>
	<li>You can still add individual permissions to a group</li>
	<li>a user can be a member of one or more groups</li>
	<li>you can add one or more roles to a group</li>
	<li>you can add one or more roles to a user</li>
</ul>
&nbsp;

&nbsp;

The concept of Groups and Roles can also be applied to resources, e.g. EC2 instances. E.g. you can a cluster of ec2 instance to a group called "projectx", and then assign a role called "projectx-db-access" to this group. This then gives all the ec2 instances access to the RDS service.

&nbsp;

Using roles means that your EC2 instances doesn't need to store any api-keys/login-credentials. Instead If a user logins into AWS and is authenticated via IAM, then the user will then get the role that specifies what instances it has permission to access.

Sometimes you might want to give users AWS access but their credentials are stored in Active Directory that is outside of AWS, or, you might want to give AWS access to third parties so that they can perform an audit on your resources.

For these scenarios, you can delegate access to AWS resources using an <span class="emphasis"><em>IAM role</em></span>. This section introduces roles and the different ways you can use them, when and how to choose among approaches, and how to create, manage, switch to (or assume), and delete roles.

http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html

&nbsp;

&nbsp;

&nbsp;

&nbsp;

http://docs.aws.amazon.com/IAM/latest/UserGuide/introduction.html

&nbsp;
<h2></h2>]]></Content>
		<Date><![CDATA[2016-01-17]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Create IAM user accounts]]></Title>
		<Content><![CDATA[After creating a new user, the user will have a unique ID called a "User ARN" account. It looks something like this for username called "admin":

arn:aws:iam:870464616830:/user/admin

The string of digits indicates the actual aws account.

&nbsp;

IAM user's can log into their account using the AWS account's custom login url, which based on the above example will look something like this:

https://870464616830.signin.aws.amazon.com/console

&nbsp;]]></Content>
		<Date><![CDATA[2016-01-17]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ansible - About this Course]]></Title>
		<Content><![CDATA[<h2>What is Ansible</h2>
Ansible can be thought of as general purpose tool for managing servers. This means that Ansible can be used as a:
<ul>
	<li>Server provisioning tool - build new vm, e.g. in aws. Ansible can also do orchestration, i.e. build+configure servers in a specific sequence. Ansible has a number of modules for communicating with aws, azure, google cloud, openstack,...etc.</li>
	<li>Configuration Management tool - i.e. configure OS and middleware tier</li>
	<li>Deployment tool - i.e. installing and configuring software that has been written in-house</li>
</ul>
&nbsp;

Ansible has a controller-client type architecture, where you have one server (aka the controller) controlling lots of other servers (aka clients). However in ansible, you don't need to install any ansible specific software on the client's themselves. You only install ansible on the controller. I.e. it is an agentless architecture.

The controller communicates with all the clients via standard ssh.

&nbsp;

Ansible playbooks are essentially 1 or more scripts. The puppet equivalent to playbooks is puppet manifests.

You can extend your ansible capability using "ansible modules", similar to how you extend puppet using puppet modules.

http://image.slidesharecdn.com/epam-symfony2-ansible-150221142047-conversion-gate01/95/deploying-symfony2-app-with-ansible-18-638.jpg?cb=1424528565

In order for a client to be controlled by the Ansible server, it needs to have the following minimum requirements:
- SSH daemon enabled (this is normally the case anyway)
- python is installed

&nbsp;
<h2>Comparison with Puppet</h2>
Key differences against puppet:
<ul>
	<li>No single master server. You can simply install ansible on any server and that can then act as the muster. In Puppet, that's not possible because the puppetmasters url is hardcoded into each puppet agent's puppet.conf file.</li>
	<li>Orchestration is done via the "push" system rather than the "pull" system. I.e. you don't need to log into ansible client and then trigger a ansible run.</li>
	<li>The code  in a playbook is executed in the order that they are specified. Where that is not the case with puppet, where if you want you can specify ordering using the before/require metaparameters</li>
	<li>Puppet is based on ruby. However Ansible is based on python, and you can write python code right into your ansible code. Python comes with a huge amount of functions/libraries that you can utilise in your ansible code.</li>
	<li>puppet code is written in it's own custom language, i.e. Domain specific  Language (DSL). However ansible code is written in yaml syntax.</li>
</ul>
&nbsp;
<h2>Advantages of Ansible</h2>
<ul>
	<li>Easy to read syntax</li>
	<li>It is a multi-purpose tool - it can do provisioning, environment orchestration, configuration management tool, deployment tool</li>
	<li>You don't need to install or configure anything on the clients. The clients needs to have ssh, and a relatively recent version of python</li>
	<li>Ansible pushed based - clients don't need to have any services running to periodically do an ansible run. Instead you trigger the run from the controller.</li>
	<li>Easy to build multiple controllers, the clients are not configured to communicate with a particular controller. Hence when number of clients goes up to thousands, then you can quickly build new controllers to handle</li>
	<li>can execute adhoc shell commands on the clients</li>
	<li>builtin modules (puppet's equivalent of resource types). These modules behaves idempotently to bring a ensure a stage.</li>
	<li></li>
</ul>
&nbsp;
<h2>Disadvantages of Ansible</h2>
<ul>
	<li>Abstractions are kept to a minimum, e.g. for installing packages on rhel based OS, you need to use the yum's built-in module, whereas for ubuntu, you use apt's built-in module instead</li>
</ul>
&nbsp;

&nbsp;

See also:

http://docs.ansible.com/ansible/]]></Content>
		<Date><![CDATA[2016-01-29]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ansible]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ansible]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ansible - A "hello world" Playbook]]></Title>
		<Content><![CDATA[Playbooks are written in yaml format, and you can actually choose where to store your playbooks. In my case I'll create a folder called "playbooks" for storing my playbooks, and I'll create this in the root user's home directory:

&nbsp;
<pre>[root@controller ~]# pwd
/root
[root@controller ~]# mkdir playbooks
[root@controller ~]# cd playbooks
[root@controller playbooks]# pwd
/root/playbooks</pre>
In this directory, I'll then create the following file:

&nbsp;
<pre>[root@controller playbooks]# cat HelloWorld.yml
---
- name: This is a hello-world example
  hosts: ansibleclient01.local
  tasks:
  - name: Create a file called '/tmp/testfile.txt' with the content 'hello world'.
    copy: content="hello world\n" dest=/tmp/testfile.txt</pre>
This playbook is designed to create the file "/tmp/testfile.txt" on the client ansibleclient01.local using ansible's <a href="http://docs.ansible.com/ansible/copy_module.html" rel="nofollow">copy module</a>.

&nbsp;

You can think of this playbook as a script that you can execute. You can execute this playbook using the "ansible-playbook" command:

&nbsp;
<pre>[root@controller playbooks]# ansible-playbook HelloWorld.yml

PLAY [This is a hello-world example] *******************************************

TASK [setup] *******************************************************************
ok: [ansibleclient01.local]

TASK [Create a file called '/tmp/testfile.txt' with the content 'hello world'.]
changed: [ansibleclient01.local]

PLAY RECAP *********************************************************************
ansibleclient01.local : ok=2 changed=1 unreachable=0 failed=0

</pre>
This results in the following file being created on the client:

&nbsp;
<pre>[root@ansibleclient01 /]# cat /tmp/testfile.txt
hello world</pre>
&nbsp;

Now let's delete the testfile.txt. Then return back to the controller.

&nbsp;

Now let's create testfile.txt in a different way, this time using a static file. we do this by first creating a static file and placing it in a logical location. In my case I created a folder called "files" and placed the testfile.txt in it:

&nbsp;
<pre>[root@controller playbooks]# tree
.
├── files
│   └── testfile.txt
└── HelloWorld.yml</pre>
<pre>1 directory, 2 files
[root@controller playbooks]# cat files/testfile.txt
hello world</pre>
&nbsp;

I then replaced the copy module's "content" attribute to "src":

&nbsp;
<pre>[root@controller playbooks]# cat HelloWorld.yml
---
- name: This is a hello-world example
 hosts: ansibleclient01.local
 tasks:
 - name: Create a file called '/tmp/testfile.txt' with the content 'hello world'.
 copy: src=./files/testfile.txt dest=/tmp/testfile.txt</pre>
&nbsp;

Notice, I think the src path is relative to where the playbook's location. You can use absolute path but that will make it inflexible to move the static file to another location.

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

http://docs.ansible.com/ansible/playbooks_intro.html

&nbsp;

&nbsp;]]></Content>
		<Date><![CDATA[2016-02-01]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ansible]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ansible]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ansible - A few terminologies compared to Puppet]]></Title>
		<Content><![CDATA[<strong>Control Machine</strong>: This is equivalent to Puppet's PuppetMaster

<strong>Managed Node</strong>: This is equivalent to Puppet's Puppet agents

<strong>Playbooks</strong>: This is ansible's equivalent to a puppet's manifest file,

&nbsp;]]></Content>
		<Date><![CDATA[2016-02-01]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ansible]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ansible]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ansible - Installing Ansible]]></Title>
		<Content><![CDATA[There are a few of ways to install ansible. For the rest of these articles I'll be running Ansible on Centos 7.
<h2>Install Ansible using yum</h2>
This is done like this:
<pre>$ yum install epel-release
$ yum install ansible
</pre>
Then check the version of ansible installed:
<pre>$ ansible --version
</pre>
It is actually recommended to install ansible on all client's and just the controller. That way the clients ends up installing all the python related dependencies.

&nbsp;

However the downside with this approach is that the version of ansible that get's installed is a few versions behind the latest stable version.
<h2>Install Ansible from source</h2>
To install a newer version, you can run this script as the root user:

[github file = "/Sher-Chowdhury/vagrant-ansible/blob/master/scripts/install-ansible.sh"]

After running the above script, log into the machine as the user, "ansible" and password "vagrant".

Then check what version of ansible is installed:
<pre>[ansible@controller ~]$ ansible --version
ansible 2.1.0 (devel c433289a8b) last updated 2016/02/03 19:41:31 (GMT +100)
  lib/ansible/modules/core: (detached HEAD 93d02189f6) last updated 2016/02/03 19:41:39 (GMT +100)
  lib/ansible/modules/extras: (detached HEAD fff5ae6994) last updated 2016/02/03 19:41:51 (GMT +100)
  config file =
  configured module search path = Default w/o overrides
</pre>
<h2>Set up ansible development environment using vagrant</h2>
I created a <a href="https://github.com/Sher-Chowdhury/vagrant-ansible">ansible-vagrant project on github</a> to create your very own ansible vagrant environment. If you are new to ansible, then I recommend this approach. That's because not only will it install a recent version of ansible, but will also set up passwordless ssh access so that your controller can access both client vms without a password prompt.

Also see:

http://docs.ansible.com/ansible/intro_installation.html]]></Content>
		<Date><![CDATA[2016-02-03]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ansible]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ansible]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ansible - Make the controller aware of it's clients]]></Title>
		<Content><![CDATA[Before the controller can manage the clients, it first need to provide ansible an inventory of the clients it is allowed to manage. This inventory needs to be in the form of a file called "hosts".

You can choose where you store the hosts file. But the default location ansible will look for the hosts file is specified in ansible's main config file:

&nbsp;
<pre>[root@controller ~]# cat /etc/ansible/ansible.cfg | grep "^inventory"
inventory = /etc/ansible/hosts</pre>
In my case, my file looks like:

&nbsp;
<pre>
$ cat /etc/ansible/hosts
# Ungrouped hosts, specify before any group headers.
ansibleclient01.local
ansibleclient02.local

# A collection of hosts belonging to the 'clients' group
[clients]
ansibleclient01.local
ansibleclient02.local

</pre>

&nbsp;

&nbsp;

&nbsp;

After that you can check if your controller can ping it's clients, you can run the following command:

&nbsp;
<pre>$ ansible ansibleclient01.local -m ping
ansibleclient01.local | SUCCESS =&gt; {
 "changed": false,
 "ping": "pong"
}</pre>
&nbsp;

Or you can ping a group of clients:

&nbsp;
<pre>$ ansible clients --module-name ping
ansibleclient01.local | SUCCESS =&gt; {
 "changed": false,
 "ping": "pong"
}
ansibleclient02.local | SUCCESS =&gt; {
 "changed": false,
 "ping": "pong"
}</pre>
Note: here the "change" is showing as false, this means the ping module hasn't change the client's state in anyway.

&nbsp;

You can also run custom commands using the "command" module:

&nbsp;
<pre>$ ansible clients --module-name command --args "cat /etc/hostname"
ansibleclient01.local | SUCCESS | rc=0 &gt;&gt;
ansibleclient01.local

ansibleclient02.local | SUCCESS | rc=0 &gt;&gt;
ansibleclient02.local</pre>
&nbsp;

Notice that the "command" module requires arguements.

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;]]></Content>
		<Date><![CDATA[2016-02-04]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ansible]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ansible]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ansible - A playbook for setting up an apache webserver]]></Title>
		<Content><![CDATA[Here's a simple playbook that set's up an apache webserver:
<pre>
[root@controller playbooks]# pwd
/root/playbooks
[root@controller playbooks]# cat httpd.yaml
---
- name: This sets up an httpd webserver
  hosts: ansibleclient01.local
  tasks:
  - name: Install the httpd rpm
    yum: name=httpd
  - name: start the httpd service
    service: name=httpd state=started
  - name: Open port 80
    firewalld: service=http permanent=true state=enabled
  - name: start the firewalld service
    service: name=firewalld state=restarted
</pre>

Here's a breakdown of what the aboout yaml syntax shows. At the top level is a single item list. This list item houses a hash with 3 key-value pairs. The first to key's values are of the type string. However the third key's (tasks) houses a list of 4 items. Each one of these list items contains a hash. Each of these hashes is made up of 2 key-value pairs.  

Now let's run this playbook:
<pre>
[root@controller playbooks]# pwd
/root/playbooks
[root@controller playbooks]# ls
httpd.yaml
$ ansible-playbook httpd.yml

</pre>
We can test if this has worked, by opening to http://ansibleclient01.local in your web browser.

&nbsp;]]></Content>
		<Date><![CDATA[2016-02-05]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ansible]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ansible]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Creating new users (in IAM)]]></Title>
		<Content><![CDATA[Once you have logged into the dashboard, you can create new AWS login accounts by clicking on the "Identity and Access Management" link.

When creating a new user, you will get prompted on whether you want to have an "API access key", if you did then you will get the following pair info generated:
<ul>
	<li>Access Key ID - a string of characters</li>
	<li>Secret Access Key - a really long string of characters</li>
</ul>
The "Secret Access Key" is only displayed the one time, and are viewable on an other setting page. Hence it gives you the option to download them into a text file, while they are displayed on screen. If you lose this info, then you would need to regenerate your API keys again, which you can do without needing to recreate the user account.

&nbsp;

Once you have created the user, you will see the user has a unique id called "user ARN". This unique looks something like this:

&nbsp;
<pre>arn:aws:iam::654065406154:user/admin</pre>
<pre>arn: amazon resource number</pre>
The number string, is actually your aws account's unique ID. This number is important because the user needs this number in order to log into the aws account, that's because this number makes up part of the aws web login console. That is, after you have created a new user account, in order for that user to log into aws, they would need to use the following url:
<pre>https://654065406154.signin.aws.amazon.com/console


</pre>
&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;]]></Content>
		<Date><![CDATA[2016-02-06]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - IAM Groups and Roles]]></Title>
		<Content><![CDATA[<h2>Groups and Roles</h2>
Roles are universal. I.e. if you create it in one region, it becomes in all other regions.

&nbsp;

Lets say you  want to grant a specific permission (e.g. create ec2 instance) to a 100 users. Then you can do this by adding it to each user one at a time. Now let's say a couple of weeks later, you want to grant another permission to these 100 users. Once again you would need to assign each user one at a time.  This can get tedious.

A better way is to create a group (with an appropriate name), and then add the 100 users to this group. After that you can then assign permissions to the group as a whole, rather then individually. You can also assign even more permissions to the individuals of the group as well. E.g. if one member in the group is the team leader who requires higher priveleges.

Let's now say that you don't have just one group, but actually 25 groups. Now let's say all these groups needs to have the same set of 15 different permissions. Then adding each permission to each group can get tedious.

A better approach is to group together all the permissions. This grouping of permissions is called a  "role". You need to give this role an appropriate name, e.g. "developers". You can then assign this role to each of the groups.

An IAM <span class="emphasis"><em>role</em></span> is a series of permission policies that are grouped together.

This means that if in a couple of weeks, a new permission needs to be added/revoked from all the groups, then instead of editing this for each group, you can instead edit the single role.

Using groups and roles gives you complete flexibility in how you manage users and permissions:
<ul>
 	<li>You can still add individual permissions to a user</li>
 	<li>You can still add individual permissions to a group</li>
 	<li>a user can be a member of one or more groups</li>
 	<li>you can add one or more roles to a group</li>
 	<li>you can add one or more roles to a user</li>
</ul>
&nbsp;

&nbsp;

The concept of Groups and Roles can also be applied to resources, e.g. EC2 instances. E.g. you can a cluster of ec2 instance to a group called "projectx", and then assign a role called "projectx-db-access" to this group. This then gives all the ec2 instances access to the RDS service.

&nbsp;

Using roles means that your EC2 instances doesn't need to store any api-keys/login-credentials. Instead If a user logins into AWS and is authenticated via IAM, then the user will then get the role that specifies what instances it has permission to access.

Note: you can only assign a role to an EC2 instance, at the time of creating the that instance. That mean's that you can't assign/change a role to an existing EC2 instance. Furthermore, you can only assign a maximum of one IAM role to an instance.

&nbsp;

&nbsp;

Sometimes you might want to give users AWS access but their credentials are stored in Active Directory that is outside of AWS, or, you might want to give AWS access to third parties so that they can perform an audit on your resources.

For these scenarios, you can delegate access to AWS resources using an <span class="emphasis"><em>IAM role</em></span>. This section introduces roles and the different ways you can use them, when and how to choose among approaches, and how to create, manage, switch to (or assume), and delete roles.

&nbsp;

&nbsp;

&nbsp;
<h2>Permission conflicts</h2>
By default, a newly created user is implicitly denied access to everything, and you need to explicitly grant priveleges to the user by directly assigning permission policies, or indirectly by assigning users to groups and roles.

However in doing so you might end up with possible conflicts, e.g. a user is a member of 2 groups, the first group is allowed to delete EC2 instances, whereas in the second group, they are explicitly denied the ability of delete EC2 instances. When there is a conflict like this, then the deny permissions have the final say.

This is handy, if you suspect one of your users account has been hacked. In this situation, instead of removing all the groups and roles from the user, you can instead apply a "deny-all" permission, on top, which will override all other allow permissions.

&nbsp;

&nbsp;

&nbsp;
<h2>Useful tips</h2>
A common thing people will want to do using be able to:
<ul>
 	<li>run aws cli commands from inside an EC2 instance</li>
 	<li>access to s3 buckets</li>
 	<li>passwordless ssh access to bitbucket/github</li>
</ul>
&nbsp;

with respect to running aws cli commands, your first ideas could be:
<ul>
 	<li>create an ami wiht the ~/.aws/credentials files backed in. The downside to this approach is ami becomes useless if those credentials are updated.</li>
 	<li>create a user data (cloud-init) script that generates the ~/.aws/credentials file during instance creation time. However the downside to this approach is that you want to keep your user-data as short as possible and this will add extra bloat. Also not very elegant approach.</li>
</ul>
The best approach is to attach an IAM role to the instance, which has API gateway access privileges.

http://docs.aws.amazon.com/apigateway/latest/developerguide/permissions.html

&nbsp;

As for the S3 access, you can take the same route as the API issue. Just give the IAM role access to the relevant S3 buckets.

&nbsp;

As for the Bitbucket/Github scenario, just store the githubs/bitbucket's private id_rsa in S3. Add IAM S3 privileges as described above. Then download this id_rsa key via user-data (cloud data) shell script, during the EC2 instance's creation.

&nbsp;

Also checkout:

https://forge.puppet.com/modules?utf-8=%E2%9C%93&amp;sort=rank&amp;q=s3

&nbsp;

&nbsp;

&nbsp;

http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html

&nbsp;

&nbsp;

&nbsp;

&nbsp;

http://docs.aws.amazon.com/IAM/latest/UserGuide/introduction.html

&nbsp;
<h2></h2>]]></Content>
		<Date><![CDATA[2016-02-06]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Create a new EC2 instance]]></Title>
		<Content><![CDATA[to create a new EC2 instance, you need to login into your dashboard, and click on "EC2", here you'll find that you are already in a default VPC. Next you:

&nbsp;

<em>click on "Launch instance" to create a new instance -&gt; Select OS type (windows, rhel, suse...etc) -&gt;  select vm size (e.g. t2.micro). </em>

&nbsp;

On the next screen, you can select the role you want to allocate to your ec2 instance. Remember this can't be removed/changed in anyway after the machine has been created.

On this screen you should also set "Auto-assign Public IP" to "enable" from the dropdown list. This is so that you can ssh into your ec2 instance using putty.

&nbsp;

The next screen let's you attach additional EBS devices to your isntance. These ebs devices appears as /dev/xvda, /dev/xvdb, /dev/xvdc, /dev/xvdd....etc on your machine.

&nbsp;

The next screen lets you tag (i.e. label) your EC2 instance, you can assign multiple tags. Each tag is a key value pair, e.g. server-type=Webserver. Sidenote: you can think of these as a becoming part of your server's environment variables, which in turn can be used in the form of "puppet facts"  and could come in handy for ansible/puppet runs.

The next screen asks for what ports the instance is allowed to listen on. port 22 (ssh) is already added by default. But you open up other ports, e.g. http (80) and https (443).

The next screen is the review screen. Here you have the "Launch" button.

When you click on the launch button, a pop up appears about private/public rsa keys. Here you can choose to create (or choose an exsiting) private/public key. What's actually is happening here is that you get to download the private counterpart of the pair, and the public part will get copied into your EC2 instance, under a "nominated user account".

You need to read your AMI image's documentation to find out what is your nominated user account's username is, that has been baked into your AMI image.

The private key that you download will have a filename with the suffix *.pem. However you can think of it as a id_rsa file.

Let's assume for all the remaining article that the "nominated user account" username is "ec2-user". Also let's assume that we named the .pem file as id_rsa.pem.

&nbsp;

from your local linux desktop machine, you can then ssh into your EC2 instance by running the command:

&nbsp;
<pre>ssh -i /path/to/id_rsa.pem ec2-user@{ip number}</pre>
&nbsp;

Here we are explicitly specifying which private key file to use by using the identity_file option on the command line.

To make it easier, we could just create this user on our local desktop:

&nbsp;
<pre>$ groupadd ec2-user
$ su - ec2-user</pre>
</pre>

And then log drop then do:


<pre>
$ mkdir /home/ec2-user/.ssh
$ cp /path/to/id_rsa.pem /home/ec2-user/.ssh/id_rsa
$ chmod 400 /home/ec2-user/.ssh/id_rsa
</pre>

After doing all that, you should now be able to just do:

<pre>[ec2-user@LinuxLaptop ~ ] $ ssh {ipnumber}</pre>

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;]]></Content>
		<Date><![CDATA[2016-02-06]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Testing what access permissions an EC2 instance]]></Title>
		<Content><![CDATA[In the previous article we cover how to create and log into your instance. During this process, lets say we assigned a role called "test-role" to this instance. 

Next we want to see what other resources our EC2 can access. To do this we first log into our ssh instance from our laptop:

<pre>
[ec2-user@LinuxLaptop ~] $ ssh {ipnumber}
</pre>

Next we need to install the aws cli utilities into our instance:

<pre>
$ yum install python-pip
$ pip install awscli
</pre>


After that we could run the following to see if your machine has access to any s3 buckets:

<pre>
$ aws ls s3
</pre>

This command will list all the buckets that exists under your aws account, assuming that the role that has been assigned to this instance has a mininmum of s3 read only access. The cool thing here is that your EC2 instance was configured with all your AWS account details during build time. That meant that the above command works without the need to provide any other aws account's id, or login credentials.

However this is not the case, if you try to run the above command on your local laptop, which has no info about your aws account. So to interact with your aws account, you need to do it using api keys, which is covered next.   




The cool thing about your exe



]]></Content>
		<Date><![CDATA[2016-02-06]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Connecting your local desktop to your AWS account using API Keys]]></Title>
		<Content><![CDATA[Everything you can do via your aws console can also be done on your local Linux desktop, via the command line. First you need to install the awscli:

<pre>
$ yum install python-pip
$ pip install awscli
</pre>

Next via the web console, via IAM, create a new user, during the creation process you will generate your IAM keys. 

Next on local desktop's command line, run the "aws configure" command:

<pre>
$ aws configure
AWS Access Key ID [None]: AESDRFKSDGJSKDF
AWS Secret Access Key ID [None]: ljlJLJljGjGLafa5454fasdf6sd5sd6sd5f4a
Default region name [None]: <a href="http://docs.aws.amazon.com/general/latest/gr/rande.html#as_region" rel="nofollow">eu-west-1</a>
Default output format [None]:
</pre>
Note: This ends up creating the ~/.aws folder.  the info  

These API keys are unique across the entire AWS worldwide. This means AWS can determine which aws account these API keys is associated to.

After that we could run the following to see if your machine has access to any s3 buckets:

<pre>
$ aws s3 ls
</pre>

Note, this could still fail if the permissions/roles that are have been assigned to the users are insufficient. ]]></Content>
		<Date><![CDATA[2016-02-06]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Monitor all user AWS activities (Cloudtrail)]]></Title>
		<Content><![CDATA[Logging activities and events are important. There are two kinds of things you want to create logs for:
<ul>
	<li>logging of capacity of EC2 instance, e.g. cpu utilisation - This is done by the Cloudwatch service.</li>
	<li>AWS interaction logging. E.g. which deleted which EC2 instance - this kind of logging is done by the Cloudtrail service.</li>
</ul>
It's good practice to log all the activities authorized AWS users are doing on AWS. The users can interact with AWS either via the web console or the aws cli, in both cases all activities will get logged by Cloudtrail, if you enable it.

Cloudtrail tightly integrates with other aws services. In particular Cloudtrail stores all the logs that it generates, in its own S3 bucket. You get to choose the name of your S3 bucket as part of activating Cloudtrail. You can also set up SNS alerts when particular events occur, e.g. someone deletes an EC2 instance. However these event level notifications are done in conjunction with cloudwatch.

You can also link up Cloudtrail with the Simple Notication Service (SNS) service. In this scenario you get set up email notifications every time a new log file is sent to S3. Each of these log entry specifies, which user did what to which resource (e.g. EC2 instance id), and when. It also gives the user's source ip address, and what region the event occurred in.

&nbsp;

&nbsp;

&nbsp;]]></Content>
		<Date><![CDATA[2016-02-06]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Overview of Elastic Block Storage (EBS)]]></Title>
		<Content><![CDATA[EBS is the way to add additional block device storage to your ec2 instance. Once attached it is still up to you to do all the tasks you need to mount it, e.g. use fdisk, mkfs.ext4, add entry to /etc/fstab...etc.

Note, you can only attach EBS devices to the an EC2 that is in the same AZ. But you can overcome this using snapshots, covered further down.

There are different types of EBS storage, each offering different levels of max capacity and performance. In terms of performance, the key performance indicator is "IOPS".

&nbsp;

<strong>IOPS</strong>: Input/output Operations Per Seconds, where the data processed in each operation is 256kb in size.

IOPS is just an alternative measurement for network bandwidth. Therefore if I an EC2 instance can interact with an EBS at 20000 IOPS, then in real term bandwith terms, this works out to be:

256kb * 20000 = 5120000kb

5120000 / 1024 = 5000 MB/s = 5GB/s.

If money was no issue, then to get the best performance you need to:
<ul>
	<li>use a EBS-optimised EC2 instance</li>
	<li>use a Network optimised EC2 instance (to ensure good bandwidth)</li>
	<li>An EBS type with high IOPS rating.</li>
</ul>
&nbsp;
<h2>Types of EBS volumes</h2>
There are three types of EBS volumes:
<ul>
	<li>EBS General Purpose (SSD)</li>
	<li>EBS Provisioned IOPS (SSD)</li>
	<li>EBS Magnetic</li>
</ul>
&nbsp;

&nbsp;
<h3>EBS General Purpose (SSD)</h3>
This is
<ul>
	<li>Used for EC2 instance's where an EBS volume is the primary block device, i.e. <a href="https://aws.amazon.com/ec2/instance-types/">T2, M4, and C4 instances</a>.</li>
	<li>used for small dev/test environments and hosting small dbs</li>
	<li>By default you get 3IOPS per GIB - which is burstable, this means you can accrue unused IOPS and use when there is a sudden surge. You can burst up to 3000 IOPS</li>
	<li>Size can range from 1GiB to 16TiB</li>
	<li>ideal for vm's that usually experiences low load, and the occassional highload.</li>
</ul>
<h3>Provisioned IOPS</h3>
This offers better performance then the General Purpose EBS:
<ul>
	<li>Suitable for productions apps that requires continuously high IOPS in order to prevent bottlenecks</li>
	<li>suitable for hosting large databases</li>
	<li>Size can range from 4GiB to 16 TiB</li>
	<li>Max Baseline IOPS is 20000 IOPS</li>
</ul>
&nbsp;
<h3>EBS Magnetic</h3>
<ul>
	<li>This is the cheapest option</li>
	<li>ideal when high performance is not important.</li>
	<li>has low IOPS</li>
	<li>Size can range from 1GiB-1024GiB (1TiB)</li>
</ul>
&nbsp;

&nbsp;
<h2>Reusing an EBS volume</h2>
If you detach an EBS volume from one EC2 instance and attach it to another, then all the data gets delete automatically for security reasons. this Auto deletions ends up using  up a lot of IOPS. That's why it's better to delete the EBS's content using the "dd" command before detaching it.

&nbsp;

&nbsp;
<h2>EBS Snapshots</h2>
You can enable snapshots for your EBS volumes. These are incremental snapshots.

Snasphots increases data durability. However performance does get impacted while a snapshot is occuring, that's why it's best to schedule snapshots to take place during off peak periods.

You can delete any snapshots as well.

snapshots are stored in S3 behind the scenes. However you can view these snapshots in S3 since they are all hidden away. Instead you view your available snapshots from inside EC2 section instead.

when creating an EBS, you have the option to specify a snapshot ID. This means that you can create another EBS in different AZ and use a snapshot generated from an EBS that was in a another AZ. This is a possible way to replicate an EBS to another EBS in a different AZ.  It is also a good way to switch between different EBS types, e.g. move from General Purpose EBS to Provisioned IOPS EBS.

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-io-characteristics.html

&nbsp;

&nbsp;

&nbsp;

https://aws.amazon.com/ebs/details/]]></Content>
		<Date><![CDATA[2016-02-07]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Bootstrapping EC2 instance using cloud-init]]></Title>
		<Content><![CDATA[When you create a new EC2 instance, there is a chance you want to run a series of shell scripts to further prepare the instance before it is ready for use. This is possible using a tool called <a href="https://cloudinit.readthedocs.org/en/latest/" rel="nofollow">cloud-init</a>.

You have to pass in "user-data" into cloud-init.

As a sidenote, After your instance is built, you can view this data, by going to the following url from inside your instance:
<pre>http://169.254.169.254/latest/meta-data</pre>
You can use the same method, to view your instance general meta data:
<pre>http://169.254.169.254/latest/user-data</pre>
Note: you have to be inside the instance before querying the above url.

&nbsp;
<pre>
$ curl http://169.254.169.254/latest/       # notice the last trailing slash, which makes curl act a bit like the ls command. 
dynamic
meta-data
user-data
</pre>
&nbsp;

These 2 urls are useful, if you want to run a ruby/python/bash script from inside your instance and want to use some of this data.

for example here is a sample output if you use curl while logged inside of your instance:


<pre>
$ curl http://169.254.169.254/latest/meta-data
ami-id
ami-launch-index
hostname
.
.
instance-id
public-ipv4
.
.
...etc
</pre>

This show list of "keys" part of key-value pairs. To get the value, for a key, e.g. for public-ipv4, you do:

<pre>
$ curl http://169.254.169.254/latest/meta-data/public-ipv4
152.12.131.152
</pre>


&nbsp;

The user-data on the other hand displays your custom script that you might have added, e.g.:

<pre>
$ curl http://169.254.169.254/latest/user-data
#!/bin/bash
echo "hello world"
export env_variable1=value    
yum install httpd -y
</pre>
 
Note, I think the user-data is only available during the instance creation time. If you reboot the vm afterwards, this info gets lost...I think.  

Cloud-init has a setting that disables people from logging in as root, this setting is specified in:

<pre>
/etc/cloud/cloud.cfg
</pre>

This has an ini setting that is:

<pre>
disable_root: root
</pre>

This actually overrides what sshd's "permitrootlogin" setting is set to. However this cloud setting is only effective once a vm is created and running for the first time. As soon as you reboot, this setting is no longer enforced.  






http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html

&nbsp;

http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html]]></Content>
		<Date><![CDATA[2016-02-07]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - EC2 Default Limits]]></Title>
		<Content><![CDATA[In the EC2 section of your AWS web console, there is a section called "limits". This sets arbitariry limits on things like how many running EC2 instance you are allowed to have at any given time. This limits are in place mainly for AWS own benefit to help them plan for the future. However you can put in a request to have these limits increased as and when needed.]]></Content>
		<Date><![CDATA[2016-02-07]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Private IP Addresses, Public IP addresses, and Elastic IP Addresses]]></Title>
		<Content><![CDATA[A public IP address is not a static IP address, i.e. it will change if you reboot your EC2 instance

&nbsp;

Elastic IP Addresses on the other hand is persistant. you can assign to one instance, or at a later date reassign it to a different instance.

Inside a vpc we can have to types of subnets:

&nbsp;
<ul>
	<li><strong>Private subnets</strong> - This is a subnet that does not have an internet gateway attached to it. This means an instance can't access the internet via this type of subnet. This address is persitant and will survive a reboot.</li>
	<li><strong>Public subnets</strong> - This is a subnet that does have an internet gateway attached to it.</li>
</ul>
&nbsp;

In order for an instance to have internet access, it first needs to be in a vpc that contains a Public Subnet. The next thing it needs is either a public address or elastic ip address.

Attaching a Public/Elastic IP address to an instance is a bit like assigning an IP address to an instance's network card.

Elastic IP address works by associating itself to a public IP address.

&nbsp;

It is possible for an instance to have a public/elastic ip address, but is a vpc that only contains a private subnet. In this situation, the instance will still not have access to the internet.

Therefore for an EC2 instance to have internet access, that instance needs to have a public/elastic IP address assigned to it, and the instance needs to also reside inside a public subnet.

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

http://aws.amazon.com/ec2/pricing/#Elastic_IP_Addresses

&nbsp;

http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html]]></Content>
		<Date><![CDATA[2016-02-08]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Security Groups and Network ACLs (firewalls)]]></Title>
		<Content><![CDATA[You can control data traffic flowing to/from your resources via security groups. Or at the higher subnet level, via Network ACLs.
<h2></h2>
<h2>Security Groups</h2>
When you create a new instance, then by default all data traffic that are attempting to reach the EC2 instance (across all ports) are denied. I think the only exception is the ssh port, port 22.

You can apply individual/collection-of firewall rules to an instance, in the form of a "security group".

Security groups are a grouping of firewall rules, that you can assign to an EC2 instance.

Security groups are things that exist at the vpc level. Therefore once you create a security group, you can then assign that security group to any EC2 instances that exists in that vpc.

Security Group firewall rules are <strong>stateful</strong>, meaning that if you allow incoming traffic for a given ip-range/security-group and port number, then the security group will allow outbound traffic too, via the same security group's firewall rule.

For each AWS account, you can have up to 5 vpc. And for each vpc, you can create up to 100 security groups. However all of these are arbitrary limits and they can be increased by submitting a request to aws.

Each security can have up to 50 firewall rules.

You can assign up to 5 security groups to an EC2 instance.

firewall rules can also specify source ip addresses, or an ip address range.

Two instances can speak to each other, as long as they both are assigned with the same security group, and you have specified a compatible traffic source.

Every time a new VPC is created, a "default security group" is automatically created as well. This security group actually has the name "default":
<p id="xdkhQcJ"><a href="http://codingbee.net/wp-content/uploads/2016/02/img_56c4c9ffe921b.png"><img class="alignnone size-full wp-image-6805 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56c4c9ffe921b.png" alt="" /></a></p>
By default the default security is configured to allow all the vpc's resources to communicate with each other on all ports.  This essentially means any traffic coming from that security group, is allowed. Note, the source field can be one of the following:
<ul>
	<li>ip address</li>
	<li>An ip address range</li>
	<li>A security group's id</li>
</ul>
This Default Security Group is automatically gets assigned to a an instance, if you don't assign different security group during EC2 instance creation time.

&nbsp;
<h2>Network ACLS</h2>
Network ACLs are similar to security groups, except that they operate at a subnet level, i.e. it can block traffic that is trying to enter a subnet itself.

Therefore you attach security groups to EC2 instances, whereas you attach Network ACLs to subnets.

Another big difference is that that in Security groups you specify "ALLOW" rules only, otherwise everything is implicitly denied. However in Network ACLS, you can specify both ALLOW and DENY rules.

&nbsp;

Also for Network ACLs, each rule has a number attached. This number is referred to as the rule number. Number=1 is top priority, and the bigger the number, the lower the priority. This is useful if you create 2 conflicting firewall rules. The firewall rule with the rule number closer to one will override the other conflicting firewall rules.

&nbsp;

However there is a special firewall rule, which has a rule number of "*". This is a catch-all rule that get's triggered if no existing firewall rules is triggered.
<p id="jgNkpPl"><a href="http://codingbee.net/wp-content/uploads/2016/02/img_56c4c79520211.png"><img class="alignnone size-full wp-image-6802 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56c4c79520211.png" alt="" /></a></p>
This catchall '*' rule essentially sets the inbound Network ACL default to deny. However in this case, the rule 100 always get's triggered, so this catch-all rule never comes into play.

&nbsp;

&nbsp;

Network ACL's are <strong>stateless</strong>, which means that you have to create inbound and outbound firewall rules, to allow two way traffic.

&nbsp;

&nbsp;

http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-network-security.html]]></Content>
		<Date><![CDATA[2016-02-09]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Cloudwatch]]></Title>
		<Content><![CDATA[<h2>Status checks</h2>
Cloudwatch is a monitoring service. It can monitor for 2 types of checks:
<ul>
 	<li><strong>System Status Checks</strong></li>
 	<li><strong>Instance Status Checks</strong></li>
</ul>
<h3>System Status Checks</h3>
These are checks that gives information about whether aws underlying hardware/software has developed a fault. If any of these checks fails then it is something AWS is required to repair. Here are some examples checks of this type:
<ul>
 	<li>Loss of network connectivity</li>
 	<li>Loss of system power</li>
 	<li>Software issues on the physical host</li>
 	<li>Hardware issues on the physical host</li>
</ul>
When one of these system checks fails, it can be fixed in one of 2 ways:
<ul>
 	<li>Wait for AWS to fix the issue</li>
 	<li>Stopping and starting an instance, or by terminating and replacing an instance. Behind the scenes, this has the effect automatically moving your instance to working hardware.</li>
</ul>
<h3>Instance Status Checks</h3>
These are checks that gives information about the software and network configuration of your individual instance. These checks detect problems that requires you to fix, e.g. fix a faulty config file. Here are some examples of this type of checks:
<ul>
 	<li>Failed system status checks - i.e. this is to show that Instant Status checks will indicate if the underlying hardware/software is at fault.</li>
 	<li>Incorrect networking or startup configuration</li>
 	<li>Exhausted memory</li>
 	<li>Corrupted file system</li>
 	<li>Incompatible kernel</li>
</ul>
<div class="itemizedlist"></div>
Faults that are identified by these types of checks requires you to investigate and fix.

&nbsp;
<h2>Cloudwatch Alarms</h2>
Cloudwatch alarms is service that sends out simple notification service (sns) message when a status check changes state. An alarm can have 3 states:
<ul>
 	<li><span class="emphasis"><em><code class="code">OK - </code></em></span>The metric is within the defined threshold</li>
 	<li><span class="emphasis"><em><code class="code">ALARM - </code></em></span>The metric is outside of the defined threshold</li>
 	<li><span class="emphasis"><em><code class="code">INSUFFICIENT_DATA </code></em><code class="code">- </code></span>This is temporary that a new alarm takes in the first few minutes after creating it</li>
</ul>
Cloudwatch Alarms are enabled by default for a few key status checks. These status checks are the ones that can be monitored at the physical host level, e.g.:
<ul>
 	<li>cpu utilisation</li>
 	<li>network traffic</li>
 	<li>disk usage</li>
</ul>
CPU credit usage/balance - i.e. it monitor how much IOPS you are accruing, on an burstable instance.

Cloudwatch alarms for each instance is kept for several weeks after an instance has been deleted.

You can also set alarms if your instance reaches a certain cost to run, e.g. £50/day. This is useful to notify if you forgot to shutdown an instance after you finished working with it. Another alarm could be set if data transfer costs becomes to high e.g. £20/day.

Alarms can do a lot more then just sending out sns notification, they can also trigger actions to try fix the issue, e.g. an alarm can trigger the creation of more instance to handle unexpected high load. Hence Cloudwatch Alarms plays a big part to enabling elasticity/auto-scaling.

&nbsp;
<h2>Basic and Detailed Monitoring</h2>
CloudWatch generates near-realtime metrics by processing raw data from Amazon EC2. Cloudwatch stores up to two weeks of data, to help analyse the history. By default, Amazon EC2 metric data is automatically sent to CloudWatch in 5-minute periods (aka basic monitoring). however you can enable more frequent monitoring on an Amazon EC2 instance, which sends data to CloudWatch in 1-minute periods (aka detailed monitoring). Pricewise, Basic Monitoring is free, but there is a charge for Detailed Monitoring.

&nbsp;

&nbsp;
<h2>Monitoring using scripts</h2>
Some EC2 instance metrics can only be collected by <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring_ec2.html">running monitoring scripts (provided by aws)</a> inside your ec2 instances. For example:
<ul>
 	<li>Memory utilization ("free -m" command)</li>
 	<li>Swap disk utilization (disk space acting as memory)</li>
 	<li>disk space utilization ("du -h" command)</li>
 	<li>All of these are displayed as time based line graphs on the aws console.</li>
</ul>
&nbsp;

http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring_ec2.html

&nbsp;

&nbsp;

&nbsp;

&nbsp;

http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-cloudwatch.html

&nbsp;

&nbsp;

&nbsp;

http://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/AlarmThatSendsEmail.html

https://aws.amazon.com/cloudwatch/

http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-system-instance-status-check.html]]></Content>
		<Date><![CDATA[2016-02-09]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Placement Groups]]></Title>
		<Content><![CDATA[<h2>Key points</h2>
<ul>
 	<li>the name of a placement group has to be unique within your aws account.</li>
 	<li>only certain instance types can be attached to placement groups, c, g, m, i</li>
 	<li>you can't move existing instances into a placement group.</li>
</ul>
&nbsp;

A<strong> placement group</strong> is a cluster of instances that are all in the same AZ. These instances have 10 Gbps networks and they need to have the "<a href="https://aws.amazon.com/ec2/instance-types/#enhanced_networking" rel="nofollow">Enhanced Networking</a>" feature. You need to puts instances into a placement group if they need to have very low latency between them.

You need to use appropriate EC2 instance types to get the best use out of placement groups.

You should try create all your ec2 (placement group) instances in a single request, this is possible because when creating an instance, there is a "number of field" to fill. This inadvertantly means that all your instances must be of the same instance type.  When creating all your placement group instances in a single request, Amazon will do it's best to create your instance on the same hardware, or at least as close as possible.

Once created, if you stop and then start an instance, then this instance will start up in the same AZ, and aws will again try to create it as close to as possible to the other placement group members.

&nbsp;

A common troubleshooting technique is to stop all your placement group instances, and start them up again. This will help aws to relocate all your instances to be as close as possible again.

&nbsp;

Placement group restrictions:
<ul>
 	<li>instances that were not originally created in a placement group, cannot be added to a placement group.</li>
 	<li>placement groups can't be combined into one. Although instances inside a placement group can communicate with outside instances.</li>
 	<li>All instances of a placement group must reside in the same AZ.</li>
 	<li>You can't have 2 or more placement group with the same name.</li>
</ul>
&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html]]></Content>
		<Date><![CDATA[2016-02-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Placement Groups Duplicate]]></Title>
		<Content><![CDATA[A<strong> placement group</strong> is a cluster of instances that are all in the same AZ. These instances have 10 Gbps networks and they need to have the "<a href="https://aws.amazon.com/ec2/instance-types/#enhanced_networking" rel="nofollow">Enhanced Networking</a>" feature. You need to puts instances into a placement group if they need to have very low latency between them.

You need to use appropriate EC2 instance types to get the best use out of placement groups.

You should try create all your ec2 (placement group) instances in a single request, this is possible because when creating an instance, there is a "number of field" to fill. This inadvertantly means that all your instances must be of the same instance type.  When creating all your placement group instances in a single request, Amazon will do it's best to create your instance on the same hardware, or at least as close as possible.

Once created, if you stop and then start an instance, then this instance will start up in the same AZ, and aws will again try to create it as close to as possible to the other placement group members.

&nbsp;

A common troubleshooting technique is to stop all your placement group instances, and start them up again. This will help aws to relocate all your instances to be as close as possible again.

&nbsp;

Placement group restrictions:
<ul>
	<li>instances that were not originally created in a placement group, cannot be added to a placement group.</li>
	<li>placement groups can't be combined into one. Although instances inside a placement group can communicate with outside instances.</li>
	<li>All instances of a placement group must reside in the same AZ.</li>
	<li>You can't have 2 or more placement group with the same name.</li>
</ul>
&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html]]></Content>
		<Date><![CDATA[2016-02-12]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[draft]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Setting up ELB to send public traffic to Private EC2 instances]]></Title>
		<Content><![CDATA[Here are the steps:
<ol>
 	<li>Create a vpc (which also creates a new route table behind the scenes)</li>
 	<li>Create 2 subnets inside this vpc. These subnets also needs to be in the same availability zone as the web-server instance, which by default both subnets ends up inheriting the vpc's route table. We'll call them <em>public-subnet</em> and <em>private-subnet</em>.</li>
 	<li>Create an "Internet Gateway" and attach it to the vpc's routing table. This ends up making both <em>public-subnet</em> and <em>private-subnet, </em>public. Later on we will set the subnet, <em>private-subnet</em><em>,</em> to a private subnet<em>.</em></li>
 	<li>attach the Internet Gateway to the route table, configure it to accept traffic from an IP address</li>
 	<li>Create an EC2 instance inside the vpc (via dropdown list), and attach the subnet, private-subnet (via dropdown list). Also assign a public ip address</li>
 	<li>Log into this EC2 instance using the public ip address, it is still accessible via the public address since private-subnet is still actually private</li>
 	<li>Install httpd, disable firewalld, start httpd service inside the instance</li>
 	<li>check that you can access the instance homepage via the ip address.</li>
 	<li>In AWS web console's ec2 section, create an ELB, attach this ELB to the public-subnet, attach your ec2 to this ELB instance.</li>
 	<li>Check you can access the instance's homepage, this time via the ELB's url, the ip address should also work too.</li>
 	<li>Now go back into the VPC section and create a new route table, call it "private-route-table", don't attach an Internet Gateway to this.</li>
 	<li>Switch the <em>private-subnet's</em> route table to this one.</li>
 	<li>The homepage should no longer be accessible via the ip address, and now only works via the elb's url. Also you can't no longer ssh into the instance. You can use the elb's url to do this, since the elb is designed to distribute traffic (also why you can't open ssh port when creating elb, only http and https). To now ssh into your instance you will need to do it in of the following ways:
<ol>
 	<li>ssh to another instance that is publicly accessible, and shares a subnet with the target instance, then ssh into that.</li>
 	<li>use <a href="https://aws.amazon.com/directconnect/" rel="nofollow">aws direct connect</a></li>
 	<li>set up a vpn</li>
</ol>
</li>
</ol>
&nbsp;
<h2>vpc and subnets</h2>
To create a vpc, you need to provide:

<a href="http://codingbee.net/wp-content/uploads/2016/02/dk8KXGB.png" rel="attachment wp-att-6691"><img class="alignnone size-full wp-image-6691" src="http://codingbee.net/wp-content/uploads/2016/02/dk8KXGB.png" alt="" width="1126" height="766" /></a>

A vpc is essentially a range of ip addresses.

&nbsp;

You can break this range, into smaller ranges, by creating subnets.

<a href="http://codingbee.net/wp-content/uploads/2016/02/RT10GyU.png" rel="attachment wp-att-6692"><img class="alignnone size-full wp-image-6692" src="http://codingbee.net/wp-content/uploads/2016/02/RT10GyU.png" alt="" width="1089" height="636" /></a>

Where:

<a href="http://codingbee.net/wp-content/uploads/2016/02/XGlk7hw.png" rel="attachment wp-att-6693"><img class="alignnone size-full wp-image-6693" src="http://codingbee.net/wp-content/uploads/2016/02/XGlk7hw.png" alt="" width="947" height="566" /></a>

&nbsp;

Let's assume in this scenario, that you only plan to set this up for a single private EC2 instance:

&nbsp;
<ol>
 	<li>Create a vpc, with the following info:
<ol>
 	<li>name=vpc1</li>
 	<li>CIDR block range=10.0.0.16</li>
 	<li>Tenancy = default</li>
</ol>
</li>
 	<li>Create a subnet in the "London" AZ. Note this AZ will be where our private EC2 instance will reside. A subnet is attached to an AZ. Here are the details you need to provide:
<ol>
 	<li> subnet name="public-subnet"</li>
 	<li>vpc name="vpc1"</li>
 	<li>AZ=london</li>
 	<li>CIDR Block=10.0.1.0/24</li>
</ol>
</li>
 	<li>Create a private subnet with the London AZ, with the following details:
<ol>
 	<li>subnet name = "private-subnet"</li>
 	<li>vpc name="vpc1"</li>
 	<li>AZ=London</li>
 	<li>CIDR block=10.0.2.0/24</li>
</ol>
</li>
</ol>
&nbsp;

&nbsp;

A Public subnet must exist in the vpc, on the same AZ as the private subnet that hosts the private ec2 instance. That's because a public ELB behind the scenes builds EC2 instance on a public subnet, and then reroutes traffic to private ec2 instances. The reason that this public subnet must be in the same AZ as the private subnet, is that it improves performance.

Note: when creating an ELB, you will be prompted to select at least 2 subnets for High Availability purposes. That's why it's best to create another public subnet in a different AZ to achieve this. Although it's unlikely this second pubsubnet get's used.

&nbsp;

&nbsp;

&nbsp;]]></Content>
		<Date><![CDATA[2016-02-12]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - VPCs & Subnets, routing tables, and Internet Gateways]]></Title>
		<Content><![CDATA[In order for an instance to have internet access, you will need the following:
<ul>
	<li>An internet gateway attached to your vpc</li>
	<li>The internet gateways is associate by an entry in your route table. Note, your VPC can have multiple route tables.</li>
	<li>This route table must be attached to your subnet, which consequently makes your subnet into a public subnet.</li>
	<li>The subnet's attached network acl, must have the appropriate inbound/outbound firewall rules in place to allow the desired traffic through</li>
	<li>your instance must have a public or Elastic IP address</li>
	<li>ensure your security groups and network ACLs allows the relevant traffic to go through</li>
	<li>setting inside instances are correct, e.g.:
<ul>
	<li>firewalld is configured</li>
	<li>network service is running</li>
	<li>services are running and listening on ports.</li>
</ul>
</li>
</ul>
&nbsp;

&nbsp;

First off, a vpc is a way to group your resources. These resources can be in different AZ. Hence a vpc isn't attached to particular AZ.

Your AWS accounts comes with pre-created vpc, called "default":

<img class="alignnone size-full wp-image-6701 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56bf33cc15fad.png" alt="" />

However in order to create a new vpc, you need to provide the following info:

<img class="alignnone size-full wp-image-6702 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56bf33e94c6b3.png" alt="" />

The description in the cidr block is:

<img class="alignnone size-full wp-image-6703 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56bf3409d8101.png" alt="" />

For my example, let's say I create a vpc with the following details:

&nbsp;
<ul>
	<li>Name tag:  cb1</li>
	<li>CIDR block:  10.0.0.0/16</li>
	<li>Tenancy: default  (the only other option in the dropdown list is "dedicated")</li>
</ul>
In this scenario, according t <a href="http://www.subnet-calculator.com/cidr.php" rel="nofollow">subnet-calculator.com</a> the full range of ip address available is:
<p id="UuollNG"><img class="alignnone size-full wp-image-6704 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56bf37a10b5f4.png" alt="" /></p>
I.e., the subnet mask of 16, locked down the first to parts "10.0".

After we have created the vpc, we can view it's details:

&nbsp;
<p id="yqAsPdh"><img class="alignnone size-full wp-image-6705 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56bf38b23598b.png" alt="" /></p>
&nbsp;

&nbsp;

&nbsp;

&nbsp;

Now we can attach 1 or more subnets to this VPC. To create a new subnet, you need to provide the following info:
<p id="mEdspWb"><img class="alignnone size-full wp-image-6712 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56bf3f27ca717.png" alt="" /></p>
<p id="bxfOLdy">Also here's the help info for the CIDR block field:</p>
<p id="enPEANn"><img class="alignnone size-full wp-image-6713 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56bf4088471ff.png" alt="" /></p>
&nbsp;

Notice here that a subnet is associated to a particular AZ, and must also belong to an existing VPC. So in my case I created a subnet with the following details:
<ul>
	<li>Name tag: public-subnet</li>
	<li>VPC: cb1</li>
	<li>Availability Zone: us-east-1a</li>
	<li>CIDR block: 10.0.0.0/24</li>
</ul>
Here the subnet's CIDR range must fall within the VPC, wider CIDR range. In this instance, the range for the cb1's range is 10.0.0.0- 10.0.255.255. So the subnet's mask of 10.0.0.0/24 falls within this range as shown here:
<p id="eMrsymJ"><img class="alignnone size-full wp-image-6715 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56bf41bb56ab1.png" alt="" /></p>
&nbsp;

After creating this subnet, we now have:
<p id="xAFlytn"><img class="alignnone size-full wp-image-6716 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/aws-vpcs-subnets-routing-tables-and-internet-gateways-6716.png" alt="" /></p>
Any EC2 instance that is connected to this subnet, will have an internal ip address which ranges between 10.0.0.0-10.0.0255.

&nbsp;

Now let's create another subnet for this vpc, which is also in the same AZ. This time with the following details:
<p id="cLtBMUQ"><img class="alignnone size-full wp-image-6718 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56bf44d239a9e.png" alt="" /></p>
The ip range for this subnet works out to be:
<p id="OcSLaaB"><img class="alignnone size-full wp-image-6719 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/aws-vpcs-subnets-routing-tables-and-internet-gateways-aws-csa-associate-6719.png" alt="" /></p>
This ip range again falls within the vpc range.

After creating this subnet we now end up with:
<p id="whZWIHx"><img class="alignnone size-full wp-image-6720 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56bf45a7d8f19.png" alt="" /></p>
&nbsp;

Notice that both subnets inherited the vpc's "route table".

Note, each newly created route table comes prelisted with a default route, referred to as the <a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Route_Tables.html">local route</a>. This local route has an IP range that covers the entire vpc IP range. This is what makes it possible for each EC2 instance in your VPC to automatically be able to communicate with each other.

In order make a subnet into public subnet (i.e. be able to receive outside traffic), it needs to be connected to a route table, which in turn is attached to an "internet gateway":

This gateway is attached to the default VPC's subnet's routing table.

<a href="http://codingbee.net/wp-content/uploads/2016/01/architecture_linux.png" rel="attachment wp-att-6421"><img class="alignnone size-full wp-image-6421" src="http://codingbee.net/wp-content/uploads/2016/01/architecture_linux.png" alt="" width="603" height="436" /></a>

&nbsp;

Source: <a href="http://docs.aws.amazon.com/gettingstarted/latest/wah-linux/web-app-hosting-intro.html">Amazon</a>

Now by default, when you create a new vpc, a "main" (default) route table also get's created with the following rule:
<p id="qdJPTCi"><a href="http://codingbee.net/wp-content/uploads/2016/02/img_56c4cd3bc021c.png"><img class="alignnone size-full wp-image-6807 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56c4cd3bc021c.png" alt="" /></a></p>
This is an example for a vpc with cidr block of 10.10.0.0/16. This routing table automatically get's assigned to any subnets that you create inside this vpc. This route allows routing of traffic from instance in one subnet to reach an instance in another subnet.

&nbsp;

&nbsp;

&nbsp;

Now let's create an internet gateway, to create a gateway, you just need to provide a name for the internet gateway:
<p id="sDEOLck"><img class="alignnone size-full wp-image-6721 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56bf49eea5b62.png" alt="" /></p>
&nbsp;

Now we have ended up with:

&nbsp;
<p id="ITTQsbn"><img class="alignnone size-full wp-image-6723 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56bf4d76ac57e.png" alt="" /></p>
&nbsp;

&nbsp;

Note, we already have an internet gateway, which by default is attached to the default vpc.

Next we need to attach our new gateway to our new vpc:
<p id="HApDHuX"><img class="alignnone size-full wp-image-6724 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56bf5013b30b2.png" alt="" /></p>
&nbsp;

Now we end up with:

&nbsp;
<p id="jplSQLt"><img class="alignnone size-full wp-image-6725 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56bf504b58d1c.png" alt="" /></p>
Note: A vpc is can only have up to 1 internet gateway attached to it at any one time.

Now we need to link our Interneet Gateway to our new vpc's route table. Here's how to create a route table:
<p id="ixWlzXC"><img class="alignnone size-full wp-image-6726 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56bf516acc75f.png" alt="" /></p>
As you can see, we simply have to give the route table a name and specify which vpc it belongs to. However we don't need to do any of this because a route table was automatically created at the time of the vpc creation. I just allocated a name "cb1-route-table" to this route table:
<p id="NOPRzYi"><img class="alignnone size-full wp-image-6727 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56bf520d1b349.png" alt="" /></p>
This route table was automatically created at the time of the vpc's creation. This route table was also automatically attached to both public-subnet, and private-subnet, at the time of both subnet's creation.

Now we need to add a route to our <a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Route_Tables.html" rel="nofollow">route table</a> that specifies that all data is allowed to go in/out via our new internet gateway:
<p id="vicfpsh"><img class="alignnone size-full wp-image-6729 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56bf554d12622.png" alt="" /></p>
&nbsp;
<p id="UVASAGQ"><img class="alignnone size-full wp-image-6730 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56bf55b410cbf.png" alt="" /></p>
So that we end up with:

&nbsp;
<p id="qmgsEEW"><img class="alignnone size-full wp-image-6731 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56bf566a1e168.png" alt="" /></p>
&nbsp;

However since both of our net subnets (private-subnet and public-subnets) are using the same route table:
<p id="ZpJQETc"><img class="alignnone size-full wp-image-6732 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56bf56bee19d1.png" alt="" /></p>
It means that both subnets are public subnets, whereas we wanted "private-subnet" to be a private subnet (which can only recieve traffic from internal resources, e.g. from an ELB).

Before we make "private-subnet" private, we'll first create the following:
<ul>
	<li>EC2 instance, ensure that this instance is part of this new vpc</li>
	<li>An ELB</li>
</ul>
&nbsp;

Follow this guide to <a href="http://codingbee.net/tutorials/aws/aws-create-a-new-ec2-instance/" rel="nofollow">create an EC2 instance</a>, also make sure to create the in our new vpc.

When creating the ELB, you need to attach it to our new vpc:
<p id="jjReQmo"><img class="alignnone size-full wp-image-6739 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56bf6d52359b6.png" alt="" /></p>
Note: I also opened up the http+https firewalls because my ec2 is going to act as a web server. We then sroll further down and only  attach the public-subnet.

In the next section we select all the what ec2 instances that we want to send data traffic to:

{need to add a screenshot}

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa

Also as part creating the elb, you have to specify which ec2 instance the elb should be attached to, i.e. send traffic to.

After you have created the elb, your elb has a url to access it with:
<p id="GwYlGxO"><img class="alignnone size-full wp-image-6743 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56bf72a6c539c.png" alt="" /></p>
Which is connected to our instance:

&nbsp;
<p id="uUqoscY"><img class="alignnone size-full wp-image-6744 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56bf72f010520.png" alt="" /></p>
&nbsp;

At this point, you should be able to access your ec2 via it's ip address:

&nbsp;
<p id="NznWcGy"><img class="alignnone size-full wp-image-6745 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56bf7365f2250.png" alt="" /></p>
Or the elb's url:

&nbsp;
<p id="ZEGngZV"><img class="alignnone size-full wp-image-6746 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56bf73b399395.png" alt="" /></p>
&nbsp;

&nbsp;

zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz
&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

At this point, you should be able to access your ec2 via it's ip address:
<p id="NznWcGy"><img class="alignnone size-full wp-image-6745 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56bf7365f2250.png" alt="" /></p>
Or the elb's url:
<p id="ZEGngZV"><img class="alignnone size-full wp-image-6746 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56bf73b399395.png" alt="" /></p>
Both of this is working because our ec2 instance is attached to the public via the elb, and via the subnet, private-subnet, which is still attached to a route table that is connected to an internet gateway.

We now want to close off direct access to our ec2 instance (i.e. ip based access), so that it can only be accessible via the ELB's url. To do this we need to first create a new route table that isn't attache to an internet gateway:
<p id="DpvfiTL"><img class="alignnone size-full wp-image-6747 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56bf75e372e2c.png" alt="" /></p>
Now let's confirm that our route table isn't attached to an internet gateway:
<p id="rsDkcAK"><img class="alignnone size-full wp-image-6748 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56bf761ff0d5d.png" alt="" /></p>
Note, the public route table looks like this:
<p id="ugZkWIt"><img class="alignnone size-full wp-image-6749 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56bf7647008d4.png" alt="" /></p>
Now let's switch our private-subnet to use this new private route-table:
<p id="wDyBBPo"><img class="alignnone size-full wp-image-6750 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56bf768f6fadc.png" alt="" /></p>
So now we have:
<p id="gEdhBhn"><img class="alignnone size-full wp-image-6751 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56bf76b47ba6a.png" alt="" /></p>
Now when we try to access via ip address we get:
<p id="iEwkeWy"><img class="alignnone size-full wp-image-6752 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56bf776b70013.png" alt="" /></p>
But it still works via the elb's url:
<p id="UXdoSOB"><img class="alignnone size-full wp-image-6753 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/02/img_56bf77c480f88.png" alt="" /></p>
&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Subnets.html

https://aws.amazon.com/vpc/

http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Subnets.html]]></Content>
		<Date><![CDATA[2016-02-13]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - NAT Instances]]></Title>
		<Content><![CDATA[If you have an ec2 instance that is attached to a private subnet only, then it won't have internet access. That's because by definition, the routing table that is associated with the private subnet, doesn't have an entry for routing traffic to/from an internet gateway.

For security reasons, this is a good thing, if the resources does not provide a service that requires direct access. However this is also a problem, because you can't use commands like yum which requires internet access.

You can overcome this problem by creating a <a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_NAT_Instance.html" rel="nofollow">NAT instance</a>. A NAT instance allows instances that are on the private subnet to initiate outbound traffic to the internet, and thereby establishing a connection session which will then allow inbound traffic for the duration fo the session. It's a bit like a temporary tunnel that is created by the private instance which is closed again as soon as the private instance received the traffic it requested for.

A NAT instance is essentially a special type of EC2 instance. A NAT instance is an EC2 instance with the following characteristics:
<ul>
	<li>A NAT instance is an EC2 isntace that is created from AMI that includes the string <strong>amzn-ami-vpc-nat</strong> in it's name. You can find these AMI's by searching in the Community AMIs sections.</li>
	<li>The NAT instance's type needs to be quite big in order to have enough network data transfer capacity.</li>
	<li>NAT instance must be connected to a public subnet and must have a public IP address (or Elastic Public IP Address). I.e. the NAT instance must have internet access.</li>
	<li>The NAT instance should have a security group attached to it called, "NATSG". This name is not manadatory but is recommended by AWS</li>
	<li>NATSG needs to have the following inbound firewall rules:
<ul>
	<li>SSH for all ip addresses (or try to limit this if possible)</li>
	<li>HTTP limited to the private subnet's ip range - That's because if you look at yum's .repo config files, you'll see baseurl is http</li>
	<li>HTTPS limited to the private subnet's ip range - That's because yum's .repo's baseurl can also be https.</li>
</ul>
</li>
	<li>The NAT Instance needs to have the <strong>source/destination checks</strong> setting disabled. Otherwise the NAT instance will terminate outbound data packets that it is not the source of. It will also terminate in bound packats that it isn't the destination of either. You can modify this setting by right clicking on your instance, and navigating to:
<p id="vpTvTeL"><a href="http://codingbee.net/wp-content/uploads/2016/04/img_570a7fab1b402.png"><img class="alignnone size-full wp-image-6897 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/04/img_570a7fab1b402.png" alt="" /></a></p>
</li>
	<li>The route table that's attached to the private subnet must have an entry that routes all other traffic (0.0.0.0) bound traffic to the NAT Instance. The "local route" will intercepts all internal traffic first. so the 0.0.0.0 will only come into play for IP ranges that are outside of the VPC IP range.</li>
</ul>
&nbsp;

The EC2 doesn't need to have a public IP address. Therefore the internet will treat the NAT instance's public IP address as the source address. When the NAT instance receives the response data traffic, it will reroute the traffic back to the EC2 instance.

&nbsp;

&nbsp;

&nbsp;]]></Content>
		<Date><![CDATA[2016-02-18]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Setting up DRBD and Pacemaker between two Centos 7 servers]]></Title>
		<Content><![CDATA[Protocol C is most commonly used:

&nbsp;

http://drbd.linbit.com/users-guide-8.4/s-replication-protocols.html

&nbsp;

http://jensd.be/156/linux/building-a-high-available-failover-cluster-with-pacemaker-corosync-pcs

&nbsp;

http://jensd.be/186/linux/use-drbd-in-a-cluster-with-corosync-and-pacemaker-on-centos-7

&nbsp;

&nbsp;]]></Content>
		<Date><![CDATA[2016-02-19]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Centos|drbd]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Spacewalk - create a channel]]></Title>
		<Content><![CDATA[First go here:

&nbsp;

<strong><a class="external-link" href="https://lv587/rhn/channels/manage/Manage.do" rel="nofollow">https://hostname/rhn/channels/manage/Manage.do</a></strong>

&nbsp;

and select a channel that you want to use a template. (and open in a new tab)

&nbsp;

click "create new channel"

&nbsp;

Copy and paste stuff across from template so to ensure consistent naming convention.

&nbsp;

ensure that "<label for="gpgkeyurl">GPG key URL:</label>" is a valid url:

&nbsp;

<a class="external-link" href="http://yum.theforeman.org/releases/1.8/RPM-GPG-KEY-foreman" rel="nofollow">http://yum.theforeman.org/releases/1.8/RPM-GPG-KEY-foreman</a>

&nbsp;

You can determine both "GPG key ID:" and "GPG key Fingerprint:" from the above file.This is done like this:

&nbsp;

&nbsp;
<pre></pre>
<pre>[root@puppetmaster ~]# wget <a class="external-link" href="http://yum.theforeman.org/releases/1.8/RPM-GPG-KEY-foreman" rel="nofollow">http://yum.theforeman.org/releases/1.8/RPM-GPG-KEY-foreman</a>
--2015-10-28 09:13:46--  <a class="external-link" href="http://yum.theforeman.org/releases/1.8/RPM-GPG-KEY-foreman" rel="nofollow">http://yum.theforeman.org/releases/1.8/RPM-GPG-KEY-foreman</a>
Resolving <a class="external-link" href="http://yum.theforeman.org" rel="nofollow">yum.theforeman.org</a> (<a class="external-link" href="http://yum.theforeman.org" rel="nofollow">yum.theforeman.org</a>)... 23.253.148.10
Connecting to <a class="external-link" href="http://yum.theforeman.org" rel="nofollow">yum.theforeman.org</a> (<a class="external-link" href="http://yum.theforeman.org" rel="nofollow">yum.theforeman.org</a>)|23.253.148.10|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 3140 (3.1K) [text/plain]
Saving to: ‘RPM-GPG-KEY-foreman’

100%[======================================&gt;] 3,140       --.-K/s   in 0s      

2015-10-28 09:13:51 (127 MB/s) - ‘RPM-GPG-KEY-foreman’ saved [3140/3140]

[root@puppetmaster ~]# gpg --with-fingerprint RPM-GPG-KEY-foreman 
gpg: directory `/root/.gnupg' created
gpg: new configuration file `/root/.gnupg/gpg.conf' created
gpg: WARNING: options in `/root/.gnupg/gpg.conf' are not yet active during this run
gpg: keyring `/root/.gnupg/secring.gpg' created
gpg: keyring `/root/.gnupg/pubring.gpg' created
pub  4096R/225C9B71 2015-03-09 Foreman Release Signing Key (1.8) &lt;packages@<a class="external-link" href="http://theforeman.org" rel="nofollow">theforeman.org</a>&gt;
      Key fingerprint = 64E3 7B1F A6C0 2416 6B53  5495 28F5 A69D 225C 9B71
sub  4096R/CC24CADC 2015-03-09 [expires: 2016-03-08]

</pre>
In this example, Here the GPG ID is: 225C9B71

and the fingerpirnt key is:
<pre>64E3 7B1F A6C0 2416 6B53  5495 28F5 A69D 225C 9B71</pre>
&nbsp;

Now we create the repo by going here:

<a class="external-link" href="https://lv587/rhn/channels/manage/repos/RepoList.do" rel="nofollow">https://hostname/rhn/channels/manage/repos/RepoList.do</a>

Here's an example of what it looks like:

&nbsp;

<a class="external-link" href="https://lv587/rhn/channels/manage/repos/RepoEdit.do?id=940" rel="nofollow">https://hostname/rhn/channels/manage/repos/RepoEdit.do?id=940</a>]]></Content>
		<Date><![CDATA[2016-02-22]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Centos|RHEL|spacewalk]]></Tags>
		<Status><![CDATA[private]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[OSX - Intro]]></Title>
		<Content><![CDATA[This is a guide for people who come from a windows and rhel/centos background. 

- homebrew
- karabiner-elements    # https://github.com/tekezo/Karabiner-Elements/blob/master/usage/README.md#installation
- iterm2

fn+-> # this is the equivalent to the 'end' button. ]]></Content>
		<Date><![CDATA[2016-03-07]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[apple|osx]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>OSX]]></Categories>
	</post>
	<post>
		<Title><![CDATA[OSX -  homebrew]]></Title>
		<Content><![CDATA[mac’s version of yum is called ‘brew’:
http://brew.sh/


Once installed here’s an analogy:

Yum     =  brew
Package  = formula
Repo = tap
Yum repolist = brew tap

here’s an example command that installs cask:

$ brew cask install slack  

To add cask to your list of repos, see:

https://caskroom.github.io/
]]></Content>
		<Date><![CDATA[2016-03-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[apple|osx]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>OSX]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Setup Dashing on EC2 instance that's built from centos 7 ami]]></Title>
		<Content><![CDATA[I had to install:

<pre>
Yum install epel-release
Yum install curl
Yum install libxml2
Yum install libxml2-devel
yum install libcurl-devel
Yum install rubygems
yum install 'gcc-c++'
Yum install ruby-devel
yum -y install nodejs
</pre>

As normal user: run:

<pre>
$ Gem install bundler
</pre>]]></Content>
		<Date><![CDATA[2016-03-31]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[aws|Centos|dashing]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Increasing the size of an EC2 instance's primary EBS volume]]></Title>
		<Content><![CDATA[This is really good guide:



http://cloud.tekgoblin.com/2013/04/29/aws-guides-how-to-increase-your-ec2-linux-root-volume-size/


basically stop your instance, detach the primary volume (/dev/sda1), create a snapshot from this ebs volume, create a volume (with increased diskspace) from the snapshot, attach the new bigger volume to the ec2 instance, as /dev/sda1. 

start the instance again. ]]></Content>
		<Date><![CDATA[2016-03-31]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[aws|ebs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS EC2 Puppet setup to use internal ip addresses]]></Title>
		<Content><![CDATA[It's best to configure your puppet setup inside your aws's vpc so that your puppet agent ec2 instance communicates with your puppet master ec2 instance, using only internal private address. 

On the puppetmaster we have:
<pre>
cat /etc/puppetlabs/puppet/puppet.conf
[main]
server = ip-10-2-40-128.eu-west-1.compute.internal      {internal-url}
external_nodes = /usr/local/bin/puppetenc
node_terminus = exec
# This file can be used to override the default puppet settings.
# See the following links for more details on what settings are available:
# - https://docs.puppetlabs.com/puppet/latest/reference/config_important_settings.html
# - https://docs.puppetlabs.com/puppet/latest/reference/config_about_settings.html
# - https://docs.puppetlabs.com/puppet/latest/reference/config_file_main.html
# - https://docs.puppetlabs.com/puppet/latest/reference/configuration.html
[master]
vardir = /opt/puppetlabs/server/data/puppetserver
logdir = /var/log/puppetlabs/puppetserver
rundir = /var/run/puppetlabs/puppetserver
pidfile = /var/run/puppetlabs/puppetserver/puppetserver.pid
codedir = /etc/puppetlabs/code
autosign = true
certname = ip-10-2-40-128.eu-west-1.compute.internal        {internal-url} 
server = ip-10-2-40-128.eu-west-1.compute.internal       {internal-url}
[agent]
certname = i-ddfe9455                                    {puppetmaster's-ec2-instance-id}
server = ip-10-2-40-128.eu-west-1.compute.internal       {internal-url}
</pre>




On the puppetagent we have: 


<pre>
$ cat /etc/puppetlabs/puppet/puppet.conf
[main]
server = ip-10-2-40-128.eu-west-1.compute.internal   {puppetmaster's-internal-url}

[agent]
certname = i-77690cff                               {puppetagent's-ec2-instance-id}
server = ip-10-2-40-128.eu-west-1.compute.internal  {puppetmaster's-internal-url}

# This file can be used to override the default puppet settings.
# See the following links for more details on what settings are available:
# - https://docs.puppetlabs.com/puppet/latest/reference/config_important_settings.html
# - https://docs.puppetlabs.com/puppet/latest/reference/config_about_settings.html
# - https://docs.puppetlabs.com/puppet/latest/reference/config_file_main.html
# - https://docs.puppetlabs.com/references/latest/configuration.html
</pre>


Note: a new cert gets generated + signed on the puppetmaster, everytime you do the following for the first time: systemctl start puppetserver

troubleshooting commands:

# commands to run on the puppetmaster:
$ puppet cert list   # only shows certs waiting to be signed
$ puppet cert list --all   # shows all certs, signed and unsigned
$ puppet cert clean {cert-name}    # deletes the cert 


Commands to run on the puppet agent:

$ puppet config print ssldir

]]></Content>
		<Date><![CDATA[2016-04-01]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[aws|Puppet]]></Tags>
		<Status><![CDATA[private]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Linux - The dig command]]></Title>
		<Content><![CDATA[You can use the nslookup command to find what ip address a url resolves to. But another way to do this is by using the dig command:
<pre>$ dig google.com +short
{list of ip address}
</pre>
This gives a list of ip addresses, since google.com website is load-balanced across several servers.

To find which dns servers gave this resolution info, do:
<pre>$ dig google.com ns +short
{list of urls}
</pre>
These are the urls of dns servers.

If you want to find out what a server's fqdn is, then do:
<pre>$ dig -x {ip-addr}</pre>
This will give the same info that you would get if you had ssh'd into the server and ran the 'hostname' command.


Also you can specify what dns server to use when using dig:

<pre>
$ dig https://example.com @8.8.8.8 
</pre>

This is using google's dns. If you want to use your company's internal dns, then do:

<pre>
$ dig https://example.com @{internal-dns} 
</pre>

]]></Content>
		<Date><![CDATA[2016-04-02]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Centos|dig]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - things to setup on new instance]]></Title>
		<Content><![CDATA[install:

montoring shell scripts
sysstat]]></Content>
		<Date><![CDATA[2016-04-07]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[aws]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHEL - Create a systemd service unit]]></Title>
		<Content><![CDATA[<pre>
cat /etc/systemd/system/qrsnew.service
[Unit]
Description=New QRS Dashboard Service (Port 4000)
After=network.target

[Service]
Type=forking
User=qrsdashboard
ExecStart=/home/qrsdashboard/qrsnew.sh start
ExecStop=/home/qrsdashboard/qrsnew.sh stop
PIDFile=/home/qrsdashboard/pid/qrsnew.pid

[Install]
WantedBy=multi-user.target
</pre>


<pre>
#!/bin/bash
# Used this as a template: http://pkgs.fedoraproject.org/cgit/rpms/openssh.git/tree/sshd.init?h=f16

start() {
  forever --sourceDir /home/qrsdashboard/qrs-new --pidFile /home/qrsdashboard/pid/qrsnew.pid --minUptime 5000 --spinSleepTime 2000 --plain -a -l /var/log/qrs-new.log start server.js
  RETVAL=$?
  sleep 5
  return $RETVAL
}


stop() {
  pid=`cat /home/qrsdashboard/pid/qrsnew.pid`
  /bin/forever stop $pid
  RETVAL=$?
  return $RETVAL
}

restart() {
  stop
  start
}


case "$1" in
  start)
	start
	;;
  stop)
	stop
	;;
  restart)
	restart
	;;
  *)
	echo "Usage: $0 {start|stop|restart}"
	;;
esac


exit $RETVAL
</pre>

<pre>
$ touch /home/qrsdashboard/pid/qrsnew.pid
</pre>
make this file read+writable]]></Content>
		<Date><![CDATA[2016-04-07]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Centos|RHEL|systemd]]></Tags>
		<Status><![CDATA[private]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - VPC Peering]]></Title>
		<Content><![CDATA[All resources inside a vpc can automatically communicate with each other via their private ip addresses, irrespective of which public/private subnets they belong to. However it is also possible for resources in one vpc to communicate with resources in another vpc. This is done by setting up a "vpc peering" connection.

A VPC peering connection is a network connection between two VPCs which lets instances from one vpc to communicate with instances in the other vpc as if they are within the same network.

There are a few conditions that needs to be met for setting up vpc peering:
<ul>
 	<li>Both VPCs needs to reside in the same region. I.e. you can't set up vpc peering between vpc's in different regions.</li>
 	<li>Each VPC's CIDR block range is not allowed to overlap. Otherwise it would potentially mean that an EC2 instance in one vpc can have the same private ip address as an EC2 instance in the other VPC, which results in conflicts.</li>
 	<li>VPCs can reside in different AWS accounts, however the region restriction still applies.</li>
</ul>
&nbsp;

There are a few options to set how much access resources in one vpc is allowed to have access to resources in another vpc:
<ul>
 	<li>vpc-vpc : all resources in one vpc can access all resources in the other vpc</li>
 	<li>subnet-vpc</li>
 	<li>subnet-subnet</li>
 	<li>instance-vpc</li>
 	<li>isntance-subnet</li>
</ul>
Note: you can't use vpc peering to setup communications between your vpc and on premise network, to do this you need to set up a vpn instead.

A common scenario you can have is have a single "master" vpc, and lots of slave VPCs, where all the slave VPCs are peered connected to the the master VPC. This means that the slave vpc's can't communicate with eachother, although they are connected to the master VPC.]]></Content>
		<Date><![CDATA[2016-04-09]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - DB Subnet Groups]]></Title>
		<Content><![CDATA[A subnet by design is attached to a particular AZ. So all instances/resources that reside in a subnet actually exists in the same AZ.

However RDS has features such as Multi-AZ and Read Replicas, which are used for improved redundancies and performance reasons. However these features needs to know what other subnets it is allowed to build read replicas and multi-AZ passive/standby db's into. You can tell this to RDS by providing a list of subnets, this list is referred to as a DB subnet group.

Note: if you want your read replicas and Mult-AZ standbys to be accessible via the internet, the subnets that you add to your DB subnet group needs to be public subnets (i.e. the subnet's route table must have an entry pointing to your vpc's internet gateway).

When you create a new rds instance, you will have the option to specify whether it is publicly accessible (whether on not to attach a public IP address to your instance).

&nbsp;]]></Content>
		<Date><![CDATA[2016-04-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Virtual Private Network (VPN)]]></Title>
		<Content><![CDATA[A VPN is essentially a subnet where it's members are a combination of AWS resources and on premise devices. I.e. a a VPN is a subnet that extends to an on premise site. There are 2 ways to set up VPN:
<ul>
 	<li>Hardware based</li>
 	<li>VPN Tunnelling (OpenVPN)</li>
</ul>
&nbsp;
<h2>Hardware based VPN</h2>
To set up a VPN, you need to have the following components:
<ul>
 	<li><strong>Virtual Private Gateway (VPG)</strong> - This is an AWS resource that you create under the VPC section of your AWS console, and then attach it to your vpc, in the same way that you attach other things, e.g. route table. When creating a VPG, all you have to do is to provide a name for the VPG, and specify which VPC to attach it to:
<p id="TYMANKl"><a href="http://codingbee.net/wp-content/uploads/2016/04/img_570c05f575ec7.png"><img class="alignnone size-full wp-image-6907 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/04/img_570c05f575ec7.png" alt="" /></a></p>
</li>
 	<li><strong>Customer Gateway</strong> - This is essentially a device on your on premise network that has a public IP address attached to it. All on premise network traffic that needs to be sent to the VPC will need to be routed through the Customer Gateway. Therefore in practice the Customer gateway is usually your on-premise router. In AWS you need to create a "Customer Gateway" resource which represents your on premise router.
<p id="KihdHXq"><a href="http://codingbee.net/wp-content/uploads/2016/04/img_570c06f3adfad.png"><img class="alignnone size-full wp-image-6908 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/04/img_570c06f3adfad.png" alt="" /></a></p>
Here you choose a name of the customer gateway and specify your router's public IP address. Not you don't directly attach the customer gateway to the VPC</li>
 	<li><strong>VPN</strong> - This is the link that connects your Customer Gateway to your VPG:
<p id="bPAcJMM"><a href="http://codingbee.net/wp-content/uploads/2016/04/img_570c0813621c7.png"><img class="alignnone size-full wp-image-6909 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/04/img_570c0813621c7.png" alt="" /></a></p>
</li>
</ul>
You can only attach 1 VPG to a VPC, however you can attach multiple Customer Gateways to a VPG, which is handy if you have several on premise sites, all with their own routers.

Internally the VPN has 2 connections to the VPG for better redundancies.

Another thing you need to do is that you need to update routing table inside your vpc as well as inside your on premise network, in order for the traffic to be routed correctly between on-premise and the vpc.
<h2>VPN Tunneling</h2>
This approach involves creating an EC2 instance that resides on a public subnet, inside this VPC. You then install a software called OpenVPN. The OpenVPN software comes in two parts the server side software and and client side. However both sides are installed by the same rpm package. On the EC2, you need to configure it as  an openvpn server.

On the on premise side, you set up your machines as open vpn clients. These clients can then use the openvpn's public IP address to establish a persistant vpn session (aka vpn tunnel) to the openvpn server.

The openvpn server then allocates an internal IP address to your openvpn client. This internal IP address isn't an IP address of any aws subnets, instead the openvpn server has an internal dhcp which aside this internal ip address to the openvpn client. This internal ip address is internally used by openvpn for routing purposes. The openvpn server has an internal route table that routes traffic coming from an openvpn client's internal ip address, and reroutes it to the permitted aws subnets (or the whole vpc).

Hence if on the openvpn client, you run:

<code>$ ssh username@{ec2-internal-ip-address}</code>

Here the openvpn client will intercept this request and route it the the openvpn server. The openvpn server will then use it's internal routing table to forward on the request to the target ec2 instance and relay responses back to the openvpn client.

This approach of setting up a vpn via openvpn, has a problem, which is that there's a single point of failure.

You can overcome this by having 3 EC2 instances for your openvpn setup:
<ul>
 	<li>main openvpn server - with an elasticip address attached</li>
 	<li>standby openvpn server - ideally in a different AZ</li>
 	<li>an EC2 instance that has a script that is constantly pinging the main openvpn server. If the pinging fails the it triggers an aws cli based script that detaches the elastic ip address form the main openvpn server and reattaches it to the standby openvpn server.</li>
</ul>
Note: you can take a similar approach to handling failover for nat instances.

Note, some routers have the openvpn client built in. This means that you can setup your router as a vpn client, and you don't need to setup openvpn clients on each of the devices that connects to the router, since that is all centralised by your onsite router which has a built in routing table to intercept internal IP bound traffic and reroute it the openvpn server.

&nbsp;

&nbsp;

&nbsp;

http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_VPN.html

http://docs.aws.amazon.com/AmazonVPC/latest/NetworkAdminGuide/Introduction.html]]></Content>
		<Date><![CDATA[2016-04-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - NAT Instances Duplicate]]></Title>
		<Content><![CDATA[If you have an ec2 instance that is attached to a private subnet only, then it won't have internet access. That's because by definition, the routing table that is associated with the private subnet, doesn't have an entry for routing traffic to/from an internet gateway.

For security reasons, this is a good thing, if the resources does not provide a service that requires direct access. However this is also a problem, because you can't use commands like yum which requires internet access.

You can overcome this problem by creating a <a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_NAT_Instance.html" rel="nofollow">NAT instance</a>. A NAT instance allows instances that are on the private subnet to initiate outbound traffic to the internet, and thereby establishing a connection session which will then allow inbound traffic for the duration fo the session. It's a bit like a temporary tunnel that is created by the private instance which is closed again as soon as the private instance received the traffic it requested for.

A NAT instance is essentially a special type of EC2 instance. A NAT instance is an EC2 instance with the following characteristics:
<ul>
	<li>A NAT instance is an EC2 isntace that is created from AMI that includes the string <strong>amzn-ami-vpc-nat</strong> in it's name. You can find these AMI's by searching in the Community AMIs sections.</li>
	<li>The NAT instance's type needs to be quite big in order to have enough network data transfer capacity.</li>
	<li>NAT instance must be connected to a public subnet and must have a public IP address (or Elastic Public IP Address). I.e. the NAT instance must have internet access.</li>
	<li>The NAT instance should have a security group attached to it called, "NATSG". This name is not manadatory but is recommended by AWS</li>
	<li>NATSG needs to have the following inbound firewall rules:
<ul>
	<li>SSH for all ip addresses (or try to limit this if possible)</li>
	<li>HTTP limited to the private subnet's ip range - That's because if you look at yum's .repo config files, you'll see baseurl is http</li>
	<li>HTTPS limited to the private subnet's ip range - That's because yum's .repo's baseurl can also be https.</li>
</ul>
</li>
	<li>The NAT Instance needs to have the <strong>source/destination checks</strong> setting disabled. Otherwise the NAT instance will terminate outbound data packets that it is not the source of. It will also terminate in bound packats that it isn't the destination of either. You can modify this setting by right clicking on your instance, and navigating to:
<p id="vpTvTeL"><a href="http://codingbee.net/wp-content/uploads/2016/04/img_570a7fab1b402.png"><img class="alignnone size-full wp-image-6897 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/04/img_570a7fab1b402.png" alt="" /></a></p>
</li>
	<li>The route table that's attached to the private subnet must have an entry that routes all other traffic (0.0.0.0) bound traffic to the NAT Instance. The "local route" will intercepts all internal traffic first. so the 0.0.0.0 will only come into play for IP ranges that are outside of the VPC IP range.</li>
</ul>
&nbsp;

The EC2 doesn't need to have a public IP address. Therefore the internet will treat the NAT instance's public IP address as the source address. When the NAT instance receives the response data traffic, it will reroute the traffic back to the EC2 instance.

&nbsp;

&nbsp;

&nbsp;]]></Content>
		<Date><![CDATA[2017-01-14]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[draft]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Amazon Machine Images (AMI)]]></Title>
		<Content><![CDATA[Note:

AMIs created in one region cannot be used to create instances in another region. However you can get round this by copying the AMI to another region first.

&nbsp;

You can also modify access permissions of the AMI, in the following ways:
<ul>
 	<li>Make the AMI public, so anyone can use it by finding it on the marketplace</li>
 	<li>Specify a list of AWS account names that can access the ami</li>
</ul>
However the region restriction still applies in both of the above cases.

&nbsp;

An AMI is made up of 3 parts:
<ul>
 	<li>a snapshot of the root volume, /dev/sda1</li>
 	<li>launch permissions, i.e. is it a public ami, or restricted to a particular aws account</li>
 	<li>block device mappings. This means that an ami can be made up of several ebs snapshots in addition to /dev/sda1</li>
</ul>
&nbsp;

You can create an AMI from an existing EBS snapshot, as long as the snapshot is taken of the root volume, /dev/sda1.

&nbsp;

&nbsp;

AWS stores Both AMI and snapshots in S3.]]></Content>
		<Date><![CDATA[2016-04-17]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[ebs automount external fact]]></Title>
		<Content><![CDATA[<pre>$ cat /opt/puppetlabs/puppet/cache/facts.d/unmounted_ebs_blocks.sh
#!/bin/bash

extra_blocks=`ls -l /dev | grep '^b.*[b-z]$' | awk '{print $NF}'`
echo '{' &gt; /opt/puppetlabs/puppet/cache/facts.d/unmounted.json
echo '"unmountedblocks":[' &gt;&gt; /opt/puppetlabs/puppet/cache/facts.d/unmounted.json

counter=`ls -l /dev | grep '^b.*[b-z]$' | awk '{print $NF}' | wc -l`
for ebsblock in $extra_blocks ; do
match=`mount | grep $ebsblock | wc -l`
if [ $match -eq 0 ] ; then
if [ ! $counter -eq 1 ] ; then
echo "\"$ebsblock\"," &gt;&gt; /opt/puppetlabs/puppet/cache/facts.d/unmounted.json
else
echo "\"$ebsblock\"" &gt;&gt; /opt/puppetlabs/puppet/cache/facts.d/unmounted.json
fi
fi
let counter=counter-1
done

echo "]" &gt;&gt; /opt/puppetlabs/puppet/cache/facts.d/unmounted.json
echo "}" &gt;&gt; /opt/puppetlabs/puppet/cache/facts.d/unmounted.json</pre>]]></Content>
		<Date><![CDATA[2016-04-19]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[]]></Tags>
		<Status><![CDATA[private]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Launch Configurations, Autoscaling Groups, and ELBs]]></Title>
		<Content><![CDATA[Launch Configurations - let's you specify a template for automatically create new instance, based on this presets. Therefore when creating a Launch Configuration resource you specify things like:
- AMI id
- Instance type
- Which vpc to build instances in 
- IAM role
- default storage requirements. E.g. number and size of ebs blocks to attach to new EC2 instances
- User data script
- security groups
- which ssh keys to use, to log into instances

Autoscaling Groups - this is where you specify under what conditions to scale up and down. This includes specifying which Launch Configuration to use when scaling up/down, in fact this is the most important your autoscaling group requires. The autoscaling group resources needs to know which launch configuration to use when starting new instance as part of the scale up process. Other info you need provide are:
- what vpc, along with what subnets to build into. You can specify multiple subnets, so it's best to specify subnets in different AZs to improve high availability, and fault tolerance.<strong> Note the ELB that you attach to this autoscaling group needs to be configured to send traffic to the same list of subnets</strong>. 
- scaling policies, e.g. min, max, and desired ec2 instance numbers. 
- you can specify whether or not the instance managed by your AGS group is to receive traffic from an ELB, if so you specify the name of the ELB. This effectively ends up attaching the ASG to an ELB. Behind the scenes, the AGS will constantly update the ELB's list of EC2 instances. As part of this you can also specify whether to use ELBs or EC2 healthcheck (this is a radio button option). I.e. if you choose ELB healthchecks, then the ASG will create new instance if ELB reports that an ec2 instance has failed.
- You can specify cloudwatch metric ranges for scaling up and down  

ELB - here ]]></Content>
		<Date><![CDATA[2016-04-22]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Direct Connect]]></Title>
		<Content><![CDATA[Some Internet Service Providers can connect your on premise devices directly to aws AZ without being rerouted via the rest of the internet.
<ul>
 	<li>this results in faster connection</li>
 	<li>more stable connection</li>
 	<li>reduced latency</li>
 	<li>No need to go via the public internet</li>
 	<li>better security</li>
 	<li>No need to have any special hardware on-site</li>
 	<li>aws charges less for data traffic going to from aws via direct connect, rather than via the internet</li>
 	<li>It gives you dedicated private network connection. I.e. you can connect to your ec2 instances via it's private ip addresses. Previously we saw that this is possible by setting up vpn, however with Direct Connect, you don't need to set up connection. This is achieved by your onsite devices connecting to your vpc via a Virtual Gateway that's attached to your vpc. To achieve this you need to setup Virtual Interfaces (vifs) on your onsite devices.</li>
</ul>
&nbsp;

private virtual interface: this is something that has to be attached to your vpc to enable direct connect connections to your ec2 instances using private ip addresses. you can think of ec2 private ip addresses as private endpoints. Each vpc must have it's own vif in order to connect to it via direct connect.

However some aws services, e.g. rds and s3 by default have public endpoints. So you can't using private vifs with things like S3. Theres 2 ways to overcome this:
<ul>
 	<li>you can actually set up private endpoints for these aws services, which will then make it possible to continue using private vifs</li>
 	<li>Use "Public VIFS", this is essentially private vifs except some extra rerouting is done on the AWS region side to reroute the traffic from aws direct connect, to the public endpoints.</li>
</ul>
You can also attach Public VIFS to your vpc too. This would then let you access your ec2 instances via their public ip addresses too, but still going through via direct connect.

public VIFS makes use of public CIDR block range, but traffic is still routed via dedicated network and hardware, due to direct connect.

&nbsp;

In order to use direct connect, your ISP needs to establish a connection from your site to it's internal traffic network. This connection is referred to as a cross-connect, aka cross network connection.

&nbsp;

Direct Connect, by design, can only connect to a single AWS region.

&nbsp;

https://aws.amazon.com/directconnect/faqs/

&nbsp;]]></Content>
		<Date><![CDATA[2016-04-23]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Cloudfront]]></Title>
		<Content><![CDATA[Cloudfront is a (Content Delivery Network) CDN that delivers content to Edge Locations around the world.

<strong>Origin</strong>: This term means the location where the content can originate from. There are a few places:
<ul>
 	<li>EC2</li>
 	<li>ELB, with ec2 instances behind it</li>
 	<li>S3 bucket</li>
 	<li>route53 - e.g. if actual source is an microsoft azure vm.</li>
</ul>
&nbsp;

&nbsp;

Edge locations effectively stores cached content.

<strong>Distribution:</strong> This term is the name for a given "cloudfront resource". You can create two types of distributions:
<ul>
 	<li>Web distribution - most commonly used, e.g. cdn for static website</li>
 	<li>RTMP - Used for media streaming.</li>
</ul>
&nbsp;

you can restrict access to your distributed content using one of the following cloudfront features:
<ul>
 	<li>signed urls</li>
 	<li>signed cookies</li>
</ul>
Objects in an Edge Location are cached for the life of the TTL (Time To Live). But can delete these sooner, but you will be charged for this.

Edge Locations are normally first responders of requests, if it doesn't have the required content cached, then it would forward the request to the origin, and it will receive the requested content. The edge location will then cache the content (for future requests) and then forward the content on to the requester.

Advantages:
<ul>
 	<li>faster response time - hence better experience to for the data requesters</li>
 	<li>less strain on the content origins</li>
</ul>
Edge location is not limited to read only. You can also write to them, and Edge Location can forward this onto the origin.

&nbsp;

Edge locations will use cached version until either:
<ul>
 	<li>We rename the source content's filename, and reference the new filename in our code. E.g. if we update the css file, then we rename the css file. After that we update our html code to reference the new name. This will work as long as our html code isn't cached but our css files are. Similar approach for jpegs, pngs,...etc.</li>
 	<li>We tell the edge location to stop using the cached version - this is done by invalidating the file. Cloudfront lets you invalidate up to 100 objects per month for free. Then it starts costing. If you need to go over this limit, then it's more cost effective to create a new cloudfront distro containing the refreshed content and delete the old cloudfront distro. Then update our route 53 entry (discussed below) to point to the new Cloudfront distro.</li>
</ul>
&nbsp;

We can attach expiration dates for cached contents. or disable caching for particular content.

&nbsp;

With CloudFront you can also create private (long) urls that has expiration dates. This is great for sharing content privately by sharing the urls, a bit like how github's gist system works.

&nbsp;

When using cloudfront, you get a unique long cloudfront url, for each distributed content sources. If you want, you can create a cname recordset in route53 to reroute traffic directly to cloudfront. This might be handy:
<ul>
 	<li>if you want to take our content offline temporarily to do maintenance work.</li>
 	<li>If you have written apps that requests for content from cloudfront. In that case you don't want to hard code the long cloudfront url, instead you want to use a userfriendly url which can easily be repointed elsewhere as and when needed.</li>
</ul>
&nbsp;
<h2>CloudFront AWS console walkthrough</h2>
To use cloudfront, we need create a cloudfront resource, which is referred to as a "distribution":
<p id="IetxgPx"><a href="http://codingbee.net/wp-content/uploads/2016/04/img_571b40df7cb29.png"><img class="alignnone size-full wp-image-6970 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/04/img_571b40df7cb29.png" alt="" /></a></p>
Next we have 2 options, in most cases the web option is the way to go:
<p id="xLDurjr"><a href="http://codingbee.net/wp-content/uploads/2016/04/img_571b41aac1909.png"><img class="alignnone size-full wp-image-6971 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/04/img_571b41aac1909.png" alt="" /></a></p>
&nbsp;

Then we have:
<p id="fEtRyhS"><a href="http://codingbee.net/wp-content/uploads/2016/04/img_571b42979de37.png"><img class="alignnone size-full wp-image-6973 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/04/img_571b42979de37.png" alt="" /></a></p>
The first field, the origin domain name is actually a drop down list where you can choose available ELBs or S3 buckets:

&nbsp;
<p id="IpgPnxD"><a href="http://codingbee.net/wp-content/uploads/2016/04/img_571b4338cc37a.png"><img class="alignnone size-full wp-image-6974 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/04/img_571b4338cc37a.png" alt="" /></a></p>
&nbsp;

Further down we have:
<p id="jJlaopc"><a href="http://codingbee.net/wp-content/uploads/2016/04/img_571b447647638.png"><img class="alignnone size-full wp-image-6975 pastedimages" src="http://codingbee.net/wp-content/uploads/2016/04/img_571b447647638.png" alt="" /></a></p>
Here you can specify how global you want your cdn to reach. E.g. us only, us+europe, or us+europe+asia, or all edge locations, which is the most expensive option.

&nbsp;

You also have to specify the route 53 alias that represents your s3 bucket or ELB.

&nbsp;

After you have created your distribution, you can then monitor various statistics for your distro:
<ul>
 	<li>cache statistics, e.g.:
<ul>
 	<li>number of objects that are currently cached</li>
 	<li>hits metric: number of objects being delivered by your cloudfront distro</li>
 	<li>misses metric: number of objects being delivered from source (and then subsequently cached for next time)</li>
 	<li>errors metric: number of 404/500 error messages being delivered</li>
 	<li>geographic origin of requests, a bit like google analytics</li>
</ul>
</li>
 	<li>You can set alarms to be triggered on certain actions. E.g. if too many objects are being delivered, which makes cloudfront expensive.</li>
 	<li>popular objects in ranked order</li>
 	<li>usage info: e.g.
<ul>
 	<li>number of http requests</li>
 	<li>number of https requests</li>
 	<li>data transfers in terms of MB, GB,..etc.</li>
</ul>
</li>
 	<li>statistics about requesting devices, e.g.
<ul>
 	<li>what os, mac, windows</li>
 	<li>what browser, chrome, firefox</li>
 	<li>what device, desktop, tablet, phone</li>
</ul>
</li>
</ul>
&nbsp;

&nbsp;

When adding an alias in route 53 for your cloudfront distro, you need to name it, with the same info you gave in the 'Alternate Domain Names' you specified above, at the time of creating the cloudfront distro.  In the alias target section, you can select your cloudfront cname  from the dropdown list, so that it all matches up

If the origin you are using is s3, then you also need to make sure your s3 permissions for shared files is changed to 'public'. Otherwise you will get a permissioned denied message. Alternatively you can set cloudfront permission to override s3 bucket permissions as described further down:

&nbsp;

&nbsp;
<h2>Customizing your cloudfront distribution</h2>
Once you have created your cloudfront resource, you can then edit it. this include:
<ul>
 	<li>you can add additional origins to your cloudfront distro. You can then specify which origins to use under the "bahaviours" tab.</li>
 	<li>you can set permissions so that cloudfront permissions overrides s3 bucket permissions, for read access. To do this you need to enable your distro's "restrict bucket access" setting, and you specify an existing pem file, then cloudfront will retrieve the files in s3, not by http, but via the aws api, using your chosen private key, .i.e. .pem file</li>
 	<li>Under the behaviours, you can specify wildcard expression (for the requesting url), e.g. *.png, or images/*, and map these expressions to an origin (which are selectable via a drop down list). you can also specify what ports to deliver content via, e.g. http. You can also specify caching retention rules here too.</li>
 	<li>You can write your own custom 404/500 error pages, that your distro can serve up on a failed request.</li>
 	<li>you can set geo based restriction, e.g. make your content inaccessible for people in a certain country.</li>
</ul>
&nbsp;

&nbsp;]]></Content>
		<Date><![CDATA[2016-04-23]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Setting up DNS failover in Route 53]]></Title>
		<Content><![CDATA[You can set up active and passive entries for the same url in route 53. So when route 53 discovers that the active (primary) source has become unhealthy it will failover to the passive (secondary) entry.

For example, we have a static website running on an EC2 instance. This EC2 instance is attached to an ELB. We have a route 53 alias that points to the ELB.

Now let's say we have a file called "index.html" that is stored in S3. Also let's assume that we configured S3 to act as a regular website. Now on the index.html file, let's assume it says something like "this website is currently down, come back later". Also let's say we created a cloudfront distribution, that used this S3 bucket as a source.

&nbsp;

Now to set up failover to the cloudfront/S3 bucket, we create 2 records in route 53. The trick is to create 2 dns entries with identical "url" name. The first one is aliased to the elb's url, and the second one is aliased to the cloudfront's url.

In both cases, you need to select "failover" from the "routing policy" dropdown list, also you leave the "Set ID" field unchanged.

For the primary route53 entry, you also need to enable the "Evaluate Target Health" radio button.

&nbsp;

&nbsp;

http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html]]></Content>
		<Date><![CDATA[2016-04-23]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Bastion hosts]]></Title>
		<Content><![CDATA[For security reasons you should not have your aws resources (e.g. ec2 instances) directly accessible via the internet unless it is necessary, i.e. keep your aws resources internal. However you still want to be able to access your VPCs, there's a few ways to achieve this:
<ul>
 	<li>Setup bastion hosts</li>
 	<li>setup vpn - covered later</li>
 	<li>set up AWS Direct Connect - covered later</li>
 	<li>A combination of the above.</li>
</ul>
A "bastion host" is a general concept and isn't something specific to AWS. A bastion host is essentially an EC2 instance that sits inside a public subnet, which in turn resides inside a vpc. You can ssh into all the other ec2 instances inside this vpc, by first ssh'ing into the bastion host (since by design, all ec2 instances inside a vpc can communicate with each other via their internal ip's irrespective of which public/private subnet they belong to).

&nbsp;

A bastion host has the following characteristics:
<ul>
 	<li>it resides inside a public subnet, therefore your vpc needs to contain at least one public subnet in order to have a bastion host.</li>
 	<li>bastion host is configured with extra security at the os level, e.g. it has selinux enabled, it has firewalld running</li>
 	<li>bastion host should only be configured to have the sshd service listening, best practice is to reconfigure away from default port 22, to another obscure port number. All other ports should be closed.</li>
 	<li>Should have it's own AWS security group which denies traffic on all ports apart from the port that ssh is listening on. This security group should also restrict source IP addresses to the range of your on-premise network.</li>
 	<li>It should store any private ssh keys, instead you use ssh forwarding to seamlessly pass through the bastion host.</li>
</ul>
&nbsp;

&nbsp;

https://10mi2.wordpress.com/2015/01/14/using-ssh-through-a-bastion-host-transparently/

https://10mi2.wordpress.com/2015/01/19/using-ssh-bastion-hosts-with-aws-and-dynamically-locating-them-with-ec2-tags/

http://sshmenu.sourceforge.net/articles/transparent-mulithop.html

Also remember, after restarting your mac, and when you open up a new bash terminal, do:

a. $ ssh-add -l
b. $ ssh-add ~/.ssh/private.pem


http://superuser.com/questions/479592/why-would-ssh-fail-to-expand-h-variable-in-ssh-config

https://www.digitalocean.com/community/tutorials/how-to-configure-custom-connection-options-for-your-ssh-client

https://aws.amazon.com/about-aws/whats-new/2016/09/linux-bastion-hosts-on-the-aws-cloud-quickstart-reference-deployment/]]></Content>
		<Date><![CDATA[2016-04-23]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Route 53 routing policy types]]></Title>
		<Content><![CDATA[In route53 you have multiple entries with the same url (aka url). In fact you have to create multiple entries with the same name in order to take advantage of the various routing policies. Here are the available routing policies:
<ul>
 	<li>Simple</li>
 	<li>Weighted</li>
 	<li>Latency</li>
 	<li>Failover</li>
 	<li>Geolocation</li>
</ul>
We have already covered Failover.

&nbsp;
<h2>Latency based routing</h2>
One thing you can do is set up the exact same VPC in 2 different regions.

You can then configure route 53 to route traffic to the VPC that is going to be the first responder to a given source's request. This usually means that the vpc that is geographically closer to the requester's location, will end up handling the request.

This indirectly means that if one vpc goes down, all traffic will failover to the vpc that is still active, since it essentially becomes the first responder.

&nbsp;

To set this up, you select "latency" from the route53 entry's "routing policy" dropdown list, and then specify a region from the down list. You then create another record for the same url again, and again choose latency. But this time specify a different region from the dropdown list. I think specifying region from the dropdown list helps route53 to route traffic faster, rather than route53 taking more time to figure out which region the corresponding ip address belongs too.

When a request is then made, route53 will ping both vpc's and the vpc that responds fastest ends up becoming the responder.
<h2>Weighted based routing</h2>
This option lets you do load balancing at the route 53 level.

Here you set a value for the "weight". This value can range from 1-100. If you set this to '1' in both entries, then route53 will send traffic to both endpoints evenly. If the 2nd entry instead has '10', then for every 11 requests, the first entry will get 1, and the second entry will get 10.

This is a useful way to gradually introduce replacement for existing architecture. E.g. existing architecture is given weight of 100, and new architecture given weight of 1. Then over the course of days and weeks, you gradually decrease the weighting value of the existing architecture, and increase the weight value of the new architecture. This is also a good way to test whether your new architecture can handle the load.

&nbsp;
<h2>Geolocation based routing</h2>
This option lets you route traffic to various endpoints based on the requester's country of origin. Hence you select a country from a massive dropdown list. This is useful if you want to direct USA customers to USA version of your website, which shows all prices in US dollar, if you run an online shop.

&nbsp;

&nbsp;

&nbsp;

&nbsp;]]></Content>
		<Date><![CDATA[2016-04-24]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Security Considerations]]></Title>
		<Content><![CDATA[This is placeholder to give an overview of AWS security]]></Content>
		<Date><![CDATA[2016-05-02]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Shared (Security) Responsibility Model]]></Title>
		<Content><![CDATA[Ensuring that your aws infrastructure is secure is a responsibility that's shared between you and Amazon.

Amazon is responsible for mainly:
<ul>
 	<li>Ensuring physical hardware that your resources (e.g. EC2 instances are running on). E.g. limit access to who is allowed to walk into AWS's AZs (data centres)</li>
 	<li>Ensuring that internal data transfers are secure, e.g. data transfers between S3 buckets and EC2 instances. Also data transfers between physical hardware</li>
</ul>
We are responsible for:
<ul>
 	<li>Ensuring we use AMIs that are secure, i.e. don't have api keys or ssh keys hardcoded in them.</li>
 	<li>Performing OS software updates and security patches</li>
 	<li>Keeping "<a href="https://en.wikipedia.org/wiki/Data_at_rest" rel="nofollow">Data at rest</a>" secure - e.g. persistant data on our EBS. We can select the ebs encrypt option when creating our instances, also encrypt our filesystems using luksformat.</li>
 	<li>OS configurations, e.g. firewalld and selinux</li>
 	<li>software configurations, e.g. httpd settings</li>
 	<li>Setting up ssl certificates</li>
 	<li>Install firewalls</li>
 	<li>securely accessing AWS, via bastion host, vpn, or AWS Direct Connect</li>
 	<li>Properly configuring security groups and network acls</li>
 	<li>ensuring our own developed apps are secure, e.g. add a login page to our apps, prompting user to log in, in order to access data.</li>
</ul>
&nbsp;

&nbsp;

https://aws.amazon.com/compliance/shared-responsibility-model/]]></Content>
		<Date><![CDATA[2016-05-02]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Natively available AWS features for enhancing security]]></Title>
		<Content><![CDATA[AWS offers a bunch of natively security features that we can use to enhance security:
<ul>
 	<li>AWS API access security - via api keys</li>
 	<li>buitin vpc firewalls - private and public subnets. Encourages us to use private subnets whenever possible</li>
 	<li>IAM - only authenticated users and apps are granted access privileges</li>
 	<li>MFA - multifactor authentication - must use android phone as part of login process</li>
 	<li>Encrypt data stores - e.g. the Encrypted EBS feature, also s3 encryption</li>
 	<li>AWS direct connect - ISP's routes AWS traffic straight to AWS AZs, without going through the rest of the internet</li>
 	<li>Monitoring aws api usage - i.e. cloudtrail. Keeps track of user activities</li>
 	<li>AWS config - Lets you compare point-in-time snapshot view of how your infrastructure has changed over time. This is useful for example if you want to see what ec2 instances have been created/stopped/terminated a 5 days go, compared to today.  Each point-in-time snapshot is documented in json format</li>
 	<li><a href="https://aws.amazon.com/kms/" rel="nofollow">Key management service</a> - A place to store your private keys</li>
 	<li>A prart of AWS that's isolated from the rest of AWS in order for use by governments, aka govcloud. This part has industry standards to satisfy goverment security grequeiments.</li>
 	<li><a href="https://aws.amazon.com/cloudhsm/" rel="nofollow">CloudHSM </a>- This is a Hardware Security Module (HSM) for hardware based encryption</li>
 	<li><a href="https://aws.amazon.com/premiumsupport/trustedadvisor/" rel="nofollow">AWS Trusted Advisor</a> - This is an AWS support service (which is available as part of premiere support) where an AWS specialist reviews your infrastructure to identify any ways to improves your AWS setup's:
<ul>
 	<li>Cost efficiency</li>
 	<li>security</li>
 	<li>High Availability and fault tolerance</li>
 	<li>performance</li>
</ul>
</li>
</ul>
&nbsp;

&nbsp;]]></Content>
		<Date><![CDATA[2016-05-02]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Minimizing impact of DDOS attacks]]></Title>
		<Content><![CDATA[We can limit DDOS attacks in the following ways:
<ul>
 	<li>identify ip range of ddos attacks and block it at the Network ACL level. Alternatively could do this at the Security Group Level, but it's quicker at the Network ACL level.</li>
 	<li>Install DDOS prevention software on our EC2 instances that will monitor for DDOS attacks and filter them out.</li>
</ul>
&nbsp;

AWS minimizes impact by:
<ul>
 	<li>use of CloudFront, which can absorb most of the impact. Hence the edge collections takes on the main brunt of the attack</li>
 	<li>If ddos against a static website that's hosted on S3, then S3 will absorb this impact.</li>
 	<li>port scanning (using the nmap command) is disabled by default in AWS (even port scanning between EC2 instances that are inside the same VPC). If you want to enable port scanning, then you need to contact AWS for permission</li>
 	<li>AWS has enabled ingress filtering on all incoming requests.</li>
</ul>]]></Content>
		<Date><![CDATA[2016-05-02]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Encryption features Overview]]></Title>
		<Content><![CDATA[You can encrypt the content of your resources. This basically means that the content can't be viewable by an AWS employee. The only way to decrypt the content is via logging into the AWS Account that created the encrypted data in the first place, and also you need to login with the appropriate account privileges.

There are 3 main resource types, whose data you can encrypt:
<ul>
 	<li>S3 Buckets
<ul>
 	<li>Uses AES-256 encription to <a href="https://en.wikipedia.org/wiki/Data_at_rest" rel="nofollow">encrypt data at rest</a>. It is decrypted only when it receives a valid request, by a valid IAM user or ec2 instance.</li>
</ul>
</li>
 	<li>EBS volumes -
<ul>
 	<li>When enabled, this essentially means that the EC2 instance will first always encrypt the data before sending it to the EBS volume for storage.</li>
 	<li>EBS snapshot therefore only stores encrypted data. only A user with the right IAM role can access this data by mounting it onto a EBS volume.</li>
</ul>
</li>
 	<li>RDS level encryption
<ul>
 	<li>here the underlying EC2 instances encrypts the data before storing it on it's block devices (which could also be an EBS device)</li>
 	<li>Automated backups and snapshots are consequently encrypted too. Note, snapshots are backups that are manually created</li>
 	<li>Read replicas are encrypted too</li>
 	<li>By default RDS endpoints are https rather than http, so that data traffic are also encrypted.</li>
</ul>
</li>
</ul>
&nbsp;

&nbsp;]]></Content>
		<Date><![CDATA[2016-05-02]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Granular User/Application/Resource Access Controls]]></Title>
		<Content><![CDATA[If there is a particular file in an S3 bucket that is available to access, then there are three things that may want to download it:
<ul>
 	<li>An AWS user - This request can be granted via IAM roles</li>
 	<li>An AD user, who doesn't have an AWS account - this user might want temporary access, in which this can be done via a token that allows them to temporarily assume a role.</li>
 	<li>An Application that's running on top of a resource (e.g. ec2 instance) - this request can be granted via tokens that have expiry dates</li>
 	<li>Resources - e.g. an ec2 instance itself might want to download the file. In which case this can be done by assigning the necessary permissions to the EC2 instance's IAM role.</li>
</ul>
&nbsp;

You can set up IAM roles that are specify permissions on specific resources, .e.g. EC2 resources.]]></Content>
		<Date><![CDATA[2016-05-02]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Cloudwatch related security features]]></Title>
		<Content><![CDATA[Cloudwatch related API requests are signed with HMAC-SHA1signature from the request and the the user's private key

Cloudwatch's (sdk) API is only accessible via https, not http, i.e. it is encrypted with ssl

An IAM user can only access cloudwatch if they are given access via IAM

You can configure cloudtrail to send notifications to SNS, which in turn sends notifications to cloudwatch, to take particular actions when something happens, e.g. restrict a certain user's permission if they do something unwanted.

&nbsp;

You can also get cloudwatch to send all of your EC2 instance system logs to your cloudwatch in real time:

http://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/WhatIsCloudWatchLogs.html

&nbsp;]]></Content>
		<Date><![CDATA[2016-05-02]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - CloudHSM]]></Title>
		<Content><![CDATA[CloudHSM (Hardware Security Module): This is essentially the name of a dedicated physical machine that is seperate from all the other AWS hardware, and it is used to store encryption keys. If an outside party gains access to these keys, then your AWS infrastructure is compromised. Hence even AWS employees don't have physical access to CloudHSM since they are locked in specially controlled rooms that is seperate from the rest of the AWS AZ's hardware.

These keys are only used from inside the CloudHSM device itself. Because of this, the CloudHSM is responsible for decrypting data it receives, and decrypting data it sends out. CloudHSM has an API that all your other AWS resources can interact with. All the AWS resources that can interact with CloudHSM are referred to as "CloudHSM clients". Therefore if our application needs data to be decrypted/encrypted then it interacts with CloudHSM via the api to get this done.

The CloudHSM devise has a lot of advanced logging feature to make it tamper resistant, and to let you know if it has been compromised.

&nbsp;

The whole concept of CloudHSM exists to satisfy vairous security requirements of certain industries, e.g. government security requirements, banking security requirements, and online retail compliance (<a href="https://www.pcicomplianceguide.org/pci-faqs-2/#1" rel="nofollow">PCI compliance</a>).

CloudHSM can be single point of failure, that's why you should have at least 2 CloudHSM devices, one in each AZ.

Here are some of the different types of keys CloudHSM can be used to store:
<ul>
 	<li>Filesystem encryption keys</li>
 	<li>database encryption keys</li>
 	<li>Digital Rights Management (DRM) related keys</li>
 	<li>S3 related encryption keys</li>
</ul>
&nbsp;

http://docs.aws.amazon.com/cloudhsm/latest/userguide/cloud-hsm-third-party-apps.html

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

https://aws.amazon.com/cloudhsm/]]></Content>
		<Date><![CDATA[2016-05-02]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Ansible - Available setting to add to a playbook's header section]]></Title>
		<Content><![CDATA[<pre>
- name:  blahblah
  hosts: hostname
  vars:
    var1: value1
    var2: value2
  vars_files:
    - /path/to/var1.yaml
    - /path/to/var2.yaml
  vars_prompt:             this key's value is a 2 item array, each item is a single item hash 
    - name: var_name         
    - prompt: the prompt message itself     # this will do a prompt for more info during runtime 
  sudo: 
  user:
  connection:
  gather_facts:
</pre>

Note: by default behind the scenes the setup module always runs behind the scenes to collect facts. You can disable this behaviour if your playbook doesn't reference any facts. To disable this set gather_facts to 'false', or 'no'. ]]></Content>
		<Date><![CDATA[2016-05-06]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[ansible]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Ansible]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Python - crash course]]></Title>
		<Content><![CDATA[To install python:

<pre>
$ yum install python
</pre>

indentation is important, since code blocks are not encased in any brackets whatsoever. 

to access python command line (aka repl) do:

<pre>
[root@ansibleclient01 ~]# python
Python 2.7.5 (default, Nov 20 2015, 02:00:19)
[GCC 4.8.5 20150623 (Red Hat 4.8.5-4)] on linux2
Type "help", "copyright", "credits" or "license" for more information.
>>> a
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'a' is not defined
>>> 4
4
>>> 2 + 2
4
>>> 2 + 3
5
>>> x = abc
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'abc' is not defined
>>> x = 'abc'
>>> x
'abc'
>>>
</pre>

<code>ctrl+d</code> to exit the python terminal

<pre>
>>> print('hello world')
hello world
>>>
</pre>


Python comes included with the standard library. This library is:

https://docs.python.org/2/library/

This shows a list of modules you can 'import' into your scripts, without needing to install them separately (using pip utility).


To view help info for the 'math' module do:

<pre>
>>> help('math')
Help on module math:

NAME
    math

FILE
    /usr/lib64/python2.7/lib-dynload/math.so

DESCRIPTION
    This module is always available.  It provides access to the
    mathematical functions defined by the C standard.

FUNCTIONS
    acos(...)
        acos(x)

        Return the arc cosine (measured in radians) of x.

    acosh(...)
        acosh(x)

        Return the hyperbolic arc cosine (measured in radians) of x.

    asin(...)
        asin(x)

        Return the arc sine (measured in radians) of x.

    asinh(...)
        asinh(x)

        Return the hyperbolic arc sine (measured in radians) of x.
</pre>

To use the math module's "sqrt" function, you do:


<pre>
>>> import math
>>> math.sqrt(81)
9.0
>>></pre>


Heres how to run a python script:


<pre>[root@ansibleclient01 ~]# cat helloworld.py
print("Hello World!!!")
[root@ansibleclient01 ~]# python helloworld.py
Hello World!!!
</pre>


You can import a python script, simply by using the import command, followed by the script's filename, but excluding the .py extension:


<pre>
[root@ansibleclient01 ~]# python
Python 2.7.5 (default, Nov 20 2015, 02:00:19)
[GCC 4.8.5 20150623 (Red Hat 4.8.5-4)] on linux2
Type "help", "copyright", "credits" or "license" for more information.
>>> import helloworldd
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
ImportError: No module named helloworldd
>>> import helloworld
Hello World!!!
>>>
</pre>

This is useful if your python script has function that you want to make use of. E.g. let's say we have the following function:

<pre>
[root@ansibleclient01 ~]# cat helloworld.py
print("Hello World!!!")

def squaringnumber(x):
    return x * x
[root@ansibleclient01 ~]# python
Python 2.7.5 (default, Nov 20 2015, 02:00:19)
[GCC 4.8.5 20150623 (Red Hat 4.8.5-4)] on linux2
Type "help", "copyright", "credits" or "license" for more information.
>>> import helloworld
Hello World!!!
>>> helloworld.squaringnumber(9)
81
>>>
</pre>

]]></Content>
		<Date><![CDATA[2016-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[python]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Python]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Python - Virtualenv]]></Title>
		<Content><![CDATA[Virtualenv is the ruby equialent of ruby's rbm.




To start using Virtualenv, you need to run the following as the root user:





<pre>
$ yum install python-pip
$ pip install virtualenv
$ pip install virtualenvwrapper  # optional but provides the workon
</pre>

From this point forward you can run all other commands as the normal user. 

By convention, we create the following folder which will house all our virtual environments, then cd into it:

<pre>
$ mkdir .virtualenvs
$ cd .virtualenvs
</pre>


Next we create the environment with the name 'testenv':

<pre>
$ virtualenv testenv
</pre>

This will create the folder in the cwd with the same name. 

Next we activate our new python virtualenv environment which we do like this:

<pre>
$ . /testenv/bin/activate
</pre>

This will end up changing your command prompt to indicate you are running in an isolated environment. To confirm this, run:

<pre>$ which python</pre>




From this point forward, any 'pip install' commands you run will install inside our testenv folder. 



to return to system level python, run:

<pre>$ deactivate</pre>

Note, when writing python code, you should do it outside of the .virtualenv folder. .virtualenv folder is mainly for internal use. 


To switch between environments, you do:

<pre>$ workon envname</pre>

Note: this needs extra config to get workon command working, including updating your .profile file ]]></Content>
		<Date><![CDATA[2016-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[python]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Python]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Python - Pip]]></Title>
		<Content><![CDATA[https://pip.pypa.io/en/stable/

To <a href="https://packaging.python.org/en/latest/install_requirements_linux/#centos-rhel">install python on centos</a>:

<pre>
$ yum install python-pip</pre>

Some useful commands:

<pre>
$ pip list    # equivalent to: yum list aailable ...

$ pip show {packagename}  # equivalent to: yum info ....   

$ pip search {keyworld}   # equivalent to: yum search ....
</pre>


https://pypi.python.org/pypi     # equivalent to rubygems.org


In Ruby, we hae the bundler gem, in python you can achiee the same result by running:

<pre>
$ pip freeze > requirements.txt
$ pip install -r requirements.txt

</pre>



]]></Content>
		<Date><![CDATA[2016-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[python]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Python]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Python - Useful pip packages]]></Title>
		<Content><![CDATA[pdb - handy python debugger tool, better than using the "print" approach. ]]></Content>
		<Date><![CDATA[2016-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[python]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Python]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - Avoid using User-Data/Cloud-init by utilizing Autoscaling + Cloudwatch Events + Lambda + Ansible]]></Title>
		<Content><![CDATA[http://docs.aws.amazon.com/lambda/latest/dg/python-programming-model-handler-types.html

http://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/EventTypes.html#auto_scaling_event_types

https://aws.amazon.com/blogs/compute/scheduling-ssh-jobs-using-aws-lambda/

https://github.com/fugue/emulambda


https://github.com/nficano/python-lambda]]></Content>
		<Date><![CDATA[2016-05-11]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Amazon Web Services (AWS)|aws|AWS Certified Solutions Architect|Cloud Computing Services]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>AWS>AWS CSA - Associate]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCE - The getent command]]></Title>
		<Content><![CDATA[The getent command essentially queries a collection of sources to return a comprehensive list. 

<pre>
$ get passwd
root:x:0:0:root:/root:/bin/bash
bin:x:1:1:bin:/bin:/sbin/nologin
daemon:x:2:2:daemon:/sbin:/sbin/nologin
adm:x:3:4:adm:/var/adm:/sbin/nologin
lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin
sync:x:5:0:sync:/sbin:/bin/sync
shutdown:x:6:0:shutdown:/sbin:/sbin/shutdown
halt:x:7:0:halt:/sbin:/sbin/halt
mail:x:8:12:mail:/var/spool/mail:/sbin/nologin
operator:x:11:0:operator:/root:/sbin/nologin
games:x:12:100:games:/usr/games:/sbin/nologin
ftp:x:14:50:FTP User:/var/ftp:/sbin/nologin
nobody:x:99:99:Nobody:/:/sbin/nologin
systemd-bus-proxy:x:999:998:systemd Bus Proxy:/:/sbin/nologin
systemd-network:x:998:997:systemd Network Management:/:/sbin/nologin
dbus:x:81:81:System message bus:/:/sbin/nologin
polkitd:x:997:996:User for polkitd:/:/sbin/nologin
sssd:x:996:995:User for sssd:/:/sbin/nologin
abrt:x:173:173::/etc/abrt:/sbin/nologin
.
.
...etc
</pre>


The above is a combined list of local accounts (i.e. those listed in /etc/hosts file) along with all LDAP/AD accounts, if this server is connected to an AD/LDAP server. Hence the getent's "passwd" db is combined listing from 2 sources. It is these two sources because:


<pre>
$ cat /etc/nsswitch.conf | grep '^passwd'
passwd:     files sss ldap
</pre>  

Here it indicates when looking for available first looking in /etc/passwd (files) followed by the System Security Subsystem (sss), which I think is depracated, and finally ldap. Note by default, ldap isn't present here, so I manually added that in.  ]]></Content>
		<Date><![CDATA[2016-07-13]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCE|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCE]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Is there a way to connect to an Amazon SES SMTP Endpoint using mailx?]]></Title>
		<Content><![CDATA[I'm trying to see if I can connect to an SMTP Endpoint from a Centos 7 machine using the mailx utility. 

I first went through the following instructions: 

https://docs.aws.amazon.com/ses/latest/DeveloperGuide/smtp-credentials.html#smtp-credentials-console


Here are some dummy info:

IAM User Name: ses-smtp-user-test
Smtp Username: AKIAABCDEFGHIJ2TPSAQ
Smtp Password: AvECdnoL1OABCDEFGHIJLejpWoGPZd4MiBm16qVTa9OI

Note: The above are dummy credentials for this example, which I have since deleted.

Note: I think the above password must only contain alphanumeric characters. If not the I think you will need to keep regenerating this
until you do. 


Next I checked if the folder existed on my centos machine:

/root/.mozilla

If not then I generated it by double clicking on the firefox icon, via the centos 7 gnome gui interface. Alternatively you might be able to copy in 
a folder from another a centos machine. 

Next followed this guide (in my case i verified email addresses rather than domains): 
http://docs.aws.amazon.com/ses/latest/DeveloperGuide/verify-addresses-and-domains.html


After that install xmail:

    $ yum install xmail 

Note: xmail is usually already installed by default

Next I appended the following lines to the file `/etc/mail.rc`:

# set smtp=smtp://smtp.server.tld:port_number
set smtp=smtp://email-smtp.eu-west-1.amazonaws.com:587
# tell mailx that it needs to authorise
set smtp-auth=login
# set the user for SMTP
# set smtp-auth-user=user@domain.tld
set smtp-auth-user=AKIAABCDEFGHIJ2TPSAQ
# set the password for authorisation
set smtp-auth-password=AvECdnoL1OABCDEFGHIJLejpWoGPZd4MiBm16qVTa9OI
set smtp-use-starttls

set nss-config-dir=/root/.mozilla/firefox/66yunioe.default    # note "66yunioe" is randomly generated, so will likely be different for you
set ssl-verify=ignore
set from=sher.chowdhury@gmail.com     # enter your (verified) email address here


Finally, run:

[root@terraform-server ~]# echo "hello world" | mail -vvvv -s "test message" sher.chowdhury@castletrust.co.uk

This should output something like this:



Resolving host email-smtp.eu-west-1.amazonaws.com . . . done.
Connecting to 54.229.133.103:587 . . . connected.
220 email-smtp.amazonaws.com ESMTP SimpleEmailService-1488614337 y5qa7L0IdNwiQfESQQvU
>>> EHLO terraform-server.local
250-email-smtp.amazonaws.com
250-8BITMIME
250-SIZE 10485760
250-STARTTLS
250-AUTH PLAIN LOGIN
250 Ok
>>> STARTTLS
220 Ready to start TLS
Error in certificate: Peer's certificate issuer is not recognized.
Comparing DNS name: "email-smtp.eu-west-1.amazonaws.com"
SSL parameters: cipher=missing, keysize=256, secretkeysize=256,
issuer=CN=Symantec Class 3 Secure Server CA - G4,OU=Symantec Trust Network,O=Symantec Corporation,C=US
subject=CN=email-smtp.eu-west-1.amazonaws.com,O="Amazon.com, Inc.",L=Seattle,ST=Washington,C=US
>>> EHLO terraform-server.local
250-email-smtp.amazonaws.com
250-8BITMIME
250-SIZE 10485760
250-STARTTLS
250-AUTH PLAIN LOGIN
250 Ok
>>> AUTH LOGIN
334 VXNlcm5hbWU6
>>> QUtJQUlBN0Y2WVpWN0syVFBTQVE=
334 UGFzc3dvcmQ6
>>> QXZFQ2Rub0wxT3JzODk4UEUwanRMZWpwV29HUFpkNE1pQm0xNnFWVGE5T0k=
235 Authentication successful.
>>> MAIL FROM:<sher.chowdhury@gmail.com>
250 Ok
>>> RCPT TO:<sher.chowdhury@yahoo.com>
250 Ok
>>> DATA
354 End data with <CR><LF>.<CR><LF>
>>> .
250 Ok 0102015611ea11fe-ff3982bb-1c49-476c-bc0e-abc9e1de23a5-000000
>>> QUIT
221 Bye
[root@terraform-server ~]#

now I logged into my yahoo account, and I found an email (that appears to have been) sent from my gmail account. 

As you can see I managed to send an email from my gmail account without actually logging into my gmail account. 
 

Other references:

http://docs.aws.amazon.com/ses/latest/DeveloperGuide/Welcome.html

http://www.systutorials.com/1411/sending-email-from-mailx-command-in-linux-using-gmails-smtp/

http://www.computerhope.com/unix/usendmai.htm

http://docs.aws.amazon.com/ses/latest/DeveloperGuide/configure-email-client.html

https://coderwall.com/p/ez1x2w/send-mail-like-a-boss

http://forums.fedoraforum.org/showthread.php?t=225852

http://forums.fedoraforum.org/showthread.php?t=214761

https://www.digitalocean.com/community/tutorials/how-to-send-e-mail-alerts-on-a-centos-vps-for-system-monitoring

http://docs.aws.amazon.com/ses/latest/DeveloperGuide/smtp-connect.html

http://aoxop.top/community/questions/how-to-configure-sendmail-to-send-mail-using-an-external-gmail-smtp-server

http://docs.aws.amazon.com/ses/latest/DeveloperGuide/verify-domains.html

]]></Content>
		<Date><![CDATA[2016-07-22]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[aws|ses]]></Tags>
		<Status><![CDATA[private]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCE - Network Teaming]]></Title>
		<Content><![CDATA[<strong>Link Aggregation</strong>

In RHEL/CentOS it is possible to combine two (physical or virtual) network interfaces so that they act as one. Having 2 network interfaces acting as one is referred to as <strong>link aggregation</strong>. There's 2 advantages to setting up link aggregations:
<ul>
 	<li>Improved redundancy - i.e. one interface is active, and the other is on standby. if the active interface fails, then then standby interface takes over.</li>
 	<li>Improved performance - i.e. having two interface acting as one will double the bandwidth.</li>
</ul>
&nbsp;

There are 2 ways to setup two interfaces to act as one:
<ul>
 	<li>bonding</li>
 	<li>teaming</li>
</ul>
Bonding is the old approach that has been around for a long while, whereas the Teaming is the newer approach. In this article we'll take a look at how to setup link aggregation via the teaming approach.

Teaming can be configured using Network Manager. That is, we can setup teaming using the nmcli, nmtui, or the graphical network manager tool.
<h2>Setup teaming using nmcli</h2>
&nbsp;
<pre>$ nmcli con add type team con-name codingbee</pre>
Here we created a new resource called codingbee]]></Content>
		<Date><![CDATA[2016-08-21]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCE|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCE]]></Categories>
	</post>
	<post>
		<Title><![CDATA[AWS - SSL termination on the ELB]]></Title>
		<Content><![CDATA[http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/using-elb-listenerconfig-quickref.html

&nbsp;

https://blog.qruizelabs.com/2014/06/06/ssl-aws-elb/

In my case I replaced:

!/elbcheck.html

with:

^(.*)$]]></Content>
		<Date><![CDATA[2016-08-23]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[aws|elb|ssl]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - querying puppetdb with postgres command line (psql)]]></Title>
		<Content><![CDATA[<h2>Exported Resources</h2>

https://docs.puppet.com/puppet/latest/reference/lang_collectors.html

https://docs.puppet.com/pe/latest/node_deactivation.html


<h2>querying puppetdb with postgres command line, psql</h2>

psql -h localhost -U puppetdb puppetdb

However to connect to puppet enterprise's puppetdb, then follow:

https://gist.github.com/tnolet/7133083

\q
\h
\l

\d
select * FROM catalog_resources
\X
\x
SELECT * FROM catalog_resourcesSELECT * FROM factsselect * from factsssselect * from reports
\q
SELECT * FROM catalog_resourcesSELECT * FROM factsselect * from factsssselect * from reports
\x
select * from facts;
\d
lsSELECT * FROM catalog_resources
\l
\c puppetdb
lsSELECT * FROM catalog_resources;
lsSELECT * FROM catalog_resources
\q
\l
\c puppetdb
\l
\d
\x
\d
SELECT * FROM certnames ;
SELECT * FROM facts ;
\d
SELECT * FROM catalog_resources ;
\x
SELECT * FROM catalog_resources ;
SELECT * FROM catalog_resources ;
\d
SELECT * FROM catalog_resources ;
\d
SELECT * FROM environments ;
SELECT * FROM catalog_resources ;
SELECT * FROM catalog_resources WHERE exported=TRUE;
SELECT * FROM catalog_resources WHERE exported=TRUE ;
SELECT * FROM catalog_resources WHERE exported=FALSE ;
\x
SELECT * FROM catalog_resources WHERE exported=FALSE ;
SELECT * FROM catalog_resources WHERE exported=FALSE ;
SELECT * FROM catalog_resources WHERE exported=TRUE ;
\q



However a better approach, as discussed in the documentation, is first create a json file with your query, e.g:


<pre>$ cat puppetdbquery.json
["and",
  ["=", "certname", "example.com"],
  ["=", "type", "file"],
  ["=", "exported", true]
]
</pre>


Then run:


<pre>
$ curl -X GET http://localhost:8080/pdb/query/v4/resources --data-urlencode query@puppetdbquery.json  --data-urlencode 'pretty=true'
</pre>

]]></Content>
		<Date><![CDATA[2016-09-20]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet|Puppetlabs]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>Puppet]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Puppet - Using AWS web console as Puppet's external node classifier (ENC)]]></Title>
		<Content><![CDATA[This is a script I wrote that queries the ec2 tags of an aws console, in order to figure out what environment a node belongs to, and what class to assign to it. 


<pre>
#!/bin/bash

# https://docs.puppetlabs.com/guides/external_nodes.html
# http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html

export AWS_ACCESS_KEY_ID=xxxxxxxxxxxxxxxxxxxxxx
export AWS_SECRET_ACCESS_KEY=xxxxxxxxxxxxxxxxxxxxxx
export AWS_DEFAULT_REGION=xxxxxxx

instanceid=`echo $1 | awk -F"_" '{print $NF}'` # $1 will match the certname

env=`/bin/aws --output text ec2 describe-instances --instance-ids $instanceid | grep '^TAGS' | grep 'env' |  awk '{print $NF}'`
role=`/bin/aws --output text ec2 describe-instances --instance-ids $instanceid | grep '^TAGS' | grep 'role' |  awk '{print $NF}'`

#aws ec2 describe-instances --instance-ids $instanceid > /tmp/enc-log.txt
#echo "puppet run occured at `date`" > /tmp/enc-log.txt
#echo "The first param value is: $1" >> /tmp/enc-log.txt
#echo "The environment is: $env" >> /tmp/enc-log.txt
#echo "The puppet role is: $role" >> /tmp/enc-log.txt

echo '---'
echo 'classes:'
echo "  - roles::$role"
echo "environment: $env"  
echo "parameters:"
echo "  role: $role"         # Needed for hiera lookup
</pre>

In order for this script to work, you need to install the aws cli utility on the puppetmaster, also ensure your puppetmaster's ec2 IAM role has the necessary privileges. 
]]></Content>
		<Date><![CDATA[2016-10-18]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[aws|Puppet]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[puppet performance tuning]]></Title>
		<Content><![CDATA[The latest version of PE 2016.4 has the capability to monitor the heap memory as a feature of puppet server 2.6.

https://docs.puppet.com/puppetserver/2.6/status-api/v1/services.html#example-request-and-response-for-a-debug-level-get-request

this guide:

https://puppet.com/blog/puppet-server-advanced-memory-debugging



https://docs.puppet.com/pe/latest/install_multimaster.html

https://support.puppet.com/hc/en-us/articles/225049688


https://docs.puppet.com/puppetserver/latest/tuning_guide.html#number-of-jrubies



Thundering herd test:


After you’ve added hundreds of nodes to your deployment you may notice that your agents are running slow or timing out. When hundreds of nodes check in simultaneously to request a catalog, it might cause a so-called thundering herd of processes that causes CPU and memory performance to suffer. To verify that you have a thundering herd condition, you can run a query on the PuppetDB node (the master in a monolithic installation) to show how many nodes check in per minute.

Log into the PuppetDB node(the master in a monolithic installation) as the pe-postgres user.

Open the PostgreSQL command line interface by running sudo su - pe-postgres -s /bin/bash -c "psql -d pe-puppetdb".

Find out how many nodes are checking in per minute for one hour by running the following query, replacing with a one hour period in the recent past:

select date_part('minute', start_time), count(*) from reports where start_time between '<DATE AND TIME>' and '<DATE AND TIME>' GROUP BY date_part('minute', start_time) ORDER BY date_part('minute', start_time) ASC;

For example, the following example shows a query for December 23rd, 2015 between 5:30 PM to 6:30 PM:

select date_part('minute', start_time), count(*) from reports where start_time between '2015-12-03 17:30:00' and '2015-12-03 18:30:00' GROUP BY date_part('minute', start_time) ORDER BY date_part('minute', start_time) ASC;

Check the results to see if you have a consistent issue with many nodes checking in simultaneously by running the query several times using different time spans.

Exit the PostgreSQL command line by typing \q.

If you find that you have a thundering herd condition, distribute agent checkins more evenly by using splay, fqdn_rand. 

https://docs.puppet.com/puppet/latest/reference/configuration.html#splay

Then confirm that any changes you make are effective by re-running the query in step 3.



e.g.

pe-puppetdb=# select date_part('minute', start_time), count(*) from reports where start_time between '2016-11-21 09:00:00' and '2016-11-21 10:00:00' GROUP BY date_part('minute', start_time) ORDER BY date_part('minute', start_time) ASC; 
date_part | count 



if puppetmaster cpu is overloaded, then it will show up in puppetserver.log as the word 'overloaded', e.g.:

2016-11-22 11:48:54,110 ERROR [qtp2038037765-49147] [p.p.jruby-request] Error 503 on SERVER at /puppet/v3/file_metadata/plugins: Attempt to borrow a JRuby instance from the pool timed out; Puppet Server is temporarily overloaded. If you get this error repeatedly, your server might be misconfigured or trying to serve too many agent nodes. Check Puppet Server settings: jruby-puppet.max-active-instances.



https://github.com/npwalker/pe_metric_curl_cron_jobs


Also you need to check whether puppetdb is storing large files, you can do this using this handy script:

https://github.com/ripienaar/puppet-reportprint


To gather per minute compilation statistics for catalogs you can run the following command on the puppetserver.log

$ grep Compiled puppetserver.log  | awk -F ':' '{print $1, $2}' | sort | uniq -c


https://docs.puppet.com/puppetdb/latest/scaling_recommendations.html


Avoid enabling the hiera deep merge setting, instead use:

https://docs.puppet.com/puppet/latest/reference/function.html#hieraarray
https://docs.puppet.com/puppet/latest/reference/function.html#hierahash

This will speed up catalog compile times. 


monolythic+compile master setup is the recommended approach. It's easier to maintain, since puppetdb, web-console, and puppetmaster are all installed on the same box, but also easier to scale horizontally by building new puppetmasters. 



]]></Content>
		<Date><![CDATA[2016-11-28]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Puppet]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Git - Handy git tools and utilities]]></Title>
		<Content><![CDATA[Find and get rid of really big files from your git repo:

https://rtyley.github.io/bfg-repo-cleaner/


Handy tool for getting rid of stale bug/feature branches that have already been merged in to the main release/master branches:

https://github.com/arc90/git-sweep



]]></Content>
		<Date><![CDATA[2016-11-28]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[git]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>git]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCE - Network teaming or bonding]]></Title>
		<Content><![CDATA[In CentOS 7 it's possible to configure to separate network interfaces to work together. Combining two network interfaces like this is referred to as 'Link Aggregation'. Link aggregations has the following benefits:
<ul>
 	<li>combined bandwith - This means improved performance, better performance</li>
 	<li>load balancing - So that one interface doesn't do all the work while the other is idle</li>
 	<li>fault tolerance - if one interface suffers a hardware failure then the other interface can take over</li>
</ul>
&nbsp;

There are 2 methods for setting up link aggregation, they are called:
<ul>
 	<li>bonding</li>
 	<li>teaming</li>
</ul>
The 'teaming' method is new in RHEL 7 and has more features compared to the bonding method. Therefore in this guide we'll focus on the teaming method.

A team based link aggregation has 4 different operating modes:
<ul>
 	<li>broadcast</li>
 	<li>roundrobin</li>
 	<li>activebackup</li>
 	<li>loadbalance</li>
 	<li>lacp</li>
</ul>
Note you can view this list by running:
<pre>$ man 5 teamd.conf</pre>
&nbsp;

&nbsp;

You can setup Link aggregation via any of the following methods:
<ul>
 	<li>nmcli command</li>
 	<li>NetworkManager gui</li>
 	<li>nmtui</li>
 	<li>manually editing the various config files</li>
</ul>
&nbsp;

&nbsp;

In this guide we'll show you how to set up Link Aggregation via the teaming method, using nmcli.

<pre>
$  nmcli connection add type team con-name CodingBeeTeam0 ifname CodingBeeTeam0 config '{"runner": {"name":"activebackup"}}'
Connection 'CodingBeeTeam0' (7a12239f-ca6f-4347-b9db-86122bb22078) successfully added.
</pre>


Behind the scenes this ends up creating the following file with the content:
<pre>$  cat /etc/sysconfig/network-scripts/ifcfg-CodingBeeTeam0
DEVICE=CodingBeeTeam0
TEAM_CONFIG="{\"runner\": {\"name\":\"activebackup\"}}"
DEVICETYPE=Team
BOOTPROTO=dhcp
DEFROUTE=yes
PEERDNS=yes
PEERROUTES=yes
IPV4_FAILURE_FATAL=no
IPV6INIT=yes
IPV6_AUTOCONF=yes
IPV6_DEFROUTE=yes
IPV6_PEERDNS=yes
IPV6_PEERROUTES=yes
IPV6_FAILURE_FATAL=no
NAME=CodingBeeTeam0
UUID=7a12239f-ca6f-4347-b9db-86122bb22078
ONBOOT=yes
</pre>
&nbsp;

This ends up creating the following connection:


<pre>
$  ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP qlen 1000
    link/ether 52:54:00:d8:71:80 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 82995sec preferred_lft 82995sec
    inet6 fe80::5054:ff:fed8:7180/64 scope link
       valid_lft forever preferred_lft forever
3: virbr0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN
    link/ether 52:54:00:38:49:3d brd ff:ff:ff:ff:ff:ff
    inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr0
       valid_lft forever preferred_lft forever
4: virbr0-nic: <BROADCAST,MULTICAST> mtu 1500 qdisc pfifo_fast master virbr0 state DOWN qlen 500
    link/ether 52:54:00:38:49:3d brd ff:ff:ff:ff:ff:ff
5: nm-team: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN
    link/ether f6:fe:47:cf:0d:a2 brd ff:ff:ff:ff:ff:ff
<strong>6: CodingBeeTeam0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN
    link/ether 3a:88:c8:68:f0:b3 brd ff:ff:ff:ff:ff:ff</strong>
</pre>

Now we assign an IP address to this connection:

<pre>
$  nmcli connection modify CodingBeeTeam0 ipv4.addresses '192.168.2.50/24'
</pre>

This ends up adding the following line to the the file:

<pre>
$  cat /etc/sysconfig/network-scripts/ifcfg-CodingBeeTeam0
DEVICE=CodingBeeTeam0
TEAM_CONFIG="{\"runner\": {\"name\":\"activebackup\"}}"
DEVICETYPE=Team
BOOTPROTO=dhcp
DEFROUTE=yes
IPV4_FAILURE_FATAL=no
IPV6INIT=yes
IPV6_AUTOCONF=yes
IPV6_DEFROUTE=yes
IPV6_FAILURE_FATAL=no
NAME=CodingBeeTeam0
UUID=7a12239f-ca6f-4347-b9db-86122bb22078
ONBOOT=yes
<strong>IPADDR=192.168.2.50
PREFIX=24</strong>
PEERDNS=yes
PEERROUTES=yes
IPV6_PEERDNS=yes
IPV6_PEERROUTES=yes
</pre>


Since we have manually assigned an IP address, it means that before using this connection we need to first disable dhcp for this connection:

<pre>
$ nmcli connection modify CodingBeeTeam0 ipv4.method manual
</pre> 

This ends up changing the following line:

<pre>
$ cat /etc/sysconfig/network-scripts/ifcfg-CodingBeeTeam0
DEVICE=CodingBeeTeam0
TEAM_CONFIG="{\"runner\": {\"name\":\"activebackup\"}}"
DEVICETYPE=Team
<strong>BOOTPROTO=none</strong>
DEFROUTE=yes
IPV4_FAILURE_FATAL=no
IPV6INIT=yes
IPV6_AUTOCONF=yes
IPV6_DEFROUTE=yes
IPV6_FAILURE_FATAL=no
NAME=CodingBeeTeam0
UUID=7a12239f-ca6f-4347-b9db-86122bb22078
ONBOOT=yes
IPADDR=192.168.2.50
PREFIX=24
IPV6_PEERDNS=yes
IPV6_PEERROUTES=yes

</pre>



&nbsp;

&nbsp;
<h2>Useful resources</h2>
https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Networking_Guide/ch-Configure_Network_Teaming.html]]></Content>
		<Date><![CDATA[2016-12-01]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCE|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCE]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Using xmllint inside bash scripts to read xml files]]></Title>
		<Content><![CDATA[xmllint --xpath "string(//channel/item/link)" test.xml

xmllint --xpath "string(//channel/item/*[name()='content:encoded'])" test.xml]]></Content>
		<Date><![CDATA[2017-01-10]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[xmllint]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[Editing the httpd.conf file using Augeas]]></Title>
		<Content><![CDATA[Sometimes you might want to edit the httpd.conf file using a shell script. For example let's say we have the following file on our CentOS 7 machine:

<pre>
$ cat /etc/httpd/conf/httpd.conf
ServerRoot "/etc/httpd"
Listen 80
Include conf.modules.d/*.conf
User apache
Group apache
ServerAdmin root@localhost
<Directory />
    AllowOverride none
    Require all denied
</Directory>
DocumentRoot "/var/www/html"
<Directory "/var/www">
    AllowOverride None
    Require all granted
</Directory>
<Directory "/var/www/html">
    Options Indexes FollowSymLinks
    AllowOverride None                    # I WANT TO EDIT THIS LINE ONLY
    Require all granted
</Directory>
<IfModule dir_module>
    DirectoryIndex index.html
</IfModule>
<Files ".ht*">
    Require all denied
</Files>
ErrorLog "logs/error_log"
LogLevel warn
<IfModule log_config_module>
    LogFormat "%h %l %u %t \"%r\" %>s %b \"%{Referer}i\" \"%{User-Agent}i\"" combined
    LogFormat "%h %l %u %t \"%r\" %>s %b" common
    <IfModule logio_module>
      LogFormat "%h %l %u %t \"%r\" %>s %b \"%{Referer}i\" \"%{User-Agent}i\" %I %O" combinedio
    </IfModule>
    CustomLog "logs/access_log" combined
</IfModule>
<IfModule alias_module>
    ScriptAlias /cgi-bin/ "/var/www/cgi-bin/"
</IfModule>
<Directory "/var/www/cgi-bin">
    AllowOverride None
    Options None
    Require all granted
</Directory>
<IfModule mime_module>
    TypesConfig /etc/mime.types
    AddType application/x-compress .Z
    AddType application/x-gzip .gz .tgz
    AddType text/html .shtml
    AddOutputFilter INCLUDES .shtml
</IfModule>
AddDefaultCharset UTF-8
<IfModule mime_magic_module>
    MIMEMagicFile conf/magic
</IfModule>
EnableSendfile on
IncludeOptional conf.d/*.conf
LimitRequestLine 81900

</pre>

The ls/print/get commands are not important, they are just there to help you see what's happening, and are handy for troubleshooting. The actual command that makes the change is the set command. the 'save' command then applies the change to the httpd.conf file.

The above can now be run automatically inside a bash shell script like this:

<pre>
#!/bin/bash
yum install -y augeas
yum install -y httpd
systemctl start httpd

augtool <<-EOF
set /files/etc/httpd/conf/httpd.conf/Directory[arg='\"/var/www/html\"']/*[self::directive='AllowOverride']/arg ALL
save
quit
EOF

systemctl restart httpd
</pre>

Side note: editing the httpd.conf file isn't necessary, since you can override them by dropping your custom configurations in the conf.d folder instead, thanks to the IncludeOptional apache setting, shown in the above sample file. 
]]></Content>
		<Date><![CDATA[2017-01-14]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[augeas]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Uncategorized]]></Categories>
	</post>
	<post>
		<Title><![CDATA[RHCSA - Installing RHEL]]></Title>
		<Content><![CDATA[]]></Content>
		<Date><![CDATA[2017-01-21]]></Date>
		<PostType><![CDATA[post]]></PostType>
		<Tags><![CDATA[Bash|Centos|Linux|Redhat|RHCSA|RHEL]]></Tags>
		<Status><![CDATA[publish]]></Status>
		<Format><![CDATA[]]></Format>
		<Categories><![CDATA[Tutorials>RHCSA]]></Categories>
	</post>
</data>